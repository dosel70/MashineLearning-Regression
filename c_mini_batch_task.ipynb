{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "948516f5-aebe-4e9e-a275-3dceaf0329b8",
   "metadata": {},
   "source": [
    "### Mini batch Task01\n",
    "- 다이아몬드의 특성을 회귀분석하여, 예상 가격을 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19223c7a-cb49-4cfe-8fbe-0818550f8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf3552db-d737-47cd-b854-b234f5e6443c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>53939</td>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>53940</td>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>53941</td>\n",
       "      <td>0.71</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.79</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53941</th>\n",
       "      <td>53942</td>\n",
       "      <td>0.71</td>\n",
       "      <td>Premium</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53942</th>\n",
       "      <td>53943</td>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS2</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53943 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  carat        cut color clarity  depth  table  price     x  \\\n",
       "0               1   0.23      Ideal     E     SI2   61.5   55.0    326  3.95   \n",
       "1               2   0.21    Premium     E     SI1   59.8   61.0    326  3.89   \n",
       "2               3   0.23       Good     E     VS1   56.9   65.0    327  4.05   \n",
       "3               4   0.29    Premium     I     VS2   62.4   58.0    334  4.20   \n",
       "4               5   0.31       Good     J     SI2   63.3   58.0    335  4.34   \n",
       "...           ...    ...        ...   ...     ...    ...    ...    ...   ...   \n",
       "53938       53939   0.86    Premium     H     SI2   61.0   58.0   2757  6.15   \n",
       "53939       53940   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83   \n",
       "53940       53941   0.71    Premium     E     SI1   60.5   55.0   2756  5.79   \n",
       "53941       53942   0.71    Premium     F     SI1   59.8   62.0   2756  5.74   \n",
       "53942       53943   0.70  Very Good     E     VS2   60.5   59.0   2757  5.71   \n",
       "\n",
       "          y     z  \n",
       "0      3.98  2.43  \n",
       "1      3.84  2.31  \n",
       "2      4.07  2.31  \n",
       "3      4.23  2.63  \n",
       "4      4.35  2.75  \n",
       "...     ...   ...  \n",
       "53938  6.12  3.74  \n",
       "53939  5.87  3.64  \n",
       "53940  5.74  3.49  \n",
       "53941  5.73  3.43  \n",
       "53942  5.76  3.47  \n",
       "\n",
       "[53943 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/diamond.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d68d5194-d043-40b7-abce-eb0b5bd42459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>0.71</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.79</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53941</th>\n",
       "      <td>0.71</td>\n",
       "      <td>Premium</td>\n",
       "      <td>F</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53942</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS2</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53943 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z\n",
       "0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
       "...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n",
       "53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74\n",
       "53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64\n",
       "53940   0.71    Premium     E     SI1   60.5   55.0   2756  5.79  5.74  3.49\n",
       "53941   0.71    Premium     F     SI1   59.8   62.0   2756  5.74  5.73  3.43\n",
       "53942   0.70  Very Good     E     VS2   60.5   59.0   2757  5.71  5.76  3.47\n",
       "\n",
       "[53943 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(labels=['Unnamed: 0'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a0464c-a310-498a-8746-2c6254fd3fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cut\n",
       "Ideal        21551\n",
       "Premium      13793\n",
       "Very Good    12083\n",
       "Good          4906\n",
       "Fair          1610\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cut.value_counts() # Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2dd837-6ed2-4af5-ac43-ea6cd90a8e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color\n",
       "G    11292\n",
       "E     9799\n",
       "F     9543\n",
       "H     8304\n",
       "D     6775\n",
       "I     5422\n",
       "J     2808\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.color.value_counts() # One_Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7330d07-41c3-4bde-bf7d-4dd98a586755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clarity\n",
       "SI1     13067\n",
       "VS2     12259\n",
       "SI2      9194\n",
       "VS1      8171\n",
       "VVS2     5066\n",
       "VVS1     3655\n",
       "IF       1790\n",
       "I1        741\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.clarity.value_counts() # One_Hot_Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7effbfd-0e4b-4eda-928c-89df874e56f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cut': array(['Fair', 'Good', 'Ideal', 'Premium', 'Very Good'], dtype=object),\n",
       " 'clarity': array(['I1', 'IF', 'SI1', 'SI2', 'VS1', 'VS2', 'VVS1', 'VVS2'],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pre_df = df.copy()\n",
    "columns = ['cut' ,'clarity']\n",
    "label_encoders = {}\n",
    "\n",
    "for column in columns:\n",
    "    encoder = LabelEncoder()\n",
    "    result = encoder.fit_transform(pre_df[column])\n",
    "    label_encoders[column] = encoder.classes_\n",
    "    pre_df[column] = result\n",
    "\n",
    "label_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd551a8-4f01-443a-81b5-868553921b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>E</td>\n",
       "      <td>3</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>4</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "      <td>J</td>\n",
       "      <td>3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>60.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.79</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53941</th>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53942</th>\n",
       "      <td>0.70</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53943 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  cut color  clarity  depth  table  price     x     y     z\n",
       "0       0.23    2     E        3   61.5   55.0    326  3.95  3.98  2.43\n",
       "1       0.21    3     E        2   59.8   61.0    326  3.89  3.84  2.31\n",
       "2       0.23    1     E        4   56.9   65.0    327  4.05  4.07  2.31\n",
       "3       0.29    3     I        5   62.4   58.0    334  4.20  4.23  2.63\n",
       "4       0.31    1     J        3   63.3   58.0    335  4.34  4.35  2.75\n",
       "...      ...  ...   ...      ...    ...    ...    ...   ...   ...   ...\n",
       "53938   0.86    3     H        3   61.0   58.0   2757  6.15  6.12  3.74\n",
       "53939   0.75    2     D        3   62.2   55.0   2757  5.83  5.87  3.64\n",
       "53940   0.71    3     E        2   60.5   55.0   2756  5.79  5.74  3.49\n",
       "53941   0.71    3     F        2   59.8   62.0   2756  5.74  5.73  3.43\n",
       "53942   0.70    4     E        5   60.5   59.0   2757  5.71  5.76  3.47\n",
       "\n",
       "[53943 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5ba8a2f-1165-4bf8-add0-b0a010d08137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>(D,)</th>\n",
       "      <th>(E,)</th>\n",
       "      <th>(F,)</th>\n",
       "      <th>(G,)</th>\n",
       "      <th>(H,)</th>\n",
       "      <th>(I,)</th>\n",
       "      <th>(J,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>E</td>\n",
       "      <td>3</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>E</td>\n",
       "      <td>4</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "      <td>J</td>\n",
       "      <td>3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>H</td>\n",
       "      <td>3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>3</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>E</td>\n",
       "      <td>2</td>\n",
       "      <td>60.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.79</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53941</th>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>F</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53942</th>\n",
       "      <td>0.70</td>\n",
       "      <td>4</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53943 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  cut color  clarity  depth  table  price     x     y     z  (D,)  \\\n",
       "0       0.23    2     E        3   61.5   55.0    326  3.95  3.98  2.43     0   \n",
       "1       0.21    3     E        2   59.8   61.0    326  3.89  3.84  2.31     0   \n",
       "2       0.23    1     E        4   56.9   65.0    327  4.05  4.07  2.31     0   \n",
       "3       0.29    3     I        5   62.4   58.0    334  4.20  4.23  2.63     0   \n",
       "4       0.31    1     J        3   63.3   58.0    335  4.34  4.35  2.75     0   \n",
       "...      ...  ...   ...      ...    ...    ...    ...   ...   ...   ...   ...   \n",
       "53938   0.86    3     H        3   61.0   58.0   2757  6.15  6.12  3.74     0   \n",
       "53939   0.75    2     D        3   62.2   55.0   2757  5.83  5.87  3.64     1   \n",
       "53940   0.71    3     E        2   60.5   55.0   2756  5.79  5.74  3.49     0   \n",
       "53941   0.71    3     F        2   59.8   62.0   2756  5.74  5.73  3.43     0   \n",
       "53942   0.70    4     E        5   60.5   59.0   2757  5.71  5.76  3.47     0   \n",
       "\n",
       "       (E,)  (F,)  (G,)  (H,)  (I,)  (J,)  \n",
       "0         1     0     0     0     0     0  \n",
       "1         1     0     0     0     0     0  \n",
       "2         1     0     0     0     0     0  \n",
       "3         0     0     0     0     1     0  \n",
       "4         0     0     0     0     0     1  \n",
       "...     ...   ...   ...   ...   ...   ...  \n",
       "53938     0     0     0     1     0     0  \n",
       "53939     0     0     0     0     0     0  \n",
       "53940     1     0     0     0     0     0  \n",
       "53941     0     1     0     0     0     0  \n",
       "53942     1     0     0     0     0     0  \n",
       "\n",
       "[53943 rows x 17 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# columns = ['color' , 'clarity']\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "result = one_hot_encoder.fit_transform(pre_df[['color']])\n",
    "\n",
    "pre_df = pd.concat([pre_df, pd.DataFrame(result, columns=one_hot_encoder.categories_).astype(np.int8)], axis=1)\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e06f6908-1c48-4e0a-80da-82dc73828bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>(D,)</th>\n",
       "      <th>(E,)</th>\n",
       "      <th>(F,)</th>\n",
       "      <th>(G,)</th>\n",
       "      <th>(H,)</th>\n",
       "      <th>(I,)</th>\n",
       "      <th>(J,)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>60.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.79</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53941</th>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53942</th>\n",
       "      <td>0.70</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53943 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  cut  clarity  depth  table  price     x     y     z  (D,)  (E,)  \\\n",
       "0       0.23    2        3   61.5   55.0    326  3.95  3.98  2.43     0     1   \n",
       "1       0.21    3        2   59.8   61.0    326  3.89  3.84  2.31     0     1   \n",
       "2       0.23    1        4   56.9   65.0    327  4.05  4.07  2.31     0     1   \n",
       "3       0.29    3        5   62.4   58.0    334  4.20  4.23  2.63     0     0   \n",
       "4       0.31    1        3   63.3   58.0    335  4.34  4.35  2.75     0     0   \n",
       "...      ...  ...      ...    ...    ...    ...   ...   ...   ...   ...   ...   \n",
       "53938   0.86    3        3   61.0   58.0   2757  6.15  6.12  3.74     0     0   \n",
       "53939   0.75    2        3   62.2   55.0   2757  5.83  5.87  3.64     1     0   \n",
       "53940   0.71    3        2   60.5   55.0   2756  5.79  5.74  3.49     0     1   \n",
       "53941   0.71    3        2   59.8   62.0   2756  5.74  5.73  3.43     0     0   \n",
       "53942   0.70    4        5   60.5   59.0   2757  5.71  5.76  3.47     0     1   \n",
       "\n",
       "       (F,)  (G,)  (H,)  (I,)  (J,)  \n",
       "0         0     0     0     0     0  \n",
       "1         0     0     0     0     0  \n",
       "2         0     0     0     0     0  \n",
       "3         0     0     0     1     0  \n",
       "4         0     0     0     0     1  \n",
       "...     ...   ...   ...   ...   ...  \n",
       "53938     0     0     1     0     0  \n",
       "53939     0     0     0     0     0  \n",
       "53940     0     0     0     0     0  \n",
       "53941     1     0     0     0     0  \n",
       "53942     0     0     0     0     0  \n",
       "\n",
       "[53943 rows x 16 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df = pre_df.drop(labels=['color'], axis=1)\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2937b60-cf06-45dc-bdd5-2eb0051527e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>h</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>60.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.79</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53941</th>\n",
       "      <td>0.71</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>59.8</td>\n",
       "      <td>62.0</td>\n",
       "      <td>2756</td>\n",
       "      <td>5.74</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53942</th>\n",
       "      <td>0.70</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>60.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.71</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53943 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat  cut  clarity  depth  table  price     x     y     z  d  e  f  g  \\\n",
       "0       0.23    2        3   61.5   55.0    326  3.95  3.98  2.43  0  1  0  0   \n",
       "1       0.21    3        2   59.8   61.0    326  3.89  3.84  2.31  0  1  0  0   \n",
       "2       0.23    1        4   56.9   65.0    327  4.05  4.07  2.31  0  1  0  0   \n",
       "3       0.29    3        5   62.4   58.0    334  4.20  4.23  2.63  0  0  0  0   \n",
       "4       0.31    1        3   63.3   58.0    335  4.34  4.35  2.75  0  0  0  0   \n",
       "...      ...  ...      ...    ...    ...    ...   ...   ...   ... .. .. .. ..   \n",
       "53938   0.86    3        3   61.0   58.0   2757  6.15  6.12  3.74  0  0  0  0   \n",
       "53939   0.75    2        3   62.2   55.0   2757  5.83  5.87  3.64  1  0  0  0   \n",
       "53940   0.71    3        2   60.5   55.0   2756  5.79  5.74  3.49  0  1  0  0   \n",
       "53941   0.71    3        2   59.8   62.0   2756  5.74  5.73  3.43  0  0  1  0   \n",
       "53942   0.70    4        5   60.5   59.0   2757  5.71  5.76  3.47  0  1  0  0   \n",
       "\n",
       "       h  i  j  \n",
       "0      0  0  0  \n",
       "1      0  0  0  \n",
       "2      0  0  0  \n",
       "3      0  1  0  \n",
       "4      0  0  1  \n",
       "...   .. .. ..  \n",
       "53938  1  0  0  \n",
       "53939  0  0  0  \n",
       "53940  0  0  0  \n",
       "53941  0  0  0  \n",
       "53942  0  0  0  \n",
       "\n",
       "[53943 rows x 16 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df.columns = ['carat' , 'cut' , 'clarity' ,\n",
    "                  'depth' , 'table' , 'price' ,\n",
    "                  'x', 'y', 'z',\n",
    "                 'd', 'e' ,'f', 'g' , 'h', 'i' ,'j']\n",
    "\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "513deac8-f0ba-42e5-bbbc-6487c90b348f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'carat'}>,\n",
       "        <Axes: title={'center': 'cut'}>,\n",
       "        <Axes: title={'center': 'clarity'}>,\n",
       "        <Axes: title={'center': 'depth'}>],\n",
       "       [<Axes: title={'center': 'table'}>,\n",
       "        <Axes: title={'center': 'price'}>, <Axes: title={'center': 'x'}>,\n",
       "        <Axes: title={'center': 'y'}>],\n",
       "       [<Axes: title={'center': 'z'}>, <Axes: title={'center': 'd'}>,\n",
       "        <Axes: title={'center': 'e'}>, <Axes: title={'center': 'f'}>],\n",
       "       [<Axes: title={'center': 'g'}>, <Axes: title={'center': 'h'}>,\n",
       "        <Axes: title={'center': 'i'}>, <Axes: title={'center': 'j'}>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_df.hist(figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "151b6981-2485-48dd-a07a-79a45861a5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x900 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_df['carat'].hist(figsize=(9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20ac1410-f2ec-4e79-bb9e-05bddb7bac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': 'carat'}>,\n",
       "        <Axes: title={'center': 'cut'}>,\n",
       "        <Axes: title={'center': 'clarity'}>,\n",
       "        <Axes: title={'center': 'depth'}>],\n",
       "       [<Axes: title={'center': 'table'}>,\n",
       "        <Axes: title={'center': 'price'}>, <Axes: title={'center': 'x'}>,\n",
       "        <Axes: title={'center': 'y'}>],\n",
       "       [<Axes: title={'center': 'z'}>, <Axes: title={'center': 'd'}>,\n",
       "        <Axes: title={'center': 'e'}>, <Axes: title={'center': 'f'}>],\n",
       "       [<Axes: title={'center': 'g'}>, <Axes: title={'center': 'h'}>,\n",
       "        <Axes: title={'center': 'i'}>, <Axes: title={'center': 'j'}>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pre_df.loc[:, 'price'] = np.log1p(pre_df['price'])\n",
    "pre_df.hist(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e62401d-b08d-4e45-91f7-163f5af8d1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x        0.958027\n",
       "y        0.936190\n",
       "z        0.935233\n",
       "carat    0.920252\n",
       "Name: price, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_df.corr()['price'].sort_values(ascending=False)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f459af53-6c32-4d6d-aba7-8a748b21e45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>carat</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.789960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.21</td>\n",
       "      <td>5.789960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.23</td>\n",
       "      <td>5.793014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.29</td>\n",
       "      <td>5.814131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.31</td>\n",
       "      <td>5.817111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "      <td>0.86</td>\n",
       "      <td>7.922261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.75</td>\n",
       "      <td>7.922261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53940</th>\n",
       "      <td>5.79</td>\n",
       "      <td>5.74</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.71</td>\n",
       "      <td>7.921898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53941</th>\n",
       "      <td>5.74</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.71</td>\n",
       "      <td>7.921898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53942</th>\n",
       "      <td>5.71</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.70</td>\n",
       "      <td>7.922261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53943 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x     y     z  carat     price\n",
       "0      3.95  3.98  2.43   0.23  5.789960\n",
       "1      3.89  3.84  2.31   0.21  5.789960\n",
       "2      4.05  4.07  2.31   0.23  5.793014\n",
       "3      4.20  4.23  2.63   0.29  5.814131\n",
       "4      4.34  4.35  2.75   0.31  5.817111\n",
       "...     ...   ...   ...    ...       ...\n",
       "53938  6.15  6.12  3.74   0.86  7.922261\n",
       "53939  5.83  5.87  3.64   0.75  7.922261\n",
       "53940  5.79  5.74  3.49   0.71  7.921898\n",
       "53941  5.74  5.73  3.43   0.71  7.921898\n",
       "53942  5.71  5.76  3.47   0.70  7.922261\n",
       "\n",
       "[53943 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['x', 'y', 'z', 'carat' , 'price']\n",
    "pre_df = pre_df.loc[:, columns]\n",
    "pre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78eadaa9-c510-4b85-bbe3-67b99237ccd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "corr = pre_df.corr()\n",
    "corr\n",
    "sns.heatmap(corr, cmap=\"Blues\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6adbe4e-755d-4718-8114-73779b98856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "\n",
    "class LinearRegressionModel(Module):\n",
    "    def __init__(self, input):\n",
    "        super().__init__()\n",
    "        self.linear = Linear(input, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f08c4f01-87bf-4bf2-a770-ae5cd78839da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/100, Batch: 1/432, W1: 0.7044, W2: 0.6184, W3: -0.0794, W4: 0.0463, b: 0.358\n",
      "loss: 0.2571\n",
      "Epoch: 10/100, Batch: 2/432, W1: 0.7042, W2: 0.6182, W3: -0.0795, W4: 0.0463, b: 0.358\n",
      "loss: 0.3071\n",
      "Epoch: 10/100, Batch: 3/432, W1: 0.7042, W2: 0.6183, W3: -0.0795, W4: 0.0462, b: 0.358\n",
      "loss: 0.2328\n",
      "Epoch: 10/100, Batch: 4/432, W1: 0.7043, W2: 0.6183, W3: -0.0795, W4: 0.0462, b: 0.358\n",
      "loss: 0.2398\n",
      "Epoch: 10/100, Batch: 5/432, W1: 0.7041, W2: 0.6182, W3: -0.0795, W4: 0.0462, b: 0.358\n",
      "loss: 0.3905\n",
      "Epoch: 10/100, Batch: 6/432, W1: 0.704, W2: 0.6181, W3: -0.0796, W4: 0.0461, b: 0.358\n",
      "loss: 0.2928\n",
      "Epoch: 10/100, Batch: 7/432, W1: 0.7041, W2: 0.6181, W3: -0.0796, W4: 0.0461, b: 0.358\n",
      "loss: 0.3423\n",
      "Epoch: 10/100, Batch: 8/432, W1: 0.704, W2: 0.6181, W3: -0.0796, W4: 0.046, b: 0.3581\n",
      "loss: 0.3371\n",
      "Epoch: 10/100, Batch: 9/432, W1: 0.7041, W2: 0.6181, W3: -0.0796, W4: 0.046, b: 0.3581\n",
      "loss: 0.31\n",
      "Epoch: 10/100, Batch: 10/432, W1: 0.7041, W2: 0.6181, W3: -0.0796, W4: 0.046, b: 0.3581\n",
      "loss: 0.3184\n",
      "Epoch: 10/100, Batch: 11/432, W1: 0.7042, W2: 0.6182, W3: -0.0795, W4: 0.046, b: 0.3581\n",
      "loss: 0.2898\n",
      "Epoch: 10/100, Batch: 12/432, W1: 0.7042, W2: 0.6182, W3: -0.0795, W4: 0.0459, b: 0.3581\n",
      "loss: 0.3393\n",
      "Epoch: 10/100, Batch: 13/432, W1: 0.7041, W2: 0.6181, W3: -0.0796, W4: 0.0459, b: 0.3581\n",
      "loss: 0.2954\n",
      "Epoch: 10/100, Batch: 14/432, W1: 0.7041, W2: 0.6181, W3: -0.0796, W4: 0.0459, b: 0.3582\n",
      "loss: 1.1071\n",
      "Epoch: 10/100, Batch: 15/432, W1: 0.7042, W2: 0.6182, W3: -0.0795, W4: 0.0459, b: 0.3582\n",
      "loss: 0.2494\n",
      "Epoch: 10/100, Batch: 16/432, W1: 0.7043, W2: 0.6183, W3: -0.0794, W4: 0.0459, b: 0.3583\n",
      "loss: 0.2869\n",
      "Epoch: 10/100, Batch: 17/432, W1: 0.7042, W2: 0.6183, W3: -0.0795, W4: 0.0458, b: 0.3583\n",
      "loss: 0.2789\n",
      "Epoch: 10/100, Batch: 18/432, W1: 0.7041, W2: 0.6182, W3: -0.0795, W4: 0.0458, b: 0.3583\n",
      "loss: 0.2942\n",
      "Epoch: 10/100, Batch: 19/432, W1: 0.7041, W2: 0.6181, W3: -0.0795, W4: 0.0458, b: 0.3583\n",
      "loss: 0.3204\n",
      "Epoch: 10/100, Batch: 20/432, W1: 0.7042, W2: 0.6182, W3: -0.0795, W4: 0.0457, b: 0.3583\n",
      "loss: 0.2526\n",
      "Epoch: 10/100, Batch: 21/432, W1: 0.7043, W2: 0.6183, W3: -0.0794, W4: 0.0457, b: 0.3583\n",
      "loss: 0.2456\n",
      "Epoch: 10/100, Batch: 22/432, W1: 0.7043, W2: 0.6183, W3: -0.0794, W4: 0.0457, b: 0.3584\n",
      "loss: 0.2416\n",
      "Epoch: 10/100, Batch: 23/432, W1: 0.7043, W2: 0.6184, W3: -0.0794, W4: 0.0457, b: 0.3584\n",
      "loss: 0.2595\n",
      "Epoch: 10/100, Batch: 24/432, W1: 0.7043, W2: 0.6183, W3: -0.0794, W4: 0.0457, b: 0.3584\n",
      "loss: 0.3537\n",
      "Epoch: 10/100, Batch: 25/432, W1: 0.7042, W2: 0.6183, W3: -0.0795, W4: 0.0456, b: 0.3584\n",
      "loss: 0.3433\n",
      "Epoch: 10/100, Batch: 26/432, W1: 0.7042, W2: 0.6182, W3: -0.0795, W4: 0.0456, b: 0.3584\n",
      "loss: 0.3007\n",
      "Epoch: 10/100, Batch: 27/432, W1: 0.7042, W2: 0.6183, W3: -0.0795, W4: 0.0455, b: 0.3584\n",
      "loss: 0.3304\n",
      "Epoch: 10/100, Batch: 28/432, W1: 0.7042, W2: 0.6183, W3: -0.0795, W4: 0.0455, b: 0.3585\n",
      "loss: 0.2758\n",
      "Epoch: 10/100, Batch: 29/432, W1: 0.7041, W2: 0.6182, W3: -0.0795, W4: 0.0455, b: 0.3585\n",
      "loss: 0.5485\n",
      "Epoch: 10/100, Batch: 30/432, W1: 0.704, W2: 0.6181, W3: -0.0796, W4: 0.0454, b: 0.3585\n",
      "loss: 0.3446\n",
      "Epoch: 10/100, Batch: 31/432, W1: 0.7039, W2: 0.618, W3: -0.0797, W4: 0.0454, b: 0.3585\n",
      "loss: 0.3633\n",
      "Epoch: 10/100, Batch: 32/432, W1: 0.7039, W2: 0.618, W3: -0.0797, W4: 0.0453, b: 0.3585\n",
      "loss: 0.2828\n",
      "Epoch: 10/100, Batch: 33/432, W1: 0.7039, W2: 0.618, W3: -0.0796, W4: 0.0453, b: 0.3585\n",
      "loss: 0.2613\n",
      "Epoch: 10/100, Batch: 34/432, W1: 0.704, W2: 0.6181, W3: -0.0796, W4: 0.0453, b: 0.3585\n",
      "loss: 0.2654\n",
      "Epoch: 10/100, Batch: 35/432, W1: 0.7041, W2: 0.6182, W3: -0.0795, W4: 0.0453, b: 0.3586\n",
      "loss: 0.2993\n",
      "Epoch: 10/100, Batch: 36/432, W1: 0.704, W2: 0.6181, W3: -0.0796, W4: 0.0452, b: 0.3586\n",
      "loss: 0.3875\n",
      "Epoch: 10/100, Batch: 37/432, W1: 0.7039, W2: 0.618, W3: -0.0796, W4: 0.0452, b: 0.3586\n",
      "loss: 0.2948\n",
      "Epoch: 10/100, Batch: 38/432, W1: 0.704, W2: 0.6181, W3: -0.0796, W4: 0.0452, b: 0.3586\n",
      "loss: 0.2771\n",
      "Epoch: 10/100, Batch: 39/432, W1: 0.704, W2: 0.6181, W3: -0.0796, W4: 0.0452, b: 0.3586\n",
      "loss: 0.3441\n",
      "Epoch: 10/100, Batch: 40/432, W1: 0.7039, W2: 0.618, W3: -0.0797, W4: 0.0451, b: 0.3586\n",
      "loss: 0.329\n",
      "Epoch: 10/100, Batch: 41/432, W1: 0.7039, W2: 0.618, W3: -0.0796, W4: 0.0451, b: 0.3587\n",
      "loss: 0.2866\n",
      "Epoch: 10/100, Batch: 42/432, W1: 0.7039, W2: 0.618, W3: -0.0797, W4: 0.045, b: 0.3587\n",
      "loss: 0.3306\n",
      "Epoch: 10/100, Batch: 43/432, W1: 0.7039, W2: 0.618, W3: -0.0797, W4: 0.045, b: 0.3587\n",
      "loss: 0.3176\n",
      "Epoch: 10/100, Batch: 44/432, W1: 0.7039, W2: 0.618, W3: -0.0797, W4: 0.045, b: 0.3587\n",
      "loss: 0.317\n",
      "Epoch: 10/100, Batch: 45/432, W1: 0.7039, W2: 0.618, W3: -0.0796, W4: 0.045, b: 0.3587\n",
      "loss: 0.3063\n",
      "Epoch: 10/100, Batch: 46/432, W1: 0.7037, W2: 0.6178, W3: -0.0798, W4: 0.0449, b: 0.3587\n",
      "loss: 0.5184\n",
      "Epoch: 10/100, Batch: 47/432, W1: 0.7036, W2: 0.6177, W3: -0.0798, W4: 0.0448, b: 0.3587\n",
      "loss: 0.2881\n",
      "Epoch: 10/100, Batch: 48/432, W1: 0.7037, W2: 0.6178, W3: -0.0798, W4: 0.0448, b: 0.3587\n",
      "loss: 0.3442\n",
      "Epoch: 10/100, Batch: 49/432, W1: 0.7037, W2: 0.6178, W3: -0.0798, W4: 0.0448, b: 0.3588\n",
      "loss: 0.353\n",
      "Epoch: 10/100, Batch: 50/432, W1: 0.7037, W2: 0.6178, W3: -0.0798, W4: 0.0447, b: 0.3588\n",
      "loss: 0.3203\n",
      "Epoch: 10/100, Batch: 51/432, W1: 0.7038, W2: 0.6179, W3: -0.0797, W4: 0.0447, b: 0.3588\n",
      "loss: 0.2382\n",
      "Epoch: 10/100, Batch: 52/432, W1: 0.7038, W2: 0.618, W3: -0.0797, W4: 0.0447, b: 0.3588\n",
      "loss: 0.2826\n",
      "Epoch: 10/100, Batch: 53/432, W1: 0.7039, W2: 0.618, W3: -0.0797, W4: 0.0447, b: 0.3589\n",
      "loss: 0.2539\n",
      "Epoch: 10/100, Batch: 54/432, W1: 0.7039, W2: 0.618, W3: -0.0796, W4: 0.0447, b: 0.3589\n",
      "loss: 0.2401\n",
      "Epoch: 10/100, Batch: 55/432, W1: 0.7038, W2: 0.6179, W3: -0.0797, W4: 0.0446, b: 0.3589\n",
      "loss: 0.2873\n",
      "Epoch: 10/100, Batch: 56/432, W1: 0.7037, W2: 0.6178, W3: -0.0798, W4: 0.0446, b: 0.3589\n",
      "loss: 0.4113\n",
      "Epoch: 10/100, Batch: 57/432, W1: 0.7036, W2: 0.6178, W3: -0.0798, W4: 0.0445, b: 0.3589\n",
      "loss: 0.2972\n",
      "Epoch: 10/100, Batch: 58/432, W1: 0.7035, W2: 0.6176, W3: -0.0799, W4: 0.0444, b: 0.3589\n",
      "loss: 0.4259\n",
      "Epoch: 10/100, Batch: 59/432, W1: 0.7034, W2: 0.6175, W3: -0.0799, W4: 0.0444, b: 0.3589\n",
      "loss: 0.2913\n",
      "Epoch: 10/100, Batch: 60/432, W1: 0.7034, W2: 0.6175, W3: -0.0799, W4: 0.0444, b: 0.3589\n",
      "loss: 0.2942\n",
      "Epoch: 10/100, Batch: 61/432, W1: 0.7035, W2: 0.6176, W3: -0.0799, W4: 0.0444, b: 0.3589\n",
      "loss: 0.2544\n",
      "Epoch: 10/100, Batch: 62/432, W1: 0.7035, W2: 0.6176, W3: -0.0799, W4: 0.0443, b: 0.359\n",
      "loss: 0.4078\n",
      "Epoch: 10/100, Batch: 63/432, W1: 0.7034, W2: 0.6175, W3: -0.08, W4: 0.0443, b: 0.359\n",
      "loss: 0.2812\n",
      "Epoch: 10/100, Batch: 64/432, W1: 0.7033, W2: 0.6174, W3: -0.08, W4: 0.0442, b: 0.359\n",
      "loss: 0.3216\n",
      "Epoch: 10/100, Batch: 65/432, W1: 0.7033, W2: 0.6174, W3: -0.08, W4: 0.0442, b: 0.359\n",
      "loss: 0.3982\n",
      "Epoch: 10/100, Batch: 66/432, W1: 0.7032, W2: 0.6173, W3: -0.0801, W4: 0.0441, b: 0.359\n",
      "loss: 0.3576\n",
      "Epoch: 10/100, Batch: 67/432, W1: 0.7033, W2: 0.6174, W3: -0.08, W4: 0.0441, b: 0.359\n",
      "loss: 0.3267\n",
      "Epoch: 10/100, Batch: 68/432, W1: 0.7033, W2: 0.6174, W3: -0.08, W4: 0.0441, b: 0.3591\n",
      "loss: 0.349\n",
      "Epoch: 10/100, Batch: 69/432, W1: 0.7033, W2: 0.6175, W3: -0.08, W4: 0.0441, b: 0.3591\n",
      "loss: 0.2562\n",
      "Epoch: 10/100, Batch: 70/432, W1: 0.7034, W2: 0.6176, W3: -0.0799, W4: 0.0441, b: 0.3591\n",
      "loss: 0.2998\n",
      "Epoch: 10/100, Batch: 71/432, W1: 0.7035, W2: 0.6176, W3: -0.0799, W4: 0.044, b: 0.3591\n",
      "loss: 0.3286\n",
      "Epoch: 10/100, Batch: 72/432, W1: 0.7036, W2: 0.6177, W3: -0.0798, W4: 0.044, b: 0.3592\n",
      "loss: 0.3088\n",
      "Epoch: 10/100, Batch: 73/432, W1: 0.7035, W2: 0.6176, W3: -0.0799, W4: 0.044, b: 0.3592\n",
      "loss: 0.3233\n",
      "Epoch: 10/100, Batch: 74/432, W1: 0.7034, W2: 0.6176, W3: -0.0799, W4: 0.0439, b: 0.3592\n",
      "loss: 0.3208\n",
      "Epoch: 10/100, Batch: 75/432, W1: 0.7034, W2: 0.6175, W3: -0.0799, W4: 0.0439, b: 0.3592\n",
      "loss: 0.2717\n",
      "Epoch: 10/100, Batch: 76/432, W1: 0.7035, W2: 0.6176, W3: -0.0799, W4: 0.0439, b: 0.3592\n",
      "loss: 0.2429\n",
      "Epoch: 10/100, Batch: 77/432, W1: 0.7036, W2: 0.6177, W3: -0.0798, W4: 0.0439, b: 0.3593\n",
      "loss: 0.3546\n",
      "Epoch: 10/100, Batch: 78/432, W1: 0.7036, W2: 0.6178, W3: -0.0798, W4: 0.0438, b: 0.3593\n",
      "loss: 0.3228\n",
      "Epoch: 10/100, Batch: 79/432, W1: 0.7032, W2: 0.614, W3: -0.0803, W4: 0.0437, b: 0.3593\n",
      "loss: 10.8156\n",
      "Epoch: 10/100, Batch: 80/432, W1: 0.7032, W2: 0.6141, W3: -0.0802, W4: 0.0437, b: 0.3593\n",
      "loss: 0.4139\n",
      "Epoch: 10/100, Batch: 81/432, W1: 0.7034, W2: 0.6142, W3: -0.0801, W4: 0.0437, b: 0.3593\n",
      "loss: 0.2664\n",
      "Epoch: 10/100, Batch: 82/432, W1: 0.7034, W2: 0.6142, W3: -0.0801, W4: 0.0436, b: 0.3593\n",
      "loss: 0.2745\n",
      "Epoch: 10/100, Batch: 83/432, W1: 0.7035, W2: 0.6144, W3: -0.08, W4: 0.0436, b: 0.3594\n",
      "loss: 0.3008\n",
      "Epoch: 10/100, Batch: 84/432, W1: 0.7035, W2: 0.6144, W3: -0.08, W4: 0.0436, b: 0.3594\n",
      "loss: 0.2713\n",
      "Epoch: 10/100, Batch: 85/432, W1: 0.7036, W2: 0.6145, W3: -0.08, W4: 0.0436, b: 0.3594\n",
      "loss: 0.3217\n",
      "Epoch: 10/100, Batch: 86/432, W1: 0.7037, W2: 0.6146, W3: -0.0799, W4: 0.0436, b: 0.3595\n",
      "loss: 0.2724\n",
      "Epoch: 10/100, Batch: 87/432, W1: 0.7037, W2: 0.6146, W3: -0.0799, W4: 0.0435, b: 0.3595\n",
      "loss: 0.3221\n",
      "Epoch: 10/100, Batch: 88/432, W1: 0.7038, W2: 0.6147, W3: -0.0798, W4: 0.0435, b: 0.3595\n",
      "loss: 0.2457\n",
      "Epoch: 10/100, Batch: 89/432, W1: 0.7038, W2: 0.6147, W3: -0.0798, W4: 0.0435, b: 0.3595\n",
      "loss: 0.3052\n",
      "Epoch: 10/100, Batch: 90/432, W1: 0.7039, W2: 0.6147, W3: -0.0798, W4: 0.0435, b: 0.3596\n",
      "loss: 0.3137\n",
      "Epoch: 10/100, Batch: 91/432, W1: 0.7038, W2: 0.6147, W3: -0.0798, W4: 0.0434, b: 0.3596\n",
      "loss: 0.3303\n",
      "Epoch: 10/100, Batch: 92/432, W1: 0.7039, W2: 0.6148, W3: -0.0797, W4: 0.0434, b: 0.3596\n",
      "loss: 0.3156\n",
      "Epoch: 10/100, Batch: 93/432, W1: 0.7039, W2: 0.6148, W3: -0.0797, W4: 0.0434, b: 0.3596\n",
      "loss: 0.3761\n",
      "Epoch: 10/100, Batch: 94/432, W1: 0.704, W2: 0.6149, W3: -0.0797, W4: 0.0434, b: 0.3597\n",
      "loss: 0.3337\n",
      "Epoch: 10/100, Batch: 95/432, W1: 0.7041, W2: 0.615, W3: -0.0796, W4: 0.0434, b: 0.3597\n",
      "loss: 0.2379\n",
      "Epoch: 10/100, Batch: 96/432, W1: 0.7042, W2: 0.615, W3: -0.0796, W4: 0.0433, b: 0.3597\n",
      "loss: 0.2793\n",
      "Epoch: 10/100, Batch: 97/432, W1: 0.7042, W2: 0.6151, W3: -0.0796, W4: 0.0433, b: 0.3598\n",
      "loss: 0.3031\n",
      "Epoch: 10/100, Batch: 98/432, W1: 0.7044, W2: 0.6152, W3: -0.0795, W4: 0.0433, b: 0.3598\n",
      "loss: 0.2724\n",
      "Epoch: 10/100, Batch: 99/432, W1: 0.7044, W2: 0.6153, W3: -0.0795, W4: 0.0433, b: 0.3598\n",
      "loss: 0.2783\n",
      "Epoch: 10/100, Batch: 100/432, W1: 0.7044, W2: 0.6153, W3: -0.0794, W4: 0.0433, b: 0.3598\n",
      "loss: 0.2609\n",
      "Epoch: 10/100, Batch: 101/432, W1: 0.7044, W2: 0.6153, W3: -0.0794, W4: 0.0432, b: 0.3599\n",
      "loss: 0.2993\n",
      "Epoch: 10/100, Batch: 102/432, W1: 0.7045, W2: 0.6154, W3: -0.0794, W4: 0.0432, b: 0.3599\n",
      "loss: 0.2591\n",
      "Epoch: 10/100, Batch: 103/432, W1: 0.7046, W2: 0.6155, W3: -0.0793, W4: 0.0432, b: 0.3599\n",
      "loss: 0.2557\n",
      "Epoch: 10/100, Batch: 104/432, W1: 0.7046, W2: 0.6155, W3: -0.0793, W4: 0.0432, b: 0.36\n",
      "loss: 0.3327\n",
      "Epoch: 10/100, Batch: 105/432, W1: 0.7047, W2: 0.6156, W3: -0.0793, W4: 0.0432, b: 0.36\n",
      "loss: 0.2892\n",
      "Epoch: 10/100, Batch: 106/432, W1: 0.7046, W2: 0.6154, W3: -0.0793, W4: 0.0431, b: 0.36\n",
      "loss: 0.2871\n",
      "Epoch: 10/100, Batch: 107/432, W1: 0.7045, W2: 0.6154, W3: -0.0794, W4: 0.0431, b: 0.36\n",
      "loss: 0.2696\n",
      "Epoch: 10/100, Batch: 108/432, W1: 0.7045, W2: 0.6154, W3: -0.0794, W4: 0.043, b: 0.36\n",
      "loss: 0.3325\n",
      "Epoch: 10/100, Batch: 109/432, W1: 0.7046, W2: 0.6154, W3: -0.0793, W4: 0.043, b: 0.36\n",
      "loss: 0.2855\n",
      "Epoch: 10/100, Batch: 110/432, W1: 0.7046, W2: 0.6154, W3: -0.0793, W4: 0.043, b: 0.36\n",
      "loss: 0.3093\n",
      "Epoch: 10/100, Batch: 111/432, W1: 0.7046, W2: 0.6155, W3: -0.0793, W4: 0.043, b: 0.3601\n",
      "loss: 0.2374\n",
      "Epoch: 10/100, Batch: 112/432, W1: 0.7047, W2: 0.6156, W3: -0.0793, W4: 0.043, b: 0.3601\n",
      "loss: 0.3278\n",
      "Epoch: 10/100, Batch: 113/432, W1: 0.7047, W2: 0.6156, W3: -0.0792, W4: 0.043, b: 0.3601\n",
      "loss: 0.2616\n",
      "Epoch: 10/100, Batch: 114/432, W1: 0.7049, W2: 0.6157, W3: -0.0791, W4: 0.0429, b: 0.3602\n",
      "loss: 0.2335\n",
      "Epoch: 10/100, Batch: 115/432, W1: 0.705, W2: 0.6159, W3: -0.079, W4: 0.0429, b: 0.3602\n",
      "loss: 0.2746\n",
      "Epoch: 10/100, Batch: 116/432, W1: 0.705, W2: 0.6159, W3: -0.0791, W4: 0.0429, b: 0.3602\n",
      "loss: 0.4084\n",
      "Epoch: 10/100, Batch: 117/432, W1: 0.705, W2: 0.6159, W3: -0.0791, W4: 0.0429, b: 0.3602\n",
      "loss: 0.3025\n",
      "Epoch: 10/100, Batch: 118/432, W1: 0.705, W2: 0.6159, W3: -0.079, W4: 0.0428, b: 0.3603\n",
      "loss: 0.2611\n",
      "Epoch: 10/100, Batch: 119/432, W1: 0.7051, W2: 0.616, W3: -0.079, W4: 0.0428, b: 0.3603\n",
      "loss: 0.7984\n",
      "Epoch: 10/100, Batch: 120/432, W1: 0.7052, W2: 0.616, W3: -0.079, W4: 0.0428, b: 0.3603\n",
      "loss: 0.2837\n",
      "Epoch: 10/100, Batch: 121/432, W1: 0.7051, W2: 0.616, W3: -0.079, W4: 0.0428, b: 0.3603\n",
      "loss: 0.3763\n",
      "Epoch: 10/100, Batch: 122/432, W1: 0.7051, W2: 0.6159, W3: -0.079, W4: 0.0427, b: 0.3604\n",
      "loss: 0.3435\n",
      "Epoch: 10/100, Batch: 123/432, W1: 0.7049, W2: 0.6158, W3: -0.0791, W4: 0.0427, b: 0.3604\n",
      "loss: 0.4695\n",
      "Epoch: 10/100, Batch: 124/432, W1: 0.7049, W2: 0.6158, W3: -0.0791, W4: 0.0426, b: 0.3604\n",
      "loss: 0.2626\n",
      "Epoch: 10/100, Batch: 125/432, W1: 0.7051, W2: 0.616, W3: -0.079, W4: 0.0426, b: 0.3604\n",
      "loss: 0.2845\n",
      "Epoch: 10/100, Batch: 126/432, W1: 0.7052, W2: 0.6161, W3: -0.0789, W4: 0.0426, b: 0.3604\n",
      "loss: 0.2789\n",
      "Epoch: 10/100, Batch: 127/432, W1: 0.705, W2: 0.6159, W3: -0.079, W4: 0.0426, b: 0.3605\n",
      "loss: 0.3888\n",
      "Epoch: 10/100, Batch: 128/432, W1: 0.705, W2: 0.6159, W3: -0.079, W4: 0.0425, b: 0.3605\n",
      "loss: 0.2651\n",
      "Epoch: 10/100, Batch: 129/432, W1: 0.7049, W2: 0.6158, W3: -0.0791, W4: 0.0425, b: 0.3605\n",
      "loss: 0.5072\n",
      "Epoch: 10/100, Batch: 130/432, W1: 0.7051, W2: 0.616, W3: -0.079, W4: 0.0425, b: 0.3605\n",
      "loss: 0.2963\n",
      "Epoch: 10/100, Batch: 131/432, W1: 0.7051, W2: 0.616, W3: -0.0789, W4: 0.0424, b: 0.3605\n",
      "loss: 0.2368\n",
      "Epoch: 10/100, Batch: 132/432, W1: 0.7052, W2: 0.6161, W3: -0.0789, W4: 0.0424, b: 0.3606\n",
      "loss: 0.3364\n",
      "Epoch: 10/100, Batch: 133/432, W1: 0.7053, W2: 0.6162, W3: -0.0788, W4: 0.0424, b: 0.3606\n",
      "loss: 0.2757\n",
      "Epoch: 10/100, Batch: 134/432, W1: 0.7052, W2: 0.6161, W3: -0.0788, W4: 0.0424, b: 0.3606\n",
      "loss: 0.3854\n",
      "Epoch: 10/100, Batch: 135/432, W1: 0.7053, W2: 0.6162, W3: -0.0788, W4: 0.0423, b: 0.3606\n",
      "loss: 0.2365\n",
      "Epoch: 10/100, Batch: 136/432, W1: 0.7052, W2: 0.6162, W3: -0.0788, W4: 0.0423, b: 0.3607\n",
      "loss: 0.3011\n",
      "Epoch: 10/100, Batch: 137/432, W1: 0.7052, W2: 0.6161, W3: -0.0789, W4: 0.0423, b: 0.3607\n",
      "loss: 0.3385\n",
      "Epoch: 10/100, Batch: 138/432, W1: 0.7053, W2: 0.6162, W3: -0.0788, W4: 0.0423, b: 0.3607\n",
      "loss: 0.2941\n",
      "Epoch: 10/100, Batch: 139/432, W1: 0.7053, W2: 0.6162, W3: -0.0788, W4: 0.0422, b: 0.3607\n",
      "loss: 0.3247\n",
      "Epoch: 10/100, Batch: 140/432, W1: 0.7054, W2: 0.6163, W3: -0.0788, W4: 0.0422, b: 0.3607\n",
      "loss: 0.3134\n",
      "Epoch: 10/100, Batch: 141/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0422, b: 0.3608\n",
      "loss: 0.2991\n",
      "Epoch: 10/100, Batch: 142/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0421, b: 0.3608\n",
      "loss: 0.3415\n",
      "Epoch: 10/100, Batch: 143/432, W1: 0.7055, W2: 0.6164, W3: -0.0787, W4: 0.0421, b: 0.3608\n",
      "loss: 0.2771\n",
      "Epoch: 10/100, Batch: 144/432, W1: 0.7055, W2: 0.6164, W3: -0.0787, W4: 0.0421, b: 0.3608\n",
      "loss: 0.2804\n",
      "Epoch: 10/100, Batch: 145/432, W1: 0.7053, W2: 0.6162, W3: -0.0788, W4: 0.042, b: 0.3608\n",
      "loss: 0.3065\n",
      "Epoch: 10/100, Batch: 146/432, W1: 0.7053, W2: 0.6162, W3: -0.0788, W4: 0.042, b: 0.3608\n",
      "loss: 0.2548\n",
      "Epoch: 10/100, Batch: 147/432, W1: 0.7053, W2: 0.6162, W3: -0.0788, W4: 0.042, b: 0.3609\n",
      "loss: 0.346\n",
      "Epoch: 10/100, Batch: 148/432, W1: 0.7053, W2: 0.6162, W3: -0.0788, W4: 0.0419, b: 0.3609\n",
      "loss: 0.3616\n",
      "Epoch: 10/100, Batch: 149/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0419, b: 0.3609\n",
      "loss: 0.3183\n",
      "Epoch: 10/100, Batch: 150/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0419, b: 0.3609\n",
      "loss: 0.237\n",
      "Epoch: 10/100, Batch: 151/432, W1: 0.7053, W2: 0.6163, W3: -0.0788, W4: 0.0419, b: 0.3609\n",
      "loss: 0.3125\n",
      "Epoch: 10/100, Batch: 152/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0418, b: 0.361\n",
      "loss: 0.2631\n",
      "Epoch: 10/100, Batch: 153/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0418, b: 0.361\n",
      "loss: 0.3348\n",
      "Epoch: 10/100, Batch: 154/432, W1: 0.7055, W2: 0.6164, W3: -0.0787, W4: 0.0418, b: 0.361\n",
      "loss: 0.221\n",
      "Epoch: 10/100, Batch: 155/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0418, b: 0.361\n",
      "loss: 0.3709\n",
      "Epoch: 10/100, Batch: 156/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0417, b: 0.361\n",
      "loss: 0.275\n",
      "Epoch: 10/100, Batch: 157/432, W1: 0.7053, W2: 0.6162, W3: -0.0788, W4: 0.0417, b: 0.361\n",
      "loss: 0.4327\n",
      "Epoch: 10/100, Batch: 158/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0417, b: 0.3611\n",
      "loss: 0.2223\n",
      "Epoch: 10/100, Batch: 159/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0416, b: 0.3611\n",
      "loss: 0.2773\n",
      "Epoch: 10/100, Batch: 160/432, W1: 0.7055, W2: 0.6164, W3: -0.0787, W4: 0.0416, b: 0.3611\n",
      "loss: 0.2451\n",
      "Epoch: 10/100, Batch: 161/432, W1: 0.7055, W2: 0.6164, W3: -0.0787, W4: 0.0416, b: 0.3611\n",
      "loss: 0.2673\n",
      "Epoch: 10/100, Batch: 162/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0415, b: 0.3611\n",
      "loss: 0.2831\n",
      "Epoch: 10/100, Batch: 163/432, W1: 0.7055, W2: 0.6164, W3: -0.0787, W4: 0.0415, b: 0.3612\n",
      "loss: 0.2533\n",
      "Epoch: 10/100, Batch: 164/432, W1: 0.7054, W2: 0.6164, W3: -0.0787, W4: 0.0415, b: 0.3612\n",
      "loss: 0.3317\n",
      "Epoch: 10/100, Batch: 165/432, W1: 0.7054, W2: 0.6164, W3: -0.0787, W4: 0.0415, b: 0.3612\n",
      "loss: 0.3394\n",
      "Epoch: 10/100, Batch: 166/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0415, b: 0.3612\n",
      "loss: 0.2774\n",
      "Epoch: 10/100, Batch: 167/432, W1: 0.7055, W2: 0.6164, W3: -0.0787, W4: 0.0414, b: 0.3612\n",
      "loss: 0.2815\n",
      "Epoch: 10/100, Batch: 168/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0414, b: 0.3613\n",
      "loss: 0.2674\n",
      "Epoch: 10/100, Batch: 169/432, W1: 0.7055, W2: 0.6165, W3: -0.0786, W4: 0.0414, b: 0.3613\n",
      "loss: 0.2891\n",
      "Epoch: 10/100, Batch: 170/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0413, b: 0.3613\n",
      "loss: 0.3929\n",
      "Epoch: 10/100, Batch: 171/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0413, b: 0.3613\n",
      "loss: 0.3439\n",
      "Epoch: 10/100, Batch: 172/432, W1: 0.7054, W2: 0.6164, W3: -0.0787, W4: 0.0413, b: 0.3613\n",
      "loss: 0.2812\n",
      "Epoch: 10/100, Batch: 173/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0412, b: 0.3613\n",
      "loss: 0.31\n",
      "Epoch: 10/100, Batch: 174/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0412, b: 0.3614\n",
      "loss: 0.2895\n",
      "Epoch: 10/100, Batch: 175/432, W1: 0.7054, W2: 0.6164, W3: -0.0787, W4: 0.0412, b: 0.3614\n",
      "loss: 0.3198\n",
      "Epoch: 10/100, Batch: 176/432, W1: 0.7053, W2: 0.6163, W3: -0.0787, W4: 0.0411, b: 0.3614\n",
      "loss: 0.3543\n",
      "Epoch: 10/100, Batch: 177/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0411, b: 0.3614\n",
      "loss: 0.2636\n",
      "Epoch: 10/100, Batch: 178/432, W1: 0.7054, W2: 0.6164, W3: -0.0787, W4: 0.0411, b: 0.3614\n",
      "loss: 0.3288\n",
      "Epoch: 10/100, Batch: 179/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0411, b: 0.3615\n",
      "loss: 0.258\n",
      "Epoch: 10/100, Batch: 180/432, W1: 0.7054, W2: 0.6164, W3: -0.0787, W4: 0.041, b: 0.3615\n",
      "loss: 0.3784\n",
      "Epoch: 10/100, Batch: 181/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.041, b: 0.3615\n",
      "loss: 0.3452\n",
      "Epoch: 10/100, Batch: 182/432, W1: 0.7053, W2: 0.6163, W3: -0.0787, W4: 0.0409, b: 0.3615\n",
      "loss: 0.2749\n",
      "Epoch: 10/100, Batch: 183/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0409, b: 0.3615\n",
      "loss: 0.3204\n",
      "Epoch: 10/100, Batch: 184/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0409, b: 0.3615\n",
      "loss: 0.2985\n",
      "Epoch: 10/100, Batch: 185/432, W1: 0.7053, W2: 0.6163, W3: -0.0787, W4: 0.0408, b: 0.3615\n",
      "loss: 0.3746\n",
      "Epoch: 10/100, Batch: 186/432, W1: 0.7053, W2: 0.6163, W3: -0.0787, W4: 0.0408, b: 0.3616\n",
      "loss: 0.3914\n",
      "Epoch: 10/100, Batch: 187/432, W1: 0.7054, W2: 0.6164, W3: -0.0787, W4: 0.0408, b: 0.3616\n",
      "loss: 0.2023\n",
      "Epoch: 10/100, Batch: 188/432, W1: 0.7055, W2: 0.6165, W3: -0.0786, W4: 0.0408, b: 0.3616\n",
      "loss: 0.3274\n",
      "Epoch: 10/100, Batch: 189/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0407, b: 0.3616\n",
      "loss: 0.3024\n",
      "Epoch: 10/100, Batch: 190/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0407, b: 0.3617\n",
      "loss: 0.374\n",
      "Epoch: 10/100, Batch: 191/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0406, b: 0.3617\n",
      "loss: 0.3594\n",
      "Epoch: 10/100, Batch: 192/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0406, b: 0.3617\n",
      "loss: 0.2081\n",
      "Epoch: 10/100, Batch: 193/432, W1: 0.7054, W2: 0.6163, W3: -0.0787, W4: 0.0406, b: 0.3617\n",
      "loss: 0.3499\n",
      "Epoch: 10/100, Batch: 194/432, W1: 0.7053, W2: 0.6163, W3: -0.0787, W4: 0.0405, b: 0.3617\n",
      "loss: 0.3402\n",
      "Epoch: 10/100, Batch: 195/432, W1: 0.7052, W2: 0.6162, W3: -0.0788, W4: 0.0405, b: 0.3617\n",
      "loss: 0.4462\n",
      "Epoch: 10/100, Batch: 196/432, W1: 0.7052, W2: 0.6162, W3: -0.0788, W4: 0.0404, b: 0.3617\n",
      "loss: 0.2734\n",
      "Epoch: 10/100, Batch: 197/432, W1: 0.7053, W2: 0.6162, W3: -0.0787, W4: 0.0404, b: 0.3618\n",
      "loss: 0.2552\n",
      "Epoch: 10/100, Batch: 198/432, W1: 0.7054, W2: 0.6164, W3: -0.0786, W4: 0.0404, b: 0.3618\n",
      "loss: 0.2891\n",
      "Epoch: 10/100, Batch: 199/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0404, b: 0.3618\n",
      "loss: 0.2777\n",
      "Epoch: 10/100, Batch: 200/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0404, b: 0.3619\n",
      "loss: 0.2539\n",
      "Epoch: 10/100, Batch: 201/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0404, b: 0.3619\n",
      "loss: 0.2269\n",
      "Epoch: 10/100, Batch: 202/432, W1: 0.7054, W2: 0.6164, W3: -0.0786, W4: 0.0403, b: 0.3619\n",
      "loss: 0.3829\n",
      "Epoch: 10/100, Batch: 203/432, W1: 0.7055, W2: 0.6164, W3: -0.0786, W4: 0.0403, b: 0.3619\n",
      "loss: 0.2788\n",
      "Epoch: 10/100, Batch: 204/432, W1: 0.7055, W2: 0.6165, W3: -0.0786, W4: 0.0403, b: 0.3619\n",
      "loss: 0.2805\n",
      "Epoch: 10/100, Batch: 205/432, W1: 0.7055, W2: 0.6165, W3: -0.0786, W4: 0.0402, b: 0.3619\n",
      "loss: 0.2523\n",
      "Epoch: 10/100, Batch: 206/432, W1: 0.7054, W2: 0.6155, W3: -0.0787, W4: 0.0402, b: 0.3619\n",
      "loss: 2.6275\n",
      "Epoch: 10/100, Batch: 207/432, W1: 0.7055, W2: 0.6157, W3: -0.0786, W4: 0.0402, b: 0.362\n",
      "loss: 0.2737\n",
      "Epoch: 10/100, Batch: 208/432, W1: 0.7056, W2: 0.6157, W3: -0.0786, W4: 0.0402, b: 0.362\n",
      "loss: 0.2322\n",
      "Epoch: 10/100, Batch: 209/432, W1: 0.7055, W2: 0.6157, W3: -0.0786, W4: 0.0402, b: 0.362\n",
      "loss: 0.3582\n",
      "Epoch: 10/100, Batch: 210/432, W1: 0.7054, W2: 0.6156, W3: -0.0787, W4: 0.0401, b: 0.362\n",
      "loss: 0.4521\n",
      "Epoch: 10/100, Batch: 211/432, W1: 0.7054, W2: 0.6156, W3: -0.0787, W4: 0.0401, b: 0.362\n",
      "loss: 0.2555\n",
      "Epoch: 10/100, Batch: 212/432, W1: 0.7054, W2: 0.6156, W3: -0.0787, W4: 0.04, b: 0.362\n",
      "loss: 0.3172\n",
      "Epoch: 10/100, Batch: 213/432, W1: 0.7055, W2: 0.6156, W3: -0.0787, W4: 0.04, b: 0.3621\n",
      "loss: 0.289\n",
      "Epoch: 10/100, Batch: 214/432, W1: 0.7056, W2: 0.6157, W3: -0.0786, W4: 0.04, b: 0.3621\n",
      "loss: 0.2965\n",
      "Epoch: 10/100, Batch: 215/432, W1: 0.7056, W2: 0.6158, W3: -0.0786, W4: 0.04, b: 0.3621\n",
      "loss: 0.3609\n",
      "Epoch: 10/100, Batch: 216/432, W1: 0.7057, W2: 0.6158, W3: -0.0785, W4: 0.0399, b: 0.3622\n",
      "loss: 0.2774\n",
      "Epoch: 10/100, Batch: 217/432, W1: 0.7057, W2: 0.6159, W3: -0.0785, W4: 0.0399, b: 0.3622\n",
      "loss: 0.2365\n",
      "Epoch: 10/100, Batch: 218/432, W1: 0.7057, W2: 0.6159, W3: -0.0785, W4: 0.0399, b: 0.3622\n",
      "loss: 0.3235\n",
      "Epoch: 10/100, Batch: 219/432, W1: 0.7057, W2: 0.6159, W3: -0.0785, W4: 0.0399, b: 0.3622\n",
      "loss: 0.2133\n",
      "Epoch: 10/100, Batch: 220/432, W1: 0.7057, W2: 0.6159, W3: -0.0785, W4: 0.0398, b: 0.3622\n",
      "loss: 0.2841\n",
      "Epoch: 10/100, Batch: 221/432, W1: 0.7057, W2: 0.6159, W3: -0.0785, W4: 0.0398, b: 0.3623\n",
      "loss: 0.3376\n",
      "Epoch: 10/100, Batch: 222/432, W1: 0.7057, W2: 0.6158, W3: -0.0785, W4: 0.0398, b: 0.3623\n",
      "loss: 0.307\n",
      "Epoch: 10/100, Batch: 223/432, W1: 0.7057, W2: 0.6158, W3: -0.0785, W4: 0.0397, b: 0.3623\n",
      "loss: 0.2998\n",
      "Epoch: 10/100, Batch: 224/432, W1: 0.7058, W2: 0.6159, W3: -0.0785, W4: 0.0397, b: 0.3623\n",
      "loss: 0.3306\n",
      "Epoch: 10/100, Batch: 225/432, W1: 0.7058, W2: 0.616, W3: -0.0784, W4: 0.0397, b: 0.3624\n",
      "loss: 0.3249\n",
      "Epoch: 10/100, Batch: 226/432, W1: 0.7057, W2: 0.6159, W3: -0.0785, W4: 0.0396, b: 0.3624\n",
      "loss: 0.3239\n",
      "Epoch: 10/100, Batch: 227/432, W1: 0.7059, W2: 0.616, W3: -0.0784, W4: 0.0397, b: 0.3624\n",
      "loss: 0.1975\n",
      "Epoch: 10/100, Batch: 228/432, W1: 0.706, W2: 0.6161, W3: -0.0783, W4: 0.0396, b: 0.3624\n",
      "loss: 0.2601\n",
      "Epoch: 10/100, Batch: 229/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0396, b: 0.3624\n",
      "loss: 0.3804\n",
      "Epoch: 10/100, Batch: 230/432, W1: 0.706, W2: 0.6162, W3: -0.0783, W4: 0.0396, b: 0.3625\n",
      "loss: 0.2747\n",
      "Epoch: 10/100, Batch: 231/432, W1: 0.706, W2: 0.6162, W3: -0.0783, W4: 0.0396, b: 0.3625\n",
      "loss: 0.304\n",
      "Epoch: 10/100, Batch: 232/432, W1: 0.706, W2: 0.6162, W3: -0.0783, W4: 0.0395, b: 0.3625\n",
      "loss: 1.1676\n",
      "Epoch: 10/100, Batch: 233/432, W1: 0.706, W2: 0.6162, W3: -0.0783, W4: 0.0395, b: 0.3625\n",
      "loss: 0.2789\n",
      "Epoch: 10/100, Batch: 234/432, W1: 0.7061, W2: 0.6163, W3: -0.0782, W4: 0.0395, b: 0.3626\n",
      "loss: 0.2957\n",
      "Epoch: 10/100, Batch: 235/432, W1: 0.706, W2: 0.6161, W3: -0.0783, W4: 0.0394, b: 0.3626\n",
      "loss: 0.3342\n",
      "Epoch: 10/100, Batch: 236/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0394, b: 0.3626\n",
      "loss: 0.2858\n",
      "Epoch: 10/100, Batch: 237/432, W1: 0.706, W2: 0.6162, W3: -0.0783, W4: 0.0394, b: 0.3626\n",
      "loss: 0.2518\n",
      "Epoch: 10/100, Batch: 238/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0394, b: 0.3626\n",
      "loss: 0.4287\n",
      "Epoch: 10/100, Batch: 239/432, W1: 0.7059, W2: 0.6161, W3: -0.0784, W4: 0.0393, b: 0.3626\n",
      "loss: 0.3704\n",
      "Epoch: 10/100, Batch: 240/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0393, b: 0.3627\n",
      "loss: 0.3145\n",
      "Epoch: 10/100, Batch: 241/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0393, b: 0.3627\n",
      "loss: 0.2709\n",
      "Epoch: 10/100, Batch: 242/432, W1: 0.706, W2: 0.6162, W3: -0.0783, W4: 0.0392, b: 0.3627\n",
      "loss: 0.9512\n",
      "Epoch: 10/100, Batch: 243/432, W1: 0.706, W2: 0.6161, W3: -0.0783, W4: 0.0392, b: 0.3627\n",
      "loss: 0.312\n",
      "Epoch: 10/100, Batch: 244/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0392, b: 0.3628\n",
      "loss: 0.2956\n",
      "Epoch: 10/100, Batch: 245/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0391, b: 0.3628\n",
      "loss: 0.3133\n",
      "Epoch: 10/100, Batch: 246/432, W1: 0.7059, W2: 0.616, W3: -0.0784, W4: 0.0391, b: 0.3628\n",
      "loss: 0.2655\n",
      "Epoch: 10/100, Batch: 247/432, W1: 0.7058, W2: 0.616, W3: -0.0784, W4: 0.0391, b: 0.3628\n",
      "loss: 0.8305\n",
      "Epoch: 10/100, Batch: 248/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0391, b: 0.3628\n",
      "loss: 0.2126\n",
      "Epoch: 10/100, Batch: 249/432, W1: 0.706, W2: 0.6162, W3: -0.0783, W4: 0.0391, b: 0.3629\n",
      "loss: 0.2286\n",
      "Epoch: 10/100, Batch: 250/432, W1: 0.706, W2: 0.6161, W3: -0.0783, W4: 0.039, b: 0.3629\n",
      "loss: 0.2403\n",
      "Epoch: 10/100, Batch: 251/432, W1: 0.706, W2: 0.6162, W3: -0.0783, W4: 0.039, b: 0.3629\n",
      "loss: 0.3318\n",
      "Epoch: 10/100, Batch: 252/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.039, b: 0.3629\n",
      "loss: 0.3034\n",
      "Epoch: 10/100, Batch: 253/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0389, b: 0.3629\n",
      "loss: 0.323\n",
      "Epoch: 10/100, Batch: 254/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0389, b: 0.3629\n",
      "loss: 0.2996\n",
      "Epoch: 10/100, Batch: 255/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0389, b: 0.363\n",
      "loss: 0.3124\n",
      "Epoch: 10/100, Batch: 256/432, W1: 0.706, W2: 0.6162, W3: -0.0783, W4: 0.0388, b: 0.363\n",
      "loss: 0.2185\n",
      "Epoch: 10/100, Batch: 257/432, W1: 0.7061, W2: 0.6163, W3: -0.0782, W4: 0.0388, b: 0.363\n",
      "loss: 0.2261\n",
      "Epoch: 10/100, Batch: 258/432, W1: 0.7062, W2: 0.6164, W3: -0.0782, W4: 0.0388, b: 0.363\n",
      "loss: 0.2715\n",
      "Epoch: 10/100, Batch: 259/432, W1: 0.7062, W2: 0.6164, W3: -0.0782, W4: 0.0388, b: 0.3631\n",
      "loss: 0.2803\n",
      "Epoch: 10/100, Batch: 260/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0388, b: 0.3631\n",
      "loss: 0.3429\n",
      "Epoch: 10/100, Batch: 261/432, W1: 0.7063, W2: 0.6165, W3: -0.0781, W4: 0.0388, b: 0.3631\n",
      "loss: 0.2457\n",
      "Epoch: 10/100, Batch: 262/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0387, b: 0.3632\n",
      "loss: 0.3721\n",
      "Epoch: 10/100, Batch: 263/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0387, b: 0.3632\n",
      "loss: 0.2376\n",
      "Epoch: 10/100, Batch: 264/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0387, b: 0.3632\n",
      "loss: 0.2872\n",
      "Epoch: 10/100, Batch: 265/432, W1: 0.7065, W2: 0.6167, W3: -0.078, W4: 0.0387, b: 0.3632\n",
      "loss: 0.2586\n",
      "Epoch: 10/100, Batch: 266/432, W1: 0.7066, W2: 0.6168, W3: -0.0779, W4: 0.0387, b: 0.3633\n",
      "loss: 0.3537\n",
      "Epoch: 10/100, Batch: 267/432, W1: 0.7065, W2: 0.6167, W3: -0.078, W4: 0.0386, b: 0.3633\n",
      "loss: 0.324\n",
      "Epoch: 10/100, Batch: 268/432, W1: 0.7065, W2: 0.6167, W3: -0.0779, W4: 0.0386, b: 0.3633\n",
      "loss: 0.2669\n",
      "Epoch: 10/100, Batch: 269/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0385, b: 0.3633\n",
      "loss: 0.3212\n",
      "Epoch: 10/100, Batch: 270/432, W1: 0.7065, W2: 0.6167, W3: -0.078, W4: 0.0385, b: 0.3633\n",
      "loss: 0.3149\n",
      "Epoch: 10/100, Batch: 271/432, W1: 0.7065, W2: 0.6167, W3: -0.078, W4: 0.0385, b: 0.3633\n",
      "loss: 0.274\n",
      "Epoch: 10/100, Batch: 272/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0384, b: 0.3633\n",
      "loss: 0.366\n",
      "Epoch: 10/100, Batch: 273/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0384, b: 0.3634\n",
      "loss: 0.2895\n",
      "Epoch: 10/100, Batch: 274/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0384, b: 0.3634\n",
      "loss: 0.3133\n",
      "Epoch: 10/100, Batch: 275/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0384, b: 0.3634\n",
      "loss: 0.3115\n",
      "Epoch: 10/100, Batch: 276/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0383, b: 0.3634\n",
      "loss: 0.2866\n",
      "Epoch: 10/100, Batch: 277/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0383, b: 0.3634\n",
      "loss: 0.3654\n",
      "Epoch: 10/100, Batch: 278/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0382, b: 0.3634\n",
      "loss: 0.4949\n",
      "Epoch: 10/100, Batch: 279/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0382, b: 0.3634\n",
      "loss: 0.3802\n",
      "Epoch: 10/100, Batch: 280/432, W1: 0.7063, W2: 0.6165, W3: -0.0781, W4: 0.0381, b: 0.3635\n",
      "loss: 0.2914\n",
      "Epoch: 10/100, Batch: 281/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0381, b: 0.3635\n",
      "loss: 0.317\n",
      "Epoch: 10/100, Batch: 282/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0381, b: 0.3635\n",
      "loss: 0.3051\n",
      "Epoch: 10/100, Batch: 283/432, W1: 0.7061, W2: 0.6164, W3: -0.0782, W4: 0.038, b: 0.3635\n",
      "loss: 0.3716\n",
      "Epoch: 10/100, Batch: 284/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.038, b: 0.3635\n",
      "loss: 0.3292\n",
      "Epoch: 10/100, Batch: 285/432, W1: 0.7062, W2: 0.6165, W3: -0.0781, W4: 0.038, b: 0.3636\n",
      "loss: 0.3504\n",
      "Epoch: 10/100, Batch: 286/432, W1: 0.7063, W2: 0.6165, W3: -0.0781, W4: 0.0379, b: 0.3636\n",
      "loss: 0.3164\n",
      "Epoch: 10/100, Batch: 287/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0379, b: 0.3636\n",
      "loss: 0.2988\n",
      "Epoch: 10/100, Batch: 288/432, W1: 0.7061, W2: 0.6164, W3: -0.0781, W4: 0.0379, b: 0.3636\n",
      "loss: 1.1624\n",
      "Epoch: 10/100, Batch: 289/432, W1: 0.7061, W2: 0.6164, W3: -0.0782, W4: 0.0379, b: 0.3636\n",
      "loss: 0.286\n",
      "Epoch: 10/100, Batch: 290/432, W1: 0.7061, W2: 0.6164, W3: -0.0782, W4: 0.0378, b: 0.3637\n",
      "loss: 0.3204\n",
      "Epoch: 10/100, Batch: 291/432, W1: 0.7061, W2: 0.6163, W3: -0.0782, W4: 0.0378, b: 0.3637\n",
      "loss: 0.2973\n",
      "Epoch: 10/100, Batch: 292/432, W1: 0.7061, W2: 0.6164, W3: -0.0781, W4: 0.0378, b: 0.3637\n",
      "loss: 0.2757\n",
      "Epoch: 10/100, Batch: 293/432, W1: 0.7062, W2: 0.6165, W3: -0.0781, W4: 0.0378, b: 0.3637\n",
      "loss: 0.2863\n",
      "Epoch: 10/100, Batch: 294/432, W1: 0.7061, W2: 0.6163, W3: -0.0782, W4: 0.0377, b: 0.3637\n",
      "loss: 0.2974\n",
      "Epoch: 10/100, Batch: 295/432, W1: 0.7061, W2: 0.6163, W3: -0.0782, W4: 0.0377, b: 0.3637\n",
      "loss: 0.2961\n",
      "Epoch: 10/100, Batch: 296/432, W1: 0.7059, W2: 0.6161, W3: -0.0783, W4: 0.0376, b: 0.3637\n",
      "loss: 0.3626\n",
      "Epoch: 10/100, Batch: 297/432, W1: 0.7059, W2: 0.6162, W3: -0.0783, W4: 0.0376, b: 0.3638\n",
      "loss: 0.3143\n",
      "Epoch: 10/100, Batch: 298/432, W1: 0.7059, W2: 0.6162, W3: -0.0783, W4: 0.0376, b: 0.3638\n",
      "loss: 0.311\n",
      "Epoch: 10/100, Batch: 299/432, W1: 0.706, W2: 0.6162, W3: -0.0782, W4: 0.0376, b: 0.3638\n",
      "loss: 0.2714\n",
      "Epoch: 10/100, Batch: 300/432, W1: 0.706, W2: 0.6162, W3: -0.0782, W4: 0.0375, b: 0.3638\n",
      "loss: 0.2736\n",
      "Epoch: 10/100, Batch: 301/432, W1: 0.706, W2: 0.6163, W3: -0.0782, W4: 0.0375, b: 0.3638\n",
      "loss: 0.2963\n",
      "Epoch: 10/100, Batch: 302/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0375, b: 0.3639\n",
      "loss: 0.2298\n",
      "Epoch: 10/100, Batch: 303/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0375, b: 0.3639\n",
      "loss: 0.258\n",
      "Epoch: 10/100, Batch: 304/432, W1: 0.7061, W2: 0.6164, W3: -0.0781, W4: 0.0374, b: 0.3639\n",
      "loss: 0.2585\n",
      "Epoch: 10/100, Batch: 305/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0374, b: 0.3639\n",
      "loss: 0.2648\n",
      "Epoch: 10/100, Batch: 306/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0374, b: 0.364\n",
      "loss: 0.3737\n",
      "Epoch: 10/100, Batch: 307/432, W1: 0.7063, W2: 0.6166, W3: -0.078, W4: 0.0374, b: 0.364\n",
      "loss: 0.2643\n",
      "Epoch: 10/100, Batch: 308/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0374, b: 0.364\n",
      "loss: 0.2535\n",
      "Epoch: 10/100, Batch: 309/432, W1: 0.7064, W2: 0.6167, W3: -0.078, W4: 0.0373, b: 0.364\n",
      "loss: 0.2691\n",
      "Epoch: 10/100, Batch: 310/432, W1: 0.7065, W2: 0.6167, W3: -0.0779, W4: 0.0373, b: 0.3641\n",
      "loss: 0.2822\n",
      "Epoch: 10/100, Batch: 311/432, W1: 0.7064, W2: 0.6167, W3: -0.078, W4: 0.0373, b: 0.3641\n",
      "loss: 0.4202\n",
      "Epoch: 10/100, Batch: 312/432, W1: 0.7062, W2: 0.6165, W3: -0.0781, W4: 0.0372, b: 0.3641\n",
      "loss: 0.3541\n",
      "Epoch: 10/100, Batch: 313/432, W1: 0.7062, W2: 0.6165, W3: -0.0781, W4: 0.0372, b: 0.3641\n",
      "loss: 0.2743\n",
      "Epoch: 10/100, Batch: 314/432, W1: 0.7063, W2: 0.6165, W3: -0.078, W4: 0.0372, b: 0.3641\n",
      "loss: 0.2344\n",
      "Epoch: 10/100, Batch: 315/432, W1: 0.7063, W2: 0.6166, W3: -0.078, W4: 0.0371, b: 0.3641\n",
      "loss: 0.2649\n",
      "Epoch: 10/100, Batch: 316/432, W1: 0.7063, W2: 0.6165, W3: -0.078, W4: 0.0371, b: 0.3642\n",
      "loss: 0.263\n",
      "Epoch: 10/100, Batch: 317/432, W1: 0.7064, W2: 0.6166, W3: -0.078, W4: 0.0371, b: 0.3642\n",
      "loss: 0.2405\n",
      "Epoch: 10/100, Batch: 318/432, W1: 0.7064, W2: 0.6167, W3: -0.0779, W4: 0.0371, b: 0.3642\n",
      "loss: 0.251\n",
      "Epoch: 10/100, Batch: 319/432, W1: 0.7064, W2: 0.6167, W3: -0.0779, W4: 0.0371, b: 0.3642\n",
      "loss: 0.2627\n",
      "Epoch: 10/100, Batch: 320/432, W1: 0.7064, W2: 0.6167, W3: -0.0779, W4: 0.037, b: 0.3642\n",
      "loss: 0.3058\n",
      "Epoch: 10/100, Batch: 321/432, W1: 0.7064, W2: 0.6166, W3: -0.0779, W4: 0.037, b: 0.3643\n",
      "loss: 0.3316\n",
      "Epoch: 10/100, Batch: 322/432, W1: 0.7064, W2: 0.6166, W3: -0.0779, W4: 0.037, b: 0.3643\n",
      "loss: 0.3508\n",
      "Epoch: 10/100, Batch: 323/432, W1: 0.7064, W2: 0.6167, W3: -0.0779, W4: 0.0369, b: 0.3643\n",
      "loss: 0.3461\n",
      "Epoch: 10/100, Batch: 324/432, W1: 0.7065, W2: 0.6167, W3: -0.0779, W4: 0.0369, b: 0.3643\n",
      "loss: 0.2825\n",
      "Epoch: 10/100, Batch: 325/432, W1: 0.7063, W2: 0.6166, W3: -0.078, W4: 0.0368, b: 0.3643\n",
      "loss: 0.431\n",
      "Epoch: 10/100, Batch: 326/432, W1: 0.7062, W2: 0.6165, W3: -0.078, W4: 0.0368, b: 0.3643\n",
      "loss: 0.4437\n",
      "Epoch: 10/100, Batch: 327/432, W1: 0.7062, W2: 0.6165, W3: -0.078, W4: 0.0368, b: 0.3643\n",
      "loss: 0.213\n",
      "Epoch: 10/100, Batch: 328/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0367, b: 0.3644\n",
      "loss: 0.2803\n",
      "Epoch: 10/100, Batch: 329/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0367, b: 0.3644\n",
      "loss: 0.325\n",
      "Epoch: 10/100, Batch: 330/432, W1: 0.7062, W2: 0.6164, W3: -0.0781, W4: 0.0367, b: 0.3644\n",
      "loss: 0.2673\n",
      "Epoch: 10/100, Batch: 331/432, W1: 0.7061, W2: 0.6164, W3: -0.0781, W4: 0.0366, b: 0.3644\n",
      "loss: 0.264\n",
      "Epoch: 10/100, Batch: 332/432, W1: 0.706, W2: 0.6163, W3: -0.0781, W4: 0.0366, b: 0.3644\n",
      "loss: 0.3286\n",
      "Epoch: 10/100, Batch: 333/432, W1: 0.706, W2: 0.6163, W3: -0.0781, W4: 0.0366, b: 0.3644\n",
      "loss: 0.2814\n",
      "Epoch: 10/100, Batch: 334/432, W1: 0.706, W2: 0.6162, W3: -0.0782, W4: 0.0365, b: 0.3644\n",
      "loss: 0.3184\n",
      "Epoch: 10/100, Batch: 335/432, W1: 0.706, W2: 0.6163, W3: -0.0782, W4: 0.0365, b: 0.3644\n",
      "loss: 0.3132\n",
      "Epoch: 10/100, Batch: 336/432, W1: 0.706, W2: 0.6163, W3: -0.078, W4: 0.0364, b: 0.3645\n",
      "loss: 0.4359\n",
      "Epoch: 10/100, Batch: 337/432, W1: 0.706, W2: 0.6162, W3: -0.078, W4: 0.0364, b: 0.3645\n",
      "loss: 0.3039\n",
      "Epoch: 10/100, Batch: 338/432, W1: 0.7059, W2: 0.6162, W3: -0.078, W4: 0.0364, b: 0.3645\n",
      "loss: 0.3267\n",
      "Epoch: 10/100, Batch: 339/432, W1: 0.7058, W2: 0.6161, W3: -0.0781, W4: 0.0363, b: 0.3645\n",
      "loss: 0.3364\n",
      "Epoch: 10/100, Batch: 340/432, W1: 0.7057, W2: 0.616, W3: -0.0782, W4: 0.0363, b: 0.3645\n",
      "loss: 0.3679\n",
      "Epoch: 10/100, Batch: 341/432, W1: 0.7056, W2: 0.6159, W3: -0.0782, W4: 0.0362, b: 0.3645\n",
      "loss: 0.3347\n",
      "Epoch: 10/100, Batch: 342/432, W1: 0.7055, W2: 0.6158, W3: -0.0783, W4: 0.0362, b: 0.3645\n",
      "loss: 0.3564\n",
      "Epoch: 10/100, Batch: 343/432, W1: 0.7055, W2: 0.6158, W3: -0.0783, W4: 0.0361, b: 0.3645\n",
      "loss: 0.2802\n",
      "Epoch: 10/100, Batch: 344/432, W1: 0.7056, W2: 0.6159, W3: -0.0782, W4: 0.0361, b: 0.3646\n",
      "loss: 0.2534\n",
      "Epoch: 10/100, Batch: 345/432, W1: 0.7055, W2: 0.6158, W3: -0.0783, W4: 0.0361, b: 0.3646\n",
      "loss: 0.3089\n",
      "Epoch: 10/100, Batch: 346/432, W1: 0.7056, W2: 0.6158, W3: -0.0782, W4: 0.036, b: 0.3646\n",
      "loss: 0.2906\n",
      "Epoch: 10/100, Batch: 347/432, W1: 0.7056, W2: 0.6158, W3: -0.0782, W4: 0.036, b: 0.3646\n",
      "loss: 0.3038\n",
      "Epoch: 10/100, Batch: 348/432, W1: 0.7056, W2: 0.6159, W3: -0.0782, W4: 0.036, b: 0.3646\n",
      "loss: 0.3442\n",
      "Epoch: 10/100, Batch: 349/432, W1: 0.7056, W2: 0.6159, W3: -0.0782, W4: 0.036, b: 0.3646\n",
      "loss: 0.2587\n",
      "Epoch: 10/100, Batch: 350/432, W1: 0.7057, W2: 0.6159, W3: -0.0782, W4: 0.0359, b: 0.3647\n",
      "loss: 0.277\n",
      "Epoch: 10/100, Batch: 351/432, W1: 0.7057, W2: 0.6159, W3: -0.0782, W4: 0.0359, b: 0.3647\n",
      "loss: 0.3558\n",
      "Epoch: 10/100, Batch: 352/432, W1: 0.7055, W2: 0.6158, W3: -0.0783, W4: 0.0359, b: 0.3647\n",
      "loss: 0.3129\n",
      "Epoch: 10/100, Batch: 353/432, W1: 0.7055, W2: 0.6158, W3: -0.0783, W4: 0.0358, b: 0.3647\n",
      "loss: 0.322\n",
      "Epoch: 10/100, Batch: 354/432, W1: 0.7056, W2: 0.6159, W3: -0.0782, W4: 0.0358, b: 0.3647\n",
      "loss: 0.2071\n",
      "Epoch: 10/100, Batch: 355/432, W1: 0.7055, W2: 0.6158, W3: -0.0782, W4: 0.0358, b: 0.3647\n",
      "loss: 0.3221\n",
      "Epoch: 10/100, Batch: 356/432, W1: 0.7055, W2: 0.6158, W3: -0.0783, W4: 0.0357, b: 0.3648\n",
      "loss: 0.3427\n",
      "Epoch: 10/100, Batch: 357/432, W1: 0.7055, W2: 0.6158, W3: -0.0783, W4: 0.0357, b: 0.3648\n",
      "loss: 0.2871\n",
      "Epoch: 10/100, Batch: 358/432, W1: 0.7054, W2: 0.6157, W3: -0.0783, W4: 0.0357, b: 0.3648\n",
      "loss: 0.3328\n",
      "Epoch: 10/100, Batch: 359/432, W1: 0.7054, W2: 0.6157, W3: -0.0783, W4: 0.0356, b: 0.3648\n",
      "loss: 0.3458\n",
      "Epoch: 10/100, Batch: 360/432, W1: 0.7054, W2: 0.6157, W3: -0.0783, W4: 0.0356, b: 0.3648\n",
      "loss: 0.2665\n",
      "Epoch: 10/100, Batch: 361/432, W1: 0.7054, W2: 0.6157, W3: -0.0783, W4: 0.0356, b: 0.3648\n",
      "loss: 0.2753\n",
      "Epoch: 10/100, Batch: 362/432, W1: 0.7055, W2: 0.6158, W3: -0.0782, W4: 0.0356, b: 0.3649\n",
      "loss: 0.2719\n",
      "Epoch: 10/100, Batch: 363/432, W1: 0.7057, W2: 0.6159, W3: -0.0782, W4: 0.0356, b: 0.3649\n",
      "loss: 0.2227\n",
      "Epoch: 10/100, Batch: 364/432, W1: 0.7057, W2: 0.616, W3: -0.0781, W4: 0.0355, b: 0.3649\n",
      "loss: 0.3096\n",
      "Epoch: 10/100, Batch: 365/432, W1: 0.7057, W2: 0.616, W3: -0.0781, W4: 0.0355, b: 0.365\n",
      "loss: 0.294\n",
      "Epoch: 10/100, Batch: 366/432, W1: 0.7056, W2: 0.6159, W3: -0.0782, W4: 0.0355, b: 0.3649\n",
      "loss: 0.2744\n",
      "Epoch: 10/100, Batch: 367/432, W1: 0.7055, W2: 0.6158, W3: -0.0782, W4: 0.0354, b: 0.365\n",
      "loss: 0.2986\n",
      "Epoch: 10/100, Batch: 368/432, W1: 0.7054, W2: 0.6157, W3: -0.0783, W4: 0.0354, b: 0.365\n",
      "loss: 0.3436\n",
      "Epoch: 10/100, Batch: 369/432, W1: 0.7053, W2: 0.6156, W3: -0.0783, W4: 0.0353, b: 0.365\n",
      "loss: 0.3514\n",
      "Epoch: 10/100, Batch: 370/432, W1: 0.7054, W2: 0.6157, W3: -0.0783, W4: 0.0353, b: 0.365\n",
      "loss: 0.323\n",
      "Epoch: 10/100, Batch: 371/432, W1: 0.7055, W2: 0.6158, W3: -0.0783, W4: 0.0353, b: 0.365\n",
      "loss: 0.2274\n",
      "Epoch: 10/100, Batch: 372/432, W1: 0.7055, W2: 0.6159, W3: -0.0782, W4: 0.0353, b: 0.3651\n",
      "loss: 0.2598\n",
      "Epoch: 10/100, Batch: 373/432, W1: 0.7055, W2: 0.6158, W3: -0.0782, W4: 0.0352, b: 0.3651\n",
      "loss: 0.3419\n",
      "Epoch: 10/100, Batch: 374/432, W1: 0.7055, W2: 0.6158, W3: -0.0782, W4: 0.0352, b: 0.3651\n",
      "loss: 0.286\n",
      "Epoch: 10/100, Batch: 375/432, W1: 0.7056, W2: 0.6159, W3: -0.0782, W4: 0.0352, b: 0.3651\n",
      "loss: 0.2444\n",
      "Epoch: 10/100, Batch: 376/432, W1: 0.7057, W2: 0.616, W3: -0.0781, W4: 0.0352, b: 0.3652\n",
      "loss: 0.2945\n",
      "Epoch: 10/100, Batch: 377/432, W1: 0.7058, W2: 0.6162, W3: -0.078, W4: 0.0352, b: 0.3652\n",
      "loss: 0.289\n",
      "Epoch: 10/100, Batch: 378/432, W1: 0.7059, W2: 0.6163, W3: -0.0779, W4: 0.0352, b: 0.3652\n",
      "loss: 0.2444\n",
      "Epoch: 10/100, Batch: 379/432, W1: 0.7059, W2: 0.6162, W3: -0.078, W4: 0.0351, b: 0.3652\n",
      "loss: 0.4109\n",
      "Epoch: 10/100, Batch: 380/432, W1: 0.7058, W2: 0.6161, W3: -0.0781, W4: 0.035, b: 0.3652\n",
      "loss: 0.2996\n",
      "Epoch: 10/100, Batch: 381/432, W1: 0.7058, W2: 0.6161, W3: -0.078, W4: 0.035, b: 0.3653\n",
      "loss: 0.3088\n",
      "Epoch: 10/100, Batch: 382/432, W1: 0.7058, W2: 0.6161, W3: -0.078, W4: 0.035, b: 0.3653\n",
      "loss: 0.2307\n",
      "Epoch: 10/100, Batch: 383/432, W1: 0.7059, W2: 0.6162, W3: -0.078, W4: 0.035, b: 0.3653\n",
      "loss: 0.2052\n",
      "Epoch: 10/100, Batch: 384/432, W1: 0.7058, W2: 0.6162, W3: -0.078, W4: 0.0349, b: 0.3653\n",
      "loss: 0.3416\n",
      "Epoch: 10/100, Batch: 385/432, W1: 0.7059, W2: 0.6163, W3: -0.0779, W4: 0.0349, b: 0.3653\n",
      "loss: 0.2218\n",
      "Epoch: 10/100, Batch: 386/432, W1: 0.706, W2: 0.6163, W3: -0.0779, W4: 0.0349, b: 0.3654\n",
      "loss: 0.2321\n",
      "Epoch: 10/100, Batch: 387/432, W1: 0.7061, W2: 0.6164, W3: -0.0779, W4: 0.0349, b: 0.3654\n",
      "loss: 0.2564\n",
      "Epoch: 10/100, Batch: 388/432, W1: 0.7059, W2: 0.6162, W3: -0.078, W4: 0.0348, b: 0.3654\n",
      "loss: 0.3966\n",
      "Epoch: 10/100, Batch: 389/432, W1: 0.7059, W2: 0.6162, W3: -0.0779, W4: 0.0348, b: 0.3654\n",
      "loss: 0.2895\n",
      "Epoch: 10/100, Batch: 390/432, W1: 0.7059, W2: 0.6163, W3: -0.0779, W4: 0.0348, b: 0.3654\n",
      "loss: 0.3221\n",
      "Epoch: 10/100, Batch: 391/432, W1: 0.706, W2: 0.6163, W3: -0.0779, W4: 0.0348, b: 0.3655\n",
      "loss: 0.3237\n",
      "Epoch: 10/100, Batch: 392/432, W1: 0.7059, W2: 0.6163, W3: -0.0779, W4: 0.0347, b: 0.3655\n",
      "loss: 0.2844\n",
      "Epoch: 10/100, Batch: 393/432, W1: 0.706, W2: 0.6164, W3: -0.0779, W4: 0.0347, b: 0.3655\n",
      "loss: 0.326\n",
      "Epoch: 10/100, Batch: 394/432, W1: 0.706, W2: 0.6163, W3: -0.0779, W4: 0.0347, b: 0.3655\n",
      "loss: 0.2761\n",
      "Epoch: 10/100, Batch: 395/432, W1: 0.7059, W2: 0.6162, W3: -0.078, W4: 0.0346, b: 0.3655\n",
      "loss: 0.3339\n",
      "Epoch: 10/100, Batch: 396/432, W1: 0.7059, W2: 0.6162, W3: -0.078, W4: 0.0346, b: 0.3655\n",
      "loss: 0.2803\n",
      "Epoch: 10/100, Batch: 397/432, W1: 0.7059, W2: 0.6162, W3: -0.0779, W4: 0.0346, b: 0.3656\n",
      "loss: 0.2409\n",
      "Epoch: 10/100, Batch: 398/432, W1: 0.7059, W2: 0.6163, W3: -0.0779, W4: 0.0346, b: 0.3656\n",
      "loss: 0.2571\n",
      "Epoch: 10/100, Batch: 399/432, W1: 0.7059, W2: 0.6163, W3: -0.0779, W4: 0.0345, b: 0.3656\n",
      "loss: 0.2881\n",
      "Epoch: 10/100, Batch: 400/432, W1: 0.706, W2: 0.6163, W3: -0.0779, W4: 0.0345, b: 0.3656\n",
      "loss: 0.3181\n",
      "Epoch: 10/100, Batch: 401/432, W1: 0.706, W2: 0.6163, W3: -0.0779, W4: 0.0345, b: 0.3656\n",
      "loss: 0.3504\n",
      "Epoch: 10/100, Batch: 402/432, W1: 0.7058, W2: 0.6162, W3: -0.078, W4: 0.0344, b: 0.3656\n",
      "loss: 0.3487\n",
      "Epoch: 10/100, Batch: 403/432, W1: 0.7056, W2: 0.616, W3: -0.0781, W4: 0.0343, b: 0.3656\n",
      "loss: 0.3889\n",
      "Epoch: 10/100, Batch: 404/432, W1: 0.7056, W2: 0.616, W3: -0.0781, W4: 0.0343, b: 0.3656\n",
      "loss: 0.3066\n",
      "Epoch: 10/100, Batch: 405/432, W1: 0.7055, W2: 0.6159, W3: -0.0782, W4: 0.0342, b: 0.3656\n",
      "loss: 0.3911\n",
      "Epoch: 10/100, Batch: 406/432, W1: 0.7056, W2: 0.6159, W3: -0.0781, W4: 0.0342, b: 0.3657\n",
      "loss: 0.2242\n",
      "Epoch: 10/100, Batch: 407/432, W1: 0.7057, W2: 0.616, W3: -0.0781, W4: 0.0342, b: 0.3657\n",
      "loss: 0.2342\n",
      "Epoch: 10/100, Batch: 408/432, W1: 0.7057, W2: 0.6161, W3: -0.078, W4: 0.0342, b: 0.3657\n",
      "loss: 0.3164\n",
      "Epoch: 10/100, Batch: 409/432, W1: 0.7059, W2: 0.6162, W3: -0.078, W4: 0.0342, b: 0.3658\n",
      "loss: 0.2413\n",
      "Epoch: 10/100, Batch: 410/432, W1: 0.7059, W2: 0.6163, W3: -0.0779, W4: 0.0342, b: 0.3658\n",
      "loss: 0.2184\n",
      "Epoch: 10/100, Batch: 411/432, W1: 0.7059, W2: 0.6162, W3: -0.0779, W4: 0.0341, b: 0.3658\n",
      "loss: 0.4405\n",
      "Epoch: 10/100, Batch: 412/432, W1: 0.7059, W2: 0.6162, W3: -0.078, W4: 0.0341, b: 0.3658\n",
      "loss: 0.247\n",
      "Epoch: 10/100, Batch: 413/432, W1: 0.7058, W2: 0.6162, W3: -0.078, W4: 0.0341, b: 0.3658\n",
      "loss: 0.2735\n",
      "Epoch: 10/100, Batch: 414/432, W1: 0.7058, W2: 0.6162, W3: -0.078, W4: 0.034, b: 0.3658\n",
      "loss: 0.2872\n",
      "Epoch: 10/100, Batch: 415/432, W1: 0.7057, W2: 0.616, W3: -0.078, W4: 0.034, b: 0.3658\n",
      "loss: 0.4016\n",
      "Epoch: 10/100, Batch: 416/432, W1: 0.7057, W2: 0.6161, W3: -0.078, W4: 0.034, b: 0.3659\n",
      "loss: 0.2832\n",
      "Epoch: 10/100, Batch: 417/432, W1: 0.7058, W2: 0.6161, W3: -0.078, W4: 0.034, b: 0.3659\n",
      "loss: 0.2383\n",
      "Epoch: 10/100, Batch: 418/432, W1: 0.7058, W2: 0.6161, W3: -0.078, W4: 0.0339, b: 0.3659\n",
      "loss: 0.4185\n",
      "Epoch: 10/100, Batch: 419/432, W1: 0.7057, W2: 0.6161, W3: -0.078, W4: 0.0339, b: 0.3659\n",
      "loss: 0.2999\n",
      "Epoch: 10/100, Batch: 420/432, W1: 0.7056, W2: 0.616, W3: -0.0781, W4: 0.0338, b: 0.3659\n",
      "loss: 0.3224\n",
      "Epoch: 10/100, Batch: 421/432, W1: 0.7058, W2: 0.6161, W3: -0.078, W4: 0.0338, b: 0.366\n",
      "loss: 0.3012\n",
      "Epoch: 10/100, Batch: 422/432, W1: 0.7058, W2: 0.6161, W3: -0.078, W4: 0.0338, b: 0.366\n",
      "loss: 0.3507\n",
      "Epoch: 10/100, Batch: 423/432, W1: 0.7059, W2: 0.6162, W3: -0.0779, W4: 0.0338, b: 0.366\n",
      "loss: 0.2862\n",
      "Epoch: 10/100, Batch: 424/432, W1: 0.7058, W2: 0.6162, W3: -0.0779, W4: 0.0337, b: 0.366\n",
      "loss: 0.3278\n",
      "Epoch: 10/100, Batch: 425/432, W1: 0.7058, W2: 0.6162, W3: -0.0779, W4: 0.0337, b: 0.3661\n",
      "loss: 0.3407\n",
      "Epoch: 10/100, Batch: 426/432, W1: 0.7058, W2: 0.6162, W3: -0.0779, W4: 0.0337, b: 0.3661\n",
      "loss: 0.2832\n",
      "Epoch: 10/100, Batch: 427/432, W1: 0.7058, W2: 0.6162, W3: -0.0779, W4: 0.0336, b: 0.3661\n",
      "loss: 0.4321\n",
      "Epoch: 10/100, Batch: 428/432, W1: 0.7059, W2: 0.6163, W3: -0.0779, W4: 0.0336, b: 0.3661\n",
      "loss: 0.3548\n",
      "Epoch: 10/100, Batch: 429/432, W1: 0.706, W2: 0.6164, W3: -0.0778, W4: 0.0336, b: 0.3662\n",
      "loss: 0.2726\n",
      "Epoch: 10/100, Batch: 430/432, W1: 0.706, W2: 0.6164, W3: -0.0778, W4: 0.0336, b: 0.3662\n",
      "loss: 0.277\n",
      "Epoch: 10/100, Batch: 431/432, W1: 0.706, W2: 0.6163, W3: -0.0778, W4: 0.0335, b: 0.3662\n",
      "loss: 0.3935\n",
      "Epoch: 10/100, Batch: 432/432, W1: 0.7059, W2: 0.6163, W3: -0.0779, W4: 0.0335, b: 0.3662\n",
      "loss: 0.2776\n",
      "Epoch: 20/100, Batch: 1/432, W1: 0.7202, W2: 0.5982, W3: -0.0641, W4: -0.0757, b: 0.4366\n",
      "loss: 0.2284\n",
      "Epoch: 20/100, Batch: 2/432, W1: 0.7203, W2: 0.5983, W3: -0.064, W4: -0.0757, b: 0.4366\n",
      "loss: 0.2151\n",
      "Epoch: 20/100, Batch: 3/432, W1: 0.7203, W2: 0.5983, W3: -0.064, W4: -0.0758, b: 0.4366\n",
      "loss: 0.2847\n",
      "Epoch: 20/100, Batch: 4/432, W1: 0.7202, W2: 0.5983, W3: -0.0641, W4: -0.0758, b: 0.4367\n",
      "loss: 0.3171\n",
      "Epoch: 20/100, Batch: 5/432, W1: 0.7202, W2: 0.5982, W3: -0.0641, W4: -0.0758, b: 0.4367\n",
      "loss: 0.2424\n",
      "Epoch: 20/100, Batch: 6/432, W1: 0.7201, W2: 0.5982, W3: -0.0641, W4: -0.0759, b: 0.4367\n",
      "loss: 0.2595\n",
      "Epoch: 20/100, Batch: 7/432, W1: 0.7201, W2: 0.5982, W3: -0.0641, W4: -0.0759, b: 0.4367\n",
      "loss: 0.2582\n",
      "Epoch: 20/100, Batch: 8/432, W1: 0.7201, W2: 0.5982, W3: -0.0641, W4: -0.0759, b: 0.4367\n",
      "loss: 0.2839\n",
      "Epoch: 20/100, Batch: 9/432, W1: 0.7201, W2: 0.5982, W3: -0.0641, W4: -0.076, b: 0.4367\n",
      "loss: 0.2736\n",
      "Epoch: 20/100, Batch: 10/432, W1: 0.7202, W2: 0.5983, W3: -0.064, W4: -0.076, b: 0.4368\n",
      "loss: 0.2289\n",
      "Epoch: 20/100, Batch: 11/432, W1: 0.7203, W2: 0.5983, W3: -0.064, W4: -0.076, b: 0.4368\n",
      "loss: 0.2288\n",
      "Epoch: 20/100, Batch: 12/432, W1: 0.7204, W2: 0.5985, W3: -0.0639, W4: -0.076, b: 0.4368\n",
      "loss: 0.1613\n",
      "Epoch: 20/100, Batch: 13/432, W1: 0.7203, W2: 0.5984, W3: -0.064, W4: -0.076, b: 0.4368\n",
      "loss: 0.3322\n",
      "Epoch: 20/100, Batch: 14/432, W1: 0.7204, W2: 0.5985, W3: -0.0639, W4: -0.076, b: 0.4368\n",
      "loss: 0.2011\n",
      "Epoch: 20/100, Batch: 15/432, W1: 0.7204, W2: 0.5985, W3: -0.0639, W4: -0.0761, b: 0.4369\n",
      "loss: 0.271\n",
      "Epoch: 20/100, Batch: 16/432, W1: 0.7205, W2: 0.5986, W3: -0.0639, W4: -0.0761, b: 0.4369\n",
      "loss: 0.2342\n",
      "Epoch: 20/100, Batch: 17/432, W1: 0.7206, W2: 0.5987, W3: -0.0638, W4: -0.0761, b: 0.4369\n",
      "loss: 0.2388\n",
      "Epoch: 20/100, Batch: 18/432, W1: 0.7206, W2: 0.5987, W3: -0.0638, W4: -0.0761, b: 0.4369\n",
      "loss: 0.2219\n",
      "Epoch: 20/100, Batch: 19/432, W1: 0.7206, W2: 0.5987, W3: -0.0638, W4: -0.0761, b: 0.437\n",
      "loss: 0.2446\n",
      "Epoch: 20/100, Batch: 20/432, W1: 0.7206, W2: 0.5987, W3: -0.0638, W4: -0.0762, b: 0.437\n",
      "loss: 0.2941\n",
      "Epoch: 20/100, Batch: 21/432, W1: 0.7206, W2: 0.5986, W3: -0.0638, W4: -0.0762, b: 0.437\n",
      "loss: 0.2042\n",
      "Epoch: 20/100, Batch: 22/432, W1: 0.7206, W2: 0.5986, W3: -0.0638, W4: -0.0762, b: 0.437\n",
      "loss: 0.2332\n",
      "Epoch: 20/100, Batch: 23/432, W1: 0.7206, W2: 0.5987, W3: -0.0638, W4: -0.0762, b: 0.437\n",
      "loss: 0.2953\n",
      "Epoch: 20/100, Batch: 24/432, W1: 0.7207, W2: 0.5987, W3: -0.0638, W4: -0.0763, b: 0.437\n",
      "loss: 0.2818\n",
      "Epoch: 20/100, Batch: 25/432, W1: 0.7206, W2: 0.5987, W3: -0.0638, W4: -0.0763, b: 0.4371\n",
      "loss: 0.2702\n",
      "Epoch: 20/100, Batch: 26/432, W1: 0.7205, W2: 0.5986, W3: -0.0639, W4: -0.0763, b: 0.4371\n",
      "loss: 0.2752\n",
      "Epoch: 20/100, Batch: 27/432, W1: 0.7205, W2: 0.5986, W3: -0.0639, W4: -0.0764, b: 0.4371\n",
      "loss: 0.1747\n",
      "Epoch: 20/100, Batch: 28/432, W1: 0.7205, W2: 0.5986, W3: -0.0639, W4: -0.0764, b: 0.4371\n",
      "loss: 0.2564\n",
      "Epoch: 20/100, Batch: 29/432, W1: 0.7204, W2: 0.5985, W3: -0.0639, W4: -0.0764, b: 0.4371\n",
      "loss: 0.2686\n",
      "Epoch: 20/100, Batch: 30/432, W1: 0.7202, W2: 0.5983, W3: -0.064, W4: -0.0765, b: 0.4371\n",
      "loss: 0.3297\n",
      "Epoch: 20/100, Batch: 31/432, W1: 0.7203, W2: 0.5984, W3: -0.064, W4: -0.0765, b: 0.4371\n",
      "loss: 0.2347\n",
      "Epoch: 20/100, Batch: 32/432, W1: 0.7203, W2: 0.5984, W3: -0.064, W4: -0.0765, b: 0.4371\n",
      "loss: 0.2402\n",
      "Epoch: 20/100, Batch: 33/432, W1: 0.7203, W2: 0.5984, W3: -0.0639, W4: -0.0765, b: 0.4371\n",
      "loss: 0.186\n",
      "Epoch: 20/100, Batch: 34/432, W1: 0.7202, W2: 0.5983, W3: -0.064, W4: -0.0766, b: 0.4371\n",
      "loss: 0.277\n",
      "Epoch: 20/100, Batch: 35/432, W1: 0.7202, W2: 0.5983, W3: -0.064, W4: -0.0766, b: 0.4371\n",
      "loss: 0.2733\n",
      "Epoch: 20/100, Batch: 36/432, W1: 0.72, W2: 0.5973, W3: -0.0642, W4: -0.0767, b: 0.4371\n",
      "loss: 2.6596\n",
      "Epoch: 20/100, Batch: 37/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0767, b: 0.4371\n",
      "loss: 0.2186\n",
      "Epoch: 20/100, Batch: 38/432, W1: 0.72, W2: 0.5973, W3: -0.0642, W4: -0.0767, b: 0.4372\n",
      "loss: 0.2649\n",
      "Epoch: 20/100, Batch: 39/432, W1: 0.7199, W2: 0.5972, W3: -0.0642, W4: -0.0768, b: 0.4372\n",
      "loss: 0.2893\n",
      "Epoch: 20/100, Batch: 40/432, W1: 0.72, W2: 0.5972, W3: -0.0642, W4: -0.0768, b: 0.4372\n",
      "loss: 0.2355\n",
      "Epoch: 20/100, Batch: 41/432, W1: 0.72, W2: 0.5973, W3: -0.0642, W4: -0.0768, b: 0.4372\n",
      "loss: 0.2226\n",
      "Epoch: 20/100, Batch: 42/432, W1: 0.72, W2: 0.5973, W3: -0.0642, W4: -0.0768, b: 0.4372\n",
      "loss: 0.2453\n",
      "Epoch: 20/100, Batch: 43/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0768, b: 0.4372\n",
      "loss: 0.2466\n",
      "Epoch: 20/100, Batch: 44/432, W1: 0.72, W2: 0.5973, W3: -0.0642, W4: -0.0769, b: 0.4372\n",
      "loss: 0.288\n",
      "Epoch: 20/100, Batch: 45/432, W1: 0.72, W2: 0.5973, W3: -0.0642, W4: -0.0769, b: 0.4373\n",
      "loss: 0.2507\n",
      "Epoch: 20/100, Batch: 46/432, W1: 0.7198, W2: 0.5971, W3: -0.0643, W4: -0.077, b: 0.4373\n",
      "loss: 0.3506\n",
      "Epoch: 20/100, Batch: 47/432, W1: 0.7199, W2: 0.5972, W3: -0.0642, W4: -0.077, b: 0.4373\n",
      "loss: 0.2174\n",
      "Epoch: 20/100, Batch: 48/432, W1: 0.7199, W2: 0.5972, W3: -0.0643, W4: -0.077, b: 0.4373\n",
      "loss: 0.1938\n",
      "Epoch: 20/100, Batch: 49/432, W1: 0.7199, W2: 0.5972, W3: -0.0642, W4: -0.077, b: 0.4373\n",
      "loss: 0.2336\n",
      "Epoch: 20/100, Batch: 50/432, W1: 0.72, W2: 0.5973, W3: -0.0642, W4: -0.077, b: 0.4373\n",
      "loss: 0.2536\n",
      "Epoch: 20/100, Batch: 51/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.077, b: 0.4374\n",
      "loss: 0.2199\n",
      "Epoch: 20/100, Batch: 52/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0771, b: 0.4374\n",
      "loss: 0.2613\n",
      "Epoch: 20/100, Batch: 53/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0771, b: 0.4374\n",
      "loss: 0.254\n",
      "Epoch: 20/100, Batch: 54/432, W1: 0.7202, W2: 0.5975, W3: -0.0641, W4: -0.0771, b: 0.4374\n",
      "loss: 0.2453\n",
      "Epoch: 20/100, Batch: 55/432, W1: 0.7202, W2: 0.5975, W3: -0.0641, W4: -0.0772, b: 0.4374\n",
      "loss: 0.2515\n",
      "Epoch: 20/100, Batch: 56/432, W1: 0.7202, W2: 0.5975, W3: -0.0641, W4: -0.0772, b: 0.4375\n",
      "loss: 0.2654\n",
      "Epoch: 20/100, Batch: 57/432, W1: 0.7202, W2: 0.5975, W3: -0.0641, W4: -0.0772, b: 0.4375\n",
      "loss: 1.1116\n",
      "Epoch: 20/100, Batch: 58/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0772, b: 0.4375\n",
      "loss: 0.2568\n",
      "Epoch: 20/100, Batch: 59/432, W1: 0.7202, W2: 0.5975, W3: -0.064, W4: -0.0772, b: 0.4375\n",
      "loss: 0.2601\n",
      "Epoch: 20/100, Batch: 60/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0773, b: 0.4375\n",
      "loss: 0.2805\n",
      "Epoch: 20/100, Batch: 61/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0773, b: 0.4375\n",
      "loss: 0.261\n",
      "Epoch: 20/100, Batch: 62/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0773, b: 0.4376\n",
      "loss: 0.2238\n",
      "Epoch: 20/100, Batch: 63/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0774, b: 0.4376\n",
      "loss: 0.2545\n",
      "Epoch: 20/100, Batch: 64/432, W1: 0.72, W2: 0.5973, W3: -0.0642, W4: -0.0774, b: 0.4376\n",
      "loss: 0.3097\n",
      "Epoch: 20/100, Batch: 65/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0774, b: 0.4376\n",
      "loss: 0.2529\n",
      "Epoch: 20/100, Batch: 66/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0774, b: 0.4376\n",
      "loss: 0.1955\n",
      "Epoch: 20/100, Batch: 67/432, W1: 0.7202, W2: 0.5975, W3: -0.0641, W4: -0.0775, b: 0.4377\n",
      "loss: 0.2517\n",
      "Epoch: 20/100, Batch: 68/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0774, b: 0.4377\n",
      "loss: 0.1955\n",
      "Epoch: 20/100, Batch: 69/432, W1: 0.7202, W2: 0.5976, W3: -0.064, W4: -0.0775, b: 0.4377\n",
      "loss: 0.2709\n",
      "Epoch: 20/100, Batch: 70/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0775, b: 0.4377\n",
      "loss: 0.2013\n",
      "Epoch: 20/100, Batch: 71/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0775, b: 0.4377\n",
      "loss: 0.2485\n",
      "Epoch: 20/100, Batch: 72/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0776, b: 0.4377\n",
      "loss: 0.2477\n",
      "Epoch: 20/100, Batch: 73/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0776, b: 0.4378\n",
      "loss: 0.245\n",
      "Epoch: 20/100, Batch: 74/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0776, b: 0.4378\n",
      "loss: 0.2653\n",
      "Epoch: 20/100, Batch: 75/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0776, b: 0.4378\n",
      "loss: 0.2031\n",
      "Epoch: 20/100, Batch: 76/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0777, b: 0.4378\n",
      "loss: 0.2277\n",
      "Epoch: 20/100, Batch: 77/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0777, b: 0.4378\n",
      "loss: 0.2183\n",
      "Epoch: 20/100, Batch: 78/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0777, b: 0.4379\n",
      "loss: 0.2313\n",
      "Epoch: 20/100, Batch: 79/432, W1: 0.7205, W2: 0.5978, W3: -0.0639, W4: -0.0777, b: 0.4379\n",
      "loss: 0.1965\n",
      "Epoch: 20/100, Batch: 80/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0777, b: 0.4379\n",
      "loss: 0.2511\n",
      "Epoch: 20/100, Batch: 81/432, W1: 0.7203, W2: 0.5977, W3: -0.0639, W4: -0.0778, b: 0.4379\n",
      "loss: 0.3053\n",
      "Epoch: 20/100, Batch: 82/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0778, b: 0.4379\n",
      "loss: 0.269\n",
      "Epoch: 20/100, Batch: 83/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0778, b: 0.4379\n",
      "loss: 0.2153\n",
      "Epoch: 20/100, Batch: 84/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0779, b: 0.4379\n",
      "loss: 0.2005\n",
      "Epoch: 20/100, Batch: 85/432, W1: 0.7202, W2: 0.5975, W3: -0.064, W4: -0.0779, b: 0.4379\n",
      "loss: 0.2609\n",
      "Epoch: 20/100, Batch: 86/432, W1: 0.7202, W2: 0.5975, W3: -0.064, W4: -0.0779, b: 0.438\n",
      "loss: 0.3315\n",
      "Epoch: 20/100, Batch: 87/432, W1: 0.7202, W2: 0.5975, W3: -0.064, W4: -0.078, b: 0.438\n",
      "loss: 0.2175\n",
      "Epoch: 20/100, Batch: 88/432, W1: 0.7202, W2: 0.5975, W3: -0.064, W4: -0.078, b: 0.438\n",
      "loss: 0.2317\n",
      "Epoch: 20/100, Batch: 89/432, W1: 0.7202, W2: 0.5975, W3: -0.064, W4: -0.078, b: 0.438\n",
      "loss: 0.2982\n",
      "Epoch: 20/100, Batch: 90/432, W1: 0.7202, W2: 0.5975, W3: -0.064, W4: -0.078, b: 0.438\n",
      "loss: 0.3596\n",
      "Epoch: 20/100, Batch: 91/432, W1: 0.72, W2: 0.5974, W3: -0.0641, W4: -0.0781, b: 0.438\n",
      "loss: 0.3219\n",
      "Epoch: 20/100, Batch: 92/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0781, b: 0.438\n",
      "loss: 0.2647\n",
      "Epoch: 20/100, Batch: 93/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0782, b: 0.4381\n",
      "loss: 0.2121\n",
      "Epoch: 20/100, Batch: 94/432, W1: 0.72, W2: 0.5974, W3: -0.0641, W4: -0.0782, b: 0.4381\n",
      "loss: 0.323\n",
      "Epoch: 20/100, Batch: 95/432, W1: 0.7201, W2: 0.5974, W3: -0.0641, W4: -0.0782, b: 0.4381\n",
      "loss: 0.2231\n",
      "Epoch: 20/100, Batch: 96/432, W1: 0.7202, W2: 0.5975, W3: -0.064, W4: -0.0782, b: 0.4381\n",
      "loss: 0.2361\n",
      "Epoch: 20/100, Batch: 97/432, W1: 0.7202, W2: 0.5976, W3: -0.064, W4: -0.0782, b: 0.4381\n",
      "loss: 0.2617\n",
      "Epoch: 20/100, Batch: 98/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0783, b: 0.4382\n",
      "loss: 0.2608\n",
      "Epoch: 20/100, Batch: 99/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0783, b: 0.4382\n",
      "loss: 0.2467\n",
      "Epoch: 20/100, Batch: 100/432, W1: 0.7203, W2: 0.5976, W3: -0.064, W4: -0.0783, b: 0.4382\n",
      "loss: 0.2629\n",
      "Epoch: 20/100, Batch: 101/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0783, b: 0.4382\n",
      "loss: 0.2476\n",
      "Epoch: 20/100, Batch: 102/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0784, b: 0.4382\n",
      "loss: 0.2868\n",
      "Epoch: 20/100, Batch: 103/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0784, b: 0.4383\n",
      "loss: 0.3128\n",
      "Epoch: 20/100, Batch: 104/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0784, b: 0.4383\n",
      "loss: 0.2383\n",
      "Epoch: 20/100, Batch: 105/432, W1: 0.7204, W2: 0.5978, W3: -0.0639, W4: -0.0784, b: 0.4383\n",
      "loss: 0.2951\n",
      "Epoch: 20/100, Batch: 106/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0785, b: 0.4383\n",
      "loss: 0.2583\n",
      "Epoch: 20/100, Batch: 107/432, W1: 0.7204, W2: 0.5977, W3: -0.0639, W4: -0.0785, b: 0.4383\n",
      "loss: 0.252\n",
      "Epoch: 20/100, Batch: 108/432, W1: 0.7205, W2: 0.5978, W3: -0.0638, W4: -0.0785, b: 0.4384\n",
      "loss: 0.2756\n",
      "Epoch: 20/100, Batch: 109/432, W1: 0.7205, W2: 0.5979, W3: -0.0638, W4: -0.0785, b: 0.4384\n",
      "loss: 0.244\n",
      "Epoch: 20/100, Batch: 110/432, W1: 0.7206, W2: 0.5979, W3: -0.0638, W4: -0.0785, b: 0.4384\n",
      "loss: 0.2527\n",
      "Epoch: 20/100, Batch: 111/432, W1: 0.7206, W2: 0.5979, W3: -0.0638, W4: -0.0786, b: 0.4384\n",
      "loss: 0.2473\n",
      "Epoch: 20/100, Batch: 112/432, W1: 0.7207, W2: 0.598, W3: -0.0637, W4: -0.0786, b: 0.4385\n",
      "loss: 0.2148\n",
      "Epoch: 20/100, Batch: 113/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0786, b: 0.4385\n",
      "loss: 0.287\n",
      "Epoch: 20/100, Batch: 114/432, W1: 0.7208, W2: 0.5981, W3: -0.0636, W4: -0.0786, b: 0.4385\n",
      "loss: 0.228\n",
      "Epoch: 20/100, Batch: 115/432, W1: 0.7207, W2: 0.598, W3: -0.0637, W4: -0.0787, b: 0.4385\n",
      "loss: 0.3491\n",
      "Epoch: 20/100, Batch: 116/432, W1: 0.7207, W2: 0.598, W3: -0.0637, W4: -0.0787, b: 0.4385\n",
      "loss: 0.3031\n",
      "Epoch: 20/100, Batch: 117/432, W1: 0.7207, W2: 0.5981, W3: -0.0637, W4: -0.0787, b: 0.4386\n",
      "loss: 0.2384\n",
      "Epoch: 20/100, Batch: 118/432, W1: 0.7208, W2: 0.5981, W3: -0.0636, W4: -0.0787, b: 0.4386\n",
      "loss: 0.2371\n",
      "Epoch: 20/100, Batch: 119/432, W1: 0.7208, W2: 0.5981, W3: -0.0636, W4: -0.0788, b: 0.4386\n",
      "loss: 0.2406\n",
      "Epoch: 20/100, Batch: 120/432, W1: 0.7207, W2: 0.5981, W3: -0.0637, W4: -0.0788, b: 0.4386\n",
      "loss: 0.3712\n",
      "Epoch: 20/100, Batch: 121/432, W1: 0.7207, W2: 0.5981, W3: -0.0636, W4: -0.0788, b: 0.4386\n",
      "loss: 0.2836\n",
      "Epoch: 20/100, Batch: 122/432, W1: 0.7208, W2: 0.5981, W3: -0.0636, W4: -0.0789, b: 0.4387\n",
      "loss: 0.2677\n",
      "Epoch: 20/100, Batch: 123/432, W1: 0.7209, W2: 0.5982, W3: -0.0636, W4: -0.0789, b: 0.4387\n",
      "loss: 0.2176\n",
      "Epoch: 20/100, Batch: 124/432, W1: 0.7208, W2: 0.5982, W3: -0.0636, W4: -0.0789, b: 0.4387\n",
      "loss: 0.2087\n",
      "Epoch: 20/100, Batch: 125/432, W1: 0.7208, W2: 0.5982, W3: -0.0636, W4: -0.0789, b: 0.4387\n",
      "loss: 0.2345\n",
      "Epoch: 20/100, Batch: 126/432, W1: 0.7209, W2: 0.5983, W3: -0.0635, W4: -0.0789, b: 0.4387\n",
      "loss: 0.2039\n",
      "Epoch: 20/100, Batch: 127/432, W1: 0.7209, W2: 0.5982, W3: -0.0635, W4: -0.079, b: 0.4387\n",
      "loss: 0.2013\n",
      "Epoch: 20/100, Batch: 128/432, W1: 0.7208, W2: 0.5982, W3: -0.0636, W4: -0.079, b: 0.4387\n",
      "loss: 0.2746\n",
      "Epoch: 20/100, Batch: 129/432, W1: 0.7208, W2: 0.5982, W3: -0.0636, W4: -0.079, b: 0.4388\n",
      "loss: 0.1858\n",
      "Epoch: 20/100, Batch: 130/432, W1: 0.7207, W2: 0.598, W3: -0.0637, W4: -0.0791, b: 0.4388\n",
      "loss: 0.2577\n",
      "Epoch: 20/100, Batch: 131/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0791, b: 0.4388\n",
      "loss: 0.257\n",
      "Epoch: 20/100, Batch: 132/432, W1: 0.7207, W2: 0.598, W3: -0.0637, W4: -0.0791, b: 0.4388\n",
      "loss: 0.2487\n",
      "Epoch: 20/100, Batch: 133/432, W1: 0.7207, W2: 0.5981, W3: -0.0636, W4: -0.0791, b: 0.4388\n",
      "loss: 0.2108\n",
      "Epoch: 20/100, Batch: 134/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0792, b: 0.4388\n",
      "loss: 0.2964\n",
      "Epoch: 20/100, Batch: 135/432, W1: 0.7205, W2: 0.5979, W3: -0.0638, W4: -0.0792, b: 0.4388\n",
      "loss: 0.2652\n",
      "Epoch: 20/100, Batch: 136/432, W1: 0.7205, W2: 0.5979, W3: -0.0638, W4: -0.0793, b: 0.4388\n",
      "loss: 0.2517\n",
      "Epoch: 20/100, Batch: 137/432, W1: 0.7206, W2: 0.5979, W3: -0.0637, W4: -0.0793, b: 0.4389\n",
      "loss: 0.1955\n",
      "Epoch: 20/100, Batch: 138/432, W1: 0.7207, W2: 0.5981, W3: -0.0636, W4: -0.0793, b: 0.4389\n",
      "loss: 0.24\n",
      "Epoch: 20/100, Batch: 139/432, W1: 0.7207, W2: 0.5981, W3: -0.0636, W4: -0.0793, b: 0.4389\n",
      "loss: 0.2887\n",
      "Epoch: 20/100, Batch: 140/432, W1: 0.7207, W2: 0.598, W3: -0.0637, W4: -0.0794, b: 0.4389\n",
      "loss: 0.2813\n",
      "Epoch: 20/100, Batch: 141/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0794, b: 0.4389\n",
      "loss: 0.2223\n",
      "Epoch: 20/100, Batch: 142/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0794, b: 0.4389\n",
      "loss: 0.2361\n",
      "Epoch: 20/100, Batch: 143/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0795, b: 0.4389\n",
      "loss: 0.2973\n",
      "Epoch: 20/100, Batch: 144/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0795, b: 0.439\n",
      "loss: 0.2348\n",
      "Epoch: 20/100, Batch: 145/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0795, b: 0.439\n",
      "loss: 0.3207\n",
      "Epoch: 20/100, Batch: 146/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0795, b: 0.439\n",
      "loss: 0.2685\n",
      "Epoch: 20/100, Batch: 147/432, W1: 0.7207, W2: 0.5981, W3: -0.0637, W4: -0.0796, b: 0.439\n",
      "loss: 0.2604\n",
      "Epoch: 20/100, Batch: 148/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0796, b: 0.439\n",
      "loss: 0.2957\n",
      "Epoch: 20/100, Batch: 149/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0796, b: 0.4391\n",
      "loss: 0.2637\n",
      "Epoch: 20/100, Batch: 150/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0797, b: 0.4391\n",
      "loss: 0.2422\n",
      "Epoch: 20/100, Batch: 151/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0797, b: 0.4391\n",
      "loss: 0.2253\n",
      "Epoch: 20/100, Batch: 152/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0797, b: 0.4391\n",
      "loss: 0.2445\n",
      "Epoch: 20/100, Batch: 153/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0797, b: 0.4391\n",
      "loss: 0.2617\n",
      "Epoch: 20/100, Batch: 154/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0798, b: 0.4391\n",
      "loss: 0.2168\n",
      "Epoch: 20/100, Batch: 155/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0798, b: 0.4391\n",
      "loss: 0.3559\n",
      "Epoch: 20/100, Batch: 156/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0798, b: 0.4392\n",
      "loss: 0.2045\n",
      "Epoch: 20/100, Batch: 157/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0798, b: 0.4392\n",
      "loss: 0.2571\n",
      "Epoch: 20/100, Batch: 158/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0799, b: 0.4392\n",
      "loss: 0.2871\n",
      "Epoch: 20/100, Batch: 159/432, W1: 0.7204, W2: 0.5978, W3: -0.0638, W4: -0.0799, b: 0.4392\n",
      "loss: 0.3313\n",
      "Epoch: 20/100, Batch: 160/432, W1: 0.7204, W2: 0.5978, W3: -0.0638, W4: -0.08, b: 0.4392\n",
      "loss: 0.2996\n",
      "Epoch: 20/100, Batch: 161/432, W1: 0.7203, W2: 0.5977, W3: -0.0638, W4: -0.08, b: 0.4392\n",
      "loss: 0.2668\n",
      "Epoch: 20/100, Batch: 162/432, W1: 0.7204, W2: 0.5978, W3: -0.0638, W4: -0.08, b: 0.4392\n",
      "loss: 0.2017\n",
      "Epoch: 20/100, Batch: 163/432, W1: 0.7204, W2: 0.5978, W3: -0.0638, W4: -0.0801, b: 0.4393\n",
      "loss: 0.3488\n",
      "Epoch: 20/100, Batch: 164/432, W1: 0.7204, W2: 0.5978, W3: -0.0638, W4: -0.0801, b: 0.4393\n",
      "loss: 0.2219\n",
      "Epoch: 20/100, Batch: 165/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0801, b: 0.4393\n",
      "loss: 0.2416\n",
      "Epoch: 20/100, Batch: 166/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0801, b: 0.4393\n",
      "loss: 0.2694\n",
      "Epoch: 20/100, Batch: 167/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0801, b: 0.4394\n",
      "loss: 0.2425\n",
      "Epoch: 20/100, Batch: 168/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0802, b: 0.4394\n",
      "loss: 0.2611\n",
      "Epoch: 20/100, Batch: 169/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0802, b: 0.4394\n",
      "loss: 0.2703\n",
      "Epoch: 20/100, Batch: 170/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0802, b: 0.4394\n",
      "loss: 0.2376\n",
      "Epoch: 20/100, Batch: 171/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0802, b: 0.4394\n",
      "loss: 0.2558\n",
      "Epoch: 20/100, Batch: 172/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0803, b: 0.4394\n",
      "loss: 0.2115\n",
      "Epoch: 20/100, Batch: 173/432, W1: 0.7207, W2: 0.5981, W3: -0.0636, W4: -0.0803, b: 0.4395\n",
      "loss: 0.8982\n",
      "Epoch: 20/100, Batch: 174/432, W1: 0.7207, W2: 0.5981, W3: -0.0636, W4: -0.0803, b: 0.4395\n",
      "loss: 0.2502\n",
      "Epoch: 20/100, Batch: 175/432, W1: 0.7207, W2: 0.5981, W3: -0.0636, W4: -0.0803, b: 0.4395\n",
      "loss: 0.2196\n",
      "Epoch: 20/100, Batch: 176/432, W1: 0.7208, W2: 0.5982, W3: -0.0636, W4: -0.0803, b: 0.4395\n",
      "loss: 0.2291\n",
      "Epoch: 20/100, Batch: 177/432, W1: 0.7207, W2: 0.5981, W3: -0.0636, W4: -0.0803, b: 0.4396\n",
      "loss: 0.2867\n",
      "Epoch: 20/100, Batch: 178/432, W1: 0.7207, W2: 0.5981, W3: -0.0636, W4: -0.0804, b: 0.4396\n",
      "loss: 0.224\n",
      "Epoch: 20/100, Batch: 179/432, W1: 0.7204, W2: 0.5979, W3: -0.0637, W4: -0.0805, b: 0.4395\n",
      "loss: 0.3442\n",
      "Epoch: 20/100, Batch: 180/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0805, b: 0.4396\n",
      "loss: 0.2077\n",
      "Epoch: 20/100, Batch: 181/432, W1: 0.7206, W2: 0.598, W3: -0.0636, W4: -0.0805, b: 0.4396\n",
      "loss: 0.1927\n",
      "Epoch: 20/100, Batch: 182/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0805, b: 0.4396\n",
      "loss: 0.3697\n",
      "Epoch: 20/100, Batch: 183/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0806, b: 0.4396\n",
      "loss: 0.3116\n",
      "Epoch: 20/100, Batch: 184/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0806, b: 0.4396\n",
      "loss: 0.223\n",
      "Epoch: 20/100, Batch: 185/432, W1: 0.7205, W2: 0.5979, W3: -0.0637, W4: -0.0806, b: 0.4397\n",
      "loss: 0.2766\n",
      "Epoch: 20/100, Batch: 186/432, W1: 0.7205, W2: 0.598, W3: -0.0637, W4: -0.0806, b: 0.4397\n",
      "loss: 1.2162\n",
      "Epoch: 20/100, Batch: 187/432, W1: 0.7206, W2: 0.598, W3: -0.0637, W4: -0.0806, b: 0.4397\n",
      "loss: 0.1619\n",
      "Epoch: 20/100, Batch: 188/432, W1: 0.7204, W2: 0.5978, W3: -0.0638, W4: -0.0807, b: 0.4397\n",
      "loss: 0.4056\n",
      "Epoch: 20/100, Batch: 189/432, W1: 0.7204, W2: 0.5978, W3: -0.0637, W4: -0.0807, b: 0.4397\n",
      "loss: 0.2437\n",
      "Epoch: 20/100, Batch: 190/432, W1: 0.7203, W2: 0.5977, W3: -0.0638, W4: -0.0808, b: 0.4397\n",
      "loss: 0.3276\n",
      "Epoch: 20/100, Batch: 191/432, W1: 0.7204, W2: 0.5978, W3: -0.0638, W4: -0.0808, b: 0.4397\n",
      "loss: 0.2304\n",
      "Epoch: 20/100, Batch: 192/432, W1: 0.7204, W2: 0.5978, W3: -0.0638, W4: -0.0808, b: 0.4398\n",
      "loss: 0.2767\n",
      "Epoch: 20/100, Batch: 193/432, W1: 0.7203, W2: 0.5977, W3: -0.0638, W4: -0.0809, b: 0.4398\n",
      "loss: 0.2875\n",
      "Epoch: 20/100, Batch: 194/432, W1: 0.7202, W2: 0.5977, W3: -0.0639, W4: -0.0809, b: 0.4398\n",
      "loss: 0.2963\n",
      "Epoch: 20/100, Batch: 195/432, W1: 0.7201, W2: 0.5975, W3: -0.064, W4: -0.081, b: 0.4398\n",
      "loss: 0.3371\n",
      "Epoch: 20/100, Batch: 196/432, W1: 0.7202, W2: 0.5976, W3: -0.0639, W4: -0.081, b: 0.4398\n",
      "loss: 0.2306\n",
      "Epoch: 20/100, Batch: 197/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.081, b: 0.4398\n",
      "loss: 0.2342\n",
      "Epoch: 20/100, Batch: 198/432, W1: 0.7201, W2: 0.5976, W3: -0.0639, W4: -0.081, b: 0.4398\n",
      "loss: 0.3265\n",
      "Epoch: 20/100, Batch: 199/432, W1: 0.7201, W2: 0.5976, W3: -0.0639, W4: -0.081, b: 0.4399\n",
      "loss: 0.2331\n",
      "Epoch: 20/100, Batch: 200/432, W1: 0.7201, W2: 0.5975, W3: -0.064, W4: -0.0811, b: 0.4399\n",
      "loss: 0.3275\n",
      "Epoch: 20/100, Batch: 201/432, W1: 0.72, W2: 0.5974, W3: -0.064, W4: -0.0811, b: 0.4399\n",
      "loss: 0.2175\n",
      "Epoch: 20/100, Batch: 202/432, W1: 0.72, W2: 0.5974, W3: -0.064, W4: -0.0812, b: 0.4399\n",
      "loss: 0.2341\n",
      "Epoch: 20/100, Batch: 203/432, W1: 0.72, W2: 0.5974, W3: -0.064, W4: -0.0812, b: 0.4399\n",
      "loss: 0.2645\n",
      "Epoch: 20/100, Batch: 204/432, W1: 0.72, W2: 0.5975, W3: -0.064, W4: -0.0812, b: 0.4399\n",
      "loss: 0.1875\n",
      "Epoch: 20/100, Batch: 205/432, W1: 0.72, W2: 0.5974, W3: -0.064, W4: -0.0812, b: 0.4399\n",
      "loss: 0.2975\n",
      "Epoch: 20/100, Batch: 206/432, W1: 0.7199, W2: 0.5974, W3: -0.064, W4: -0.0813, b: 0.4399\n",
      "loss: 0.3246\n",
      "Epoch: 20/100, Batch: 207/432, W1: 0.72, W2: 0.5974, W3: -0.064, W4: -0.0813, b: 0.44\n",
      "loss: 0.2431\n",
      "Epoch: 20/100, Batch: 208/432, W1: 0.7201, W2: 0.5975, W3: -0.0639, W4: -0.0813, b: 0.44\n",
      "loss: 0.184\n",
      "Epoch: 20/100, Batch: 209/432, W1: 0.7201, W2: 0.5975, W3: -0.0639, W4: -0.0813, b: 0.44\n",
      "loss: 0.2625\n",
      "Epoch: 20/100, Batch: 210/432, W1: 0.7201, W2: 0.5975, W3: -0.0639, W4: -0.0814, b: 0.44\n",
      "loss: 0.2224\n",
      "Epoch: 20/100, Batch: 211/432, W1: 0.7201, W2: 0.5975, W3: -0.0639, W4: -0.0814, b: 0.44\n",
      "loss: 0.2443\n",
      "Epoch: 20/100, Batch: 212/432, W1: 0.7202, W2: 0.5976, W3: -0.0639, W4: -0.0814, b: 0.4401\n",
      "loss: 0.2629\n",
      "Epoch: 20/100, Batch: 213/432, W1: 0.7201, W2: 0.5975, W3: -0.0639, W4: -0.0814, b: 0.4401\n",
      "loss: 0.2354\n",
      "Epoch: 20/100, Batch: 214/432, W1: 0.7202, W2: 0.5976, W3: -0.0639, W4: -0.0815, b: 0.4401\n",
      "loss: 0.2893\n",
      "Epoch: 20/100, Batch: 215/432, W1: 0.7201, W2: 0.5975, W3: -0.0639, W4: -0.0815, b: 0.4401\n",
      "loss: 0.3956\n",
      "Epoch: 20/100, Batch: 216/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.0815, b: 0.4401\n",
      "loss: 0.2497\n",
      "Epoch: 20/100, Batch: 217/432, W1: 0.7202, W2: 0.5976, W3: -0.0639, W4: -0.0816, b: 0.4402\n",
      "loss: 0.2971\n",
      "Epoch: 20/100, Batch: 218/432, W1: 0.7202, W2: 0.5976, W3: -0.0639, W4: -0.0816, b: 0.4402\n",
      "loss: 0.2262\n",
      "Epoch: 20/100, Batch: 219/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.0816, b: 0.4402\n",
      "loss: 0.2276\n",
      "Epoch: 20/100, Batch: 220/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.0816, b: 0.4402\n",
      "loss: 0.2413\n",
      "Epoch: 20/100, Batch: 221/432, W1: 0.7202, W2: 0.5976, W3: -0.0639, W4: -0.0817, b: 0.4402\n",
      "loss: 0.4183\n",
      "Epoch: 20/100, Batch: 222/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.0817, b: 0.4402\n",
      "loss: 0.2047\n",
      "Epoch: 20/100, Batch: 223/432, W1: 0.7201, W2: 0.5975, W3: -0.0639, W4: -0.0818, b: 0.4402\n",
      "loss: 0.3131\n",
      "Epoch: 20/100, Batch: 224/432, W1: 0.72, W2: 0.5975, W3: -0.064, W4: -0.0818, b: 0.4402\n",
      "loss: 0.2572\n",
      "Epoch: 20/100, Batch: 225/432, W1: 0.7201, W2: 0.5975, W3: -0.0639, W4: -0.0818, b: 0.4403\n",
      "loss: 0.2886\n",
      "Epoch: 20/100, Batch: 226/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.0818, b: 0.4403\n",
      "loss: 0.222\n",
      "Epoch: 20/100, Batch: 227/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.0818, b: 0.4404\n",
      "loss: 1.0725\n",
      "Epoch: 20/100, Batch: 228/432, W1: 0.7203, W2: 0.5978, W3: -0.0638, W4: -0.0818, b: 0.4404\n",
      "loss: 0.1941\n",
      "Epoch: 20/100, Batch: 229/432, W1: 0.7204, W2: 0.5978, W3: -0.0637, W4: -0.0818, b: 0.4404\n",
      "loss: 0.267\n",
      "Epoch: 20/100, Batch: 230/432, W1: 0.7203, W2: 0.5978, W3: -0.0638, W4: -0.0819, b: 0.4404\n",
      "loss: 0.2583\n",
      "Epoch: 20/100, Batch: 231/432, W1: 0.7203, W2: 0.5978, W3: -0.0638, W4: -0.0819, b: 0.4404\n",
      "loss: 0.2845\n",
      "Epoch: 20/100, Batch: 232/432, W1: 0.7203, W2: 0.5978, W3: -0.0638, W4: -0.0819, b: 0.4404\n",
      "loss: 0.2167\n",
      "Epoch: 20/100, Batch: 233/432, W1: 0.7204, W2: 0.5978, W3: -0.0637, W4: -0.0819, b: 0.4405\n",
      "loss: 0.25\n",
      "Epoch: 20/100, Batch: 234/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.082, b: 0.4405\n",
      "loss: 0.3355\n",
      "Epoch: 20/100, Batch: 235/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.082, b: 0.4405\n",
      "loss: 0.2486\n",
      "Epoch: 20/100, Batch: 236/432, W1: 0.7202, W2: 0.5977, W3: -0.0638, W4: -0.0821, b: 0.4405\n",
      "loss: 0.2603\n",
      "Epoch: 20/100, Batch: 237/432, W1: 0.7202, W2: 0.5976, W3: -0.0639, W4: -0.0821, b: 0.4405\n",
      "loss: 0.246\n",
      "Epoch: 20/100, Batch: 238/432, W1: 0.7202, W2: 0.5976, W3: -0.0639, W4: -0.0821, b: 0.4405\n",
      "loss: 0.2347\n",
      "Epoch: 20/100, Batch: 239/432, W1: 0.7203, W2: 0.5977, W3: -0.0638, W4: -0.0821, b: 0.4406\n",
      "loss: 0.2698\n",
      "Epoch: 20/100, Batch: 240/432, W1: 0.7201, W2: 0.5976, W3: -0.0639, W4: -0.0822, b: 0.4405\n",
      "loss: 0.3172\n",
      "Epoch: 20/100, Batch: 241/432, W1: 0.7199, W2: 0.5974, W3: -0.064, W4: -0.0823, b: 0.4405\n",
      "loss: 0.2722\n",
      "Epoch: 20/100, Batch: 242/432, W1: 0.7199, W2: 0.5974, W3: -0.064, W4: -0.0823, b: 0.4406\n",
      "loss: 0.2743\n",
      "Epoch: 20/100, Batch: 243/432, W1: 0.7199, W2: 0.5974, W3: -0.064, W4: -0.0823, b: 0.4406\n",
      "loss: 0.3572\n",
      "Epoch: 20/100, Batch: 244/432, W1: 0.7197, W2: 0.5972, W3: -0.0641, W4: -0.0824, b: 0.4405\n",
      "loss: 0.3397\n",
      "Epoch: 20/100, Batch: 245/432, W1: 0.7197, W2: 0.5971, W3: -0.0641, W4: -0.0824, b: 0.4406\n",
      "loss: 0.3394\n",
      "Epoch: 20/100, Batch: 246/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0825, b: 0.4406\n",
      "loss: 0.2342\n",
      "Epoch: 20/100, Batch: 247/432, W1: 0.7196, W2: 0.5971, W3: -0.0642, W4: -0.0825, b: 0.4406\n",
      "loss: 0.2291\n",
      "Epoch: 20/100, Batch: 248/432, W1: 0.7195, W2: 0.597, W3: -0.0642, W4: -0.0825, b: 0.4406\n",
      "loss: 0.3106\n",
      "Epoch: 20/100, Batch: 249/432, W1: 0.7195, W2: 0.597, W3: -0.0642, W4: -0.0826, b: 0.4406\n",
      "loss: 0.2127\n",
      "Epoch: 20/100, Batch: 250/432, W1: 0.7196, W2: 0.5971, W3: -0.0642, W4: -0.0826, b: 0.4406\n",
      "loss: 0.2259\n",
      "Epoch: 20/100, Batch: 251/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0826, b: 0.4407\n",
      "loss: 0.1834\n",
      "Epoch: 20/100, Batch: 252/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0826, b: 0.4407\n",
      "loss: 0.2636\n",
      "Epoch: 20/100, Batch: 253/432, W1: 0.7197, W2: 0.5972, W3: -0.0641, W4: -0.0826, b: 0.4407\n",
      "loss: 0.1916\n",
      "Epoch: 20/100, Batch: 254/432, W1: 0.7197, W2: 0.5971, W3: -0.0641, W4: -0.0827, b: 0.4407\n",
      "loss: 0.3396\n",
      "Epoch: 20/100, Batch: 255/432, W1: 0.7197, W2: 0.5972, W3: -0.0641, W4: -0.0827, b: 0.4407\n",
      "loss: 0.2228\n",
      "Epoch: 20/100, Batch: 256/432, W1: 0.7197, W2: 0.5972, W3: -0.0641, W4: -0.0827, b: 0.4407\n",
      "loss: 0.2553\n",
      "Epoch: 20/100, Batch: 257/432, W1: 0.7196, W2: 0.5971, W3: -0.0642, W4: -0.0828, b: 0.4407\n",
      "loss: 0.3011\n",
      "Epoch: 20/100, Batch: 258/432, W1: 0.7196, W2: 0.5971, W3: -0.0642, W4: -0.0828, b: 0.4408\n",
      "loss: 0.2331\n",
      "Epoch: 20/100, Batch: 259/432, W1: 0.7196, W2: 0.5971, W3: -0.0642, W4: -0.0828, b: 0.4408\n",
      "loss: 0.2147\n",
      "Epoch: 20/100, Batch: 260/432, W1: 0.7196, W2: 0.5971, W3: -0.0642, W4: -0.0828, b: 0.4408\n",
      "loss: 0.2934\n",
      "Epoch: 20/100, Batch: 261/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0829, b: 0.4408\n",
      "loss: 0.3014\n",
      "Epoch: 20/100, Batch: 262/432, W1: 0.7195, W2: 0.597, W3: -0.0642, W4: -0.0829, b: 0.4408\n",
      "loss: 0.2436\n",
      "Epoch: 20/100, Batch: 263/432, W1: 0.7197, W2: 0.5972, W3: -0.0641, W4: -0.0829, b: 0.4409\n",
      "loss: 0.189\n",
      "Epoch: 20/100, Batch: 264/432, W1: 0.7197, W2: 0.5972, W3: -0.064, W4: -0.0829, b: 0.4409\n",
      "loss: 0.2568\n",
      "Epoch: 20/100, Batch: 265/432, W1: 0.7197, W2: 0.5972, W3: -0.0641, W4: -0.083, b: 0.4409\n",
      "loss: 0.3038\n",
      "Epoch: 20/100, Batch: 266/432, W1: 0.7197, W2: 0.5972, W3: -0.0641, W4: -0.083, b: 0.4409\n",
      "loss: 0.1896\n",
      "Epoch: 20/100, Batch: 267/432, W1: 0.7198, W2: 0.5973, W3: -0.064, W4: -0.083, b: 0.4409\n",
      "loss: 0.2165\n",
      "Epoch: 20/100, Batch: 268/432, W1: 0.7199, W2: 0.5974, W3: -0.064, W4: -0.083, b: 0.441\n",
      "loss: 0.2582\n",
      "Epoch: 20/100, Batch: 269/432, W1: 0.7199, W2: 0.5974, W3: -0.0639, W4: -0.083, b: 0.441\n",
      "loss: 0.2474\n",
      "Epoch: 20/100, Batch: 270/432, W1: 0.7199, W2: 0.5974, W3: -0.0639, W4: -0.083, b: 0.441\n",
      "loss: 0.2266\n",
      "Epoch: 20/100, Batch: 271/432, W1: 0.7198, W2: 0.5973, W3: -0.064, W4: -0.0831, b: 0.441\n",
      "loss: 0.2966\n",
      "Epoch: 20/100, Batch: 272/432, W1: 0.7197, W2: 0.5972, W3: -0.064, W4: -0.0831, b: 0.441\n",
      "loss: 0.2782\n",
      "Epoch: 20/100, Batch: 273/432, W1: 0.7197, W2: 0.5972, W3: -0.0641, W4: -0.0832, b: 0.441\n",
      "loss: 0.2328\n",
      "Epoch: 20/100, Batch: 274/432, W1: 0.7198, W2: 0.5973, W3: -0.064, W4: -0.0832, b: 0.441\n",
      "loss: 0.1988\n",
      "Epoch: 20/100, Batch: 275/432, W1: 0.7197, W2: 0.5972, W3: -0.064, W4: -0.0832, b: 0.4411\n",
      "loss: 0.3604\n",
      "Epoch: 20/100, Batch: 276/432, W1: 0.7198, W2: 0.5973, W3: -0.064, W4: -0.0833, b: 0.4411\n",
      "loss: 0.2685\n",
      "Epoch: 20/100, Batch: 277/432, W1: 0.7197, W2: 0.5972, W3: -0.064, W4: -0.0833, b: 0.4411\n",
      "loss: 0.2296\n",
      "Epoch: 20/100, Batch: 278/432, W1: 0.7196, W2: 0.5972, W3: -0.0641, W4: -0.0833, b: 0.4411\n",
      "loss: 0.2711\n",
      "Epoch: 20/100, Batch: 279/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0834, b: 0.4411\n",
      "loss: 0.2157\n",
      "Epoch: 20/100, Batch: 280/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0834, b: 0.4411\n",
      "loss: 0.3226\n",
      "Epoch: 20/100, Batch: 281/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0834, b: 0.4411\n",
      "loss: 0.2447\n",
      "Epoch: 20/100, Batch: 282/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0835, b: 0.4411\n",
      "loss: 0.3142\n",
      "Epoch: 20/100, Batch: 283/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0835, b: 0.4412\n",
      "loss: 0.3008\n",
      "Epoch: 20/100, Batch: 284/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0835, b: 0.4412\n",
      "loss: 0.2289\n",
      "Epoch: 20/100, Batch: 285/432, W1: 0.7196, W2: 0.5972, W3: -0.0641, W4: -0.0835, b: 0.4412\n",
      "loss: 0.2522\n",
      "Epoch: 20/100, Batch: 286/432, W1: 0.7197, W2: 0.5972, W3: -0.064, W4: -0.0835, b: 0.4412\n",
      "loss: 0.2357\n",
      "Epoch: 20/100, Batch: 287/432, W1: 0.7198, W2: 0.5973, W3: -0.064, W4: -0.0836, b: 0.4413\n",
      "loss: 0.2271\n",
      "Epoch: 20/100, Batch: 288/432, W1: 0.7198, W2: 0.5973, W3: -0.064, W4: -0.0836, b: 0.4413\n",
      "loss: 0.2026\n",
      "Epoch: 20/100, Batch: 289/432, W1: 0.7197, W2: 0.5973, W3: -0.064, W4: -0.0836, b: 0.4413\n",
      "loss: 0.2224\n",
      "Epoch: 20/100, Batch: 290/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0837, b: 0.4413\n",
      "loss: 0.3038\n",
      "Epoch: 20/100, Batch: 291/432, W1: 0.7195, W2: 0.597, W3: -0.0641, W4: -0.0837, b: 0.4413\n",
      "loss: 0.2693\n",
      "Epoch: 20/100, Batch: 292/432, W1: 0.7195, W2: 0.597, W3: -0.0641, W4: -0.0837, b: 0.4413\n",
      "loss: 0.2659\n",
      "Epoch: 20/100, Batch: 293/432, W1: 0.7196, W2: 0.5971, W3: -0.0641, W4: -0.0837, b: 0.4413\n",
      "loss: 0.199\n",
      "Epoch: 20/100, Batch: 294/432, W1: 0.7197, W2: 0.5972, W3: -0.064, W4: -0.0838, b: 0.4414\n",
      "loss: 0.2537\n",
      "Epoch: 20/100, Batch: 295/432, W1: 0.7197, W2: 0.5972, W3: -0.064, W4: -0.0838, b: 0.4414\n",
      "loss: 0.8384\n",
      "Epoch: 20/100, Batch: 296/432, W1: 0.7197, W2: 0.5972, W3: -0.064, W4: -0.0838, b: 0.4414\n",
      "loss: 0.2541\n",
      "Epoch: 20/100, Batch: 297/432, W1: 0.7196, W2: 0.5972, W3: -0.0641, W4: -0.0838, b: 0.4414\n",
      "loss: 0.2894\n",
      "Epoch: 20/100, Batch: 298/432, W1: 0.7196, W2: 0.5972, W3: -0.0641, W4: -0.0839, b: 0.4414\n",
      "loss: 0.2173\n",
      "Epoch: 20/100, Batch: 299/432, W1: 0.7198, W2: 0.5973, W3: -0.064, W4: -0.0839, b: 0.4415\n",
      "loss: 0.2188\n",
      "Epoch: 20/100, Batch: 300/432, W1: 0.7198, W2: 0.5973, W3: -0.064, W4: -0.0839, b: 0.4415\n",
      "loss: 0.2476\n",
      "Epoch: 20/100, Batch: 301/432, W1: 0.7199, W2: 0.5974, W3: -0.0637, W4: -0.0839, b: 0.4415\n",
      "loss: 0.3228\n",
      "Epoch: 20/100, Batch: 302/432, W1: 0.7198, W2: 0.5974, W3: -0.0638, W4: -0.084, b: 0.4415\n",
      "loss: 0.3771\n",
      "Epoch: 20/100, Batch: 303/432, W1: 0.7199, W2: 0.5974, W3: -0.0638, W4: -0.084, b: 0.4416\n",
      "loss: 0.2133\n",
      "Epoch: 20/100, Batch: 304/432, W1: 0.7198, W2: 0.5973, W3: -0.0638, W4: -0.084, b: 0.4416\n",
      "loss: 0.2874\n",
      "Epoch: 20/100, Batch: 305/432, W1: 0.7198, W2: 0.5974, W3: -0.0638, W4: -0.084, b: 0.4416\n",
      "loss: 0.2338\n",
      "Epoch: 20/100, Batch: 306/432, W1: 0.7197, W2: 0.5973, W3: -0.0639, W4: -0.0841, b: 0.4416\n",
      "loss: 0.2308\n",
      "Epoch: 20/100, Batch: 307/432, W1: 0.7198, W2: 0.5973, W3: -0.0638, W4: -0.0841, b: 0.4416\n",
      "loss: 0.2713\n",
      "Epoch: 20/100, Batch: 308/432, W1: 0.7198, W2: 0.5973, W3: -0.0638, W4: -0.0841, b: 0.4416\n",
      "loss: 0.2388\n",
      "Epoch: 20/100, Batch: 309/432, W1: 0.7198, W2: 0.5974, W3: -0.0638, W4: -0.0841, b: 0.4416\n",
      "loss: 0.2432\n",
      "Epoch: 20/100, Batch: 310/432, W1: 0.7197, W2: 0.5973, W3: -0.0638, W4: -0.0842, b: 0.4416\n",
      "loss: 0.2727\n",
      "Epoch: 20/100, Batch: 311/432, W1: 0.7198, W2: 0.5973, W3: -0.0638, W4: -0.0842, b: 0.4417\n",
      "loss: 0.2434\n",
      "Epoch: 20/100, Batch: 312/432, W1: 0.7198, W2: 0.5974, W3: -0.0638, W4: -0.0842, b: 0.4417\n",
      "loss: 0.2986\n",
      "Epoch: 20/100, Batch: 313/432, W1: 0.7199, W2: 0.5974, W3: -0.0637, W4: -0.0842, b: 0.4417\n",
      "loss: 0.2352\n",
      "Epoch: 20/100, Batch: 314/432, W1: 0.7199, W2: 0.5975, W3: -0.0637, W4: -0.0843, b: 0.4417\n",
      "loss: 0.224\n",
      "Epoch: 20/100, Batch: 315/432, W1: 0.7199, W2: 0.5975, W3: -0.0637, W4: -0.0843, b: 0.4418\n",
      "loss: 0.207\n",
      "Epoch: 20/100, Batch: 316/432, W1: 0.72, W2: 0.5975, W3: -0.0637, W4: -0.0843, b: 0.4418\n",
      "loss: 0.236\n",
      "Epoch: 20/100, Batch: 317/432, W1: 0.72, W2: 0.5975, W3: -0.0637, W4: -0.0843, b: 0.4418\n",
      "loss: 0.3176\n",
      "Epoch: 20/100, Batch: 318/432, W1: 0.72, W2: 0.5975, W3: -0.0637, W4: -0.0844, b: 0.4418\n",
      "loss: 0.2955\n",
      "Epoch: 20/100, Batch: 319/432, W1: 0.72, W2: 0.5976, W3: -0.0636, W4: -0.0844, b: 0.4418\n",
      "loss: 0.2411\n",
      "Epoch: 20/100, Batch: 320/432, W1: 0.72, W2: 0.5976, W3: -0.0636, W4: -0.0844, b: 0.4419\n",
      "loss: 0.2117\n",
      "Epoch: 20/100, Batch: 321/432, W1: 0.7201, W2: 0.5976, W3: -0.0636, W4: -0.0844, b: 0.4419\n",
      "loss: 0.2463\n",
      "Epoch: 20/100, Batch: 322/432, W1: 0.7202, W2: 0.5977, W3: -0.0635, W4: -0.0844, b: 0.4419\n",
      "loss: 0.2597\n",
      "Epoch: 20/100, Batch: 323/432, W1: 0.7202, W2: 0.5977, W3: -0.0635, W4: -0.0845, b: 0.4419\n",
      "loss: 0.2964\n",
      "Epoch: 20/100, Batch: 324/432, W1: 0.7201, W2: 0.5977, W3: -0.0635, W4: -0.0845, b: 0.4419\n",
      "loss: 0.2411\n",
      "Epoch: 20/100, Batch: 325/432, W1: 0.7202, W2: 0.5977, W3: -0.0635, W4: -0.0845, b: 0.442\n",
      "loss: 0.2431\n",
      "Epoch: 20/100, Batch: 326/432, W1: 0.7202, W2: 0.5978, W3: -0.0635, W4: -0.0845, b: 0.442\n",
      "loss: 0.1779\n",
      "Epoch: 20/100, Batch: 327/432, W1: 0.7203, W2: 0.5978, W3: -0.0635, W4: -0.0846, b: 0.442\n",
      "loss: 0.292\n",
      "Epoch: 20/100, Batch: 328/432, W1: 0.7201, W2: 0.5976, W3: -0.0636, W4: -0.0846, b: 0.442\n",
      "loss: 0.3308\n",
      "Epoch: 20/100, Batch: 329/432, W1: 0.72, W2: 0.5976, W3: -0.0636, W4: -0.0847, b: 0.442\n",
      "loss: 0.2237\n",
      "Epoch: 20/100, Batch: 330/432, W1: 0.7199, W2: 0.5975, W3: -0.0637, W4: -0.0847, b: 0.442\n",
      "loss: 0.2615\n",
      "Epoch: 20/100, Batch: 331/432, W1: 0.7199, W2: 0.5975, W3: -0.0637, W4: -0.0847, b: 0.442\n",
      "loss: 0.2348\n",
      "Epoch: 20/100, Batch: 332/432, W1: 0.7199, W2: 0.5975, W3: -0.0637, W4: -0.0847, b: 0.442\n",
      "loss: 0.2043\n",
      "Epoch: 20/100, Batch: 333/432, W1: 0.7199, W2: 0.5975, W3: -0.0637, W4: -0.0848, b: 0.4421\n",
      "loss: 0.289\n",
      "Epoch: 20/100, Batch: 334/432, W1: 0.7199, W2: 0.5975, W3: -0.0637, W4: -0.0848, b: 0.4421\n",
      "loss: 0.216\n",
      "Epoch: 20/100, Batch: 335/432, W1: 0.7199, W2: 0.5975, W3: -0.0637, W4: -0.0848, b: 0.4421\n",
      "loss: 0.219\n",
      "Epoch: 20/100, Batch: 336/432, W1: 0.7199, W2: 0.5975, W3: -0.0637, W4: -0.0848, b: 0.4421\n",
      "loss: 0.2513\n",
      "Epoch: 20/100, Batch: 337/432, W1: 0.72, W2: 0.5975, W3: -0.0636, W4: -0.0849, b: 0.4421\n",
      "loss: 0.2478\n",
      "Epoch: 20/100, Batch: 338/432, W1: 0.72, W2: 0.5976, W3: -0.0636, W4: -0.0849, b: 0.4421\n",
      "loss: 0.2274\n",
      "Epoch: 20/100, Batch: 339/432, W1: 0.7201, W2: 0.5977, W3: -0.0636, W4: -0.0849, b: 0.4422\n",
      "loss: 0.226\n",
      "Epoch: 20/100, Batch: 340/432, W1: 0.7202, W2: 0.5977, W3: -0.0635, W4: -0.0849, b: 0.4422\n",
      "loss: 0.1791\n",
      "Epoch: 20/100, Batch: 341/432, W1: 0.7201, W2: 0.5976, W3: -0.0636, W4: -0.085, b: 0.4422\n",
      "loss: 0.3124\n",
      "Epoch: 20/100, Batch: 342/432, W1: 0.7201, W2: 0.5976, W3: -0.0636, W4: -0.085, b: 0.4422\n",
      "loss: 0.1982\n",
      "Epoch: 20/100, Batch: 343/432, W1: 0.7201, W2: 0.5976, W3: -0.0636, W4: -0.085, b: 0.4422\n",
      "loss: 0.2234\n",
      "Epoch: 20/100, Batch: 344/432, W1: 0.72, W2: 0.5976, W3: -0.0636, W4: -0.085, b: 0.4422\n",
      "loss: 0.2901\n",
      "Epoch: 20/100, Batch: 345/432, W1: 0.72, W2: 0.5976, W3: -0.0636, W4: -0.0851, b: 0.4423\n",
      "loss: 0.2435\n",
      "Epoch: 20/100, Batch: 346/432, W1: 0.72, W2: 0.5976, W3: -0.0636, W4: -0.0851, b: 0.4423\n",
      "loss: 0.2358\n",
      "Epoch: 20/100, Batch: 347/432, W1: 0.7202, W2: 0.5978, W3: -0.0635, W4: -0.0851, b: 0.4423\n",
      "loss: 0.2305\n",
      "Epoch: 20/100, Batch: 348/432, W1: 0.7202, W2: 0.5978, W3: -0.0635, W4: -0.0851, b: 0.4423\n",
      "loss: 0.2513\n",
      "Epoch: 20/100, Batch: 349/432, W1: 0.7202, W2: 0.5978, W3: -0.0635, W4: -0.0851, b: 0.4423\n",
      "loss: 0.2509\n",
      "Epoch: 20/100, Batch: 350/432, W1: 0.7203, W2: 0.5979, W3: -0.0634, W4: -0.0851, b: 0.4424\n",
      "loss: 0.2272\n",
      "Epoch: 20/100, Batch: 351/432, W1: 0.7203, W2: 0.5979, W3: -0.0634, W4: -0.0852, b: 0.4424\n",
      "loss: 0.2514\n",
      "Epoch: 20/100, Batch: 352/432, W1: 0.7204, W2: 0.598, W3: -0.0634, W4: -0.0852, b: 0.4424\n",
      "loss: 0.282\n",
      "Epoch: 20/100, Batch: 353/432, W1: 0.7205, W2: 0.5981, W3: -0.0633, W4: -0.0852, b: 0.4425\n",
      "loss: 0.2156\n",
      "Epoch: 20/100, Batch: 354/432, W1: 0.7205, W2: 0.5981, W3: -0.0633, W4: -0.0852, b: 0.4425\n",
      "loss: 0.2376\n",
      "Epoch: 20/100, Batch: 355/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.0853, b: 0.4425\n",
      "loss: 0.324\n",
      "Epoch: 20/100, Batch: 356/432, W1: 0.7204, W2: 0.5979, W3: -0.0634, W4: -0.0853, b: 0.4425\n",
      "loss: 0.2798\n",
      "Epoch: 20/100, Batch: 357/432, W1: 0.7205, W2: 0.598, W3: -0.0633, W4: -0.0853, b: 0.4425\n",
      "loss: 0.2492\n",
      "Epoch: 20/100, Batch: 358/432, W1: 0.7205, W2: 0.5981, W3: -0.0633, W4: -0.0853, b: 0.4425\n",
      "loss: 0.2834\n",
      "Epoch: 20/100, Batch: 359/432, W1: 0.7203, W2: 0.5979, W3: -0.0634, W4: -0.0854, b: 0.4425\n",
      "loss: 0.286\n",
      "Epoch: 20/100, Batch: 360/432, W1: 0.7204, W2: 0.598, W3: -0.0634, W4: -0.0854, b: 0.4426\n",
      "loss: 0.2594\n",
      "Epoch: 20/100, Batch: 361/432, W1: 0.7203, W2: 0.5979, W3: -0.0634, W4: -0.0854, b: 0.4426\n",
      "loss: 0.2479\n",
      "Epoch: 20/100, Batch: 362/432, W1: 0.7204, W2: 0.598, W3: -0.0634, W4: -0.0855, b: 0.4426\n",
      "loss: 0.3225\n",
      "Epoch: 20/100, Batch: 363/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.0855, b: 0.4426\n",
      "loss: 0.2393\n",
      "Epoch: 20/100, Batch: 364/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.0855, b: 0.4426\n",
      "loss: 0.2578\n",
      "Epoch: 20/100, Batch: 365/432, W1: 0.7205, W2: 0.5981, W3: -0.0633, W4: -0.0855, b: 0.4427\n",
      "loss: 0.2027\n",
      "Epoch: 20/100, Batch: 366/432, W1: 0.7205, W2: 0.598, W3: -0.0633, W4: -0.0856, b: 0.4427\n",
      "loss: 0.2247\n",
      "Epoch: 20/100, Batch: 367/432, W1: 0.7205, W2: 0.5981, W3: -0.0633, W4: -0.0856, b: 0.4427\n",
      "loss: 0.1995\n",
      "Epoch: 20/100, Batch: 368/432, W1: 0.7205, W2: 0.5981, W3: -0.0633, W4: -0.0856, b: 0.4427\n",
      "loss: 0.2545\n",
      "Epoch: 20/100, Batch: 369/432, W1: 0.7205, W2: 0.5981, W3: -0.0633, W4: -0.0856, b: 0.4427\n",
      "loss: 0.2337\n",
      "Epoch: 20/100, Batch: 370/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.0857, b: 0.4427\n",
      "loss: 0.3433\n",
      "Epoch: 20/100, Batch: 371/432, W1: 0.7204, W2: 0.5979, W3: -0.0634, W4: -0.0857, b: 0.4427\n",
      "loss: 0.2236\n",
      "Epoch: 20/100, Batch: 372/432, W1: 0.7204, W2: 0.5979, W3: -0.0634, W4: -0.0857, b: 0.4428\n",
      "loss: 0.3214\n",
      "Epoch: 20/100, Batch: 373/432, W1: 0.7203, W2: 0.5979, W3: -0.0634, W4: -0.0858, b: 0.4428\n",
      "loss: 0.2866\n",
      "Epoch: 20/100, Batch: 374/432, W1: 0.7203, W2: 0.5979, W3: -0.0634, W4: -0.0858, b: 0.4428\n",
      "loss: 0.2206\n",
      "Epoch: 20/100, Batch: 375/432, W1: 0.7202, W2: 0.5978, W3: -0.0634, W4: -0.0858, b: 0.4428\n",
      "loss: 0.2661\n",
      "Epoch: 20/100, Batch: 376/432, W1: 0.7202, W2: 0.5978, W3: -0.0634, W4: -0.0859, b: 0.4428\n",
      "loss: 0.208\n",
      "Epoch: 20/100, Batch: 377/432, W1: 0.7202, W2: 0.5978, W3: -0.0634, W4: -0.0859, b: 0.4428\n",
      "loss: 0.3471\n",
      "Epoch: 20/100, Batch: 378/432, W1: 0.7203, W2: 0.5979, W3: -0.0634, W4: -0.0859, b: 0.4428\n",
      "loss: 0.2667\n",
      "Epoch: 20/100, Batch: 379/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.0859, b: 0.4429\n",
      "loss: 0.255\n",
      "Epoch: 20/100, Batch: 380/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.086, b: 0.4429\n",
      "loss: 0.2476\n",
      "Epoch: 20/100, Batch: 381/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.086, b: 0.4429\n",
      "loss: 0.2224\n",
      "Epoch: 20/100, Batch: 382/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.086, b: 0.4429\n",
      "loss: 0.2318\n",
      "Epoch: 20/100, Batch: 383/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.086, b: 0.4429\n",
      "loss: 0.2561\n",
      "Epoch: 20/100, Batch: 384/432, W1: 0.7204, W2: 0.598, W3: -0.0633, W4: -0.0861, b: 0.4429\n",
      "loss: 0.2101\n",
      "Epoch: 20/100, Batch: 385/432, W1: 0.7203, W2: 0.598, W3: -0.0634, W4: -0.0861, b: 0.443\n",
      "loss: 0.2453\n",
      "Epoch: 20/100, Batch: 386/432, W1: 0.7203, W2: 0.5979, W3: -0.0634, W4: -0.0861, b: 0.443\n",
      "loss: 0.2231\n",
      "Epoch: 20/100, Batch: 387/432, W1: 0.7205, W2: 0.5981, W3: -0.0633, W4: -0.0861, b: 0.443\n",
      "loss: 0.1897\n",
      "Epoch: 20/100, Batch: 388/432, W1: 0.7206, W2: 0.5982, W3: -0.0632, W4: -0.0861, b: 0.443\n",
      "loss: 0.2184\n",
      "Epoch: 20/100, Batch: 389/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0861, b: 0.443\n",
      "loss: 0.2193\n",
      "Epoch: 20/100, Batch: 390/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0862, b: 0.4431\n",
      "loss: 0.2375\n",
      "Epoch: 20/100, Batch: 391/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0862, b: 0.4431\n",
      "loss: 0.2378\n",
      "Epoch: 20/100, Batch: 392/432, W1: 0.7206, W2: 0.5983, W3: -0.0632, W4: -0.0862, b: 0.4431\n",
      "loss: 0.197\n",
      "Epoch: 20/100, Batch: 393/432, W1: 0.7207, W2: 0.5983, W3: -0.0631, W4: -0.0862, b: 0.4431\n",
      "loss: 0.2555\n",
      "Epoch: 20/100, Batch: 394/432, W1: 0.7207, W2: 0.5983, W3: -0.0631, W4: -0.0862, b: 0.4431\n",
      "loss: 0.3018\n",
      "Epoch: 20/100, Batch: 395/432, W1: 0.7206, W2: 0.5982, W3: -0.0632, W4: -0.0863, b: 0.4431\n",
      "loss: 0.2573\n",
      "Epoch: 20/100, Batch: 396/432, W1: 0.7206, W2: 0.5982, W3: -0.0632, W4: -0.0863, b: 0.4432\n",
      "loss: 0.1899\n",
      "Epoch: 20/100, Batch: 397/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0863, b: 0.4432\n",
      "loss: 0.2467\n",
      "Epoch: 20/100, Batch: 398/432, W1: 0.7205, W2: 0.5981, W3: -0.0632, W4: -0.0864, b: 0.4432\n",
      "loss: 0.7621\n",
      "Epoch: 20/100, Batch: 399/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0864, b: 0.4432\n",
      "loss: 0.2384\n",
      "Epoch: 20/100, Batch: 400/432, W1: 0.7206, W2: 0.5982, W3: -0.0632, W4: -0.0864, b: 0.4432\n",
      "loss: 0.2068\n",
      "Epoch: 20/100, Batch: 401/432, W1: 0.7205, W2: 0.5981, W3: -0.0632, W4: -0.0864, b: 0.4432\n",
      "loss: 0.2931\n",
      "Epoch: 20/100, Batch: 402/432, W1: 0.7206, W2: 0.5983, W3: -0.0631, W4: -0.0864, b: 0.4433\n",
      "loss: 0.2575\n",
      "Epoch: 20/100, Batch: 403/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0865, b: 0.4433\n",
      "loss: 0.5019\n",
      "Epoch: 20/100, Batch: 404/432, W1: 0.7205, W2: 0.5981, W3: -0.0632, W4: -0.0865, b: 0.4433\n",
      "loss: 0.2174\n",
      "Epoch: 20/100, Batch: 405/432, W1: 0.7204, W2: 0.5981, W3: -0.0633, W4: -0.0866, b: 0.4433\n",
      "loss: 0.2718\n",
      "Epoch: 20/100, Batch: 406/432, W1: 0.7204, W2: 0.5981, W3: -0.0633, W4: -0.0866, b: 0.4433\n",
      "loss: 0.2666\n",
      "Epoch: 20/100, Batch: 407/432, W1: 0.7204, W2: 0.5981, W3: -0.0632, W4: -0.0866, b: 0.4433\n",
      "loss: 0.2144\n",
      "Epoch: 20/100, Batch: 408/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0866, b: 0.4434\n",
      "loss: 0.2867\n",
      "Epoch: 20/100, Batch: 409/432, W1: 0.7204, W2: 0.5981, W3: -0.0633, W4: -0.0867, b: 0.4434\n",
      "loss: 0.2547\n",
      "Epoch: 20/100, Batch: 410/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0867, b: 0.4434\n",
      "loss: 0.2542\n",
      "Epoch: 20/100, Batch: 411/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0867, b: 0.4434\n",
      "loss: 0.1777\n",
      "Epoch: 20/100, Batch: 412/432, W1: 0.7205, W2: 0.5982, W3: -0.0632, W4: -0.0867, b: 0.4434\n",
      "loss: 0.2679\n",
      "Epoch: 20/100, Batch: 413/432, W1: 0.7201, W2: 0.5946, W3: -0.0636, W4: -0.0869, b: 0.4434\n",
      "loss: 10.0848\n",
      "Epoch: 20/100, Batch: 414/432, W1: 0.7201, W2: 0.5946, W3: -0.0636, W4: -0.0869, b: 0.4434\n",
      "loss: 0.2426\n",
      "Epoch: 20/100, Batch: 415/432, W1: 0.7201, W2: 0.5946, W3: -0.0636, W4: -0.0869, b: 0.4434\n",
      "loss: 0.3469\n",
      "Epoch: 20/100, Batch: 416/432, W1: 0.7201, W2: 0.5946, W3: -0.0637, W4: -0.087, b: 0.4434\n",
      "loss: 0.2287\n",
      "Epoch: 20/100, Batch: 417/432, W1: 0.7201, W2: 0.5946, W3: -0.0636, W4: -0.087, b: 0.4435\n",
      "loss: 0.2588\n",
      "Epoch: 20/100, Batch: 418/432, W1: 0.7202, W2: 0.5947, W3: -0.0636, W4: -0.087, b: 0.4435\n",
      "loss: 0.1991\n",
      "Epoch: 20/100, Batch: 419/432, W1: 0.7202, W2: 0.5947, W3: -0.0636, W4: -0.087, b: 0.4435\n",
      "loss: 0.2769\n",
      "Epoch: 20/100, Batch: 420/432, W1: 0.7203, W2: 0.5948, W3: -0.0635, W4: -0.087, b: 0.4435\n",
      "loss: 0.2458\n",
      "Epoch: 20/100, Batch: 421/432, W1: 0.7204, W2: 0.5949, W3: -0.0635, W4: -0.087, b: 0.4436\n",
      "loss: 0.2379\n",
      "Epoch: 20/100, Batch: 422/432, W1: 0.7204, W2: 0.5949, W3: -0.0635, W4: -0.0871, b: 0.4436\n",
      "loss: 0.2902\n",
      "Epoch: 20/100, Batch: 423/432, W1: 0.7204, W2: 0.5949, W3: -0.0635, W4: -0.0871, b: 0.4436\n",
      "loss: 0.277\n",
      "Epoch: 20/100, Batch: 424/432, W1: 0.7203, W2: 0.5949, W3: -0.0635, W4: -0.0871, b: 0.4436\n",
      "loss: 0.3344\n",
      "Epoch: 20/100, Batch: 425/432, W1: 0.7204, W2: 0.5949, W3: -0.0634, W4: -0.0871, b: 0.4437\n",
      "loss: 0.2918\n",
      "Epoch: 20/100, Batch: 426/432, W1: 0.7205, W2: 0.595, W3: -0.0634, W4: -0.0872, b: 0.4437\n",
      "loss: 0.2196\n",
      "Epoch: 20/100, Batch: 427/432, W1: 0.7206, W2: 0.5951, W3: -0.0633, W4: -0.0872, b: 0.4437\n",
      "loss: 0.2285\n",
      "Epoch: 20/100, Batch: 428/432, W1: 0.7206, W2: 0.5952, W3: -0.0633, W4: -0.0872, b: 0.4437\n",
      "loss: 0.252\n",
      "Epoch: 20/100, Batch: 429/432, W1: 0.7207, W2: 0.5952, W3: -0.0632, W4: -0.0872, b: 0.4438\n",
      "loss: 0.2366\n",
      "Epoch: 20/100, Batch: 430/432, W1: 0.7206, W2: 0.5951, W3: -0.0633, W4: -0.0872, b: 0.4438\n",
      "loss: 0.2625\n",
      "Epoch: 20/100, Batch: 431/432, W1: 0.7207, W2: 0.5952, W3: -0.0633, W4: -0.0873, b: 0.4438\n",
      "loss: 0.2169\n",
      "Epoch: 20/100, Batch: 432/432, W1: 0.7206, W2: 0.5951, W3: -0.0633, W4: -0.0873, b: 0.4438\n",
      "loss: 0.32\n",
      "Epoch: 30/100, Batch: 1/432, W1: 0.7355, W2: 0.5787, W3: -0.0495, W4: -0.1841, b: 0.507\n",
      "loss: 0.2114\n",
      "Epoch: 30/100, Batch: 2/432, W1: 0.7355, W2: 0.5787, W3: -0.0495, W4: -0.1841, b: 0.5071\n",
      "loss: 0.1596\n",
      "Epoch: 30/100, Batch: 3/432, W1: 0.7355, W2: 0.5786, W3: -0.0495, W4: -0.1841, b: 0.5071\n",
      "loss: 0.266\n",
      "Epoch: 30/100, Batch: 4/432, W1: 0.7355, W2: 0.5787, W3: -0.0495, W4: -0.1842, b: 0.5071\n",
      "loss: 0.1643\n",
      "Epoch: 30/100, Batch: 5/432, W1: 0.7356, W2: 0.5787, W3: -0.0495, W4: -0.1842, b: 0.5071\n",
      "loss: 0.2245\n",
      "Epoch: 30/100, Batch: 6/432, W1: 0.7356, W2: 0.5788, W3: -0.0494, W4: -0.1842, b: 0.5071\n",
      "loss: 0.2439\n",
      "Epoch: 30/100, Batch: 7/432, W1: 0.7357, W2: 0.5789, W3: -0.0494, W4: -0.1842, b: 0.5072\n",
      "loss: 0.2002\n",
      "Epoch: 30/100, Batch: 8/432, W1: 0.7357, W2: 0.5788, W3: -0.0494, W4: -0.1842, b: 0.5072\n",
      "loss: 0.2553\n",
      "Epoch: 30/100, Batch: 9/432, W1: 0.7356, W2: 0.5788, W3: -0.0494, W4: -0.1843, b: 0.5072\n",
      "loss: 0.219\n",
      "Epoch: 30/100, Batch: 10/432, W1: 0.7355, W2: 0.5787, W3: -0.0495, W4: -0.1843, b: 0.5072\n",
      "loss: 0.2507\n",
      "Epoch: 30/100, Batch: 11/432, W1: 0.7356, W2: 0.5787, W3: -0.0495, W4: -0.1843, b: 0.5072\n",
      "loss: 0.1951\n",
      "Epoch: 30/100, Batch: 12/432, W1: 0.7355, W2: 0.5787, W3: -0.0495, W4: -0.1844, b: 0.5072\n",
      "loss: 0.2222\n",
      "Epoch: 30/100, Batch: 13/432, W1: 0.7355, W2: 0.5787, W3: -0.0495, W4: -0.1844, b: 0.5072\n",
      "loss: 0.2064\n",
      "Epoch: 30/100, Batch: 14/432, W1: 0.7354, W2: 0.5785, W3: -0.0496, W4: -0.1844, b: 0.5072\n",
      "loss: 0.2781\n",
      "Epoch: 30/100, Batch: 15/432, W1: 0.7349, W2: 0.575, W3: -0.0501, W4: -0.1846, b: 0.5072\n",
      "loss: 9.4488\n",
      "Epoch: 30/100, Batch: 16/432, W1: 0.7349, W2: 0.5749, W3: -0.0501, W4: -0.1846, b: 0.5072\n",
      "loss: 0.245\n",
      "Epoch: 30/100, Batch: 17/432, W1: 0.7349, W2: 0.575, W3: -0.0501, W4: -0.1846, b: 0.5072\n",
      "loss: 0.2474\n",
      "Epoch: 30/100, Batch: 18/432, W1: 0.7348, W2: 0.5749, W3: -0.0501, W4: -0.1847, b: 0.5072\n",
      "loss: 0.208\n",
      "Epoch: 30/100, Batch: 19/432, W1: 0.7349, W2: 0.575, W3: -0.05, W4: -0.1847, b: 0.5072\n",
      "loss: 0.224\n",
      "Epoch: 30/100, Batch: 20/432, W1: 0.7349, W2: 0.575, W3: -0.0501, W4: -0.1847, b: 0.5072\n",
      "loss: 0.352\n",
      "Epoch: 30/100, Batch: 21/432, W1: 0.7348, W2: 0.5749, W3: -0.0501, W4: -0.1848, b: 0.5072\n",
      "loss: 0.2438\n",
      "Epoch: 30/100, Batch: 22/432, W1: 0.7349, W2: 0.575, W3: -0.0501, W4: -0.1848, b: 0.5073\n",
      "loss: 0.1962\n",
      "Epoch: 30/100, Batch: 23/432, W1: 0.7348, W2: 0.5749, W3: -0.0501, W4: -0.1848, b: 0.5073\n",
      "loss: 0.2439\n",
      "Epoch: 30/100, Batch: 24/432, W1: 0.7348, W2: 0.5749, W3: -0.0501, W4: -0.1849, b: 0.5073\n",
      "loss: 0.1855\n",
      "Epoch: 30/100, Batch: 25/432, W1: 0.7349, W2: 0.575, W3: -0.0501, W4: -0.1849, b: 0.5073\n",
      "loss: 0.2021\n",
      "Epoch: 30/100, Batch: 26/432, W1: 0.7348, W2: 0.5749, W3: -0.0501, W4: -0.1849, b: 0.5073\n",
      "loss: 0.8241\n",
      "Epoch: 30/100, Batch: 27/432, W1: 0.7349, W2: 0.575, W3: -0.05, W4: -0.1849, b: 0.5074\n",
      "loss: 0.2116\n",
      "Epoch: 30/100, Batch: 28/432, W1: 0.735, W2: 0.5751, W3: -0.05, W4: -0.1849, b: 0.5074\n",
      "loss: 0.2151\n",
      "Epoch: 30/100, Batch: 29/432, W1: 0.735, W2: 0.5751, W3: -0.05, W4: -0.1849, b: 0.5074\n",
      "loss: 0.2199\n",
      "Epoch: 30/100, Batch: 30/432, W1: 0.7351, W2: 0.5752, W3: -0.0499, W4: -0.1849, b: 0.5074\n",
      "loss: 0.2298\n",
      "Epoch: 30/100, Batch: 31/432, W1: 0.7351, W2: 0.5752, W3: -0.0499, W4: -0.185, b: 0.5075\n",
      "loss: 0.2328\n",
      "Epoch: 30/100, Batch: 32/432, W1: 0.7351, W2: 0.5752, W3: -0.0499, W4: -0.185, b: 0.5075\n",
      "loss: 0.2047\n",
      "Epoch: 30/100, Batch: 33/432, W1: 0.7351, W2: 0.5752, W3: -0.0499, W4: -0.185, b: 0.5075\n",
      "loss: 0.2457\n",
      "Epoch: 30/100, Batch: 34/432, W1: 0.7351, W2: 0.5752, W3: -0.0499, W4: -0.185, b: 0.5075\n",
      "loss: 0.2277\n",
      "Epoch: 30/100, Batch: 35/432, W1: 0.7351, W2: 0.5752, W3: -0.0499, W4: -0.1851, b: 0.5075\n",
      "loss: 0.1924\n",
      "Epoch: 30/100, Batch: 36/432, W1: 0.7351, W2: 0.5752, W3: -0.0499, W4: -0.1851, b: 0.5075\n",
      "loss: 0.2039\n",
      "Epoch: 30/100, Batch: 37/432, W1: 0.7352, W2: 0.5753, W3: -0.0498, W4: -0.1851, b: 0.5076\n",
      "loss: 0.2001\n",
      "Epoch: 30/100, Batch: 38/432, W1: 0.7352, W2: 0.5753, W3: -0.0498, W4: -0.1851, b: 0.5076\n",
      "loss: 0.1995\n",
      "Epoch: 30/100, Batch: 39/432, W1: 0.7352, W2: 0.5753, W3: -0.0499, W4: -0.1851, b: 0.5076\n",
      "loss: 0.1997\n",
      "Epoch: 30/100, Batch: 40/432, W1: 0.7353, W2: 0.5754, W3: -0.0498, W4: -0.1851, b: 0.5076\n",
      "loss: 0.1318\n",
      "Epoch: 30/100, Batch: 41/432, W1: 0.7354, W2: 0.5755, W3: -0.0497, W4: -0.1852, b: 0.5076\n",
      "loss: 0.2065\n",
      "Epoch: 30/100, Batch: 42/432, W1: 0.7354, W2: 0.5755, W3: -0.0497, W4: -0.1852, b: 0.5077\n",
      "loss: 0.7267\n",
      "Epoch: 30/100, Batch: 43/432, W1: 0.7354, W2: 0.5755, W3: -0.0497, W4: -0.1852, b: 0.5077\n",
      "loss: 0.2545\n",
      "Epoch: 30/100, Batch: 44/432, W1: 0.7354, W2: 0.5755, W3: -0.0497, W4: -0.1852, b: 0.5077\n",
      "loss: 0.1995\n",
      "Epoch: 30/100, Batch: 45/432, W1: 0.7355, W2: 0.5756, W3: -0.0497, W4: -0.1852, b: 0.5077\n",
      "loss: 0.1691\n",
      "Epoch: 30/100, Batch: 46/432, W1: 0.7355, W2: 0.5757, W3: -0.0496, W4: -0.1852, b: 0.5078\n",
      "loss: 0.2404\n",
      "Epoch: 30/100, Batch: 47/432, W1: 0.7355, W2: 0.5756, W3: -0.0497, W4: -0.1853, b: 0.5078\n",
      "loss: 0.2771\n",
      "Epoch: 30/100, Batch: 48/432, W1: 0.7356, W2: 0.5757, W3: -0.0496, W4: -0.1853, b: 0.5078\n",
      "loss: 0.2044\n",
      "Epoch: 30/100, Batch: 49/432, W1: 0.7356, W2: 0.5757, W3: -0.0496, W4: -0.1853, b: 0.5078\n",
      "loss: 0.2321\n",
      "Epoch: 30/100, Batch: 50/432, W1: 0.7356, W2: 0.5757, W3: -0.0496, W4: -0.1853, b: 0.5078\n",
      "loss: 0.1884\n",
      "Epoch: 30/100, Batch: 51/432, W1: 0.7358, W2: 0.5759, W3: -0.0495, W4: -0.1853, b: 0.5079\n",
      "loss: 0.1647\n",
      "Epoch: 30/100, Batch: 52/432, W1: 0.7358, W2: 0.5759, W3: -0.0495, W4: -0.1853, b: 0.5079\n",
      "loss: 0.178\n",
      "Epoch: 30/100, Batch: 53/432, W1: 0.7358, W2: 0.576, W3: -0.0494, W4: -0.1853, b: 0.5079\n",
      "loss: 0.2179\n",
      "Epoch: 30/100, Batch: 54/432, W1: 0.7358, W2: 0.5759, W3: -0.0495, W4: -0.1854, b: 0.5079\n",
      "loss: 0.2665\n",
      "Epoch: 30/100, Batch: 55/432, W1: 0.7358, W2: 0.5759, W3: -0.0495, W4: -0.1854, b: 0.5079\n",
      "loss: 0.1712\n",
      "Epoch: 30/100, Batch: 56/432, W1: 0.7358, W2: 0.5759, W3: -0.0495, W4: -0.1854, b: 0.5079\n",
      "loss: 0.2382\n",
      "Epoch: 30/100, Batch: 57/432, W1: 0.7359, W2: 0.576, W3: -0.0494, W4: -0.1854, b: 0.508\n",
      "loss: 0.2167\n",
      "Epoch: 30/100, Batch: 58/432, W1: 0.7358, W2: 0.5759, W3: -0.0494, W4: -0.1855, b: 0.508\n",
      "loss: 0.2793\n",
      "Epoch: 30/100, Batch: 59/432, W1: 0.7358, W2: 0.5759, W3: -0.0494, W4: -0.1855, b: 0.508\n",
      "loss: 0.2831\n",
      "Epoch: 30/100, Batch: 60/432, W1: 0.7358, W2: 0.576, W3: -0.0494, W4: -0.1855, b: 0.508\n",
      "loss: 0.1924\n",
      "Epoch: 30/100, Batch: 61/432, W1: 0.7359, W2: 0.576, W3: -0.0494, W4: -0.1855, b: 0.508\n",
      "loss: 0.2202\n",
      "Epoch: 30/100, Batch: 62/432, W1: 0.7358, W2: 0.576, W3: -0.0494, W4: -0.1856, b: 0.508\n",
      "loss: 0.2905\n",
      "Epoch: 30/100, Batch: 63/432, W1: 0.736, W2: 0.5761, W3: -0.0493, W4: -0.1856, b: 0.5081\n",
      "loss: 0.1694\n",
      "Epoch: 30/100, Batch: 64/432, W1: 0.736, W2: 0.5761, W3: -0.0493, W4: -0.1856, b: 0.5081\n",
      "loss: 0.2167\n",
      "Epoch: 30/100, Batch: 65/432, W1: 0.7361, W2: 0.5762, W3: -0.0493, W4: -0.1856, b: 0.5081\n",
      "loss: 0.1889\n",
      "Epoch: 30/100, Batch: 66/432, W1: 0.7361, W2: 0.5762, W3: -0.0493, W4: -0.1856, b: 0.5081\n",
      "loss: 0.2281\n",
      "Epoch: 30/100, Batch: 67/432, W1: 0.7362, W2: 0.5764, W3: -0.0492, W4: -0.1856, b: 0.5082\n",
      "loss: 0.2057\n",
      "Epoch: 30/100, Batch: 68/432, W1: 0.7363, W2: 0.5765, W3: -0.0491, W4: -0.1856, b: 0.5082\n",
      "loss: 0.1931\n",
      "Epoch: 30/100, Batch: 69/432, W1: 0.7363, W2: 0.5764, W3: -0.0491, W4: -0.1857, b: 0.5082\n",
      "loss: 0.1962\n",
      "Epoch: 30/100, Batch: 70/432, W1: 0.7364, W2: 0.5765, W3: -0.0491, W4: -0.1857, b: 0.5082\n",
      "loss: 0.192\n",
      "Epoch: 30/100, Batch: 71/432, W1: 0.7364, W2: 0.5765, W3: -0.0491, W4: -0.1857, b: 0.5083\n",
      "loss: 0.2429\n",
      "Epoch: 30/100, Batch: 72/432, W1: 0.7364, W2: 0.5765, W3: -0.0491, W4: -0.1857, b: 0.5083\n",
      "loss: 0.1915\n",
      "Epoch: 30/100, Batch: 73/432, W1: 0.7363, W2: 0.5764, W3: -0.0491, W4: -0.1858, b: 0.5083\n",
      "loss: 0.2437\n",
      "Epoch: 30/100, Batch: 74/432, W1: 0.7363, W2: 0.5764, W3: -0.0491, W4: -0.1858, b: 0.5083\n",
      "loss: 0.172\n",
      "Epoch: 30/100, Batch: 75/432, W1: 0.7364, W2: 0.5765, W3: -0.0491, W4: -0.1858, b: 0.5083\n",
      "loss: 0.1798\n",
      "Epoch: 30/100, Batch: 76/432, W1: 0.7363, W2: 0.5764, W3: -0.0491, W4: -0.1858, b: 0.5083\n",
      "loss: 0.2482\n",
      "Epoch: 30/100, Batch: 77/432, W1: 0.7364, W2: 0.5765, W3: -0.0491, W4: -0.1858, b: 0.5083\n",
      "loss: 0.2282\n",
      "Epoch: 30/100, Batch: 78/432, W1: 0.7363, W2: 0.5765, W3: -0.0491, W4: -0.1859, b: 0.5083\n",
      "loss: 0.2325\n",
      "Epoch: 30/100, Batch: 79/432, W1: 0.7364, W2: 0.5765, W3: -0.0491, W4: -0.1859, b: 0.5084\n",
      "loss: 0.2243\n",
      "Epoch: 30/100, Batch: 80/432, W1: 0.7364, W2: 0.5765, W3: -0.0491, W4: -0.1859, b: 0.5084\n",
      "loss: 0.1896\n",
      "Epoch: 30/100, Batch: 81/432, W1: 0.7363, W2: 0.5765, W3: -0.0491, W4: -0.1859, b: 0.5084\n",
      "loss: 0.8796\n",
      "Epoch: 30/100, Batch: 82/432, W1: 0.7363, W2: 0.5764, W3: -0.0491, W4: -0.186, b: 0.5084\n",
      "loss: 0.2729\n",
      "Epoch: 30/100, Batch: 83/432, W1: 0.7362, W2: 0.5764, W3: -0.0492, W4: -0.186, b: 0.5084\n",
      "loss: 0.2597\n",
      "Epoch: 30/100, Batch: 84/432, W1: 0.7363, W2: 0.5764, W3: -0.0491, W4: -0.186, b: 0.5084\n",
      "loss: 0.2039\n",
      "Epoch: 30/100, Batch: 85/432, W1: 0.7362, W2: 0.5764, W3: -0.0492, W4: -0.186, b: 0.5084\n",
      "loss: 0.2101\n",
      "Epoch: 30/100, Batch: 86/432, W1: 0.7362, W2: 0.5764, W3: -0.0491, W4: -0.1861, b: 0.5085\n",
      "loss: 0.2227\n",
      "Epoch: 30/100, Batch: 87/432, W1: 0.7361, W2: 0.5762, W3: -0.0492, W4: -0.1861, b: 0.5085\n",
      "loss: 0.3304\n",
      "Epoch: 30/100, Batch: 88/432, W1: 0.736, W2: 0.5761, W3: -0.0493, W4: -0.1862, b: 0.5085\n",
      "loss: 0.241\n",
      "Epoch: 30/100, Batch: 89/432, W1: 0.736, W2: 0.5762, W3: -0.0493, W4: -0.1862, b: 0.5085\n",
      "loss: 0.2812\n",
      "Epoch: 30/100, Batch: 90/432, W1: 0.7361, W2: 0.5762, W3: -0.0493, W4: -0.1862, b: 0.5085\n",
      "loss: 0.1855\n",
      "Epoch: 30/100, Batch: 91/432, W1: 0.7362, W2: 0.5763, W3: -0.0492, W4: -0.1862, b: 0.5085\n",
      "loss: 0.1965\n",
      "Epoch: 30/100, Batch: 92/432, W1: 0.7362, W2: 0.5764, W3: -0.0492, W4: -0.1862, b: 0.5086\n",
      "loss: 0.2065\n",
      "Epoch: 30/100, Batch: 93/432, W1: 0.736, W2: 0.5762, W3: -0.0493, W4: -0.1863, b: 0.5085\n",
      "loss: 0.29\n",
      "Epoch: 30/100, Batch: 94/432, W1: 0.736, W2: 0.5762, W3: -0.0493, W4: -0.1863, b: 0.5086\n",
      "loss: 0.2366\n",
      "Epoch: 30/100, Batch: 95/432, W1: 0.736, W2: 0.5762, W3: -0.0493, W4: -0.1863, b: 0.5086\n",
      "loss: 0.2099\n",
      "Epoch: 30/100, Batch: 96/432, W1: 0.7361, W2: 0.5763, W3: -0.0492, W4: -0.1863, b: 0.5086\n",
      "loss: 0.209\n",
      "Epoch: 30/100, Batch: 97/432, W1: 0.736, W2: 0.5762, W3: -0.0493, W4: -0.1864, b: 0.5086\n",
      "loss: 0.2762\n",
      "Epoch: 30/100, Batch: 98/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1864, b: 0.5086\n",
      "loss: 0.2483\n",
      "Epoch: 30/100, Batch: 99/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1865, b: 0.5086\n",
      "loss: 0.2038\n",
      "Epoch: 30/100, Batch: 100/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1865, b: 0.5086\n",
      "loss: 0.2152\n",
      "Epoch: 30/100, Batch: 101/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1865, b: 0.5087\n",
      "loss: 0.1869\n",
      "Epoch: 30/100, Batch: 102/432, W1: 0.7358, W2: 0.576, W3: -0.0494, W4: -0.1865, b: 0.5086\n",
      "loss: 0.2074\n",
      "Epoch: 30/100, Batch: 103/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1865, b: 0.5087\n",
      "loss: 0.2026\n",
      "Epoch: 30/100, Batch: 104/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1866, b: 0.5087\n",
      "loss: 0.2448\n",
      "Epoch: 30/100, Batch: 105/432, W1: 0.7359, W2: 0.576, W3: -0.0494, W4: -0.1866, b: 0.5087\n",
      "loss: 0.1946\n",
      "Epoch: 30/100, Batch: 106/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1866, b: 0.5087\n",
      "loss: 0.2161\n",
      "Epoch: 30/100, Batch: 107/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1866, b: 0.5087\n",
      "loss: 0.2267\n",
      "Epoch: 30/100, Batch: 108/432, W1: 0.7358, W2: 0.576, W3: -0.0494, W4: -0.1867, b: 0.5087\n",
      "loss: 0.2454\n",
      "Epoch: 30/100, Batch: 109/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1867, b: 0.5088\n",
      "loss: 0.2298\n",
      "Epoch: 30/100, Batch: 110/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1867, b: 0.5088\n",
      "loss: 1.0989\n",
      "Epoch: 30/100, Batch: 111/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1867, b: 0.5088\n",
      "loss: 0.2073\n",
      "Epoch: 30/100, Batch: 112/432, W1: 0.736, W2: 0.5762, W3: -0.0493, W4: -0.1867, b: 0.5088\n",
      "loss: 0.1546\n",
      "Epoch: 30/100, Batch: 113/432, W1: 0.7359, W2: 0.5761, W3: -0.0493, W4: -0.1868, b: 0.5088\n",
      "loss: 0.2395\n",
      "Epoch: 30/100, Batch: 114/432, W1: 0.7358, W2: 0.576, W3: -0.0493, W4: -0.1868, b: 0.5088\n",
      "loss: 0.3491\n",
      "Epoch: 30/100, Batch: 115/432, W1: 0.7358, W2: 0.576, W3: -0.0494, W4: -0.1868, b: 0.5089\n",
      "loss: 0.2185\n",
      "Epoch: 30/100, Batch: 116/432, W1: 0.7358, W2: 0.576, W3: -0.0494, W4: -0.1869, b: 0.5089\n",
      "loss: 0.1754\n",
      "Epoch: 30/100, Batch: 117/432, W1: 0.7358, W2: 0.576, W3: -0.0494, W4: -0.1869, b: 0.5089\n",
      "loss: 0.224\n",
      "Epoch: 30/100, Batch: 118/432, W1: 0.7358, W2: 0.576, W3: -0.0494, W4: -0.1869, b: 0.5089\n",
      "loss: 0.2148\n",
      "Epoch: 30/100, Batch: 119/432, W1: 0.7359, W2: 0.576, W3: -0.0493, W4: -0.1869, b: 0.5089\n",
      "loss: 0.2125\n",
      "Epoch: 30/100, Batch: 120/432, W1: 0.7359, W2: 0.5761, W3: -0.0492, W4: -0.1869, b: 0.5089\n",
      "loss: 0.2618\n",
      "Epoch: 30/100, Batch: 121/432, W1: 0.7358, W2: 0.576, W3: -0.0492, W4: -0.187, b: 0.5089\n",
      "loss: 0.2409\n",
      "Epoch: 30/100, Batch: 122/432, W1: 0.7358, W2: 0.5759, W3: -0.0493, W4: -0.187, b: 0.5089\n",
      "loss: 0.2055\n",
      "Epoch: 30/100, Batch: 123/432, W1: 0.7358, W2: 0.576, W3: -0.0493, W4: -0.187, b: 0.509\n",
      "loss: 0.2089\n",
      "Epoch: 30/100, Batch: 124/432, W1: 0.7358, W2: 0.576, W3: -0.0493, W4: -0.1871, b: 0.509\n",
      "loss: 0.2042\n",
      "Epoch: 30/100, Batch: 125/432, W1: 0.7358, W2: 0.576, W3: -0.0492, W4: -0.1871, b: 0.509\n",
      "loss: 0.2342\n",
      "Epoch: 30/100, Batch: 126/432, W1: 0.736, W2: 0.5762, W3: -0.0491, W4: -0.1871, b: 0.509\n",
      "loss: 0.1696\n",
      "Epoch: 30/100, Batch: 127/432, W1: 0.7361, W2: 0.5762, W3: -0.0491, W4: -0.1871, b: 0.5091\n",
      "loss: 0.1719\n",
      "Epoch: 30/100, Batch: 128/432, W1: 0.736, W2: 0.5762, W3: -0.0491, W4: -0.1871, b: 0.5091\n",
      "loss: 0.2296\n",
      "Epoch: 30/100, Batch: 129/432, W1: 0.7362, W2: 0.5763, W3: -0.049, W4: -0.1871, b: 0.5091\n",
      "loss: 0.1932\n",
      "Epoch: 30/100, Batch: 130/432, W1: 0.7362, W2: 0.5764, W3: -0.049, W4: -0.1871, b: 0.5091\n",
      "loss: 0.2128\n",
      "Epoch: 30/100, Batch: 131/432, W1: 0.7363, W2: 0.5764, W3: -0.049, W4: -0.1871, b: 0.5092\n",
      "loss: 0.1834\n",
      "Epoch: 30/100, Batch: 132/432, W1: 0.7362, W2: 0.5764, W3: -0.049, W4: -0.1872, b: 0.5092\n",
      "loss: 0.1981\n",
      "Epoch: 30/100, Batch: 133/432, W1: 0.7361, W2: 0.5763, W3: -0.0491, W4: -0.1872, b: 0.5091\n",
      "loss: 0.2318\n",
      "Epoch: 30/100, Batch: 134/432, W1: 0.7361, W2: 0.5763, W3: -0.0491, W4: -0.1872, b: 0.5092\n",
      "loss: 0.2301\n",
      "Epoch: 30/100, Batch: 135/432, W1: 0.7362, W2: 0.5764, W3: -0.049, W4: -0.1872, b: 0.5092\n",
      "loss: 0.2101\n",
      "Epoch: 30/100, Batch: 136/432, W1: 0.7362, W2: 0.5764, W3: -0.049, W4: -0.1873, b: 0.5092\n",
      "loss: 0.2481\n",
      "Epoch: 30/100, Batch: 137/432, W1: 0.7363, W2: 0.5765, W3: -0.0489, W4: -0.1873, b: 0.5092\n",
      "loss: 0.1886\n",
      "Epoch: 30/100, Batch: 138/432, W1: 0.7363, W2: 0.5765, W3: -0.0489, W4: -0.1873, b: 0.5093\n",
      "loss: 0.1709\n",
      "Epoch: 30/100, Batch: 139/432, W1: 0.7364, W2: 0.5766, W3: -0.0488, W4: -0.1873, b: 0.5093\n",
      "loss: 0.2006\n",
      "Epoch: 30/100, Batch: 140/432, W1: 0.7364, W2: 0.5766, W3: -0.0488, W4: -0.1873, b: 0.5093\n",
      "loss: 0.2479\n",
      "Epoch: 30/100, Batch: 141/432, W1: 0.7363, W2: 0.5765, W3: -0.0489, W4: -0.1874, b: 0.5093\n",
      "loss: 0.3102\n",
      "Epoch: 30/100, Batch: 142/432, W1: 0.7364, W2: 0.5766, W3: -0.0489, W4: -0.1874, b: 0.5093\n",
      "loss: 0.2419\n",
      "Epoch: 30/100, Batch: 143/432, W1: 0.7365, W2: 0.5766, W3: -0.0488, W4: -0.1874, b: 0.5094\n",
      "loss: 0.2402\n",
      "Epoch: 30/100, Batch: 144/432, W1: 0.7365, W2: 0.5767, W3: -0.0488, W4: -0.1874, b: 0.5094\n",
      "loss: 0.1745\n",
      "Epoch: 30/100, Batch: 145/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1874, b: 0.5094\n",
      "loss: 0.1892\n",
      "Epoch: 30/100, Batch: 146/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1875, b: 0.5094\n",
      "loss: 0.293\n",
      "Epoch: 30/100, Batch: 147/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1875, b: 0.5094\n",
      "loss: 0.1944\n",
      "Epoch: 30/100, Batch: 148/432, W1: 0.7365, W2: 0.5767, W3: -0.0488, W4: -0.1875, b: 0.5094\n",
      "loss: 0.1929\n",
      "Epoch: 30/100, Batch: 149/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1875, b: 0.5095\n",
      "loss: 0.1643\n",
      "Epoch: 30/100, Batch: 150/432, W1: 0.7367, W2: 0.5769, W3: -0.0487, W4: -0.1875, b: 0.5095\n",
      "loss: 0.2354\n",
      "Epoch: 30/100, Batch: 151/432, W1: 0.7367, W2: 0.5769, W3: -0.0487, W4: -0.1876, b: 0.5095\n",
      "loss: 0.185\n",
      "Epoch: 30/100, Batch: 152/432, W1: 0.7366, W2: 0.5769, W3: -0.0487, W4: -0.1876, b: 0.5095\n",
      "loss: 0.1769\n",
      "Epoch: 30/100, Batch: 153/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1876, b: 0.5095\n",
      "loss: 0.2279\n",
      "Epoch: 30/100, Batch: 154/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1876, b: 0.5095\n",
      "loss: 0.1806\n",
      "Epoch: 30/100, Batch: 155/432, W1: 0.7365, W2: 0.5767, W3: -0.0488, W4: -0.1877, b: 0.5095\n",
      "loss: 0.3463\n",
      "Epoch: 30/100, Batch: 156/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1877, b: 0.5096\n",
      "loss: 0.2195\n",
      "Epoch: 30/100, Batch: 157/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1877, b: 0.5096\n",
      "loss: 0.2324\n",
      "Epoch: 30/100, Batch: 158/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1877, b: 0.5096\n",
      "loss: 0.1968\n",
      "Epoch: 30/100, Batch: 159/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1878, b: 0.5096\n",
      "loss: 0.236\n",
      "Epoch: 30/100, Batch: 160/432, W1: 0.7364, W2: 0.5766, W3: -0.0488, W4: -0.1878, b: 0.5096\n",
      "loss: 0.2124\n",
      "Epoch: 30/100, Batch: 161/432, W1: 0.7364, W2: 0.5766, W3: -0.0489, W4: -0.1879, b: 0.5096\n",
      "loss: 0.2078\n",
      "Epoch: 30/100, Batch: 162/432, W1: 0.7363, W2: 0.5766, W3: -0.0489, W4: -0.1879, b: 0.5096\n",
      "loss: 0.2183\n",
      "Epoch: 30/100, Batch: 163/432, W1: 0.7364, W2: 0.5766, W3: -0.0489, W4: -0.1879, b: 0.5096\n",
      "loss: 0.2395\n",
      "Epoch: 30/100, Batch: 164/432, W1: 0.7364, W2: 0.5766, W3: -0.0488, W4: -0.1879, b: 0.5097\n",
      "loss: 0.1766\n",
      "Epoch: 30/100, Batch: 165/432, W1: 0.7364, W2: 0.5766, W3: -0.0488, W4: -0.188, b: 0.5097\n",
      "loss: 0.2423\n",
      "Epoch: 30/100, Batch: 166/432, W1: 0.7365, W2: 0.5768, W3: -0.0487, W4: -0.188, b: 0.5097\n",
      "loss: 0.2265\n",
      "Epoch: 30/100, Batch: 167/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.188, b: 0.5097\n",
      "loss: 0.1987\n",
      "Epoch: 30/100, Batch: 168/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.188, b: 0.5097\n",
      "loss: 0.2223\n",
      "Epoch: 30/100, Batch: 169/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.188, b: 0.5097\n",
      "loss: 0.2539\n",
      "Epoch: 30/100, Batch: 170/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1881, b: 0.5098\n",
      "loss: 0.2108\n",
      "Epoch: 30/100, Batch: 171/432, W1: 0.7364, W2: 0.5766, W3: -0.0488, W4: -0.1881, b: 0.5098\n",
      "loss: 0.2261\n",
      "Epoch: 30/100, Batch: 172/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1881, b: 0.5098\n",
      "loss: 0.2018\n",
      "Epoch: 30/100, Batch: 173/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1881, b: 0.5098\n",
      "loss: 0.245\n",
      "Epoch: 30/100, Batch: 174/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1881, b: 0.5098\n",
      "loss: 0.1796\n",
      "Epoch: 30/100, Batch: 175/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1882, b: 0.5098\n",
      "loss: 0.2378\n",
      "Epoch: 30/100, Batch: 176/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1882, b: 0.5099\n",
      "loss: 0.1647\n",
      "Epoch: 30/100, Batch: 177/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1882, b: 0.5099\n",
      "loss: 0.1687\n",
      "Epoch: 30/100, Batch: 178/432, W1: 0.7366, W2: 0.5769, W3: -0.0486, W4: -0.1882, b: 0.5099\n",
      "loss: 0.1808\n",
      "Epoch: 30/100, Batch: 179/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1882, b: 0.5099\n",
      "loss: 1.1516\n",
      "Epoch: 30/100, Batch: 180/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1882, b: 0.5099\n",
      "loss: 0.1914\n",
      "Epoch: 30/100, Batch: 181/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1882, b: 0.5099\n",
      "loss: 0.1791\n",
      "Epoch: 30/100, Batch: 182/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1883, b: 0.51\n",
      "loss: 0.202\n",
      "Epoch: 30/100, Batch: 183/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1883, b: 0.51\n",
      "loss: 0.1898\n",
      "Epoch: 30/100, Batch: 184/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1883, b: 0.51\n",
      "loss: 0.217\n",
      "Epoch: 30/100, Batch: 185/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1883, b: 0.51\n",
      "loss: 0.2034\n",
      "Epoch: 30/100, Batch: 186/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1884, b: 0.51\n",
      "loss: 0.2165\n",
      "Epoch: 30/100, Batch: 187/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1884, b: 0.51\n",
      "loss: 0.2146\n",
      "Epoch: 30/100, Batch: 188/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1884, b: 0.51\n",
      "loss: 0.152\n",
      "Epoch: 30/100, Batch: 189/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1884, b: 0.51\n",
      "loss: 0.1851\n",
      "Epoch: 30/100, Batch: 190/432, W1: 0.7366, W2: 0.5769, W3: -0.0486, W4: -0.1884, b: 0.5101\n",
      "loss: 0.1963\n",
      "Epoch: 30/100, Batch: 191/432, W1: 0.7365, W2: 0.5768, W3: -0.0487, W4: -0.1885, b: 0.5101\n",
      "loss: 0.2852\n",
      "Epoch: 30/100, Batch: 192/432, W1: 0.7364, W2: 0.5767, W3: -0.0487, W4: -0.1885, b: 0.5101\n",
      "loss: 0.2399\n",
      "Epoch: 30/100, Batch: 193/432, W1: 0.7364, W2: 0.5766, W3: -0.0488, W4: -0.1885, b: 0.5101\n",
      "loss: 0.1932\n",
      "Epoch: 30/100, Batch: 194/432, W1: 0.7364, W2: 0.5767, W3: -0.0488, W4: -0.1886, b: 0.5101\n",
      "loss: 0.1807\n",
      "Epoch: 30/100, Batch: 195/432, W1: 0.7364, W2: 0.5767, W3: -0.0487, W4: -0.1886, b: 0.5101\n",
      "loss: 0.1789\n",
      "Epoch: 30/100, Batch: 196/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1886, b: 0.5101\n",
      "loss: 0.2079\n",
      "Epoch: 30/100, Batch: 197/432, W1: 0.7365, W2: 0.5768, W3: -0.0487, W4: -0.1886, b: 0.5101\n",
      "loss: 0.1781\n",
      "Epoch: 30/100, Batch: 198/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1886, b: 0.5102\n",
      "loss: 0.2418\n",
      "Epoch: 30/100, Batch: 199/432, W1: 0.7365, W2: 0.5768, W3: -0.0487, W4: -0.1887, b: 0.5102\n",
      "loss: 0.2185\n",
      "Epoch: 30/100, Batch: 200/432, W1: 0.7366, W2: 0.5768, W3: -0.0486, W4: -0.1887, b: 0.5102\n",
      "loss: 0.1554\n",
      "Epoch: 30/100, Batch: 201/432, W1: 0.7366, W2: 0.5768, W3: -0.0487, W4: -0.1887, b: 0.5102\n",
      "loss: 0.1964\n",
      "Epoch: 30/100, Batch: 202/432, W1: 0.7364, W2: 0.5766, W3: -0.0487, W4: -0.1887, b: 0.5102\n",
      "loss: 0.2803\n",
      "Epoch: 30/100, Batch: 203/432, W1: 0.7364, W2: 0.5766, W3: -0.0488, W4: -0.1888, b: 0.5102\n",
      "loss: 0.2392\n",
      "Epoch: 30/100, Batch: 204/432, W1: 0.7364, W2: 0.5767, W3: -0.0487, W4: -0.1888, b: 0.5102\n",
      "loss: 0.1749\n",
      "Epoch: 30/100, Batch: 205/432, W1: 0.7364, W2: 0.5767, W3: -0.0487, W4: -0.1888, b: 0.5102\n",
      "loss: 0.2596\n",
      "Epoch: 30/100, Batch: 206/432, W1: 0.7364, W2: 0.5766, W3: -0.0487, W4: -0.1888, b: 0.5103\n",
      "loss: 0.1992\n",
      "Epoch: 30/100, Batch: 207/432, W1: 0.7364, W2: 0.5766, W3: -0.0487, W4: -0.1889, b: 0.5103\n",
      "loss: 0.1797\n",
      "Epoch: 30/100, Batch: 208/432, W1: 0.7364, W2: 0.5767, W3: -0.0487, W4: -0.1889, b: 0.5103\n",
      "loss: 0.2291\n",
      "Epoch: 30/100, Batch: 209/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1889, b: 0.5103\n",
      "loss: 0.2647\n",
      "Epoch: 30/100, Batch: 210/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1889, b: 0.5103\n",
      "loss: 0.1507\n",
      "Epoch: 30/100, Batch: 211/432, W1: 0.7365, W2: 0.5768, W3: -0.0487, W4: -0.1889, b: 0.5103\n",
      "loss: 0.2058\n",
      "Epoch: 30/100, Batch: 212/432, W1: 0.7365, W2: 0.5768, W3: -0.0486, W4: -0.189, b: 0.5104\n",
      "loss: 0.2346\n",
      "Epoch: 30/100, Batch: 213/432, W1: 0.7365, W2: 0.5768, W3: -0.0487, W4: -0.189, b: 0.5104\n",
      "loss: 0.2268\n",
      "Epoch: 30/100, Batch: 214/432, W1: 0.7365, W2: 0.5768, W3: -0.0487, W4: -0.189, b: 0.5104\n",
      "loss: 0.2351\n",
      "Epoch: 30/100, Batch: 215/432, W1: 0.7366, W2: 0.5769, W3: -0.0486, W4: -0.189, b: 0.5104\n",
      "loss: 0.1947\n",
      "Epoch: 30/100, Batch: 216/432, W1: 0.7366, W2: 0.5769, W3: -0.0486, W4: -0.1891, b: 0.5104\n",
      "loss: 0.2136\n",
      "Epoch: 30/100, Batch: 217/432, W1: 0.7366, W2: 0.5768, W3: -0.0486, W4: -0.1891, b: 0.5104\n",
      "loss: 0.2705\n",
      "Epoch: 30/100, Batch: 218/432, W1: 0.7366, W2: 0.5768, W3: -0.0486, W4: -0.1891, b: 0.5105\n",
      "loss: 0.1851\n",
      "Epoch: 30/100, Batch: 219/432, W1: 0.7365, W2: 0.5768, W3: -0.0486, W4: -0.1891, b: 0.5105\n",
      "loss: 0.2206\n",
      "Epoch: 30/100, Batch: 220/432, W1: 0.7365, W2: 0.5767, W3: -0.0487, W4: -0.1892, b: 0.5105\n",
      "loss: 0.253\n",
      "Epoch: 30/100, Batch: 221/432, W1: 0.7364, W2: 0.5766, W3: -0.0487, W4: -0.1892, b: 0.5105\n",
      "loss: 0.2562\n",
      "Epoch: 30/100, Batch: 222/432, W1: 0.7364, W2: 0.5766, W3: -0.0487, W4: -0.1892, b: 0.5105\n",
      "loss: 0.2487\n",
      "Epoch: 30/100, Batch: 223/432, W1: 0.7364, W2: 0.5767, W3: -0.0487, W4: -0.1893, b: 0.5105\n",
      "loss: 0.2136\n",
      "Epoch: 30/100, Batch: 224/432, W1: 0.7364, W2: 0.5767, W3: -0.0487, W4: -0.1893, b: 0.5105\n",
      "loss: 0.2286\n",
      "Epoch: 30/100, Batch: 225/432, W1: 0.7363, W2: 0.5766, W3: -0.0488, W4: -0.1893, b: 0.5105\n",
      "loss: 0.2185\n",
      "Epoch: 30/100, Batch: 226/432, W1: 0.7363, W2: 0.5765, W3: -0.0488, W4: -0.1894, b: 0.5105\n",
      "loss: 0.2988\n",
      "Epoch: 30/100, Batch: 227/432, W1: 0.7363, W2: 0.5765, W3: -0.0488, W4: -0.1894, b: 0.5105\n",
      "loss: 0.1867\n",
      "Epoch: 30/100, Batch: 228/432, W1: 0.7363, W2: 0.5765, W3: -0.0488, W4: -0.1894, b: 0.5106\n",
      "loss: 0.1696\n",
      "Epoch: 30/100, Batch: 229/432, W1: 0.7362, W2: 0.5765, W3: -0.0488, W4: -0.1895, b: 0.5106\n",
      "loss: 0.2664\n",
      "Epoch: 30/100, Batch: 230/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1895, b: 0.5106\n",
      "loss: 0.2529\n",
      "Epoch: 30/100, Batch: 231/432, W1: 0.7362, W2: 0.5765, W3: -0.0489, W4: -0.1895, b: 0.5106\n",
      "loss: 0.2187\n",
      "Epoch: 30/100, Batch: 232/432, W1: 0.7363, W2: 0.5765, W3: -0.0488, W4: -0.1895, b: 0.5106\n",
      "loss: 0.1678\n",
      "Epoch: 30/100, Batch: 233/432, W1: 0.7362, W2: 0.5765, W3: -0.0488, W4: -0.1895, b: 0.5106\n",
      "loss: 0.2008\n",
      "Epoch: 30/100, Batch: 234/432, W1: 0.7363, W2: 0.5766, W3: -0.0488, W4: -0.1896, b: 0.5107\n",
      "loss: 0.2304\n",
      "Epoch: 30/100, Batch: 235/432, W1: 0.7363, W2: 0.5766, W3: -0.0487, W4: -0.1896, b: 0.5107\n",
      "loss: 0.2718\n",
      "Epoch: 30/100, Batch: 236/432, W1: 0.7362, W2: 0.5765, W3: -0.0488, W4: -0.1896, b: 0.5107\n",
      "loss: 0.2236\n",
      "Epoch: 30/100, Batch: 237/432, W1: 0.7362, W2: 0.5764, W3: -0.0489, W4: -0.1897, b: 0.5107\n",
      "loss: 0.2178\n",
      "Epoch: 30/100, Batch: 238/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1897, b: 0.5107\n",
      "loss: 0.1981\n",
      "Epoch: 30/100, Batch: 239/432, W1: 0.7361, W2: 0.5763, W3: -0.0489, W4: -0.1897, b: 0.5107\n",
      "loss: 0.194\n",
      "Epoch: 30/100, Batch: 240/432, W1: 0.7362, W2: 0.5765, W3: -0.0488, W4: -0.1897, b: 0.5107\n",
      "loss: 0.1807\n",
      "Epoch: 30/100, Batch: 241/432, W1: 0.7362, W2: 0.5765, W3: -0.0488, W4: -0.1897, b: 0.5107\n",
      "loss: 0.2004\n",
      "Epoch: 30/100, Batch: 242/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1898, b: 0.5107\n",
      "loss: 0.2236\n",
      "Epoch: 30/100, Batch: 243/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1898, b: 0.5107\n",
      "loss: 0.1863\n",
      "Epoch: 30/100, Batch: 244/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1898, b: 0.5108\n",
      "loss: 0.216\n",
      "Epoch: 30/100, Batch: 245/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1899, b: 0.5108\n",
      "loss: 0.2738\n",
      "Epoch: 30/100, Batch: 246/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1899, b: 0.5108\n",
      "loss: 0.2759\n",
      "Epoch: 30/100, Batch: 247/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1899, b: 0.5108\n",
      "loss: 0.2193\n",
      "Epoch: 30/100, Batch: 248/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1899, b: 0.5108\n",
      "loss: 0.1998\n",
      "Epoch: 30/100, Batch: 249/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.19, b: 0.5108\n",
      "loss: 0.1919\n",
      "Epoch: 30/100, Batch: 250/432, W1: 0.7362, W2: 0.5765, W3: -0.0488, W4: -0.19, b: 0.5109\n",
      "loss: 0.2033\n",
      "Epoch: 30/100, Batch: 251/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.19, b: 0.5109\n",
      "loss: 0.2224\n",
      "Epoch: 30/100, Batch: 252/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.19, b: 0.5109\n",
      "loss: 0.3106\n",
      "Epoch: 30/100, Batch: 253/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1901, b: 0.5109\n",
      "loss: 0.2892\n",
      "Epoch: 30/100, Batch: 254/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.1901, b: 0.5109\n",
      "loss: 0.1647\n",
      "Epoch: 30/100, Batch: 255/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.1901, b: 0.5109\n",
      "loss: 0.2377\n",
      "Epoch: 30/100, Batch: 256/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1902, b: 0.511\n",
      "loss: 0.2268\n",
      "Epoch: 30/100, Batch: 257/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.1902, b: 0.511\n",
      "loss: 0.1963\n",
      "Epoch: 30/100, Batch: 258/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1902, b: 0.511\n",
      "loss: 0.2477\n",
      "Epoch: 30/100, Batch: 259/432, W1: 0.7361, W2: 0.5764, W3: -0.0489, W4: -0.1902, b: 0.511\n",
      "loss: 0.163\n",
      "Epoch: 30/100, Batch: 260/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1903, b: 0.511\n",
      "loss: 0.2825\n",
      "Epoch: 30/100, Batch: 261/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1903, b: 0.511\n",
      "loss: 0.2003\n",
      "Epoch: 30/100, Batch: 262/432, W1: 0.7359, W2: 0.5763, W3: -0.0489, W4: -0.1903, b: 0.511\n",
      "loss: 0.2902\n",
      "Epoch: 30/100, Batch: 263/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1903, b: 0.511\n",
      "loss: 0.2327\n",
      "Epoch: 30/100, Batch: 264/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1904, b: 0.5111\n",
      "loss: 0.2012\n",
      "Epoch: 30/100, Batch: 265/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1904, b: 0.5111\n",
      "loss: 0.2669\n",
      "Epoch: 30/100, Batch: 266/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.1904, b: 0.5111\n",
      "loss: 0.1787\n",
      "Epoch: 30/100, Batch: 267/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.1904, b: 0.5111\n",
      "loss: 0.1586\n",
      "Epoch: 30/100, Batch: 268/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.1904, b: 0.5111\n",
      "loss: 0.2255\n",
      "Epoch: 30/100, Batch: 269/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1905, b: 0.5111\n",
      "loss: 0.2649\n",
      "Epoch: 30/100, Batch: 270/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1905, b: 0.5111\n",
      "loss: 0.2308\n",
      "Epoch: 30/100, Batch: 271/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.1905, b: 0.5111\n",
      "loss: 0.1768\n",
      "Epoch: 30/100, Batch: 272/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.1905, b: 0.5112\n",
      "loss: 0.1802\n",
      "Epoch: 30/100, Batch: 273/432, W1: 0.736, W2: 0.5764, W3: -0.0489, W4: -0.1906, b: 0.5112\n",
      "loss: 0.1421\n",
      "Epoch: 30/100, Batch: 274/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.1906, b: 0.5112\n",
      "loss: 0.2257\n",
      "Epoch: 30/100, Batch: 275/432, W1: 0.7362, W2: 0.5765, W3: -0.0488, W4: -0.1906, b: 0.5112\n",
      "loss: 0.177\n",
      "Epoch: 30/100, Batch: 276/432, W1: 0.7362, W2: 0.5765, W3: -0.0487, W4: -0.1906, b: 0.5113\n",
      "loss: 0.213\n",
      "Epoch: 30/100, Batch: 277/432, W1: 0.7362, W2: 0.5765, W3: -0.0488, W4: -0.1906, b: 0.5113\n",
      "loss: 0.2163\n",
      "Epoch: 30/100, Batch: 278/432, W1: 0.7361, W2: 0.5764, W3: -0.0488, W4: -0.1907, b: 0.5113\n",
      "loss: 0.3391\n",
      "Epoch: 30/100, Batch: 279/432, W1: 0.7359, W2: 0.5763, W3: -0.0489, W4: -0.1907, b: 0.5113\n",
      "loss: 0.2983\n",
      "Epoch: 30/100, Batch: 280/432, W1: 0.7359, W2: 0.5763, W3: -0.0489, W4: -0.1908, b: 0.5113\n",
      "loss: 0.2626\n",
      "Epoch: 30/100, Batch: 281/432, W1: 0.7359, W2: 0.5762, W3: -0.0489, W4: -0.1908, b: 0.5113\n",
      "loss: 0.1834\n",
      "Epoch: 30/100, Batch: 282/432, W1: 0.7359, W2: 0.5762, W3: -0.049, W4: -0.1908, b: 0.5113\n",
      "loss: 0.4575\n",
      "Epoch: 30/100, Batch: 283/432, W1: 0.7359, W2: 0.5763, W3: -0.0489, W4: -0.1908, b: 0.5113\n",
      "loss: 0.1837\n",
      "Epoch: 30/100, Batch: 284/432, W1: 0.7359, W2: 0.5763, W3: -0.0489, W4: -0.1909, b: 0.5113\n",
      "loss: 0.1824\n",
      "Epoch: 30/100, Batch: 285/432, W1: 0.7358, W2: 0.5762, W3: -0.049, W4: -0.1909, b: 0.5113\n",
      "loss: 0.2221\n",
      "Epoch: 30/100, Batch: 286/432, W1: 0.7358, W2: 0.5762, W3: -0.049, W4: -0.1909, b: 0.5113\n",
      "loss: 0.2731\n",
      "Epoch: 30/100, Batch: 287/432, W1: 0.7358, W2: 0.5762, W3: -0.049, W4: -0.191, b: 0.5114\n",
      "loss: 0.1919\n",
      "Epoch: 30/100, Batch: 288/432, W1: 0.7359, W2: 0.5762, W3: -0.049, W4: -0.191, b: 0.5114\n",
      "loss: 0.2339\n",
      "Epoch: 30/100, Batch: 289/432, W1: 0.7359, W2: 0.5763, W3: -0.049, W4: -0.191, b: 0.5114\n",
      "loss: 0.1793\n",
      "Epoch: 30/100, Batch: 290/432, W1: 0.736, W2: 0.5763, W3: -0.0489, W4: -0.191, b: 0.5114\n",
      "loss: 0.1869\n",
      "Epoch: 30/100, Batch: 291/432, W1: 0.736, W2: 0.5764, W3: -0.0488, W4: -0.191, b: 0.5115\n",
      "loss: 0.1992\n",
      "Epoch: 30/100, Batch: 292/432, W1: 0.736, W2: 0.5764, W3: -0.0488, W4: -0.191, b: 0.5115\n",
      "loss: 0.235\n",
      "Epoch: 30/100, Batch: 293/432, W1: 0.7361, W2: 0.5765, W3: -0.0488, W4: -0.1911, b: 0.5115\n",
      "loss: 0.2514\n",
      "Epoch: 30/100, Batch: 294/432, W1: 0.7361, W2: 0.5765, W3: -0.0488, W4: -0.1911, b: 0.5115\n",
      "loss: 0.211\n",
      "Epoch: 30/100, Batch: 295/432, W1: 0.7361, W2: 0.5765, W3: -0.0488, W4: -0.1911, b: 0.5115\n",
      "loss: 0.2403\n",
      "Epoch: 30/100, Batch: 296/432, W1: 0.736, W2: 0.5764, W3: -0.0488, W4: -0.1911, b: 0.5115\n",
      "loss: 0.2492\n",
      "Epoch: 30/100, Batch: 297/432, W1: 0.736, W2: 0.5764, W3: -0.0489, W4: -0.1912, b: 0.5115\n",
      "loss: 0.2716\n",
      "Epoch: 30/100, Batch: 298/432, W1: 0.7361, W2: 0.5765, W3: -0.0488, W4: -0.1912, b: 0.5116\n",
      "loss: 0.1664\n",
      "Epoch: 30/100, Batch: 299/432, W1: 0.7361, W2: 0.5765, W3: -0.0488, W4: -0.1912, b: 0.5116\n",
      "loss: 0.1999\n",
      "Epoch: 30/100, Batch: 300/432, W1: 0.7361, W2: 0.5765, W3: -0.0488, W4: -0.1912, b: 0.5116\n",
      "loss: 0.2109\n",
      "Epoch: 30/100, Batch: 301/432, W1: 0.7362, W2: 0.5766, W3: -0.0487, W4: -0.1912, b: 0.5116\n",
      "loss: 0.2022\n",
      "Epoch: 30/100, Batch: 302/432, W1: 0.7362, W2: 0.5766, W3: -0.0487, W4: -0.1912, b: 0.5116\n",
      "loss: 0.2058\n",
      "Epoch: 30/100, Batch: 303/432, W1: 0.7362, W2: 0.5766, W3: -0.0487, W4: -0.1912, b: 0.5117\n",
      "loss: 1.043\n",
      "Epoch: 30/100, Batch: 304/432, W1: 0.7362, W2: 0.5766, W3: -0.0487, W4: -0.1913, b: 0.5117\n",
      "loss: 0.2222\n",
      "Epoch: 30/100, Batch: 305/432, W1: 0.7362, W2: 0.5766, W3: -0.0487, W4: -0.1913, b: 0.5117\n",
      "loss: 0.2135\n",
      "Epoch: 30/100, Batch: 306/432, W1: 0.7362, W2: 0.5766, W3: -0.0487, W4: -0.1913, b: 0.5117\n",
      "loss: 0.1653\n",
      "Epoch: 30/100, Batch: 307/432, W1: 0.7362, W2: 0.5766, W3: -0.0487, W4: -0.1913, b: 0.5117\n",
      "loss: 0.2368\n",
      "Epoch: 30/100, Batch: 308/432, W1: 0.7362, W2: 0.5766, W3: -0.0487, W4: -0.1914, b: 0.5117\n",
      "loss: 0.2161\n",
      "Epoch: 30/100, Batch: 309/432, W1: 0.7362, W2: 0.5766, W3: -0.0487, W4: -0.1914, b: 0.5118\n",
      "loss: 0.1904\n",
      "Epoch: 30/100, Batch: 310/432, W1: 0.7361, W2: 0.5757, W3: -0.0489, W4: -0.1914, b: 0.5117\n",
      "loss: 2.3703\n",
      "Epoch: 30/100, Batch: 311/432, W1: 0.736, W2: 0.5756, W3: -0.0489, W4: -0.1915, b: 0.5117\n",
      "loss: 0.255\n",
      "Epoch: 30/100, Batch: 312/432, W1: 0.736, W2: 0.5756, W3: -0.0489, W4: -0.1915, b: 0.5118\n",
      "loss: 0.2247\n",
      "Epoch: 30/100, Batch: 313/432, W1: 0.7361, W2: 0.5757, W3: -0.0488, W4: -0.1915, b: 0.5118\n",
      "loss: 0.2162\n",
      "Epoch: 30/100, Batch: 314/432, W1: 0.7361, W2: 0.5757, W3: -0.0488, W4: -0.1915, b: 0.5118\n",
      "loss: 0.2663\n",
      "Epoch: 30/100, Batch: 315/432, W1: 0.7362, W2: 0.5758, W3: -0.0488, W4: -0.1915, b: 0.5118\n",
      "loss: 0.176\n",
      "Epoch: 30/100, Batch: 316/432, W1: 0.7361, W2: 0.5757, W3: -0.0488, W4: -0.1916, b: 0.5118\n",
      "loss: 0.1985\n",
      "Epoch: 30/100, Batch: 317/432, W1: 0.736, W2: 0.5757, W3: -0.0489, W4: -0.1916, b: 0.5118\n",
      "loss: 0.2281\n",
      "Epoch: 30/100, Batch: 318/432, W1: 0.736, W2: 0.5756, W3: -0.0489, W4: -0.1916, b: 0.5119\n",
      "loss: 0.2056\n",
      "Epoch: 30/100, Batch: 319/432, W1: 0.736, W2: 0.5756, W3: -0.0489, W4: -0.1916, b: 0.5119\n",
      "loss: 0.2648\n",
      "Epoch: 30/100, Batch: 320/432, W1: 0.7361, W2: 0.5757, W3: -0.0488, W4: -0.1917, b: 0.5119\n",
      "loss: 0.1744\n",
      "Epoch: 30/100, Batch: 321/432, W1: 0.7361, W2: 0.5757, W3: -0.0488, W4: -0.1917, b: 0.5119\n",
      "loss: 0.2688\n",
      "Epoch: 30/100, Batch: 322/432, W1: 0.736, W2: 0.5757, W3: -0.0489, W4: -0.1917, b: 0.5119\n",
      "loss: 0.2032\n",
      "Epoch: 30/100, Batch: 323/432, W1: 0.7361, W2: 0.5758, W3: -0.0488, W4: -0.1917, b: 0.5119\n",
      "loss: 0.2107\n",
      "Epoch: 30/100, Batch: 324/432, W1: 0.7361, W2: 0.5758, W3: -0.0488, W4: -0.1917, b: 0.512\n",
      "loss: 0.2499\n",
      "Epoch: 30/100, Batch: 325/432, W1: 0.7362, W2: 0.5758, W3: -0.0488, W4: -0.1918, b: 0.512\n",
      "loss: 0.18\n",
      "Epoch: 30/100, Batch: 326/432, W1: 0.7362, W2: 0.5758, W3: -0.0487, W4: -0.1918, b: 0.512\n",
      "loss: 0.2165\n",
      "Epoch: 30/100, Batch: 327/432, W1: 0.7362, W2: 0.5758, W3: -0.0488, W4: -0.1918, b: 0.512\n",
      "loss: 0.2484\n",
      "Epoch: 30/100, Batch: 328/432, W1: 0.7362, W2: 0.5759, W3: -0.0487, W4: -0.1918, b: 0.512\n",
      "loss: 0.1825\n",
      "Epoch: 30/100, Batch: 329/432, W1: 0.7362, W2: 0.5758, W3: -0.0488, W4: -0.1919, b: 0.512\n",
      "loss: 0.1876\n",
      "Epoch: 30/100, Batch: 330/432, W1: 0.7362, W2: 0.5759, W3: -0.0487, W4: -0.1919, b: 0.5121\n",
      "loss: 0.2421\n",
      "Epoch: 30/100, Batch: 331/432, W1: 0.7362, W2: 0.5758, W3: -0.0488, W4: -0.1919, b: 0.5121\n",
      "loss: 0.2277\n",
      "Epoch: 30/100, Batch: 332/432, W1: 0.7362, W2: 0.5759, W3: -0.0487, W4: -0.1919, b: 0.5121\n",
      "loss: 0.1544\n",
      "Epoch: 30/100, Batch: 333/432, W1: 0.7362, W2: 0.5758, W3: -0.0487, W4: -0.192, b: 0.5121\n",
      "loss: 0.2488\n",
      "Epoch: 30/100, Batch: 334/432, W1: 0.7363, W2: 0.5759, W3: -0.0487, W4: -0.192, b: 0.5121\n",
      "loss: 0.2651\n",
      "Epoch: 30/100, Batch: 335/432, W1: 0.7361, W2: 0.5758, W3: -0.0488, W4: -0.192, b: 0.5121\n",
      "loss: 0.2766\n",
      "Epoch: 30/100, Batch: 336/432, W1: 0.7361, W2: 0.5757, W3: -0.0488, W4: -0.1921, b: 0.5121\n",
      "loss: 0.246\n",
      "Epoch: 30/100, Batch: 337/432, W1: 0.7362, W2: 0.5758, W3: -0.0487, W4: -0.1921, b: 0.5122\n",
      "loss: 0.2155\n",
      "Epoch: 30/100, Batch: 338/432, W1: 0.7362, W2: 0.5758, W3: -0.0487, W4: -0.1921, b: 0.5122\n",
      "loss: 0.2163\n",
      "Epoch: 30/100, Batch: 339/432, W1: 0.7362, W2: 0.5759, W3: -0.0487, W4: -0.1921, b: 0.5122\n",
      "loss: 0.1973\n",
      "Epoch: 30/100, Batch: 340/432, W1: 0.7362, W2: 0.5759, W3: -0.0487, W4: -0.1921, b: 0.5122\n",
      "loss: 0.2267\n",
      "Epoch: 30/100, Batch: 341/432, W1: 0.7362, W2: 0.5758, W3: -0.0487, W4: -0.1922, b: 0.5122\n",
      "loss: 0.1947\n",
      "Epoch: 30/100, Batch: 342/432, W1: 0.7363, W2: 0.5759, W3: -0.0487, W4: -0.1922, b: 0.5122\n",
      "loss: 0.1299\n",
      "Epoch: 30/100, Batch: 343/432, W1: 0.7362, W2: 0.5759, W3: -0.0487, W4: -0.1922, b: 0.5123\n",
      "loss: 0.2287\n",
      "Epoch: 30/100, Batch: 344/432, W1: 0.7361, W2: 0.5758, W3: -0.0488, W4: -0.1922, b: 0.5122\n",
      "loss: 0.2619\n",
      "Epoch: 30/100, Batch: 345/432, W1: 0.7362, W2: 0.5758, W3: -0.0487, W4: -0.1923, b: 0.5123\n",
      "loss: 0.1872\n",
      "Epoch: 30/100, Batch: 346/432, W1: 0.7364, W2: 0.576, W3: -0.0486, W4: -0.1922, b: 0.5123\n",
      "loss: 0.1927\n",
      "Epoch: 30/100, Batch: 347/432, W1: 0.7363, W2: 0.576, W3: -0.0486, W4: -0.1923, b: 0.5123\n",
      "loss: 0.1965\n",
      "Epoch: 30/100, Batch: 348/432, W1: 0.7363, W2: 0.576, W3: -0.0486, W4: -0.1923, b: 0.5123\n",
      "loss: 0.1956\n",
      "Epoch: 30/100, Batch: 349/432, W1: 0.7363, W2: 0.576, W3: -0.0486, W4: -0.1923, b: 0.5123\n",
      "loss: 0.2223\n",
      "Epoch: 30/100, Batch: 350/432, W1: 0.7363, W2: 0.5759, W3: -0.0487, W4: -0.1923, b: 0.5124\n",
      "loss: 0.1736\n",
      "Epoch: 30/100, Batch: 351/432, W1: 0.7362, W2: 0.5759, W3: -0.0487, W4: -0.1924, b: 0.5124\n",
      "loss: 0.2106\n",
      "Epoch: 30/100, Batch: 352/432, W1: 0.7361, W2: 0.5758, W3: -0.0488, W4: -0.1924, b: 0.5124\n",
      "loss: 0.2087\n",
      "Epoch: 30/100, Batch: 353/432, W1: 0.7362, W2: 0.5758, W3: -0.0487, W4: -0.1924, b: 0.5124\n",
      "loss: 0.1691\n",
      "Epoch: 30/100, Batch: 354/432, W1: 0.7363, W2: 0.5759, W3: -0.0487, W4: -0.1924, b: 0.5124\n",
      "loss: 0.2375\n",
      "Epoch: 30/100, Batch: 355/432, W1: 0.7362, W2: 0.5758, W3: -0.0487, W4: -0.1925, b: 0.5124\n",
      "loss: 0.2286\n",
      "Epoch: 30/100, Batch: 356/432, W1: 0.7363, W2: 0.5759, W3: -0.0487, W4: -0.1925, b: 0.5124\n",
      "loss: 0.1602\n",
      "Epoch: 30/100, Batch: 357/432, W1: 0.7363, W2: 0.5759, W3: -0.0487, W4: -0.1925, b: 0.5125\n",
      "loss: 0.2037\n",
      "Epoch: 30/100, Batch: 358/432, W1: 0.7362, W2: 0.5759, W3: -0.0487, W4: -0.1925, b: 0.5125\n",
      "loss: 0.2292\n",
      "Epoch: 30/100, Batch: 359/432, W1: 0.7363, W2: 0.576, W3: -0.0486, W4: -0.1926, b: 0.5125\n",
      "loss: 0.2138\n",
      "Epoch: 30/100, Batch: 360/432, W1: 0.7363, W2: 0.576, W3: -0.0486, W4: -0.1926, b: 0.5125\n",
      "loss: 0.21\n",
      "Epoch: 30/100, Batch: 361/432, W1: 0.7364, W2: 0.5761, W3: -0.0486, W4: -0.1926, b: 0.5125\n",
      "loss: 0.1844\n",
      "Epoch: 30/100, Batch: 362/432, W1: 0.7364, W2: 0.576, W3: -0.0486, W4: -0.1926, b: 0.5125\n",
      "loss: 0.2679\n",
      "Epoch: 30/100, Batch: 363/432, W1: 0.7364, W2: 0.5761, W3: -0.0485, W4: -0.1926, b: 0.5126\n",
      "loss: 0.2084\n",
      "Epoch: 30/100, Batch: 364/432, W1: 0.7365, W2: 0.5761, W3: -0.0485, W4: -0.1927, b: 0.5126\n",
      "loss: 0.1957\n",
      "Epoch: 30/100, Batch: 365/432, W1: 0.7365, W2: 0.5762, W3: -0.0485, W4: -0.1927, b: 0.5126\n",
      "loss: 0.2137\n",
      "Epoch: 30/100, Batch: 366/432, W1: 0.7366, W2: 0.5763, W3: -0.0484, W4: -0.1927, b: 0.5126\n",
      "loss: 0.2193\n",
      "Epoch: 30/100, Batch: 367/432, W1: 0.7365, W2: 0.5762, W3: -0.0485, W4: -0.1927, b: 0.5126\n",
      "loss: 0.2702\n",
      "Epoch: 30/100, Batch: 368/432, W1: 0.7366, W2: 0.5763, W3: -0.0484, W4: -0.1927, b: 0.5127\n",
      "loss: 0.1765\n",
      "Epoch: 30/100, Batch: 369/432, W1: 0.7367, W2: 0.5763, W3: -0.0484, W4: -0.1927, b: 0.5127\n",
      "loss: 0.2445\n",
      "Epoch: 30/100, Batch: 370/432, W1: 0.7367, W2: 0.5763, W3: -0.0484, W4: -0.1928, b: 0.5127\n",
      "loss: 0.2275\n",
      "Epoch: 30/100, Batch: 371/432, W1: 0.7367, W2: 0.5764, W3: -0.0484, W4: -0.1928, b: 0.5127\n",
      "loss: 0.197\n",
      "Epoch: 30/100, Batch: 372/432, W1: 0.7368, W2: 0.5764, W3: -0.0483, W4: -0.1928, b: 0.5127\n",
      "loss: 0.1912\n",
      "Epoch: 30/100, Batch: 373/432, W1: 0.7367, W2: 0.5763, W3: -0.0484, W4: -0.1928, b: 0.5127\n",
      "loss: 0.2201\n",
      "Epoch: 30/100, Batch: 374/432, W1: 0.7368, W2: 0.5764, W3: -0.0483, W4: -0.1928, b: 0.5128\n",
      "loss: 0.1742\n",
      "Epoch: 30/100, Batch: 375/432, W1: 0.7368, W2: 0.5765, W3: -0.0483, W4: -0.1929, b: 0.5128\n",
      "loss: 0.2275\n",
      "Epoch: 30/100, Batch: 376/432, W1: 0.7367, W2: 0.5764, W3: -0.0484, W4: -0.1929, b: 0.5128\n",
      "loss: 0.2528\n",
      "Epoch: 30/100, Batch: 377/432, W1: 0.7367, W2: 0.5763, W3: -0.0484, W4: -0.1929, b: 0.5128\n",
      "loss: 0.2201\n",
      "Epoch: 30/100, Batch: 378/432, W1: 0.7367, W2: 0.5764, W3: -0.0484, W4: -0.193, b: 0.5128\n",
      "loss: 0.2202\n",
      "Epoch: 30/100, Batch: 379/432, W1: 0.7367, W2: 0.5764, W3: -0.0484, W4: -0.193, b: 0.5128\n",
      "loss: 0.2407\n",
      "Epoch: 30/100, Batch: 380/432, W1: 0.7366, W2: 0.5763, W3: -0.0484, W4: -0.193, b: 0.5128\n",
      "loss: 0.239\n",
      "Epoch: 30/100, Batch: 381/432, W1: 0.7365, W2: 0.5762, W3: -0.0485, W4: -0.1931, b: 0.5128\n",
      "loss: 0.2119\n",
      "Epoch: 30/100, Batch: 382/432, W1: 0.7365, W2: 0.5762, W3: -0.0485, W4: -0.1931, b: 0.5129\n",
      "loss: 0.1927\n",
      "Epoch: 30/100, Batch: 383/432, W1: 0.7365, W2: 0.5762, W3: -0.0485, W4: -0.1931, b: 0.5129\n",
      "loss: 0.2047\n",
      "Epoch: 30/100, Batch: 384/432, W1: 0.7366, W2: 0.5763, W3: -0.0484, W4: -0.1931, b: 0.5129\n",
      "loss: 0.1909\n",
      "Epoch: 30/100, Batch: 385/432, W1: 0.7366, W2: 0.5763, W3: -0.0484, W4: -0.1931, b: 0.5129\n",
      "loss: 0.2044\n",
      "Epoch: 30/100, Batch: 386/432, W1: 0.7365, W2: 0.5762, W3: -0.0485, W4: -0.1932, b: 0.5129\n",
      "loss: 0.307\n",
      "Epoch: 30/100, Batch: 387/432, W1: 0.7366, W2: 0.5762, W3: -0.0484, W4: -0.1932, b: 0.5129\n",
      "loss: 0.2451\n",
      "Epoch: 30/100, Batch: 388/432, W1: 0.7366, W2: 0.5762, W3: -0.0484, W4: -0.1933, b: 0.513\n",
      "loss: 0.2789\n",
      "Epoch: 30/100, Batch: 389/432, W1: 0.7366, W2: 0.5763, W3: -0.0484, W4: -0.1933, b: 0.513\n",
      "loss: 0.2327\n",
      "Epoch: 30/100, Batch: 390/432, W1: 0.7367, W2: 0.5764, W3: -0.0484, W4: -0.1933, b: 0.513\n",
      "loss: 0.2069\n",
      "Epoch: 30/100, Batch: 391/432, W1: 0.7368, W2: 0.5765, W3: -0.0483, W4: -0.1933, b: 0.513\n",
      "loss: 0.1724\n",
      "Epoch: 30/100, Batch: 392/432, W1: 0.7368, W2: 0.5765, W3: -0.0483, W4: -0.1933, b: 0.5131\n",
      "loss: 0.2395\n",
      "Epoch: 30/100, Batch: 393/432, W1: 0.7368, W2: 0.5764, W3: -0.0483, W4: -0.1933, b: 0.5131\n",
      "loss: 0.1883\n",
      "Epoch: 30/100, Batch: 394/432, W1: 0.7368, W2: 0.5765, W3: -0.0483, W4: -0.1934, b: 0.5131\n",
      "loss: 0.2216\n",
      "Epoch: 30/100, Batch: 395/432, W1: 0.7369, W2: 0.5766, W3: -0.0482, W4: -0.1934, b: 0.5131\n",
      "loss: 0.2307\n",
      "Epoch: 30/100, Batch: 396/432, W1: 0.737, W2: 0.5767, W3: -0.0482, W4: -0.1934, b: 0.5131\n",
      "loss: 0.21\n",
      "Epoch: 30/100, Batch: 397/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1934, b: 0.5132\n",
      "loss: 0.1991\n",
      "Epoch: 30/100, Batch: 398/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1934, b: 0.5132\n",
      "loss: 0.1459\n",
      "Epoch: 30/100, Batch: 399/432, W1: 0.7372, W2: 0.5769, W3: -0.048, W4: -0.1934, b: 0.5132\n",
      "loss: 0.1822\n",
      "Epoch: 30/100, Batch: 400/432, W1: 0.737, W2: 0.5767, W3: -0.0481, W4: -0.1935, b: 0.5132\n",
      "loss: 0.2817\n",
      "Epoch: 30/100, Batch: 401/432, W1: 0.737, W2: 0.5767, W3: -0.0481, W4: -0.1935, b: 0.5132\n",
      "loss: 0.1992\n",
      "Epoch: 30/100, Batch: 402/432, W1: 0.737, W2: 0.5767, W3: -0.0481, W4: -0.1935, b: 0.5132\n",
      "loss: 0.1655\n",
      "Epoch: 30/100, Batch: 403/432, W1: 0.737, W2: 0.5767, W3: -0.0482, W4: -0.1935, b: 0.5132\n",
      "loss: 0.2714\n",
      "Epoch: 30/100, Batch: 404/432, W1: 0.737, W2: 0.5767, W3: -0.0482, W4: -0.1936, b: 0.5133\n",
      "loss: 0.1766\n",
      "Epoch: 30/100, Batch: 405/432, W1: 0.7371, W2: 0.5769, W3: -0.0481, W4: -0.1935, b: 0.5133\n",
      "loss: 0.1409\n",
      "Epoch: 30/100, Batch: 406/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1936, b: 0.5133\n",
      "loss: 0.2948\n",
      "Epoch: 30/100, Batch: 407/432, W1: 0.7372, W2: 0.5769, W3: -0.048, W4: -0.1936, b: 0.5133\n",
      "loss: 0.198\n",
      "Epoch: 30/100, Batch: 408/432, W1: 0.737, W2: 0.5767, W3: -0.0481, W4: -0.1936, b: 0.5133\n",
      "loss: 0.2119\n",
      "Epoch: 30/100, Batch: 409/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1937, b: 0.5133\n",
      "loss: 0.1975\n",
      "Epoch: 30/100, Batch: 410/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1937, b: 0.5134\n",
      "loss: 0.2063\n",
      "Epoch: 30/100, Batch: 411/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1937, b: 0.5134\n",
      "loss: 0.2067\n",
      "Epoch: 30/100, Batch: 412/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1937, b: 0.5134\n",
      "loss: 0.1571\n",
      "Epoch: 30/100, Batch: 413/432, W1: 0.737, W2: 0.5767, W3: -0.0481, W4: -0.1938, b: 0.5134\n",
      "loss: 0.2561\n",
      "Epoch: 30/100, Batch: 414/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1938, b: 0.5134\n",
      "loss: 0.202\n",
      "Epoch: 30/100, Batch: 415/432, W1: 0.7372, W2: 0.5769, W3: -0.048, W4: -0.1938, b: 0.5134\n",
      "loss: 0.2059\n",
      "Epoch: 30/100, Batch: 416/432, W1: 0.7372, W2: 0.5769, W3: -0.048, W4: -0.1938, b: 0.5135\n",
      "loss: 0.1868\n",
      "Epoch: 30/100, Batch: 417/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1939, b: 0.5135\n",
      "loss: 0.2614\n",
      "Epoch: 30/100, Batch: 418/432, W1: 0.7371, W2: 0.5769, W3: -0.048, W4: -0.1939, b: 0.5135\n",
      "loss: 0.1978\n",
      "Epoch: 30/100, Batch: 419/432, W1: 0.737, W2: 0.5768, W3: -0.0481, W4: -0.1939, b: 0.5135\n",
      "loss: 0.2478\n",
      "Epoch: 30/100, Batch: 420/432, W1: 0.737, W2: 0.5767, W3: -0.0481, W4: -0.1939, b: 0.5135\n",
      "loss: 0.2199\n",
      "Epoch: 30/100, Batch: 421/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.194, b: 0.5135\n",
      "loss: 0.2164\n",
      "Epoch: 30/100, Batch: 422/432, W1: 0.7372, W2: 0.5769, W3: -0.048, W4: -0.194, b: 0.5136\n",
      "loss: 0.2204\n",
      "Epoch: 30/100, Batch: 423/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.194, b: 0.5135\n",
      "loss: 0.1898\n",
      "Epoch: 30/100, Batch: 424/432, W1: 0.737, W2: 0.5767, W3: -0.0481, W4: -0.194, b: 0.5135\n",
      "loss: 0.2368\n",
      "Epoch: 30/100, Batch: 425/432, W1: 0.737, W2: 0.5767, W3: -0.0481, W4: -0.1941, b: 0.5136\n",
      "loss: 0.2164\n",
      "Epoch: 30/100, Batch: 426/432, W1: 0.737, W2: 0.5768, W3: -0.0481, W4: -0.1941, b: 0.5136\n",
      "loss: 0.2084\n",
      "Epoch: 30/100, Batch: 427/432, W1: 0.7371, W2: 0.5768, W3: -0.0481, W4: -0.1941, b: 0.5136\n",
      "loss: 0.2067\n",
      "Epoch: 30/100, Batch: 428/432, W1: 0.737, W2: 0.5767, W3: -0.0481, W4: -0.1941, b: 0.5136\n",
      "loss: 0.1851\n",
      "Epoch: 30/100, Batch: 429/432, W1: 0.7371, W2: 0.5768, W3: -0.048, W4: -0.1941, b: 0.5136\n",
      "loss: 0.1884\n",
      "Epoch: 30/100, Batch: 430/432, W1: 0.7372, W2: 0.5769, W3: -0.048, W4: -0.1941, b: 0.5137\n",
      "loss: 0.1771\n",
      "Epoch: 30/100, Batch: 431/432, W1: 0.7371, W2: 0.5768, W3: -0.048, W4: -0.1942, b: 0.5137\n",
      "loss: 0.2189\n",
      "Epoch: 30/100, Batch: 432/432, W1: 0.7371, W2: 0.5769, W3: -0.048, W4: -0.1942, b: 0.5137\n",
      "loss: 0.2332\n",
      "Epoch: 40/100, Batch: 1/432, W1: 0.7496, W2: 0.5589, W3: -0.0361, W4: -0.2804, b: 0.5701\n",
      "loss: 0.2428\n",
      "Epoch: 40/100, Batch: 2/432, W1: 0.7496, W2: 0.5588, W3: -0.0361, W4: -0.2805, b: 0.5701\n",
      "loss: 0.2265\n",
      "Epoch: 40/100, Batch: 3/432, W1: 0.7496, W2: 0.5589, W3: -0.0361, W4: -0.2805, b: 0.5701\n",
      "loss: 0.2038\n",
      "Epoch: 40/100, Batch: 4/432, W1: 0.7491, W2: 0.5554, W3: -0.0366, W4: -0.2806, b: 0.5701\n",
      "loss: 8.7928\n",
      "Epoch: 40/100, Batch: 5/432, W1: 0.7492, W2: 0.5555, W3: -0.0366, W4: -0.2806, b: 0.5701\n",
      "loss: 0.1852\n",
      "Epoch: 40/100, Batch: 6/432, W1: 0.7492, W2: 0.5555, W3: -0.0365, W4: -0.2806, b: 0.5701\n",
      "loss: 0.1704\n",
      "Epoch: 40/100, Batch: 7/432, W1: 0.7493, W2: 0.5556, W3: -0.0365, W4: -0.2806, b: 0.5701\n",
      "loss: 0.1976\n",
      "Epoch: 40/100, Batch: 8/432, W1: 0.7493, W2: 0.5556, W3: -0.0365, W4: -0.2807, b: 0.5701\n",
      "loss: 0.2173\n",
      "Epoch: 40/100, Batch: 9/432, W1: 0.7493, W2: 0.5556, W3: -0.0365, W4: -0.2807, b: 0.5702\n",
      "loss: 0.195\n",
      "Epoch: 40/100, Batch: 10/432, W1: 0.7493, W2: 0.5556, W3: -0.0365, W4: -0.2807, b: 0.5702\n",
      "loss: 0.2016\n",
      "Epoch: 40/100, Batch: 11/432, W1: 0.7494, W2: 0.5557, W3: -0.0364, W4: -0.2807, b: 0.5702\n",
      "loss: 0.1901\n",
      "Epoch: 40/100, Batch: 12/432, W1: 0.7494, W2: 0.5557, W3: -0.0364, W4: -0.2807, b: 0.5702\n",
      "loss: 0.192\n",
      "Epoch: 40/100, Batch: 13/432, W1: 0.7494, W2: 0.5557, W3: -0.0364, W4: -0.2808, b: 0.5702\n",
      "loss: 0.1824\n",
      "Epoch: 40/100, Batch: 14/432, W1: 0.7495, W2: 0.5558, W3: -0.0364, W4: -0.2808, b: 0.5702\n",
      "loss: 0.1575\n",
      "Epoch: 40/100, Batch: 15/432, W1: 0.7495, W2: 0.5558, W3: -0.0363, W4: -0.2808, b: 0.5703\n",
      "loss: 0.1937\n",
      "Epoch: 40/100, Batch: 16/432, W1: 0.7495, W2: 0.5558, W3: -0.0363, W4: -0.2808, b: 0.5703\n",
      "loss: 0.2268\n",
      "Epoch: 40/100, Batch: 17/432, W1: 0.7496, W2: 0.5559, W3: -0.0363, W4: -0.2808, b: 0.5703\n",
      "loss: 0.1216\n",
      "Epoch: 40/100, Batch: 18/432, W1: 0.7497, W2: 0.556, W3: -0.0362, W4: -0.2808, b: 0.5703\n",
      "loss: 0.1546\n",
      "Epoch: 40/100, Batch: 19/432, W1: 0.7497, W2: 0.556, W3: -0.0362, W4: -0.2808, b: 0.5703\n",
      "loss: 0.2009\n",
      "Epoch: 40/100, Batch: 20/432, W1: 0.7497, W2: 0.556, W3: -0.0362, W4: -0.2809, b: 0.5704\n",
      "loss: 0.1531\n",
      "Epoch: 40/100, Batch: 21/432, W1: 0.7496, W2: 0.5559, W3: -0.0363, W4: -0.2809, b: 0.5704\n",
      "loss: 0.221\n",
      "Epoch: 40/100, Batch: 22/432, W1: 0.7497, W2: 0.556, W3: -0.0362, W4: -0.2809, b: 0.5704\n",
      "loss: 0.1461\n",
      "Epoch: 40/100, Batch: 23/432, W1: 0.7497, W2: 0.556, W3: -0.0362, W4: -0.2809, b: 0.5704\n",
      "loss: 0.1491\n",
      "Epoch: 40/100, Batch: 24/432, W1: 0.7496, W2: 0.556, W3: -0.0362, W4: -0.2809, b: 0.5704\n",
      "loss: 0.1773\n",
      "Epoch: 40/100, Batch: 25/432, W1: 0.7496, W2: 0.5559, W3: -0.0362, W4: -0.281, b: 0.5704\n",
      "loss: 0.1916\n",
      "Epoch: 40/100, Batch: 26/432, W1: 0.7496, W2: 0.5559, W3: -0.0363, W4: -0.281, b: 0.5704\n",
      "loss: 0.1952\n",
      "Epoch: 40/100, Batch: 27/432, W1: 0.7496, W2: 0.5559, W3: -0.0362, W4: -0.281, b: 0.5704\n",
      "loss: 0.268\n",
      "Epoch: 40/100, Batch: 28/432, W1: 0.7495, W2: 0.5559, W3: -0.0363, W4: -0.2811, b: 0.5704\n",
      "loss: 0.2191\n",
      "Epoch: 40/100, Batch: 29/432, W1: 0.7496, W2: 0.5559, W3: -0.0362, W4: -0.2811, b: 0.5705\n",
      "loss: 0.245\n",
      "Epoch: 40/100, Batch: 30/432, W1: 0.7497, W2: 0.556, W3: -0.0362, W4: -0.2811, b: 0.5705\n",
      "loss: 0.1288\n",
      "Epoch: 40/100, Batch: 31/432, W1: 0.7497, W2: 0.556, W3: -0.0362, W4: -0.2811, b: 0.5705\n",
      "loss: 0.2135\n",
      "Epoch: 40/100, Batch: 32/432, W1: 0.7497, W2: 0.5561, W3: -0.0362, W4: -0.2811, b: 0.5705\n",
      "loss: 0.1542\n",
      "Epoch: 40/100, Batch: 33/432, W1: 0.7499, W2: 0.5562, W3: -0.0361, W4: -0.2811, b: 0.5706\n",
      "loss: 0.1551\n",
      "Epoch: 40/100, Batch: 34/432, W1: 0.7499, W2: 0.5562, W3: -0.0361, W4: -0.2811, b: 0.5706\n",
      "loss: 0.1921\n",
      "Epoch: 40/100, Batch: 35/432, W1: 0.75, W2: 0.5563, W3: -0.036, W4: -0.2811, b: 0.5706\n",
      "loss: 0.1566\n",
      "Epoch: 40/100, Batch: 36/432, W1: 0.75, W2: 0.5563, W3: -0.036, W4: -0.2812, b: 0.5706\n",
      "loss: 0.2552\n",
      "Epoch: 40/100, Batch: 37/432, W1: 0.75, W2: 0.5564, W3: -0.036, W4: -0.2812, b: 0.5706\n",
      "loss: 0.1592\n",
      "Epoch: 40/100, Batch: 38/432, W1: 0.7502, W2: 0.5565, W3: -0.0359, W4: -0.2812, b: 0.5707\n",
      "loss: 0.1924\n",
      "Epoch: 40/100, Batch: 39/432, W1: 0.7503, W2: 0.5566, W3: -0.0358, W4: -0.2812, b: 0.5707\n",
      "loss: 0.1233\n",
      "Epoch: 40/100, Batch: 40/432, W1: 0.7503, W2: 0.5566, W3: -0.0358, W4: -0.2812, b: 0.5707\n",
      "loss: 0.1712\n",
      "Epoch: 40/100, Batch: 41/432, W1: 0.7503, W2: 0.5567, W3: -0.0358, W4: -0.2812, b: 0.5707\n",
      "loss: 0.1664\n",
      "Epoch: 40/100, Batch: 42/432, W1: 0.7504, W2: 0.5567, W3: -0.0357, W4: -0.2812, b: 0.5707\n",
      "loss: 0.2088\n",
      "Epoch: 40/100, Batch: 43/432, W1: 0.7505, W2: 0.5568, W3: -0.0357, W4: -0.2812, b: 0.5708\n",
      "loss: 0.1502\n",
      "Epoch: 40/100, Batch: 44/432, W1: 0.7505, W2: 0.5568, W3: -0.0357, W4: -0.2812, b: 0.5708\n",
      "loss: 0.1861\n",
      "Epoch: 40/100, Batch: 45/432, W1: 0.7505, W2: 0.5568, W3: -0.0357, W4: -0.2813, b: 0.5708\n",
      "loss: 0.1938\n",
      "Epoch: 40/100, Batch: 46/432, W1: 0.7506, W2: 0.5569, W3: -0.0356, W4: -0.2813, b: 0.5708\n",
      "loss: 0.1448\n",
      "Epoch: 40/100, Batch: 47/432, W1: 0.7506, W2: 0.5569, W3: -0.0356, W4: -0.2813, b: 0.5708\n",
      "loss: 0.1711\n",
      "Epoch: 40/100, Batch: 48/432, W1: 0.7507, W2: 0.557, W3: -0.0356, W4: -0.2813, b: 0.5709\n",
      "loss: 0.1753\n",
      "Epoch: 40/100, Batch: 49/432, W1: 0.7507, W2: 0.5571, W3: -0.0355, W4: -0.2813, b: 0.5709\n",
      "loss: 0.1486\n",
      "Epoch: 40/100, Batch: 50/432, W1: 0.7508, W2: 0.5571, W3: -0.0355, W4: -0.2813, b: 0.5709\n",
      "loss: 0.1579\n",
      "Epoch: 40/100, Batch: 51/432, W1: 0.7508, W2: 0.5571, W3: -0.0355, W4: -0.2813, b: 0.5709\n",
      "loss: 0.1675\n",
      "Epoch: 40/100, Batch: 52/432, W1: 0.7509, W2: 0.5572, W3: -0.0354, W4: -0.2813, b: 0.571\n",
      "loss: 0.1429\n",
      "Epoch: 40/100, Batch: 53/432, W1: 0.751, W2: 0.5573, W3: -0.0354, W4: -0.2813, b: 0.571\n",
      "loss: 0.1931\n",
      "Epoch: 40/100, Batch: 54/432, W1: 0.7509, W2: 0.5572, W3: -0.0354, W4: -0.2814, b: 0.571\n",
      "loss: 0.2383\n",
      "Epoch: 40/100, Batch: 55/432, W1: 0.7509, W2: 0.5572, W3: -0.0354, W4: -0.2814, b: 0.571\n",
      "loss: 0.227\n",
      "Epoch: 40/100, Batch: 56/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2814, b: 0.571\n",
      "loss: 0.2153\n",
      "Epoch: 40/100, Batch: 57/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2815, b: 0.571\n",
      "loss: 0.193\n",
      "Epoch: 40/100, Batch: 58/432, W1: 0.751, W2: 0.5573, W3: -0.0354, W4: -0.2815, b: 0.571\n",
      "loss: 0.1619\n",
      "Epoch: 40/100, Batch: 59/432, W1: 0.751, W2: 0.5573, W3: -0.0354, W4: -0.2815, b: 0.5711\n",
      "loss: 0.1592\n",
      "Epoch: 40/100, Batch: 60/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2815, b: 0.5711\n",
      "loss: 0.1881\n",
      "Epoch: 40/100, Batch: 61/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2815, b: 0.5711\n",
      "loss: 0.2183\n",
      "Epoch: 40/100, Batch: 62/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2815, b: 0.5711\n",
      "loss: 0.1156\n",
      "Epoch: 40/100, Batch: 63/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2816, b: 0.5711\n",
      "loss: 0.2011\n",
      "Epoch: 40/100, Batch: 64/432, W1: 0.7509, W2: 0.5573, W3: -0.0354, W4: -0.2816, b: 0.5711\n",
      "loss: 0.1788\n",
      "Epoch: 40/100, Batch: 65/432, W1: 0.7509, W2: 0.5573, W3: -0.0354, W4: -0.2816, b: 0.5711\n",
      "loss: 0.1835\n",
      "Epoch: 40/100, Batch: 66/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2816, b: 0.5712\n",
      "loss: 0.1482\n",
      "Epoch: 40/100, Batch: 67/432, W1: 0.7511, W2: 0.5574, W3: -0.0353, W4: -0.2816, b: 0.5712\n",
      "loss: 0.1728\n",
      "Epoch: 40/100, Batch: 68/432, W1: 0.7511, W2: 0.5574, W3: -0.0353, W4: -0.2816, b: 0.5712\n",
      "loss: 0.1607\n",
      "Epoch: 40/100, Batch: 69/432, W1: 0.7511, W2: 0.5575, W3: -0.0353, W4: -0.2817, b: 0.5712\n",
      "loss: 0.2241\n",
      "Epoch: 40/100, Batch: 70/432, W1: 0.7511, W2: 0.5574, W3: -0.0353, W4: -0.2817, b: 0.5712\n",
      "loss: 0.1906\n",
      "Epoch: 40/100, Batch: 71/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2817, b: 0.5712\n",
      "loss: 0.2442\n",
      "Epoch: 40/100, Batch: 72/432, W1: 0.7511, W2: 0.5574, W3: -0.0353, W4: -0.2817, b: 0.5712\n",
      "loss: 0.159\n",
      "Epoch: 40/100, Batch: 73/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2818, b: 0.5712\n",
      "loss: 0.1889\n",
      "Epoch: 40/100, Batch: 74/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2818, b: 0.5713\n",
      "loss: 0.1951\n",
      "Epoch: 40/100, Batch: 75/432, W1: 0.7511, W2: 0.5574, W3: -0.0353, W4: -0.2818, b: 0.5713\n",
      "loss: 0.2067\n",
      "Epoch: 40/100, Batch: 76/432, W1: 0.7511, W2: 0.5575, W3: -0.0353, W4: -0.2818, b: 0.5713\n",
      "loss: 0.1684\n",
      "Epoch: 40/100, Batch: 77/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2819, b: 0.5713\n",
      "loss: 0.231\n",
      "Epoch: 40/100, Batch: 78/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2819, b: 0.5713\n",
      "loss: 0.1813\n",
      "Epoch: 40/100, Batch: 79/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2819, b: 0.5713\n",
      "loss: 0.1809\n",
      "Epoch: 40/100, Batch: 80/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2819, b: 0.5713\n",
      "loss: 0.1624\n",
      "Epoch: 40/100, Batch: 81/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.282, b: 0.5713\n",
      "loss: 0.2019\n",
      "Epoch: 40/100, Batch: 82/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.282, b: 0.5714\n",
      "loss: 0.1998\n",
      "Epoch: 40/100, Batch: 83/432, W1: 0.7512, W2: 0.5575, W3: -0.0352, W4: -0.282, b: 0.5714\n",
      "loss: 0.1176\n",
      "Epoch: 40/100, Batch: 84/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.282, b: 0.5714\n",
      "loss: 0.1672\n",
      "Epoch: 40/100, Batch: 85/432, W1: 0.7511, W2: 0.5574, W3: -0.0353, W4: -0.282, b: 0.5714\n",
      "loss: 0.1777\n",
      "Epoch: 40/100, Batch: 86/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.282, b: 0.5714\n",
      "loss: 0.1599\n",
      "Epoch: 40/100, Batch: 87/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2821, b: 0.5714\n",
      "loss: 0.1447\n",
      "Epoch: 40/100, Batch: 88/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2821, b: 0.5714\n",
      "loss: 0.1807\n",
      "Epoch: 40/100, Batch: 89/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2821, b: 0.5714\n",
      "loss: 0.1871\n",
      "Epoch: 40/100, Batch: 90/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2821, b: 0.5715\n",
      "loss: 0.2046\n",
      "Epoch: 40/100, Batch: 91/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2821, b: 0.5715\n",
      "loss: 0.1398\n",
      "Epoch: 40/100, Batch: 92/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2822, b: 0.5715\n",
      "loss: 0.1528\n",
      "Epoch: 40/100, Batch: 93/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2822, b: 0.5715\n",
      "loss: 0.266\n",
      "Epoch: 40/100, Batch: 94/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2822, b: 0.5715\n",
      "loss: 0.173\n",
      "Epoch: 40/100, Batch: 95/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2822, b: 0.5715\n",
      "loss: 0.144\n",
      "Epoch: 40/100, Batch: 96/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2823, b: 0.5715\n",
      "loss: 0.1987\n",
      "Epoch: 40/100, Batch: 97/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2823, b: 0.5715\n",
      "loss: 0.1819\n",
      "Epoch: 40/100, Batch: 98/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2823, b: 0.5716\n",
      "loss: 0.1703\n",
      "Epoch: 40/100, Batch: 99/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2824, b: 0.5715\n",
      "loss: 0.3019\n",
      "Epoch: 40/100, Batch: 100/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2824, b: 0.5716\n",
      "loss: 0.1666\n",
      "Epoch: 40/100, Batch: 101/432, W1: 0.7509, W2: 0.5572, W3: -0.0354, W4: -0.2824, b: 0.5716\n",
      "loss: 0.123\n",
      "Epoch: 40/100, Batch: 102/432, W1: 0.7509, W2: 0.5573, W3: -0.0354, W4: -0.2824, b: 0.5716\n",
      "loss: 0.1965\n",
      "Epoch: 40/100, Batch: 103/432, W1: 0.751, W2: 0.5573, W3: -0.0353, W4: -0.2824, b: 0.5716\n",
      "loss: 0.1828\n",
      "Epoch: 40/100, Batch: 104/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2824, b: 0.5716\n",
      "loss: 0.1575\n",
      "Epoch: 40/100, Batch: 105/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2824, b: 0.5717\n",
      "loss: 0.1637\n",
      "Epoch: 40/100, Batch: 106/432, W1: 0.7511, W2: 0.5574, W3: -0.0352, W4: -0.2825, b: 0.5717\n",
      "loss: 0.1928\n",
      "Epoch: 40/100, Batch: 107/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2825, b: 0.5717\n",
      "loss: 0.1471\n",
      "Epoch: 40/100, Batch: 108/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2825, b: 0.5717\n",
      "loss: 0.1727\n",
      "Epoch: 40/100, Batch: 109/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2825, b: 0.5717\n",
      "loss: 0.1965\n",
      "Epoch: 40/100, Batch: 110/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2825, b: 0.5717\n",
      "loss: 0.2209\n",
      "Epoch: 40/100, Batch: 111/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2826, b: 0.5717\n",
      "loss: 0.2\n",
      "Epoch: 40/100, Batch: 112/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2826, b: 0.5717\n",
      "loss: 0.1513\n",
      "Epoch: 40/100, Batch: 113/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2826, b: 0.5717\n",
      "loss: 0.1887\n",
      "Epoch: 40/100, Batch: 114/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2826, b: 0.5718\n",
      "loss: 0.1929\n",
      "Epoch: 40/100, Batch: 115/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2827, b: 0.5718\n",
      "loss: 0.212\n",
      "Epoch: 40/100, Batch: 116/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2827, b: 0.5718\n",
      "loss: 0.2046\n",
      "Epoch: 40/100, Batch: 117/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2827, b: 0.5718\n",
      "loss: 0.1757\n",
      "Epoch: 40/100, Batch: 118/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2827, b: 0.5718\n",
      "loss: 0.8742\n",
      "Epoch: 40/100, Batch: 119/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2827, b: 0.5719\n",
      "loss: 1.0488\n",
      "Epoch: 40/100, Batch: 120/432, W1: 0.7511, W2: 0.5574, W3: -0.0352, W4: -0.2827, b: 0.5719\n",
      "loss: 0.188\n",
      "Epoch: 40/100, Batch: 121/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2827, b: 0.5719\n",
      "loss: 0.1557\n",
      "Epoch: 40/100, Batch: 122/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2827, b: 0.5719\n",
      "loss: 0.1742\n",
      "Epoch: 40/100, Batch: 123/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2828, b: 0.5719\n",
      "loss: 0.1802\n",
      "Epoch: 40/100, Batch: 124/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2828, b: 0.5719\n",
      "loss: 0.1639\n",
      "Epoch: 40/100, Batch: 125/432, W1: 0.7512, W2: 0.5576, W3: -0.0351, W4: -0.2828, b: 0.572\n",
      "loss: 0.1873\n",
      "Epoch: 40/100, Batch: 126/432, W1: 0.7512, W2: 0.5576, W3: -0.0351, W4: -0.2828, b: 0.572\n",
      "loss: 0.1646\n",
      "Epoch: 40/100, Batch: 127/432, W1: 0.7512, W2: 0.5576, W3: -0.0351, W4: -0.2828, b: 0.572\n",
      "loss: 0.131\n",
      "Epoch: 40/100, Batch: 128/432, W1: 0.7512, W2: 0.5575, W3: -0.0352, W4: -0.2829, b: 0.572\n",
      "loss: 0.2186\n",
      "Epoch: 40/100, Batch: 129/432, W1: 0.7512, W2: 0.5576, W3: -0.0351, W4: -0.2829, b: 0.572\n",
      "loss: 0.1845\n",
      "Epoch: 40/100, Batch: 130/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2829, b: 0.572\n",
      "loss: 0.1937\n",
      "Epoch: 40/100, Batch: 131/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2829, b: 0.572\n",
      "loss: 0.194\n",
      "Epoch: 40/100, Batch: 132/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.283, b: 0.572\n",
      "loss: 0.2507\n",
      "Epoch: 40/100, Batch: 133/432, W1: 0.7512, W2: 0.5575, W3: -0.0352, W4: -0.283, b: 0.5721\n",
      "loss: 0.2177\n",
      "Epoch: 40/100, Batch: 134/432, W1: 0.7512, W2: 0.5575, W3: -0.0352, W4: -0.283, b: 0.5721\n",
      "loss: 0.1965\n",
      "Epoch: 40/100, Batch: 135/432, W1: 0.7513, W2: 0.5576, W3: -0.0351, W4: -0.283, b: 0.5721\n",
      "loss: 0.1811\n",
      "Epoch: 40/100, Batch: 136/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.283, b: 0.5721\n",
      "loss: 0.2422\n",
      "Epoch: 40/100, Batch: 137/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2831, b: 0.5721\n",
      "loss: 0.1663\n",
      "Epoch: 40/100, Batch: 138/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2831, b: 0.5721\n",
      "loss: 0.2735\n",
      "Epoch: 40/100, Batch: 139/432, W1: 0.751, W2: 0.5574, W3: -0.0353, W4: -0.2831, b: 0.5721\n",
      "loss: 0.2044\n",
      "Epoch: 40/100, Batch: 140/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2832, b: 0.5721\n",
      "loss: 0.1968\n",
      "Epoch: 40/100, Batch: 141/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2832, b: 0.5721\n",
      "loss: 0.1957\n",
      "Epoch: 40/100, Batch: 142/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2832, b: 0.5721\n",
      "loss: 0.2414\n",
      "Epoch: 40/100, Batch: 143/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2833, b: 0.5721\n",
      "loss: 0.1387\n",
      "Epoch: 40/100, Batch: 144/432, W1: 0.7506, W2: 0.5571, W3: -0.0355, W4: -0.2833, b: 0.5721\n",
      "loss: 0.2438\n",
      "Epoch: 40/100, Batch: 145/432, W1: 0.7506, W2: 0.557, W3: -0.0355, W4: -0.2833, b: 0.5721\n",
      "loss: 0.1489\n",
      "Epoch: 40/100, Batch: 146/432, W1: 0.7506, W2: 0.557, W3: -0.0355, W4: -0.2834, b: 0.5722\n",
      "loss: 0.2088\n",
      "Epoch: 40/100, Batch: 147/432, W1: 0.7507, W2: 0.5571, W3: -0.0354, W4: -0.2834, b: 0.5722\n",
      "loss: 0.2136\n",
      "Epoch: 40/100, Batch: 148/432, W1: 0.7507, W2: 0.5571, W3: -0.0354, W4: -0.2834, b: 0.5722\n",
      "loss: 0.193\n",
      "Epoch: 40/100, Batch: 149/432, W1: 0.7507, W2: 0.5571, W3: -0.0354, W4: -0.2834, b: 0.5722\n",
      "loss: 0.1773\n",
      "Epoch: 40/100, Batch: 150/432, W1: 0.7507, W2: 0.5571, W3: -0.0354, W4: -0.2834, b: 0.5722\n",
      "loss: 0.1393\n",
      "Epoch: 40/100, Batch: 151/432, W1: 0.7507, W2: 0.5571, W3: -0.0354, W4: -0.2834, b: 0.5722\n",
      "loss: 0.152\n",
      "Epoch: 40/100, Batch: 152/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2834, b: 0.5723\n",
      "loss: 0.2151\n",
      "Epoch: 40/100, Batch: 153/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2835, b: 0.5723\n",
      "loss: 0.1409\n",
      "Epoch: 40/100, Batch: 154/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2835, b: 0.5723\n",
      "loss: 0.1864\n",
      "Epoch: 40/100, Batch: 155/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2835, b: 0.5723\n",
      "loss: 0.1381\n",
      "Epoch: 40/100, Batch: 156/432, W1: 0.7508, W2: 0.5572, W3: -0.0354, W4: -0.2835, b: 0.5723\n",
      "loss: 0.1429\n",
      "Epoch: 40/100, Batch: 157/432, W1: 0.7508, W2: 0.5572, W3: -0.0353, W4: -0.2835, b: 0.5723\n",
      "loss: 0.2148\n",
      "Epoch: 40/100, Batch: 158/432, W1: 0.7508, W2: 0.5572, W3: -0.0353, W4: -0.2836, b: 0.5723\n",
      "loss: 0.1935\n",
      "Epoch: 40/100, Batch: 159/432, W1: 0.7508, W2: 0.5572, W3: -0.0353, W4: -0.2836, b: 0.5723\n",
      "loss: 0.1994\n",
      "Epoch: 40/100, Batch: 160/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2836, b: 0.5724\n",
      "loss: 0.1555\n",
      "Epoch: 40/100, Batch: 161/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2836, b: 0.5724\n",
      "loss: 0.1792\n",
      "Epoch: 40/100, Batch: 162/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2836, b: 0.5724\n",
      "loss: 0.2186\n",
      "Epoch: 40/100, Batch: 163/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2837, b: 0.5724\n",
      "loss: 0.2038\n",
      "Epoch: 40/100, Batch: 164/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2837, b: 0.5724\n",
      "loss: 0.1943\n",
      "Epoch: 40/100, Batch: 165/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2837, b: 0.5724\n",
      "loss: 0.1629\n",
      "Epoch: 40/100, Batch: 166/432, W1: 0.7509, W2: 0.5574, W3: -0.0352, W4: -0.2837, b: 0.5725\n",
      "loss: 0.1742\n",
      "Epoch: 40/100, Batch: 167/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2837, b: 0.5725\n",
      "loss: 0.2314\n",
      "Epoch: 40/100, Batch: 168/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2837, b: 0.5725\n",
      "loss: 0.1667\n",
      "Epoch: 40/100, Batch: 169/432, W1: 0.751, W2: 0.5575, W3: -0.0352, W4: -0.2838, b: 0.5725\n",
      "loss: 0.1863\n",
      "Epoch: 40/100, Batch: 170/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2838, b: 0.5725\n",
      "loss: 0.2148\n",
      "Epoch: 40/100, Batch: 171/432, W1: 0.7511, W2: 0.5575, W3: -0.0352, W4: -0.2838, b: 0.5725\n",
      "loss: 0.1671\n",
      "Epoch: 40/100, Batch: 172/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2838, b: 0.5726\n",
      "loss: 0.2283\n",
      "Epoch: 40/100, Batch: 173/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2838, b: 0.5726\n",
      "loss: 0.1654\n",
      "Epoch: 40/100, Batch: 174/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2838, b: 0.5726\n",
      "loss: 0.1326\n",
      "Epoch: 40/100, Batch: 175/432, W1: 0.7512, W2: 0.5576, W3: -0.0351, W4: -0.2839, b: 0.5726\n",
      "loss: 0.1683\n",
      "Epoch: 40/100, Batch: 176/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2839, b: 0.5726\n",
      "loss: 0.1798\n",
      "Epoch: 40/100, Batch: 177/432, W1: 0.7512, W2: 0.5576, W3: -0.0351, W4: -0.2839, b: 0.5726\n",
      "loss: 0.1808\n",
      "Epoch: 40/100, Batch: 178/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2839, b: 0.5726\n",
      "loss: 0.2062\n",
      "Epoch: 40/100, Batch: 179/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.284, b: 0.5727\n",
      "loss: 0.1999\n",
      "Epoch: 40/100, Batch: 180/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.284, b: 0.5727\n",
      "loss: 0.1534\n",
      "Epoch: 40/100, Batch: 181/432, W1: 0.7511, W2: 0.5575, W3: -0.0351, W4: -0.284, b: 0.5727\n",
      "loss: 0.2015\n",
      "Epoch: 40/100, Batch: 182/432, W1: 0.7511, W2: 0.5575, W3: -0.0351, W4: -0.284, b: 0.5727\n",
      "loss: 0.1942\n",
      "Epoch: 40/100, Batch: 183/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2841, b: 0.5727\n",
      "loss: 0.22\n",
      "Epoch: 40/100, Batch: 184/432, W1: 0.751, W2: 0.5575, W3: -0.0352, W4: -0.2841, b: 0.5727\n",
      "loss: 0.2238\n",
      "Epoch: 40/100, Batch: 185/432, W1: 0.751, W2: 0.5575, W3: -0.0352, W4: -0.2841, b: 0.5727\n",
      "loss: 0.1647\n",
      "Epoch: 40/100, Batch: 186/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2841, b: 0.5727\n",
      "loss: 0.1639\n",
      "Epoch: 40/100, Batch: 187/432, W1: 0.7509, W2: 0.5574, W3: -0.0352, W4: -0.2842, b: 0.5727\n",
      "loss: 0.2184\n",
      "Epoch: 40/100, Batch: 188/432, W1: 0.7509, W2: 0.5573, W3: -0.0352, W4: -0.2842, b: 0.5727\n",
      "loss: 0.2383\n",
      "Epoch: 40/100, Batch: 189/432, W1: 0.7508, W2: 0.5573, W3: -0.0353, W4: -0.2842, b: 0.5727\n",
      "loss: 0.2147\n",
      "Epoch: 40/100, Batch: 190/432, W1: 0.7508, W2: 0.5572, W3: -0.0353, W4: -0.2843, b: 0.5727\n",
      "loss: 0.2188\n",
      "Epoch: 40/100, Batch: 191/432, W1: 0.7507, W2: 0.5572, W3: -0.0354, W4: -0.2843, b: 0.5727\n",
      "loss: 0.2271\n",
      "Epoch: 40/100, Batch: 192/432, W1: 0.7508, W2: 0.5573, W3: -0.0353, W4: -0.2843, b: 0.5728\n",
      "loss: 0.15\n",
      "Epoch: 40/100, Batch: 193/432, W1: 0.7509, W2: 0.5573, W3: -0.0353, W4: -0.2843, b: 0.5728\n",
      "loss: 0.1974\n",
      "Epoch: 40/100, Batch: 194/432, W1: 0.7509, W2: 0.5573, W3: -0.0352, W4: -0.2843, b: 0.5728\n",
      "loss: 0.1757\n",
      "Epoch: 40/100, Batch: 195/432, W1: 0.7508, W2: 0.5573, W3: -0.0353, W4: -0.2844, b: 0.5728\n",
      "loss: 0.1805\n",
      "Epoch: 40/100, Batch: 196/432, W1: 0.7509, W2: 0.5573, W3: -0.0352, W4: -0.2844, b: 0.5728\n",
      "loss: 0.1801\n",
      "Epoch: 40/100, Batch: 197/432, W1: 0.7509, W2: 0.5574, W3: -0.0352, W4: -0.2844, b: 0.5729\n",
      "loss: 0.1769\n",
      "Epoch: 40/100, Batch: 198/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2844, b: 0.5729\n",
      "loss: 0.2086\n",
      "Epoch: 40/100, Batch: 199/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2844, b: 0.5729\n",
      "loss: 0.1744\n",
      "Epoch: 40/100, Batch: 200/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2844, b: 0.5729\n",
      "loss: 0.1781\n",
      "Epoch: 40/100, Batch: 201/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2844, b: 0.5729\n",
      "loss: 0.1407\n",
      "Epoch: 40/100, Batch: 202/432, W1: 0.7511, W2: 0.5575, W3: -0.0351, W4: -0.2845, b: 0.5729\n",
      "loss: 0.148\n",
      "Epoch: 40/100, Batch: 203/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2845, b: 0.573\n",
      "loss: 0.1886\n",
      "Epoch: 40/100, Batch: 204/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2845, b: 0.573\n",
      "loss: 0.2144\n",
      "Epoch: 40/100, Batch: 205/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2845, b: 0.573\n",
      "loss: 0.152\n",
      "Epoch: 40/100, Batch: 206/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2845, b: 0.573\n",
      "loss: 0.1649\n",
      "Epoch: 40/100, Batch: 207/432, W1: 0.751, W2: 0.5575, W3: -0.0351, W4: -0.2846, b: 0.573\n",
      "loss: 0.2305\n",
      "Epoch: 40/100, Batch: 208/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2846, b: 0.573\n",
      "loss: 0.2048\n",
      "Epoch: 40/100, Batch: 209/432, W1: 0.7509, W2: 0.5574, W3: -0.0352, W4: -0.2847, b: 0.573\n",
      "loss: 0.1761\n",
      "Epoch: 40/100, Batch: 210/432, W1: 0.751, W2: 0.5575, W3: -0.0352, W4: -0.2847, b: 0.573\n",
      "loss: 0.1632\n",
      "Epoch: 40/100, Batch: 211/432, W1: 0.751, W2: 0.5575, W3: -0.0352, W4: -0.2847, b: 0.573\n",
      "loss: 0.1615\n",
      "Epoch: 40/100, Batch: 212/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2847, b: 0.5731\n",
      "loss: 0.1267\n",
      "Epoch: 40/100, Batch: 213/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2847, b: 0.5731\n",
      "loss: 0.2095\n",
      "Epoch: 40/100, Batch: 214/432, W1: 0.751, W2: 0.5575, W3: -0.0351, W4: -0.2847, b: 0.5731\n",
      "loss: 0.1504\n",
      "Epoch: 40/100, Batch: 215/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2847, b: 0.5731\n",
      "loss: 0.1493\n",
      "Epoch: 40/100, Batch: 216/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2848, b: 0.5731\n",
      "loss: 0.1745\n",
      "Epoch: 40/100, Batch: 217/432, W1: 0.7512, W2: 0.5576, W3: -0.0351, W4: -0.2848, b: 0.5732\n",
      "loss: 0.1626\n",
      "Epoch: 40/100, Batch: 218/432, W1: 0.7512, W2: 0.5577, W3: -0.035, W4: -0.2848, b: 0.5732\n",
      "loss: 1.0396\n",
      "Epoch: 40/100, Batch: 219/432, W1: 0.7512, W2: 0.5577, W3: -0.035, W4: -0.2848, b: 0.5732\n",
      "loss: 0.1324\n",
      "Epoch: 40/100, Batch: 220/432, W1: 0.7512, W2: 0.5577, W3: -0.035, W4: -0.2848, b: 0.5732\n",
      "loss: 0.1552\n",
      "Epoch: 40/100, Batch: 221/432, W1: 0.7512, W2: 0.5577, W3: -0.035, W4: -0.2848, b: 0.5732\n",
      "loss: 0.1944\n",
      "Epoch: 40/100, Batch: 222/432, W1: 0.7513, W2: 0.5578, W3: -0.035, W4: -0.2848, b: 0.5733\n",
      "loss: 0.1527\n",
      "Epoch: 40/100, Batch: 223/432, W1: 0.7513, W2: 0.5578, W3: -0.035, W4: -0.2848, b: 0.5733\n",
      "loss: 0.154\n",
      "Epoch: 40/100, Batch: 224/432, W1: 0.7513, W2: 0.5578, W3: -0.035, W4: -0.2849, b: 0.5733\n",
      "loss: 0.1268\n",
      "Epoch: 40/100, Batch: 225/432, W1: 0.7513, W2: 0.5578, W3: -0.035, W4: -0.2849, b: 0.5733\n",
      "loss: 0.2017\n",
      "Epoch: 40/100, Batch: 226/432, W1: 0.7512, W2: 0.5577, W3: -0.035, W4: -0.2849, b: 0.5733\n",
      "loss: 0.2235\n",
      "Epoch: 40/100, Batch: 227/432, W1: 0.7512, W2: 0.5577, W3: -0.035, W4: -0.2849, b: 0.5733\n",
      "loss: 0.1394\n",
      "Epoch: 40/100, Batch: 228/432, W1: 0.7512, W2: 0.5577, W3: -0.035, W4: -0.285, b: 0.5733\n",
      "loss: 0.2426\n",
      "Epoch: 40/100, Batch: 229/432, W1: 0.7512, W2: 0.5576, W3: -0.035, W4: -0.285, b: 0.5733\n",
      "loss: 0.1745\n",
      "Epoch: 40/100, Batch: 230/432, W1: 0.7512, W2: 0.5576, W3: -0.035, W4: -0.285, b: 0.5733\n",
      "loss: 0.1742\n",
      "Epoch: 40/100, Batch: 231/432, W1: 0.7512, W2: 0.5577, W3: -0.035, W4: -0.285, b: 0.5733\n",
      "loss: 0.2198\n",
      "Epoch: 40/100, Batch: 232/432, W1: 0.7511, W2: 0.5576, W3: -0.035, W4: -0.2851, b: 0.5734\n",
      "loss: 0.2374\n",
      "Epoch: 40/100, Batch: 233/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.285, b: 0.5734\n",
      "loss: 1.12\n",
      "Epoch: 40/100, Batch: 234/432, W1: 0.7511, W2: 0.5576, W3: -0.0351, W4: -0.2851, b: 0.5734\n",
      "loss: 0.2688\n",
      "Epoch: 40/100, Batch: 235/432, W1: 0.751, W2: 0.5575, W3: -0.0351, W4: -0.2851, b: 0.5734\n",
      "loss: 0.201\n",
      "Epoch: 40/100, Batch: 236/432, W1: 0.751, W2: 0.5575, W3: -0.0351, W4: -0.2851, b: 0.5734\n",
      "loss: 0.1744\n",
      "Epoch: 40/100, Batch: 237/432, W1: 0.751, W2: 0.5575, W3: -0.0351, W4: -0.2852, b: 0.5734\n",
      "loss: 0.1602\n",
      "Epoch: 40/100, Batch: 238/432, W1: 0.751, W2: 0.5574, W3: -0.0352, W4: -0.2852, b: 0.5734\n",
      "loss: 0.1887\n",
      "Epoch: 40/100, Batch: 239/432, W1: 0.751, W2: 0.5575, W3: -0.0351, W4: -0.2852, b: 0.5734\n",
      "loss: 0.2496\n",
      "Epoch: 40/100, Batch: 240/432, W1: 0.7509, W2: 0.5574, W3: -0.0352, W4: -0.2852, b: 0.5734\n",
      "loss: 0.2104\n",
      "Epoch: 40/100, Batch: 241/432, W1: 0.7509, W2: 0.5574, W3: -0.0352, W4: -0.2853, b: 0.5735\n",
      "loss: 0.2291\n",
      "Epoch: 40/100, Batch: 242/432, W1: 0.7508, W2: 0.5573, W3: -0.0353, W4: -0.2853, b: 0.5735\n",
      "loss: 0.1888\n",
      "Epoch: 40/100, Batch: 243/432, W1: 0.7509, W2: 0.5574, W3: -0.0352, W4: -0.2853, b: 0.5735\n",
      "loss: 0.138\n",
      "Epoch: 40/100, Batch: 244/432, W1: 0.7509, W2: 0.5574, W3: -0.0352, W4: -0.2853, b: 0.5735\n",
      "loss: 0.1707\n",
      "Epoch: 40/100, Batch: 245/432, W1: 0.7508, W2: 0.5573, W3: -0.0352, W4: -0.2854, b: 0.5735\n",
      "loss: 0.2188\n",
      "Epoch: 40/100, Batch: 246/432, W1: 0.7508, W2: 0.5573, W3: -0.0353, W4: -0.2854, b: 0.5735\n",
      "loss: 0.1912\n",
      "Epoch: 40/100, Batch: 247/432, W1: 0.7507, W2: 0.5572, W3: -0.0353, W4: -0.2854, b: 0.5735\n",
      "loss: 0.1918\n",
      "Epoch: 40/100, Batch: 248/432, W1: 0.7508, W2: 0.5573, W3: -0.0352, W4: -0.2854, b: 0.5735\n",
      "loss: 0.1731\n",
      "Epoch: 40/100, Batch: 249/432, W1: 0.7508, W2: 0.5573, W3: -0.0353, W4: -0.2855, b: 0.5735\n",
      "loss: 0.2103\n",
      "Epoch: 40/100, Batch: 250/432, W1: 0.7507, W2: 0.5572, W3: -0.0353, W4: -0.2855, b: 0.5735\n",
      "loss: 0.2012\n",
      "Epoch: 40/100, Batch: 251/432, W1: 0.7507, W2: 0.5571, W3: -0.0353, W4: -0.2855, b: 0.5736\n",
      "loss: 0.241\n",
      "Epoch: 40/100, Batch: 252/432, W1: 0.7506, W2: 0.5571, W3: -0.0353, W4: -0.2856, b: 0.5736\n",
      "loss: 0.1822\n",
      "Epoch: 40/100, Batch: 253/432, W1: 0.7507, W2: 0.5572, W3: -0.0353, W4: -0.2856, b: 0.5736\n",
      "loss: 0.1812\n",
      "Epoch: 40/100, Batch: 254/432, W1: 0.7507, W2: 0.5572, W3: -0.0353, W4: -0.2856, b: 0.5736\n",
      "loss: 0.2399\n",
      "Epoch: 40/100, Batch: 255/432, W1: 0.7506, W2: 0.5571, W3: -0.0353, W4: -0.2856, b: 0.5736\n",
      "loss: 0.2407\n",
      "Epoch: 40/100, Batch: 256/432, W1: 0.7506, W2: 0.5571, W3: -0.0353, W4: -0.2857, b: 0.5736\n",
      "loss: 0.1823\n",
      "Epoch: 40/100, Batch: 257/432, W1: 0.7506, W2: 0.5572, W3: -0.0353, W4: -0.2857, b: 0.5736\n",
      "loss: 0.1778\n",
      "Epoch: 40/100, Batch: 258/432, W1: 0.7507, W2: 0.5572, W3: -0.0353, W4: -0.2857, b: 0.5736\n",
      "loss: 0.1731\n",
      "Epoch: 40/100, Batch: 259/432, W1: 0.7507, W2: 0.5572, W3: -0.0352, W4: -0.2857, b: 0.5737\n",
      "loss: 0.2521\n",
      "Epoch: 40/100, Batch: 260/432, W1: 0.7507, W2: 0.5572, W3: -0.0352, W4: -0.2857, b: 0.5737\n",
      "loss: 0.1706\n",
      "Epoch: 40/100, Batch: 261/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2857, b: 0.5737\n",
      "loss: 0.1524\n",
      "Epoch: 40/100, Batch: 262/432, W1: 0.7507, W2: 0.5572, W3: -0.0352, W4: -0.2858, b: 0.5737\n",
      "loss: 0.2403\n",
      "Epoch: 40/100, Batch: 263/432, W1: 0.7507, W2: 0.5572, W3: -0.0352, W4: -0.2858, b: 0.5737\n",
      "loss: 0.2107\n",
      "Epoch: 40/100, Batch: 264/432, W1: 0.7507, W2: 0.5572, W3: -0.0352, W4: -0.2858, b: 0.5737\n",
      "loss: 0.194\n",
      "Epoch: 40/100, Batch: 265/432, W1: 0.7506, W2: 0.5571, W3: -0.0352, W4: -0.2859, b: 0.5737\n",
      "loss: 0.1953\n",
      "Epoch: 40/100, Batch: 266/432, W1: 0.7507, W2: 0.5572, W3: -0.0352, W4: -0.2859, b: 0.5738\n",
      "loss: 0.1414\n",
      "Epoch: 40/100, Batch: 267/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2859, b: 0.5738\n",
      "loss: 0.1846\n",
      "Epoch: 40/100, Batch: 268/432, W1: 0.7509, W2: 0.5574, W3: -0.0351, W4: -0.2859, b: 0.5738\n",
      "loss: 0.1732\n",
      "Epoch: 40/100, Batch: 269/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2859, b: 0.5738\n",
      "loss: 0.2002\n",
      "Epoch: 40/100, Batch: 270/432, W1: 0.7509, W2: 0.5574, W3: -0.0351, W4: -0.2859, b: 0.5738\n",
      "loss: 0.2106\n",
      "Epoch: 40/100, Batch: 271/432, W1: 0.7509, W2: 0.5574, W3: -0.0351, W4: -0.286, b: 0.5739\n",
      "loss: 0.2202\n",
      "Epoch: 40/100, Batch: 272/432, W1: 0.7509, W2: 0.5575, W3: -0.035, W4: -0.286, b: 0.5739\n",
      "loss: 0.1616\n",
      "Epoch: 40/100, Batch: 273/432, W1: 0.7509, W2: 0.5574, W3: -0.035, W4: -0.286, b: 0.5739\n",
      "loss: 0.2317\n",
      "Epoch: 40/100, Batch: 274/432, W1: 0.7509, W2: 0.5574, W3: -0.035, W4: -0.286, b: 0.5739\n",
      "loss: 0.1728\n",
      "Epoch: 40/100, Batch: 275/432, W1: 0.7509, W2: 0.5574, W3: -0.035, W4: -0.286, b: 0.5739\n",
      "loss: 0.1687\n",
      "Epoch: 40/100, Batch: 276/432, W1: 0.7509, W2: 0.5574, W3: -0.0351, W4: -0.2861, b: 0.5739\n",
      "loss: 0.2017\n",
      "Epoch: 40/100, Batch: 277/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2861, b: 0.5739\n",
      "loss: 0.2169\n",
      "Epoch: 40/100, Batch: 278/432, W1: 0.7508, W2: 0.5574, W3: -0.0351, W4: -0.2861, b: 0.574\n",
      "loss: 0.218\n",
      "Epoch: 40/100, Batch: 279/432, W1: 0.7508, W2: 0.5574, W3: -0.0351, W4: -0.2861, b: 0.574\n",
      "loss: 0.2008\n",
      "Epoch: 40/100, Batch: 280/432, W1: 0.7509, W2: 0.5574, W3: -0.0351, W4: -0.2862, b: 0.574\n",
      "loss: 0.1446\n",
      "Epoch: 40/100, Batch: 281/432, W1: 0.7509, W2: 0.5574, W3: -0.035, W4: -0.2862, b: 0.574\n",
      "loss: 0.1618\n",
      "Epoch: 40/100, Batch: 282/432, W1: 0.7509, W2: 0.5575, W3: -0.035, W4: -0.2862, b: 0.574\n",
      "loss: 0.181\n",
      "Epoch: 40/100, Batch: 283/432, W1: 0.751, W2: 0.5576, W3: -0.0349, W4: -0.2862, b: 0.574\n",
      "loss: 0.1694\n",
      "Epoch: 40/100, Batch: 284/432, W1: 0.751, W2: 0.5576, W3: -0.0349, W4: -0.2862, b: 0.5741\n",
      "loss: 0.1952\n",
      "Epoch: 40/100, Batch: 285/432, W1: 0.751, W2: 0.5575, W3: -0.0349, W4: -0.2862, b: 0.5741\n",
      "loss: 0.1939\n",
      "Epoch: 40/100, Batch: 286/432, W1: 0.751, W2: 0.5575, W3: -0.035, W4: -0.2863, b: 0.5741\n",
      "loss: 0.2109\n",
      "Epoch: 40/100, Batch: 287/432, W1: 0.751, W2: 0.5575, W3: -0.035, W4: -0.2863, b: 0.5741\n",
      "loss: 0.1544\n",
      "Epoch: 40/100, Batch: 288/432, W1: 0.7509, W2: 0.5575, W3: -0.035, W4: -0.2863, b: 0.5741\n",
      "loss: 0.2146\n",
      "Epoch: 40/100, Batch: 289/432, W1: 0.7509, W2: 0.5574, W3: -0.035, W4: -0.2864, b: 0.5741\n",
      "loss: 0.2235\n",
      "Epoch: 40/100, Batch: 290/432, W1: 0.7508, W2: 0.5574, W3: -0.0351, W4: -0.2864, b: 0.5741\n",
      "loss: 0.1561\n",
      "Epoch: 40/100, Batch: 291/432, W1: 0.7509, W2: 0.5574, W3: -0.035, W4: -0.2864, b: 0.5741\n",
      "loss: 0.2033\n",
      "Epoch: 40/100, Batch: 292/432, W1: 0.7508, W2: 0.5574, W3: -0.0351, W4: -0.2864, b: 0.5741\n",
      "loss: 0.1735\n",
      "Epoch: 40/100, Batch: 293/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2865, b: 0.5741\n",
      "loss: 0.2642\n",
      "Epoch: 40/100, Batch: 294/432, W1: 0.7508, W2: 0.5574, W3: -0.035, W4: -0.2865, b: 0.5742\n",
      "loss: 0.1368\n",
      "Epoch: 40/100, Batch: 295/432, W1: 0.7508, W2: 0.5574, W3: -0.035, W4: -0.2865, b: 0.5742\n",
      "loss: 0.1498\n",
      "Epoch: 40/100, Batch: 296/432, W1: 0.7508, W2: 0.5574, W3: -0.035, W4: -0.2865, b: 0.5742\n",
      "loss: 0.1909\n",
      "Epoch: 40/100, Batch: 297/432, W1: 0.7508, W2: 0.5574, W3: -0.035, W4: -0.2865, b: 0.5742\n",
      "loss: 0.2116\n",
      "Epoch: 40/100, Batch: 298/432, W1: 0.7508, W2: 0.5574, W3: -0.035, W4: -0.2866, b: 0.5742\n",
      "loss: 0.2274\n",
      "Epoch: 40/100, Batch: 299/432, W1: 0.7508, W2: 0.5574, W3: -0.035, W4: -0.2866, b: 0.5742\n",
      "loss: 0.2087\n",
      "Epoch: 40/100, Batch: 300/432, W1: 0.7509, W2: 0.5574, W3: -0.035, W4: -0.2866, b: 0.5742\n",
      "loss: 0.2161\n",
      "Epoch: 40/100, Batch: 301/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2866, b: 0.5742\n",
      "loss: 0.1987\n",
      "Epoch: 40/100, Batch: 302/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2867, b: 0.5743\n",
      "loss: 0.1575\n",
      "Epoch: 40/100, Batch: 303/432, W1: 0.7508, W2: 0.5574, W3: -0.035, W4: -0.2867, b: 0.5743\n",
      "loss: 0.1705\n",
      "Epoch: 40/100, Batch: 304/432, W1: 0.7508, W2: 0.5574, W3: -0.035, W4: -0.2867, b: 0.5743\n",
      "loss: 0.1849\n",
      "Epoch: 40/100, Batch: 305/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2867, b: 0.5743\n",
      "loss: 0.1643\n",
      "Epoch: 40/100, Batch: 306/432, W1: 0.7507, W2: 0.5573, W3: -0.0351, W4: -0.2867, b: 0.5743\n",
      "loss: 0.1658\n",
      "Epoch: 40/100, Batch: 307/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2868, b: 0.5743\n",
      "loss: 0.1832\n",
      "Epoch: 40/100, Batch: 308/432, W1: 0.7508, W2: 0.5573, W3: -0.035, W4: -0.2868, b: 0.5743\n",
      "loss: 0.1993\n",
      "Epoch: 40/100, Batch: 309/432, W1: 0.7508, W2: 0.5573, W3: -0.035, W4: -0.2868, b: 0.5743\n",
      "loss: 0.1441\n",
      "Epoch: 40/100, Batch: 310/432, W1: 0.7509, W2: 0.5574, W3: -0.035, W4: -0.2868, b: 0.5744\n",
      "loss: 0.1592\n",
      "Epoch: 40/100, Batch: 311/432, W1: 0.7508, W2: 0.5574, W3: -0.035, W4: -0.2868, b: 0.5744\n",
      "loss: 0.222\n",
      "Epoch: 40/100, Batch: 312/432, W1: 0.7508, W2: 0.5573, W3: -0.0351, W4: -0.2869, b: 0.5744\n",
      "loss: 0.2972\n",
      "Epoch: 40/100, Batch: 313/432, W1: 0.7507, W2: 0.5572, W3: -0.0351, W4: -0.2869, b: 0.5744\n",
      "loss: 0.2015\n",
      "Epoch: 40/100, Batch: 314/432, W1: 0.7505, W2: 0.5563, W3: -0.0353, W4: -0.287, b: 0.5744\n",
      "loss: 2.1796\n",
      "Epoch: 40/100, Batch: 315/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.287, b: 0.5744\n",
      "loss: 0.1804\n",
      "Epoch: 40/100, Batch: 316/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.287, b: 0.5744\n",
      "loss: 0.1985\n",
      "Epoch: 40/100, Batch: 317/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.287, b: 0.5744\n",
      "loss: 0.1853\n",
      "Epoch: 40/100, Batch: 318/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.2871, b: 0.5744\n",
      "loss: 0.1873\n",
      "Epoch: 40/100, Batch: 319/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.2871, b: 0.5744\n",
      "loss: 0.1467\n",
      "Epoch: 40/100, Batch: 320/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.2871, b: 0.5744\n",
      "loss: 0.1981\n",
      "Epoch: 40/100, Batch: 321/432, W1: 0.7505, W2: 0.5563, W3: -0.0353, W4: -0.2871, b: 0.5744\n",
      "loss: 0.1974\n",
      "Epoch: 40/100, Batch: 322/432, W1: 0.7506, W2: 0.5564, W3: -0.0352, W4: -0.2871, b: 0.5745\n",
      "loss: 0.1666\n",
      "Epoch: 40/100, Batch: 323/432, W1: 0.7505, W2: 0.5563, W3: -0.0353, W4: -0.2871, b: 0.5745\n",
      "loss: 0.1575\n",
      "Epoch: 40/100, Batch: 324/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.2872, b: 0.5745\n",
      "loss: 0.2245\n",
      "Epoch: 40/100, Batch: 325/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.2872, b: 0.5745\n",
      "loss: 0.1497\n",
      "Epoch: 40/100, Batch: 326/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.2872, b: 0.5745\n",
      "loss: 0.1913\n",
      "Epoch: 40/100, Batch: 327/432, W1: 0.7504, W2: 0.5562, W3: -0.0353, W4: -0.2872, b: 0.5745\n",
      "loss: 0.208\n",
      "Epoch: 40/100, Batch: 328/432, W1: 0.7505, W2: 0.5563, W3: -0.0353, W4: -0.2873, b: 0.5745\n",
      "loss: 0.1999\n",
      "Epoch: 40/100, Batch: 329/432, W1: 0.7505, W2: 0.5564, W3: -0.0352, W4: -0.2873, b: 0.5746\n",
      "loss: 0.4051\n",
      "Epoch: 40/100, Batch: 330/432, W1: 0.7505, W2: 0.5564, W3: -0.0352, W4: -0.2873, b: 0.5746\n",
      "loss: 0.7222\n",
      "Epoch: 40/100, Batch: 331/432, W1: 0.7506, W2: 0.5565, W3: -0.0352, W4: -0.2873, b: 0.5746\n",
      "loss: 0.1863\n",
      "Epoch: 40/100, Batch: 332/432, W1: 0.7506, W2: 0.5565, W3: -0.0351, W4: -0.2873, b: 0.5746\n",
      "loss: 0.1528\n",
      "Epoch: 40/100, Batch: 333/432, W1: 0.7506, W2: 0.5565, W3: -0.0352, W4: -0.2873, b: 0.5746\n",
      "loss: 0.1951\n",
      "Epoch: 40/100, Batch: 334/432, W1: 0.7507, W2: 0.5565, W3: -0.0351, W4: -0.2873, b: 0.5747\n",
      "loss: 0.2212\n",
      "Epoch: 40/100, Batch: 335/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.2873, b: 0.5747\n",
      "loss: 0.1805\n",
      "Epoch: 40/100, Batch: 336/432, W1: 0.7507, W2: 0.5565, W3: -0.0351, W4: -0.2874, b: 0.5747\n",
      "loss: 0.1829\n",
      "Epoch: 40/100, Batch: 337/432, W1: 0.7506, W2: 0.5565, W3: -0.0352, W4: -0.2874, b: 0.5747\n",
      "loss: 0.1888\n",
      "Epoch: 40/100, Batch: 338/432, W1: 0.7506, W2: 0.5565, W3: -0.0351, W4: -0.2874, b: 0.5747\n",
      "loss: 0.1588\n",
      "Epoch: 40/100, Batch: 339/432, W1: 0.7506, W2: 0.5565, W3: -0.0352, W4: -0.2874, b: 0.5747\n",
      "loss: 0.1975\n",
      "Epoch: 40/100, Batch: 340/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.2875, b: 0.5747\n",
      "loss: 0.2047\n",
      "Epoch: 40/100, Batch: 341/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.2875, b: 0.5748\n",
      "loss: 0.2301\n",
      "Epoch: 40/100, Batch: 342/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.2875, b: 0.5748\n",
      "loss: 0.1336\n",
      "Epoch: 40/100, Batch: 343/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.2875, b: 0.5748\n",
      "loss: 0.1574\n",
      "Epoch: 40/100, Batch: 344/432, W1: 0.7508, W2: 0.5566, W3: -0.0351, W4: -0.2875, b: 0.5748\n",
      "loss: 0.2423\n",
      "Epoch: 40/100, Batch: 345/432, W1: 0.7508, W2: 0.5567, W3: -0.0351, W4: -0.2876, b: 0.5748\n",
      "loss: 0.2487\n",
      "Epoch: 40/100, Batch: 346/432, W1: 0.7508, W2: 0.5566, W3: -0.0351, W4: -0.2876, b: 0.5748\n",
      "loss: 0.2036\n",
      "Epoch: 40/100, Batch: 347/432, W1: 0.7508, W2: 0.5567, W3: -0.035, W4: -0.2876, b: 0.5748\n",
      "loss: 0.1759\n",
      "Epoch: 40/100, Batch: 348/432, W1: 0.7508, W2: 0.5567, W3: -0.035, W4: -0.2876, b: 0.5749\n",
      "loss: 0.155\n",
      "Epoch: 40/100, Batch: 349/432, W1: 0.7508, W2: 0.5567, W3: -0.035, W4: -0.2876, b: 0.5749\n",
      "loss: 0.2181\n",
      "Epoch: 40/100, Batch: 350/432, W1: 0.7508, W2: 0.5567, W3: -0.035, W4: -0.2877, b: 0.5749\n",
      "loss: 0.1688\n",
      "Epoch: 40/100, Batch: 351/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.2877, b: 0.5749\n",
      "loss: 0.2084\n",
      "Epoch: 40/100, Batch: 352/432, W1: 0.7508, W2: 0.5567, W3: -0.035, W4: -0.2877, b: 0.5749\n",
      "loss: 0.2451\n",
      "Epoch: 40/100, Batch: 353/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.2878, b: 0.5749\n",
      "loss: 0.2464\n",
      "Epoch: 40/100, Batch: 354/432, W1: 0.7507, W2: 0.5565, W3: -0.0351, W4: -0.2878, b: 0.5749\n",
      "loss: 0.1925\n",
      "Epoch: 40/100, Batch: 355/432, W1: 0.7506, W2: 0.5565, W3: -0.0351, W4: -0.2878, b: 0.5749\n",
      "loss: 0.1889\n",
      "Epoch: 40/100, Batch: 356/432, W1: 0.7506, W2: 0.5565, W3: -0.0351, W4: -0.2878, b: 0.575\n",
      "loss: 0.7209\n",
      "Epoch: 40/100, Batch: 357/432, W1: 0.7506, W2: 0.5565, W3: -0.0351, W4: -0.2879, b: 0.575\n",
      "loss: 0.1808\n",
      "Epoch: 40/100, Batch: 358/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.2879, b: 0.575\n",
      "loss: 0.188\n",
      "Epoch: 40/100, Batch: 359/432, W1: 0.7508, W2: 0.5567, W3: -0.035, W4: -0.2879, b: 0.575\n",
      "loss: 0.171\n",
      "Epoch: 40/100, Batch: 360/432, W1: 0.7508, W2: 0.5567, W3: -0.035, W4: -0.2879, b: 0.575\n",
      "loss: 0.2008\n",
      "Epoch: 40/100, Batch: 361/432, W1: 0.7506, W2: 0.5565, W3: -0.0351, W4: -0.288, b: 0.575\n",
      "loss: 0.2827\n",
      "Epoch: 40/100, Batch: 362/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.288, b: 0.5751\n",
      "loss: 0.1793\n",
      "Epoch: 40/100, Batch: 363/432, W1: 0.7506, W2: 0.5565, W3: -0.0351, W4: -0.288, b: 0.5751\n",
      "loss: 0.1968\n",
      "Epoch: 40/100, Batch: 364/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.288, b: 0.5751\n",
      "loss: 0.1923\n",
      "Epoch: 40/100, Batch: 365/432, W1: 0.7507, W2: 0.5566, W3: -0.0351, W4: -0.288, b: 0.5751\n",
      "loss: 0.1727\n",
      "Epoch: 40/100, Batch: 366/432, W1: 0.7508, W2: 0.5567, W3: -0.035, W4: -0.288, b: 0.5751\n",
      "loss: 0.1789\n",
      "Epoch: 40/100, Batch: 367/432, W1: 0.7509, W2: 0.5568, W3: -0.035, W4: -0.2881, b: 0.5752\n",
      "loss: 0.1901\n",
      "Epoch: 40/100, Batch: 368/432, W1: 0.7508, W2: 0.5567, W3: -0.035, W4: -0.2881, b: 0.5752\n",
      "loss: 0.1746\n",
      "Epoch: 40/100, Batch: 369/432, W1: 0.7509, W2: 0.5568, W3: -0.035, W4: -0.2881, b: 0.5752\n",
      "loss: 0.2094\n",
      "Epoch: 40/100, Batch: 370/432, W1: 0.751, W2: 0.5569, W3: -0.0349, W4: -0.2881, b: 0.5752\n",
      "loss: 0.1973\n",
      "Epoch: 40/100, Batch: 371/432, W1: 0.7509, W2: 0.5568, W3: -0.0349, W4: -0.2881, b: 0.5752\n",
      "loss: 0.2011\n",
      "Epoch: 40/100, Batch: 372/432, W1: 0.751, W2: 0.5569, W3: -0.0348, W4: -0.2881, b: 0.5753\n",
      "loss: 0.1949\n",
      "Epoch: 40/100, Batch: 373/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2882, b: 0.5753\n",
      "loss: 0.1634\n",
      "Epoch: 40/100, Batch: 374/432, W1: 0.751, W2: 0.5569, W3: -0.0349, W4: -0.2882, b: 0.5753\n",
      "loss: 0.1836\n",
      "Epoch: 40/100, Batch: 375/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2882, b: 0.5753\n",
      "loss: 0.1496\n",
      "Epoch: 40/100, Batch: 376/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2882, b: 0.5753\n",
      "loss: 0.18\n",
      "Epoch: 40/100, Batch: 377/432, W1: 0.7512, W2: 0.5571, W3: -0.0348, W4: -0.2882, b: 0.5753\n",
      "loss: 0.1726\n",
      "Epoch: 40/100, Batch: 378/432, W1: 0.7512, W2: 0.5571, W3: -0.0348, W4: -0.2882, b: 0.5753\n",
      "loss: 0.1447\n",
      "Epoch: 40/100, Batch: 379/432, W1: 0.7512, W2: 0.5571, W3: -0.0348, W4: -0.2883, b: 0.5754\n",
      "loss: 0.2191\n",
      "Epoch: 40/100, Batch: 380/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.2883, b: 0.5754\n",
      "loss: 0.1426\n",
      "Epoch: 40/100, Batch: 381/432, W1: 0.7513, W2: 0.5572, W3: -0.0347, W4: -0.2883, b: 0.5754\n",
      "loss: 0.1956\n",
      "Epoch: 40/100, Batch: 382/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.2883, b: 0.5754\n",
      "loss: 0.188\n",
      "Epoch: 40/100, Batch: 383/432, W1: 0.7511, W2: 0.5571, W3: -0.0348, W4: -0.2883, b: 0.5754\n",
      "loss: 0.1465\n",
      "Epoch: 40/100, Batch: 384/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2884, b: 0.5754\n",
      "loss: 0.1724\n",
      "Epoch: 40/100, Batch: 385/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2884, b: 0.5754\n",
      "loss: 0.1448\n",
      "Epoch: 40/100, Batch: 386/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2884, b: 0.5754\n",
      "loss: 0.1978\n",
      "Epoch: 40/100, Batch: 387/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2884, b: 0.5754\n",
      "loss: 0.1979\n",
      "Epoch: 40/100, Batch: 388/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2884, b: 0.5755\n",
      "loss: 0.1395\n",
      "Epoch: 40/100, Batch: 389/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.2885, b: 0.5755\n",
      "loss: 0.1615\n",
      "Epoch: 40/100, Batch: 390/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.2885, b: 0.5755\n",
      "loss: 0.1757\n",
      "Epoch: 40/100, Batch: 391/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2885, b: 0.5755\n",
      "loss: 0.1925\n",
      "Epoch: 40/100, Batch: 392/432, W1: 0.751, W2: 0.5569, W3: -0.0349, W4: -0.2885, b: 0.5755\n",
      "loss: 0.1708\n",
      "Epoch: 40/100, Batch: 393/432, W1: 0.751, W2: 0.557, W3: -0.0348, W4: -0.2886, b: 0.5755\n",
      "loss: 0.1606\n",
      "Epoch: 40/100, Batch: 394/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2886, b: 0.5755\n",
      "loss: 0.1846\n",
      "Epoch: 40/100, Batch: 395/432, W1: 0.7512, W2: 0.5572, W3: -0.0347, W4: -0.2886, b: 0.5756\n",
      "loss: 0.1552\n",
      "Epoch: 40/100, Batch: 396/432, W1: 0.7513, W2: 0.5573, W3: -0.0346, W4: -0.2886, b: 0.5756\n",
      "loss: 0.1909\n",
      "Epoch: 40/100, Batch: 397/432, W1: 0.7513, W2: 0.5572, W3: -0.0347, W4: -0.2886, b: 0.5756\n",
      "loss: 0.1967\n",
      "Epoch: 40/100, Batch: 398/432, W1: 0.7514, W2: 0.5573, W3: -0.0346, W4: -0.2886, b: 0.5756\n",
      "loss: 0.1399\n",
      "Epoch: 40/100, Batch: 399/432, W1: 0.7513, W2: 0.5572, W3: -0.0347, W4: -0.2886, b: 0.5756\n",
      "loss: 0.1519\n",
      "Epoch: 40/100, Batch: 400/432, W1: 0.7513, W2: 0.5573, W3: -0.0346, W4: -0.2886, b: 0.5756\n",
      "loss: 0.2053\n",
      "Epoch: 40/100, Batch: 401/432, W1: 0.7514, W2: 0.5573, W3: -0.0346, W4: -0.2887, b: 0.5757\n",
      "loss: 0.1458\n",
      "Epoch: 40/100, Batch: 402/432, W1: 0.7513, W2: 0.5572, W3: -0.0347, W4: -0.2887, b: 0.5757\n",
      "loss: 0.2484\n",
      "Epoch: 40/100, Batch: 403/432, W1: 0.7513, W2: 0.5572, W3: -0.0347, W4: -0.2887, b: 0.5757\n",
      "loss: 0.2194\n",
      "Epoch: 40/100, Batch: 404/432, W1: 0.7513, W2: 0.5572, W3: -0.0347, W4: -0.2887, b: 0.5757\n",
      "loss: 0.1585\n",
      "Epoch: 40/100, Batch: 405/432, W1: 0.7513, W2: 0.5573, W3: -0.0346, W4: -0.2888, b: 0.5757\n",
      "loss: 0.1837\n",
      "Epoch: 40/100, Batch: 406/432, W1: 0.7514, W2: 0.5573, W3: -0.0346, W4: -0.2888, b: 0.5757\n",
      "loss: 0.1468\n",
      "Epoch: 40/100, Batch: 407/432, W1: 0.7514, W2: 0.5574, W3: -0.0346, W4: -0.2888, b: 0.5757\n",
      "loss: 0.1575\n",
      "Epoch: 40/100, Batch: 408/432, W1: 0.7515, W2: 0.5574, W3: -0.0345, W4: -0.2888, b: 0.5758\n",
      "loss: 0.1522\n",
      "Epoch: 40/100, Batch: 409/432, W1: 0.7515, W2: 0.5574, W3: -0.0345, W4: -0.2888, b: 0.5758\n",
      "loss: 0.1514\n",
      "Epoch: 40/100, Batch: 410/432, W1: 0.7515, W2: 0.5574, W3: -0.0345, W4: -0.2888, b: 0.5758\n",
      "loss: 0.2062\n",
      "Epoch: 40/100, Batch: 411/432, W1: 0.7515, W2: 0.5574, W3: -0.0345, W4: -0.2889, b: 0.5758\n",
      "loss: 0.2122\n",
      "Epoch: 40/100, Batch: 412/432, W1: 0.7513, W2: 0.5573, W3: -0.0346, W4: -0.2889, b: 0.5758\n",
      "loss: 0.232\n",
      "Epoch: 40/100, Batch: 413/432, W1: 0.7513, W2: 0.5572, W3: -0.0346, W4: -0.2889, b: 0.5758\n",
      "loss: 0.2086\n",
      "Epoch: 40/100, Batch: 414/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.289, b: 0.5758\n",
      "loss: 0.194\n",
      "Epoch: 40/100, Batch: 415/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.289, b: 0.5758\n",
      "loss: 0.2016\n",
      "Epoch: 40/100, Batch: 416/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.289, b: 0.5758\n",
      "loss: 0.1838\n",
      "Epoch: 40/100, Batch: 417/432, W1: 0.7511, W2: 0.5571, W3: -0.0347, W4: -0.2891, b: 0.5758\n",
      "loss: 0.2337\n",
      "Epoch: 40/100, Batch: 418/432, W1: 0.7511, W2: 0.5571, W3: -0.0347, W4: -0.2891, b: 0.5758\n",
      "loss: 0.1345\n",
      "Epoch: 40/100, Batch: 419/432, W1: 0.7511, W2: 0.5571, W3: -0.0347, W4: -0.2891, b: 0.5759\n",
      "loss: 0.2472\n",
      "Epoch: 40/100, Batch: 420/432, W1: 0.7511, W2: 0.5571, W3: -0.0347, W4: -0.2891, b: 0.5759\n",
      "loss: 0.1708\n",
      "Epoch: 40/100, Batch: 421/432, W1: 0.7511, W2: 0.5571, W3: -0.0347, W4: -0.2891, b: 0.5759\n",
      "loss: 0.1631\n",
      "Epoch: 40/100, Batch: 422/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.2891, b: 0.5759\n",
      "loss: 0.1261\n",
      "Epoch: 40/100, Batch: 423/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.2892, b: 0.5759\n",
      "loss: 0.1658\n",
      "Epoch: 40/100, Batch: 424/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2892, b: 0.5759\n",
      "loss: 0.2366\n",
      "Epoch: 40/100, Batch: 425/432, W1: 0.751, W2: 0.557, W3: -0.0348, W4: -0.2893, b: 0.5759\n",
      "loss: 0.2362\n",
      "Epoch: 40/100, Batch: 426/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2893, b: 0.5759\n",
      "loss: 0.1657\n",
      "Epoch: 40/100, Batch: 427/432, W1: 0.7511, W2: 0.557, W3: -0.0348, W4: -0.2893, b: 0.5759\n",
      "loss: 0.1834\n",
      "Epoch: 40/100, Batch: 428/432, W1: 0.7511, W2: 0.557, W3: -0.0347, W4: -0.2893, b: 0.576\n",
      "loss: 0.2184\n",
      "Epoch: 40/100, Batch: 429/432, W1: 0.7511, W2: 0.5571, W3: -0.0347, W4: -0.2893, b: 0.576\n",
      "loss: 0.1605\n",
      "Epoch: 40/100, Batch: 430/432, W1: 0.7511, W2: 0.5571, W3: -0.0347, W4: -0.2893, b: 0.576\n",
      "loss: 0.1885\n",
      "Epoch: 40/100, Batch: 431/432, W1: 0.751, W2: 0.557, W3: -0.0348, W4: -0.2894, b: 0.576\n",
      "loss: 0.2695\n",
      "Epoch: 40/100, Batch: 432/432, W1: 0.7512, W2: 0.5571, W3: -0.0347, W4: -0.2894, b: 0.576\n",
      "loss: 0.1487\n",
      "Epoch: 50/100, Batch: 1/432, W1: 0.7639, W2: 0.5404, W3: -0.0229, W4: -0.3658, b: 0.6267\n",
      "loss: 0.1559\n",
      "Epoch: 50/100, Batch: 2/432, W1: 0.7639, W2: 0.5403, W3: -0.023, W4: -0.3658, b: 0.6267\n",
      "loss: 0.1881\n",
      "Epoch: 50/100, Batch: 3/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.3658, b: 0.6267\n",
      "loss: 0.1923\n",
      "Epoch: 50/100, Batch: 4/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.3659, b: 0.6267\n",
      "loss: 0.1447\n",
      "Epoch: 50/100, Batch: 5/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3659, b: 0.6268\n",
      "loss: 0.1327\n",
      "Epoch: 50/100, Batch: 6/432, W1: 0.7639, W2: 0.5403, W3: -0.023, W4: -0.3659, b: 0.6268\n",
      "loss: 0.1427\n",
      "Epoch: 50/100, Batch: 7/432, W1: 0.7638, W2: 0.5402, W3: -0.023, W4: -0.3659, b: 0.6268\n",
      "loss: 0.2443\n",
      "Epoch: 50/100, Batch: 8/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.3659, b: 0.6268\n",
      "loss: 0.1354\n",
      "Epoch: 50/100, Batch: 9/432, W1: 0.7638, W2: 0.5402, W3: -0.023, W4: -0.3659, b: 0.6268\n",
      "loss: 0.1435\n",
      "Epoch: 50/100, Batch: 10/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.366, b: 0.6268\n",
      "loss: 0.1506\n",
      "Epoch: 50/100, Batch: 11/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.366, b: 0.6268\n",
      "loss: 0.143\n",
      "Epoch: 50/100, Batch: 12/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.366, b: 0.6268\n",
      "loss: 0.139\n",
      "Epoch: 50/100, Batch: 13/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.366, b: 0.6268\n",
      "loss: 0.1713\n",
      "Epoch: 50/100, Batch: 14/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.366, b: 0.6269\n",
      "loss: 0.1375\n",
      "Epoch: 50/100, Batch: 15/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.366, b: 0.6269\n",
      "loss: 0.1233\n",
      "Epoch: 50/100, Batch: 16/432, W1: 0.7639, W2: 0.5403, W3: -0.023, W4: -0.366, b: 0.6269\n",
      "loss: 0.1775\n",
      "Epoch: 50/100, Batch: 17/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.3661, b: 0.6269\n",
      "loss: 0.1477\n",
      "Epoch: 50/100, Batch: 18/432, W1: 0.7638, W2: 0.5402, W3: -0.023, W4: -0.3661, b: 0.6269\n",
      "loss: 0.1644\n",
      "Epoch: 50/100, Batch: 19/432, W1: 0.7638, W2: 0.5402, W3: -0.023, W4: -0.3661, b: 0.6269\n",
      "loss: 0.1588\n",
      "Epoch: 50/100, Batch: 20/432, W1: 0.7637, W2: 0.5402, W3: -0.023, W4: -0.3661, b: 0.6269\n",
      "loss: 0.202\n",
      "Epoch: 50/100, Batch: 21/432, W1: 0.7638, W2: 0.5402, W3: -0.023, W4: -0.3662, b: 0.6269\n",
      "loss: 0.1763\n",
      "Epoch: 50/100, Batch: 22/432, W1: 0.7638, W2: 0.5402, W3: -0.023, W4: -0.3662, b: 0.6269\n",
      "loss: 0.1643\n",
      "Epoch: 50/100, Batch: 23/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.3662, b: 0.6269\n",
      "loss: 0.1394\n",
      "Epoch: 50/100, Batch: 24/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.3662, b: 0.627\n",
      "loss: 0.1354\n",
      "Epoch: 50/100, Batch: 25/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3662, b: 0.627\n",
      "loss: 0.1713\n",
      "Epoch: 50/100, Batch: 26/432, W1: 0.764, W2: 0.5404, W3: -0.0229, W4: -0.3662, b: 0.627\n",
      "loss: 0.1543\n",
      "Epoch: 50/100, Batch: 27/432, W1: 0.764, W2: 0.5404, W3: -0.0229, W4: -0.3662, b: 0.627\n",
      "loss: 0.1758\n",
      "Epoch: 50/100, Batch: 28/432, W1: 0.7639, W2: 0.5404, W3: -0.0229, W4: -0.3663, b: 0.627\n",
      "loss: 0.1392\n",
      "Epoch: 50/100, Batch: 29/432, W1: 0.7639, W2: 0.5404, W3: -0.0229, W4: -0.3663, b: 0.627\n",
      "loss: 0.1357\n",
      "Epoch: 50/100, Batch: 30/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3663, b: 0.627\n",
      "loss: 0.1858\n",
      "Epoch: 50/100, Batch: 31/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3663, b: 0.627\n",
      "loss: 0.2332\n",
      "Epoch: 50/100, Batch: 32/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3664, b: 0.6271\n",
      "loss: 0.1435\n",
      "Epoch: 50/100, Batch: 33/432, W1: 0.7638, W2: 0.5403, W3: -0.023, W4: -0.3664, b: 0.6271\n",
      "loss: 0.1889\n",
      "Epoch: 50/100, Batch: 34/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3664, b: 0.6271\n",
      "loss: 0.1857\n",
      "Epoch: 50/100, Batch: 35/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3664, b: 0.6271\n",
      "loss: 0.1499\n",
      "Epoch: 50/100, Batch: 36/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3665, b: 0.6271\n",
      "loss: 0.1653\n",
      "Epoch: 50/100, Batch: 37/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3665, b: 0.6271\n",
      "loss: 0.1752\n",
      "Epoch: 50/100, Batch: 38/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3665, b: 0.6271\n",
      "loss: 0.1622\n",
      "Epoch: 50/100, Batch: 39/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3665, b: 0.6271\n",
      "loss: 0.1505\n",
      "Epoch: 50/100, Batch: 40/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3665, b: 0.6272\n",
      "loss: 0.1399\n",
      "Epoch: 50/100, Batch: 41/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3665, b: 0.6272\n",
      "loss: 0.1308\n",
      "Epoch: 50/100, Batch: 42/432, W1: 0.764, W2: 0.5404, W3: -0.0229, W4: -0.3665, b: 0.6272\n",
      "loss: 0.1559\n",
      "Epoch: 50/100, Batch: 43/432, W1: 0.764, W2: 0.5405, W3: -0.0228, W4: -0.3665, b: 0.6272\n",
      "loss: 0.1414\n",
      "Epoch: 50/100, Batch: 44/432, W1: 0.7641, W2: 0.5405, W3: -0.0228, W4: -0.3666, b: 0.6272\n",
      "loss: 0.167\n",
      "Epoch: 50/100, Batch: 45/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3666, b: 0.6272\n",
      "loss: 0.1326\n",
      "Epoch: 50/100, Batch: 46/432, W1: 0.7641, W2: 0.5406, W3: -0.0228, W4: -0.3666, b: 0.6272\n",
      "loss: 0.1295\n",
      "Epoch: 50/100, Batch: 47/432, W1: 0.7642, W2: 0.5406, W3: -0.0227, W4: -0.3666, b: 0.6273\n",
      "loss: 0.1732\n",
      "Epoch: 50/100, Batch: 48/432, W1: 0.7642, W2: 0.5406, W3: -0.0227, W4: -0.3666, b: 0.6273\n",
      "loss: 0.1514\n",
      "Epoch: 50/100, Batch: 49/432, W1: 0.7642, W2: 0.5406, W3: -0.0227, W4: -0.3666, b: 0.6273\n",
      "loss: 0.1188\n",
      "Epoch: 50/100, Batch: 50/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3667, b: 0.6273\n",
      "loss: 0.1947\n",
      "Epoch: 50/100, Batch: 51/432, W1: 0.7641, W2: 0.5405, W3: -0.0228, W4: -0.3667, b: 0.6273\n",
      "loss: 0.1841\n",
      "Epoch: 50/100, Batch: 52/432, W1: 0.7641, W2: 0.5406, W3: -0.0228, W4: -0.3667, b: 0.6273\n",
      "loss: 0.1512\n",
      "Epoch: 50/100, Batch: 53/432, W1: 0.7641, W2: 0.5405, W3: -0.0228, W4: -0.3667, b: 0.6273\n",
      "loss: 0.1754\n",
      "Epoch: 50/100, Batch: 54/432, W1: 0.764, W2: 0.5404, W3: -0.0228, W4: -0.3668, b: 0.6273\n",
      "loss: 0.1929\n",
      "Epoch: 50/100, Batch: 55/432, W1: 0.764, W2: 0.5405, W3: -0.0228, W4: -0.3668, b: 0.6273\n",
      "loss: 0.1687\n",
      "Epoch: 50/100, Batch: 56/432, W1: 0.7641, W2: 0.5406, W3: -0.0228, W4: -0.3668, b: 0.6274\n",
      "loss: 0.1457\n",
      "Epoch: 50/100, Batch: 57/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3668, b: 0.6274\n",
      "loss: 0.1441\n",
      "Epoch: 50/100, Batch: 58/432, W1: 0.7642, W2: 0.5406, W3: -0.0227, W4: -0.3668, b: 0.6274\n",
      "loss: 0.1387\n",
      "Epoch: 50/100, Batch: 59/432, W1: 0.7642, W2: 0.5407, W3: -0.0227, W4: -0.3668, b: 0.6274\n",
      "loss: 0.1515\n",
      "Epoch: 50/100, Batch: 60/432, W1: 0.7642, W2: 0.5407, W3: -0.0227, W4: -0.3668, b: 0.6274\n",
      "loss: 0.1589\n",
      "Epoch: 50/100, Batch: 61/432, W1: 0.7642, W2: 0.5407, W3: -0.0227, W4: -0.3669, b: 0.6274\n",
      "loss: 0.1495\n",
      "Epoch: 50/100, Batch: 62/432, W1: 0.7642, W2: 0.5406, W3: -0.0227, W4: -0.3669, b: 0.6274\n",
      "loss: 0.1732\n",
      "Epoch: 50/100, Batch: 63/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3669, b: 0.6274\n",
      "loss: 0.256\n",
      "Epoch: 50/100, Batch: 64/432, W1: 0.7641, W2: 0.5406, W3: -0.0228, W4: -0.3669, b: 0.6275\n",
      "loss: 0.1757\n",
      "Epoch: 50/100, Batch: 65/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3669, b: 0.6275\n",
      "loss: 1.1851\n",
      "Epoch: 50/100, Batch: 66/432, W1: 0.7642, W2: 0.5407, W3: -0.0227, W4: -0.3669, b: 0.6275\n",
      "loss: 0.148\n",
      "Epoch: 50/100, Batch: 67/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3669, b: 0.6275\n",
      "loss: 0.1575\n",
      "Epoch: 50/100, Batch: 68/432, W1: 0.7641, W2: 0.5405, W3: -0.0228, W4: -0.367, b: 0.6275\n",
      "loss: 0.1324\n",
      "Epoch: 50/100, Batch: 69/432, W1: 0.764, W2: 0.5405, W3: -0.0228, W4: -0.367, b: 0.6275\n",
      "loss: 0.171\n",
      "Epoch: 50/100, Batch: 70/432, W1: 0.7639, W2: 0.5404, W3: -0.0229, W4: -0.367, b: 0.6275\n",
      "loss: 0.1616\n",
      "Epoch: 50/100, Batch: 71/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3671, b: 0.6275\n",
      "loss: 0.1538\n",
      "Epoch: 50/100, Batch: 72/432, W1: 0.7638, W2: 0.5403, W3: -0.0229, W4: -0.3671, b: 0.6275\n",
      "loss: 0.2599\n",
      "Epoch: 50/100, Batch: 73/432, W1: 0.7638, W2: 0.5403, W3: -0.0229, W4: -0.3671, b: 0.6275\n",
      "loss: 0.1171\n",
      "Epoch: 50/100, Batch: 74/432, W1: 0.7638, W2: 0.5403, W3: -0.0229, W4: -0.3671, b: 0.6275\n",
      "loss: 0.1807\n",
      "Epoch: 50/100, Batch: 75/432, W1: 0.7639, W2: 0.5403, W3: -0.0229, W4: -0.3671, b: 0.6276\n",
      "loss: 0.1586\n",
      "Epoch: 50/100, Batch: 76/432, W1: 0.7639, W2: 0.5404, W3: -0.0229, W4: -0.3672, b: 0.6276\n",
      "loss: 0.1635\n",
      "Epoch: 50/100, Batch: 77/432, W1: 0.7639, W2: 0.5404, W3: -0.0228, W4: -0.3672, b: 0.6276\n",
      "loss: 0.1788\n",
      "Epoch: 50/100, Batch: 78/432, W1: 0.764, W2: 0.5404, W3: -0.0228, W4: -0.3672, b: 0.6276\n",
      "loss: 0.1547\n",
      "Epoch: 50/100, Batch: 79/432, W1: 0.764, W2: 0.5405, W3: -0.0228, W4: -0.3672, b: 0.6276\n",
      "loss: 0.1569\n",
      "Epoch: 50/100, Batch: 80/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3672, b: 0.6277\n",
      "loss: 0.149\n",
      "Epoch: 50/100, Batch: 81/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3672, b: 0.6277\n",
      "loss: 0.1766\n",
      "Epoch: 50/100, Batch: 82/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3672, b: 0.6277\n",
      "loss: 0.1655\n",
      "Epoch: 50/100, Batch: 83/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3673, b: 0.6277\n",
      "loss: 0.1801\n",
      "Epoch: 50/100, Batch: 84/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3673, b: 0.6277\n",
      "loss: 0.6854\n",
      "Epoch: 50/100, Batch: 85/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3673, b: 0.6277\n",
      "loss: 0.175\n",
      "Epoch: 50/100, Batch: 86/432, W1: 0.7642, W2: 0.5406, W3: -0.0227, W4: -0.3673, b: 0.6278\n",
      "loss: 0.1392\n",
      "Epoch: 50/100, Batch: 87/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3673, b: 0.6278\n",
      "loss: 0.1356\n",
      "Epoch: 50/100, Batch: 88/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3673, b: 0.6278\n",
      "loss: 0.1231\n",
      "Epoch: 50/100, Batch: 89/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3673, b: 0.6278\n",
      "loss: 0.1409\n",
      "Epoch: 50/100, Batch: 90/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3673, b: 0.6278\n",
      "loss: 0.172\n",
      "Epoch: 50/100, Batch: 91/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3674, b: 0.6278\n",
      "loss: 0.2362\n",
      "Epoch: 50/100, Batch: 92/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3674, b: 0.6278\n",
      "loss: 0.1351\n",
      "Epoch: 50/100, Batch: 93/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3674, b: 0.6279\n",
      "loss: 0.1599\n",
      "Epoch: 50/100, Batch: 94/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3674, b: 0.6279\n",
      "loss: 0.1318\n",
      "Epoch: 50/100, Batch: 95/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3675, b: 0.6279\n",
      "loss: 0.2111\n",
      "Epoch: 50/100, Batch: 96/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3675, b: 0.6279\n",
      "loss: 0.1359\n",
      "Epoch: 50/100, Batch: 97/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3675, b: 0.6279\n",
      "loss: 0.1825\n",
      "Epoch: 50/100, Batch: 98/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3675, b: 0.6279\n",
      "loss: 0.173\n",
      "Epoch: 50/100, Batch: 99/432, W1: 0.7642, W2: 0.5407, W3: -0.0227, W4: -0.3675, b: 0.6279\n",
      "loss: 0.1563\n",
      "Epoch: 50/100, Batch: 100/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3676, b: 0.6279\n",
      "loss: 0.1119\n",
      "Epoch: 50/100, Batch: 101/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3676, b: 0.6279\n",
      "loss: 0.1551\n",
      "Epoch: 50/100, Batch: 102/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3676, b: 0.6279\n",
      "loss: 0.1686\n",
      "Epoch: 50/100, Batch: 103/432, W1: 0.7642, W2: 0.5407, W3: -0.0227, W4: -0.3676, b: 0.6279\n",
      "loss: 0.1143\n",
      "Epoch: 50/100, Batch: 104/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3676, b: 0.628\n",
      "loss: 0.1467\n",
      "Epoch: 50/100, Batch: 105/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3676, b: 0.628\n",
      "loss: 0.162\n",
      "Epoch: 50/100, Batch: 106/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3677, b: 0.628\n",
      "loss: 0.1291\n",
      "Epoch: 50/100, Batch: 107/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3677, b: 0.628\n",
      "loss: 0.1866\n",
      "Epoch: 50/100, Batch: 108/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3677, b: 0.628\n",
      "loss: 0.1559\n",
      "Epoch: 50/100, Batch: 109/432, W1: 0.7642, W2: 0.5407, W3: -0.0227, W4: -0.3677, b: 0.628\n",
      "loss: 0.1703\n",
      "Epoch: 50/100, Batch: 110/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3678, b: 0.628\n",
      "loss: 0.1625\n",
      "Epoch: 50/100, Batch: 111/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3678, b: 0.628\n",
      "loss: 0.1503\n",
      "Epoch: 50/100, Batch: 112/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3678, b: 0.628\n",
      "loss: 0.1536\n",
      "Epoch: 50/100, Batch: 113/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3678, b: 0.6281\n",
      "loss: 0.1919\n",
      "Epoch: 50/100, Batch: 114/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3678, b: 0.6281\n",
      "loss: 0.1273\n",
      "Epoch: 50/100, Batch: 115/432, W1: 0.7642, W2: 0.5408, W3: -0.0226, W4: -0.3678, b: 0.6281\n",
      "loss: 0.1618\n",
      "Epoch: 50/100, Batch: 116/432, W1: 0.7642, W2: 0.5408, W3: -0.0226, W4: -0.3678, b: 0.6281\n",
      "loss: 0.1659\n",
      "Epoch: 50/100, Batch: 117/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3679, b: 0.6281\n",
      "loss: 0.1794\n",
      "Epoch: 50/100, Batch: 118/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3679, b: 0.6281\n",
      "loss: 0.1672\n",
      "Epoch: 50/100, Batch: 119/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3679, b: 0.6281\n",
      "loss: 0.1656\n",
      "Epoch: 50/100, Batch: 120/432, W1: 0.7643, W2: 0.5408, W3: -0.0226, W4: -0.3679, b: 0.6282\n",
      "loss: 0.1331\n",
      "Epoch: 50/100, Batch: 121/432, W1: 0.7643, W2: 0.5408, W3: -0.0225, W4: -0.3679, b: 0.6282\n",
      "loss: 0.1388\n",
      "Epoch: 50/100, Batch: 122/432, W1: 0.7644, W2: 0.5409, W3: -0.0225, W4: -0.3679, b: 0.6282\n",
      "loss: 0.1381\n",
      "Epoch: 50/100, Batch: 123/432, W1: 0.7643, W2: 0.5409, W3: -0.0225, W4: -0.368, b: 0.6282\n",
      "loss: 0.1482\n",
      "Epoch: 50/100, Batch: 124/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.368, b: 0.6282\n",
      "loss: 0.1765\n",
      "Epoch: 50/100, Batch: 125/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.368, b: 0.6282\n",
      "loss: 0.1737\n",
      "Epoch: 50/100, Batch: 126/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.368, b: 0.6282\n",
      "loss: 0.1395\n",
      "Epoch: 50/100, Batch: 127/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3681, b: 0.6282\n",
      "loss: 0.1136\n",
      "Epoch: 50/100, Batch: 128/432, W1: 0.7641, W2: 0.5406, W3: -0.0226, W4: -0.3681, b: 0.6282\n",
      "loss: 0.1749\n",
      "Epoch: 50/100, Batch: 129/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3681, b: 0.6282\n",
      "loss: 0.1774\n",
      "Epoch: 50/100, Batch: 130/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3681, b: 0.6283\n",
      "loss: 0.1477\n",
      "Epoch: 50/100, Batch: 131/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3681, b: 0.6283\n",
      "loss: 0.1874\n",
      "Epoch: 50/100, Batch: 132/432, W1: 0.7642, W2: 0.5408, W3: -0.0226, W4: -0.3681, b: 0.6283\n",
      "loss: 0.1464\n",
      "Epoch: 50/100, Batch: 133/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3682, b: 0.6283\n",
      "loss: 0.185\n",
      "Epoch: 50/100, Batch: 134/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3682, b: 0.6283\n",
      "loss: 0.1514\n",
      "Epoch: 50/100, Batch: 135/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3682, b: 0.6283\n",
      "loss: 0.167\n",
      "Epoch: 50/100, Batch: 136/432, W1: 0.7642, W2: 0.5407, W3: -0.0226, W4: -0.3682, b: 0.6283\n",
      "loss: 0.171\n",
      "Epoch: 50/100, Batch: 137/432, W1: 0.7641, W2: 0.5406, W3: -0.0226, W4: -0.3683, b: 0.6283\n",
      "loss: 0.1517\n",
      "Epoch: 50/100, Batch: 138/432, W1: 0.7641, W2: 0.5407, W3: -0.0226, W4: -0.3683, b: 0.6283\n",
      "loss: 0.1243\n",
      "Epoch: 50/100, Batch: 139/432, W1: 0.764, W2: 0.5406, W3: -0.0227, W4: -0.3683, b: 0.6283\n",
      "loss: 0.1855\n",
      "Epoch: 50/100, Batch: 140/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3683, b: 0.6283\n",
      "loss: 0.134\n",
      "Epoch: 50/100, Batch: 141/432, W1: 0.7641, W2: 0.5406, W3: -0.0227, W4: -0.3683, b: 0.6284\n",
      "loss: 0.1469\n",
      "Epoch: 50/100, Batch: 142/432, W1: 0.7641, W2: 0.5406, W3: -0.0226, W4: -0.3684, b: 0.6284\n",
      "loss: 0.1924\n",
      "Epoch: 50/100, Batch: 143/432, W1: 0.764, W2: 0.5406, W3: -0.0227, W4: -0.3684, b: 0.6284\n",
      "loss: 0.1739\n",
      "Epoch: 50/100, Batch: 144/432, W1: 0.7641, W2: 0.5406, W3: -0.0226, W4: -0.3684, b: 0.6284\n",
      "loss: 0.1474\n",
      "Epoch: 50/100, Batch: 145/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3684, b: 0.6284\n",
      "loss: 0.1539\n",
      "Epoch: 50/100, Batch: 146/432, W1: 0.7639, W2: 0.5405, W3: -0.0227, W4: -0.3685, b: 0.6284\n",
      "loss: 0.2114\n",
      "Epoch: 50/100, Batch: 147/432, W1: 0.7639, W2: 0.5404, W3: -0.0227, W4: -0.3685, b: 0.6284\n",
      "loss: 0.1815\n",
      "Epoch: 50/100, Batch: 148/432, W1: 0.7639, W2: 0.5404, W3: -0.0228, W4: -0.3685, b: 0.6284\n",
      "loss: 0.2014\n",
      "Epoch: 50/100, Batch: 149/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3685, b: 0.6284\n",
      "loss: 0.1929\n",
      "Epoch: 50/100, Batch: 150/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3685, b: 0.6284\n",
      "loss: 0.175\n",
      "Epoch: 50/100, Batch: 151/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3685, b: 0.6285\n",
      "loss: 1.0234\n",
      "Epoch: 50/100, Batch: 152/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3685, b: 0.6285\n",
      "loss: 0.1827\n",
      "Epoch: 50/100, Batch: 153/432, W1: 0.7641, W2: 0.5406, W3: -0.0226, W4: -0.3686, b: 0.6285\n",
      "loss: 0.1423\n",
      "Epoch: 50/100, Batch: 154/432, W1: 0.7639, W2: 0.5405, W3: -0.0227, W4: -0.3686, b: 0.6285\n",
      "loss: 0.1759\n",
      "Epoch: 50/100, Batch: 155/432, W1: 0.7639, W2: 0.5404, W3: -0.0227, W4: -0.3686, b: 0.6285\n",
      "loss: 0.1622\n",
      "Epoch: 50/100, Batch: 156/432, W1: 0.7639, W2: 0.5404, W3: -0.0227, W4: -0.3686, b: 0.6285\n",
      "loss: 0.1541\n",
      "Epoch: 50/100, Batch: 157/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3686, b: 0.6286\n",
      "loss: 0.1707\n",
      "Epoch: 50/100, Batch: 158/432, W1: 0.764, W2: 0.5406, W3: -0.0226, W4: -0.3686, b: 0.6286\n",
      "loss: 0.1368\n",
      "Epoch: 50/100, Batch: 159/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3687, b: 0.6286\n",
      "loss: 0.8856\n",
      "Epoch: 50/100, Batch: 160/432, W1: 0.7639, W2: 0.5405, W3: -0.0227, W4: -0.3687, b: 0.6286\n",
      "loss: 0.1837\n",
      "Epoch: 50/100, Batch: 161/432, W1: 0.7638, W2: 0.5404, W3: -0.0228, W4: -0.3687, b: 0.6286\n",
      "loss: 0.1975\n",
      "Epoch: 50/100, Batch: 162/432, W1: 0.7637, W2: 0.5403, W3: -0.0228, W4: -0.3688, b: 0.6286\n",
      "loss: 0.1727\n",
      "Epoch: 50/100, Batch: 163/432, W1: 0.7637, W2: 0.5402, W3: -0.0228, W4: -0.3688, b: 0.6286\n",
      "loss: 0.1753\n",
      "Epoch: 50/100, Batch: 164/432, W1: 0.7637, W2: 0.5402, W3: -0.0229, W4: -0.3688, b: 0.6286\n",
      "loss: 0.1485\n",
      "Epoch: 50/100, Batch: 165/432, W1: 0.7638, W2: 0.5403, W3: -0.0228, W4: -0.3688, b: 0.6286\n",
      "loss: 0.1644\n",
      "Epoch: 50/100, Batch: 166/432, W1: 0.7637, W2: 0.5402, W3: -0.0229, W4: -0.3689, b: 0.6286\n",
      "loss: 0.1658\n",
      "Epoch: 50/100, Batch: 167/432, W1: 0.7637, W2: 0.5402, W3: -0.0229, W4: -0.3689, b: 0.6286\n",
      "loss: 0.1739\n",
      "Epoch: 50/100, Batch: 168/432, W1: 0.7636, W2: 0.5402, W3: -0.0229, W4: -0.3689, b: 0.6286\n",
      "loss: 0.1702\n",
      "Epoch: 50/100, Batch: 169/432, W1: 0.7637, W2: 0.5402, W3: -0.0228, W4: -0.3689, b: 0.6287\n",
      "loss: 0.1333\n",
      "Epoch: 50/100, Batch: 170/432, W1: 0.7636, W2: 0.5402, W3: -0.0229, W4: -0.3689, b: 0.6287\n",
      "loss: 0.1782\n",
      "Epoch: 50/100, Batch: 171/432, W1: 0.7636, W2: 0.5402, W3: -0.0229, W4: -0.369, b: 0.6287\n",
      "loss: 0.1659\n",
      "Epoch: 50/100, Batch: 172/432, W1: 0.7637, W2: 0.5402, W3: -0.0228, W4: -0.369, b: 0.6287\n",
      "loss: 0.1182\n",
      "Epoch: 50/100, Batch: 173/432, W1: 0.7637, W2: 0.5403, W3: -0.0228, W4: -0.369, b: 0.6287\n",
      "loss: 0.1322\n",
      "Epoch: 50/100, Batch: 174/432, W1: 0.7637, W2: 0.5403, W3: -0.0228, W4: -0.369, b: 0.6287\n",
      "loss: 0.1601\n",
      "Epoch: 50/100, Batch: 175/432, W1: 0.7638, W2: 0.5403, W3: -0.0228, W4: -0.369, b: 0.6287\n",
      "loss: 0.1381\n",
      "Epoch: 50/100, Batch: 176/432, W1: 0.7638, W2: 0.5404, W3: -0.0227, W4: -0.369, b: 0.6288\n",
      "loss: 0.1868\n",
      "Epoch: 50/100, Batch: 177/432, W1: 0.7638, W2: 0.5404, W3: -0.0228, W4: -0.369, b: 0.6288\n",
      "loss: 0.1772\n",
      "Epoch: 50/100, Batch: 178/432, W1: 0.7639, W2: 0.5404, W3: -0.0227, W4: -0.3691, b: 0.6288\n",
      "loss: 0.1878\n",
      "Epoch: 50/100, Batch: 179/432, W1: 0.7638, W2: 0.5404, W3: -0.0227, W4: -0.3691, b: 0.6288\n",
      "loss: 0.1606\n",
      "Epoch: 50/100, Batch: 180/432, W1: 0.7639, W2: 0.5404, W3: -0.0227, W4: -0.3691, b: 0.6288\n",
      "loss: 0.1844\n",
      "Epoch: 50/100, Batch: 181/432, W1: 0.7639, W2: 0.5405, W3: -0.0227, W4: -0.3691, b: 0.6288\n",
      "loss: 0.168\n",
      "Epoch: 50/100, Batch: 182/432, W1: 0.7639, W2: 0.5405, W3: -0.0227, W4: -0.3691, b: 0.6289\n",
      "loss: 0.1153\n",
      "Epoch: 50/100, Batch: 183/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3691, b: 0.6289\n",
      "loss: 0.159\n",
      "Epoch: 50/100, Batch: 184/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3692, b: 0.6289\n",
      "loss: 0.1891\n",
      "Epoch: 50/100, Batch: 185/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3692, b: 0.6289\n",
      "loss: 0.1202\n",
      "Epoch: 50/100, Batch: 186/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3692, b: 0.6289\n",
      "loss: 0.1695\n",
      "Epoch: 50/100, Batch: 187/432, W1: 0.764, W2: 0.5405, W3: -0.0227, W4: -0.3692, b: 0.6289\n",
      "loss: 0.2097\n",
      "Epoch: 50/100, Batch: 188/432, W1: 0.7639, W2: 0.5405, W3: -0.0227, W4: -0.3692, b: 0.6289\n",
      "loss: 0.1474\n",
      "Epoch: 50/100, Batch: 189/432, W1: 0.764, W2: 0.5405, W3: -0.0226, W4: -0.3693, b: 0.6289\n",
      "loss: 0.1439\n",
      "Epoch: 50/100, Batch: 190/432, W1: 0.764, W2: 0.5405, W3: -0.0226, W4: -0.3693, b: 0.6289\n",
      "loss: 0.1556\n",
      "Epoch: 50/100, Batch: 191/432, W1: 0.764, W2: 0.5405, W3: -0.0226, W4: -0.3693, b: 0.629\n",
      "loss: 0.1881\n",
      "Epoch: 50/100, Batch: 192/432, W1: 0.7641, W2: 0.5407, W3: -0.0226, W4: -0.3693, b: 0.629\n",
      "loss: 0.1596\n",
      "Epoch: 50/100, Batch: 193/432, W1: 0.7641, W2: 0.5406, W3: -0.0226, W4: -0.3693, b: 0.629\n",
      "loss: 0.1771\n",
      "Epoch: 50/100, Batch: 194/432, W1: 0.7641, W2: 0.5407, W3: -0.0226, W4: -0.3693, b: 0.629\n",
      "loss: 0.1727\n",
      "Epoch: 50/100, Batch: 195/432, W1: 0.7641, W2: 0.5406, W3: -0.0225, W4: -0.3694, b: 0.629\n",
      "loss: 0.213\n",
      "Epoch: 50/100, Batch: 196/432, W1: 0.7641, W2: 0.5407, W3: -0.0225, W4: -0.3694, b: 0.629\n",
      "loss: 0.1365\n",
      "Epoch: 50/100, Batch: 197/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3694, b: 0.6291\n",
      "loss: 0.1926\n",
      "Epoch: 50/100, Batch: 198/432, W1: 0.7641, W2: 0.5406, W3: -0.0225, W4: -0.3694, b: 0.6291\n",
      "loss: 0.1707\n",
      "Epoch: 50/100, Batch: 199/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3695, b: 0.6291\n",
      "loss: 0.1683\n",
      "Epoch: 50/100, Batch: 200/432, W1: 0.764, W2: 0.5405, W3: -0.0226, W4: -0.3695, b: 0.6291\n",
      "loss: 0.1638\n",
      "Epoch: 50/100, Batch: 201/432, W1: 0.764, W2: 0.5405, W3: -0.0226, W4: -0.3695, b: 0.6291\n",
      "loss: 0.1422\n",
      "Epoch: 50/100, Batch: 202/432, W1: 0.764, W2: 0.5406, W3: -0.0226, W4: -0.3695, b: 0.6291\n",
      "loss: 0.1464\n",
      "Epoch: 50/100, Batch: 203/432, W1: 0.764, W2: 0.5406, W3: -0.0226, W4: -0.3695, b: 0.6291\n",
      "loss: 0.1638\n",
      "Epoch: 50/100, Batch: 204/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3696, b: 0.6291\n",
      "loss: 0.1606\n",
      "Epoch: 50/100, Batch: 205/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3696, b: 0.6291\n",
      "loss: 0.2001\n",
      "Epoch: 50/100, Batch: 206/432, W1: 0.7641, W2: 0.5406, W3: -0.0225, W4: -0.3696, b: 0.6292\n",
      "loss: 0.1677\n",
      "Epoch: 50/100, Batch: 207/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3696, b: 0.6292\n",
      "loss: 0.1667\n",
      "Epoch: 50/100, Batch: 208/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3696, b: 0.6292\n",
      "loss: 0.1571\n",
      "Epoch: 50/100, Batch: 209/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3696, b: 0.6292\n",
      "loss: 0.1226\n",
      "Epoch: 50/100, Batch: 210/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3697, b: 0.6292\n",
      "loss: 0.2232\n",
      "Epoch: 50/100, Batch: 211/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3697, b: 0.6292\n",
      "loss: 0.1759\n",
      "Epoch: 50/100, Batch: 212/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3697, b: 0.6292\n",
      "loss: 0.1646\n",
      "Epoch: 50/100, Batch: 213/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3697, b: 0.6292\n",
      "loss: 0.1863\n",
      "Epoch: 50/100, Batch: 214/432, W1: 0.764, W2: 0.5406, W3: -0.0225, W4: -0.3698, b: 0.6292\n",
      "loss: 0.1567\n",
      "Epoch: 50/100, Batch: 215/432, W1: 0.7639, W2: 0.5405, W3: -0.0226, W4: -0.3698, b: 0.6292\n",
      "loss: 0.2\n",
      "Epoch: 50/100, Batch: 216/432, W1: 0.7638, W2: 0.5404, W3: -0.0226, W4: -0.3698, b: 0.6292\n",
      "loss: 0.1859\n",
      "Epoch: 50/100, Batch: 217/432, W1: 0.7633, W2: 0.537, W3: -0.0231, W4: -0.37, b: 0.6292\n",
      "loss: 8.1878\n",
      "Epoch: 50/100, Batch: 218/432, W1: 0.7633, W2: 0.537, W3: -0.0231, W4: -0.37, b: 0.6292\n",
      "loss: 0.1541\n",
      "Epoch: 50/100, Batch: 219/432, W1: 0.7632, W2: 0.537, W3: -0.0232, W4: -0.37, b: 0.6292\n",
      "loss: 0.1399\n",
      "Epoch: 50/100, Batch: 220/432, W1: 0.7632, W2: 0.5369, W3: -0.0232, W4: -0.3701, b: 0.6292\n",
      "loss: 0.1923\n",
      "Epoch: 50/100, Batch: 221/432, W1: 0.7632, W2: 0.5369, W3: -0.0232, W4: -0.3701, b: 0.6292\n",
      "loss: 0.1383\n",
      "Epoch: 50/100, Batch: 222/432, W1: 0.7633, W2: 0.537, W3: -0.0231, W4: -0.3701, b: 0.6292\n",
      "loss: 0.168\n",
      "Epoch: 50/100, Batch: 223/432, W1: 0.7633, W2: 0.537, W3: -0.0231, W4: -0.3701, b: 0.6293\n",
      "loss: 0.1695\n",
      "Epoch: 50/100, Batch: 224/432, W1: 0.7634, W2: 0.5371, W3: -0.0231, W4: -0.3701, b: 0.6293\n",
      "loss: 0.1579\n",
      "Epoch: 50/100, Batch: 225/432, W1: 0.7633, W2: 0.5371, W3: -0.0231, W4: -0.3701, b: 0.6293\n",
      "loss: 0.142\n",
      "Epoch: 50/100, Batch: 226/432, W1: 0.7634, W2: 0.5371, W3: -0.0231, W4: -0.3701, b: 0.6293\n",
      "loss: 0.1466\n",
      "Epoch: 50/100, Batch: 227/432, W1: 0.7634, W2: 0.5372, W3: -0.023, W4: -0.3701, b: 0.6293\n",
      "loss: 0.1408\n",
      "Epoch: 50/100, Batch: 228/432, W1: 0.7634, W2: 0.5372, W3: -0.023, W4: -0.3702, b: 0.6293\n",
      "loss: 0.1518\n",
      "Epoch: 50/100, Batch: 229/432, W1: 0.7634, W2: 0.5372, W3: -0.023, W4: -0.3702, b: 0.6293\n",
      "loss: 0.1639\n",
      "Epoch: 50/100, Batch: 230/432, W1: 0.7635, W2: 0.5373, W3: -0.023, W4: -0.3702, b: 0.6294\n",
      "loss: 0.1609\n",
      "Epoch: 50/100, Batch: 231/432, W1: 0.7636, W2: 0.5373, W3: -0.0229, W4: -0.3702, b: 0.6294\n",
      "loss: 0.1605\n",
      "Epoch: 50/100, Batch: 232/432, W1: 0.7636, W2: 0.5373, W3: -0.0229, W4: -0.3702, b: 0.6294\n",
      "loss: 0.1499\n",
      "Epoch: 50/100, Batch: 233/432, W1: 0.7636, W2: 0.5374, W3: -0.0229, W4: -0.3702, b: 0.6294\n",
      "loss: 0.1662\n",
      "Epoch: 50/100, Batch: 234/432, W1: 0.7637, W2: 0.5374, W3: -0.0229, W4: -0.3702, b: 0.6294\n",
      "loss: 0.13\n",
      "Epoch: 50/100, Batch: 235/432, W1: 0.7638, W2: 0.5375, W3: -0.0228, W4: -0.3702, b: 0.6295\n",
      "loss: 0.1031\n",
      "Epoch: 50/100, Batch: 236/432, W1: 0.7638, W2: 0.5376, W3: -0.0228, W4: -0.3702, b: 0.6295\n",
      "loss: 0.1955\n",
      "Epoch: 50/100, Batch: 237/432, W1: 0.7639, W2: 0.5376, W3: -0.0228, W4: -0.3702, b: 0.6295\n",
      "loss: 0.1481\n",
      "Epoch: 50/100, Batch: 238/432, W1: 0.7638, W2: 0.5375, W3: -0.0228, W4: -0.3703, b: 0.6295\n",
      "loss: 0.1846\n",
      "Epoch: 50/100, Batch: 239/432, W1: 0.7638, W2: 0.5376, W3: -0.0228, W4: -0.3703, b: 0.6295\n",
      "loss: 0.1734\n",
      "Epoch: 50/100, Batch: 240/432, W1: 0.7639, W2: 0.5376, W3: -0.0227, W4: -0.3703, b: 0.6295\n",
      "loss: 0.2051\n",
      "Epoch: 50/100, Batch: 241/432, W1: 0.7638, W2: 0.5376, W3: -0.0228, W4: -0.3703, b: 0.6295\n",
      "loss: 0.1846\n",
      "Epoch: 50/100, Batch: 242/432, W1: 0.7639, W2: 0.5376, W3: -0.0227, W4: -0.3704, b: 0.6296\n",
      "loss: 0.1763\n",
      "Epoch: 50/100, Batch: 243/432, W1: 0.764, W2: 0.5377, W3: -0.0227, W4: -0.3704, b: 0.6296\n",
      "loss: 0.1534\n",
      "Epoch: 50/100, Batch: 244/432, W1: 0.7639, W2: 0.5377, W3: -0.0227, W4: -0.3704, b: 0.6296\n",
      "loss: 0.196\n",
      "Epoch: 50/100, Batch: 245/432, W1: 0.7639, W2: 0.5376, W3: -0.0227, W4: -0.3704, b: 0.6296\n",
      "loss: 0.2134\n",
      "Epoch: 50/100, Batch: 246/432, W1: 0.7639, W2: 0.5376, W3: -0.0227, W4: -0.3704, b: 0.6296\n",
      "loss: 0.1378\n",
      "Epoch: 50/100, Batch: 247/432, W1: 0.7639, W2: 0.5376, W3: -0.0227, W4: -0.3705, b: 0.6296\n",
      "loss: 0.1394\n",
      "Epoch: 50/100, Batch: 248/432, W1: 0.7638, W2: 0.5376, W3: -0.0228, W4: -0.3705, b: 0.6296\n",
      "loss: 0.2003\n",
      "Epoch: 50/100, Batch: 249/432, W1: 0.7639, W2: 0.5376, W3: -0.0227, W4: -0.3705, b: 0.6296\n",
      "loss: 0.1222\n",
      "Epoch: 50/100, Batch: 250/432, W1: 0.7638, W2: 0.5375, W3: -0.0228, W4: -0.3705, b: 0.6296\n",
      "loss: 0.1692\n",
      "Epoch: 50/100, Batch: 251/432, W1: 0.7638, W2: 0.5375, W3: -0.0228, W4: -0.3705, b: 0.6297\n",
      "loss: 0.1804\n",
      "Epoch: 50/100, Batch: 252/432, W1: 0.7638, W2: 0.5375, W3: -0.0228, W4: -0.3706, b: 0.6297\n",
      "loss: 0.1975\n",
      "Epoch: 50/100, Batch: 253/432, W1: 0.7637, W2: 0.5374, W3: -0.0228, W4: -0.3706, b: 0.6297\n",
      "loss: 0.1968\n",
      "Epoch: 50/100, Batch: 254/432, W1: 0.7636, W2: 0.5374, W3: -0.0229, W4: -0.3706, b: 0.6297\n",
      "loss: 0.1751\n",
      "Epoch: 50/100, Batch: 255/432, W1: 0.7637, W2: 0.5374, W3: -0.0229, W4: -0.3707, b: 0.6297\n",
      "loss: 0.1455\n",
      "Epoch: 50/100, Batch: 256/432, W1: 0.7637, W2: 0.5374, W3: -0.0228, W4: -0.3707, b: 0.6297\n",
      "loss: 0.1593\n",
      "Epoch: 50/100, Batch: 257/432, W1: 0.7637, W2: 0.5374, W3: -0.0228, W4: -0.3707, b: 0.6297\n",
      "loss: 0.1594\n",
      "Epoch: 50/100, Batch: 258/432, W1: 0.7638, W2: 0.5375, W3: -0.0228, W4: -0.3707, b: 0.6297\n",
      "loss: 0.1817\n",
      "Epoch: 50/100, Batch: 259/432, W1: 0.7638, W2: 0.5375, W3: -0.0228, W4: -0.3707, b: 0.6297\n",
      "loss: 0.1188\n",
      "Epoch: 50/100, Batch: 260/432, W1: 0.7639, W2: 0.5376, W3: -0.0227, W4: -0.3707, b: 0.6298\n",
      "loss: 0.1477\n",
      "Epoch: 50/100, Batch: 261/432, W1: 0.7639, W2: 0.5377, W3: -0.0227, W4: -0.3707, b: 0.6298\n",
      "loss: 0.1445\n",
      "Epoch: 50/100, Batch: 262/432, W1: 0.7639, W2: 0.5376, W3: -0.0227, W4: -0.3708, b: 0.6298\n",
      "loss: 0.1483\n",
      "Epoch: 50/100, Batch: 263/432, W1: 0.7639, W2: 0.5377, W3: -0.0227, W4: -0.3708, b: 0.6298\n",
      "loss: 0.1489\n",
      "Epoch: 50/100, Batch: 264/432, W1: 0.7639, W2: 0.5377, W3: -0.0227, W4: -0.3708, b: 0.6298\n",
      "loss: 0.1259\n",
      "Epoch: 50/100, Batch: 265/432, W1: 0.764, W2: 0.5377, W3: -0.0226, W4: -0.3708, b: 0.6298\n",
      "loss: 0.1385\n",
      "Epoch: 50/100, Batch: 266/432, W1: 0.764, W2: 0.5378, W3: -0.0226, W4: -0.3708, b: 0.6299\n",
      "loss: 0.1662\n",
      "Epoch: 50/100, Batch: 267/432, W1: 0.764, W2: 0.5377, W3: -0.0226, W4: -0.3708, b: 0.6299\n",
      "loss: 0.1429\n",
      "Epoch: 50/100, Batch: 268/432, W1: 0.764, W2: 0.5378, W3: -0.0226, W4: -0.3708, b: 0.6299\n",
      "loss: 0.1566\n",
      "Epoch: 50/100, Batch: 269/432, W1: 0.7641, W2: 0.5379, W3: -0.0226, W4: -0.3708, b: 0.6299\n",
      "loss: 1.03\n",
      "Epoch: 50/100, Batch: 270/432, W1: 0.7641, W2: 0.5378, W3: -0.0226, W4: -0.3708, b: 0.6299\n",
      "loss: 0.187\n",
      "Epoch: 50/100, Batch: 271/432, W1: 0.764, W2: 0.5378, W3: -0.0226, W4: -0.3709, b: 0.6299\n",
      "loss: 0.171\n",
      "Epoch: 50/100, Batch: 272/432, W1: 0.7639, W2: 0.5377, W3: -0.0227, W4: -0.3709, b: 0.6299\n",
      "loss: 0.2182\n",
      "Epoch: 50/100, Batch: 273/432, W1: 0.7639, W2: 0.5377, W3: -0.0227, W4: -0.3709, b: 0.63\n",
      "loss: 0.1502\n",
      "Epoch: 50/100, Batch: 274/432, W1: 0.764, W2: 0.5377, W3: -0.0226, W4: -0.3709, b: 0.63\n",
      "loss: 0.1676\n",
      "Epoch: 50/100, Batch: 275/432, W1: 0.764, W2: 0.5378, W3: -0.0226, W4: -0.3709, b: 0.63\n",
      "loss: 0.1482\n",
      "Epoch: 50/100, Batch: 276/432, W1: 0.7641, W2: 0.5378, W3: -0.0226, W4: -0.3709, b: 0.63\n",
      "loss: 0.1501\n",
      "Epoch: 50/100, Batch: 277/432, W1: 0.7641, W2: 0.5379, W3: -0.0225, W4: -0.371, b: 0.63\n",
      "loss: 0.1935\n",
      "Epoch: 50/100, Batch: 278/432, W1: 0.7642, W2: 0.5379, W3: -0.0225, W4: -0.371, b: 0.63\n",
      "loss: 0.1631\n",
      "Epoch: 50/100, Batch: 279/432, W1: 0.7643, W2: 0.538, W3: -0.0225, W4: -0.371, b: 0.6301\n",
      "loss: 0.137\n",
      "Epoch: 50/100, Batch: 280/432, W1: 0.7642, W2: 0.538, W3: -0.0225, W4: -0.371, b: 0.6301\n",
      "loss: 0.2467\n",
      "Epoch: 50/100, Batch: 281/432, W1: 0.7642, W2: 0.5379, W3: -0.0225, W4: -0.371, b: 0.6301\n",
      "loss: 0.1845\n",
      "Epoch: 50/100, Batch: 282/432, W1: 0.7641, W2: 0.5379, W3: -0.0225, W4: -0.3711, b: 0.6301\n",
      "loss: 0.2186\n",
      "Epoch: 50/100, Batch: 283/432, W1: 0.764, W2: 0.5378, W3: -0.0226, W4: -0.3711, b: 0.6301\n",
      "loss: 0.2003\n",
      "Epoch: 50/100, Batch: 284/432, W1: 0.764, W2: 0.5378, W3: -0.0226, W4: -0.3711, b: 0.6301\n",
      "loss: 0.1587\n",
      "Epoch: 50/100, Batch: 285/432, W1: 0.7641, W2: 0.5379, W3: -0.0225, W4: -0.3711, b: 0.6301\n",
      "loss: 0.1284\n",
      "Epoch: 50/100, Batch: 286/432, W1: 0.7641, W2: 0.5379, W3: -0.0225, W4: -0.3711, b: 0.6301\n",
      "loss: 0.1449\n",
      "Epoch: 50/100, Batch: 287/432, W1: 0.7642, W2: 0.5379, W3: -0.0225, W4: -0.3712, b: 0.6302\n",
      "loss: 0.1451\n",
      "Epoch: 50/100, Batch: 288/432, W1: 0.7642, W2: 0.538, W3: -0.0225, W4: -0.3712, b: 0.6302\n",
      "loss: 0.1648\n",
      "Epoch: 50/100, Batch: 289/432, W1: 0.7641, W2: 0.5379, W3: -0.0225, W4: -0.3712, b: 0.6302\n",
      "loss: 0.1645\n",
      "Epoch: 50/100, Batch: 290/432, W1: 0.7642, W2: 0.5379, W3: -0.0225, W4: -0.3712, b: 0.6302\n",
      "loss: 0.1274\n",
      "Epoch: 50/100, Batch: 291/432, W1: 0.7642, W2: 0.5379, W3: -0.0225, W4: -0.3712, b: 0.6302\n",
      "loss: 0.2223\n",
      "Epoch: 50/100, Batch: 292/432, W1: 0.7642, W2: 0.538, W3: -0.0225, W4: -0.3713, b: 0.6302\n",
      "loss: 0.1642\n",
      "Epoch: 50/100, Batch: 293/432, W1: 0.7642, W2: 0.5379, W3: -0.0225, W4: -0.3713, b: 0.6302\n",
      "loss: 0.1468\n",
      "Epoch: 50/100, Batch: 294/432, W1: 0.7642, W2: 0.538, W3: -0.0225, W4: -0.3713, b: 0.6302\n",
      "loss: 0.1382\n",
      "Epoch: 50/100, Batch: 295/432, W1: 0.7643, W2: 0.538, W3: -0.0224, W4: -0.3713, b: 0.6303\n",
      "loss: 0.1672\n",
      "Epoch: 50/100, Batch: 296/432, W1: 0.7642, W2: 0.538, W3: -0.0225, W4: -0.3713, b: 0.6303\n",
      "loss: 0.2143\n",
      "Epoch: 50/100, Batch: 297/432, W1: 0.7642, W2: 0.538, W3: -0.0225, W4: -0.3714, b: 0.6303\n",
      "loss: 0.1339\n",
      "Epoch: 50/100, Batch: 298/432, W1: 0.7642, W2: 0.5379, W3: -0.0225, W4: -0.3714, b: 0.6303\n",
      "loss: 0.1712\n",
      "Epoch: 50/100, Batch: 299/432, W1: 0.7642, W2: 0.538, W3: -0.0225, W4: -0.3714, b: 0.6303\n",
      "loss: 0.1367\n",
      "Epoch: 50/100, Batch: 300/432, W1: 0.7642, W2: 0.538, W3: -0.0225, W4: -0.3714, b: 0.6303\n",
      "loss: 0.1597\n",
      "Epoch: 50/100, Batch: 301/432, W1: 0.7642, W2: 0.538, W3: -0.0224, W4: -0.3714, b: 0.6303\n",
      "loss: 0.139\n",
      "Epoch: 50/100, Batch: 302/432, W1: 0.7643, W2: 0.538, W3: -0.0224, W4: -0.3714, b: 0.6303\n",
      "loss: 0.1355\n",
      "Epoch: 50/100, Batch: 303/432, W1: 0.7643, W2: 0.5381, W3: -0.0224, W4: -0.3714, b: 0.6304\n",
      "loss: 0.1346\n",
      "Epoch: 50/100, Batch: 304/432, W1: 0.7643, W2: 0.5381, W3: -0.0224, W4: -0.3715, b: 0.6304\n",
      "loss: 0.1522\n",
      "Epoch: 50/100, Batch: 305/432, W1: 0.7643, W2: 0.5381, W3: -0.0224, W4: -0.3715, b: 0.6304\n",
      "loss: 0.1503\n",
      "Epoch: 50/100, Batch: 306/432, W1: 0.7643, W2: 0.5381, W3: -0.0224, W4: -0.3715, b: 0.6304\n",
      "loss: 0.1835\n",
      "Epoch: 50/100, Batch: 307/432, W1: 0.7644, W2: 0.5382, W3: -0.0223, W4: -0.3715, b: 0.6304\n",
      "loss: 0.1447\n",
      "Epoch: 50/100, Batch: 308/432, W1: 0.7644, W2: 0.5382, W3: -0.0223, W4: -0.3715, b: 0.6304\n",
      "loss: 0.1426\n",
      "Epoch: 50/100, Batch: 309/432, W1: 0.7644, W2: 0.5382, W3: -0.0223, W4: -0.3715, b: 0.6304\n",
      "loss: 0.1333\n",
      "Epoch: 50/100, Batch: 310/432, W1: 0.7645, W2: 0.5383, W3: -0.0223, W4: -0.3715, b: 0.6305\n",
      "loss: 0.1364\n",
      "Epoch: 50/100, Batch: 311/432, W1: 0.7645, W2: 0.5382, W3: -0.0223, W4: -0.3716, b: 0.6305\n",
      "loss: 0.2109\n",
      "Epoch: 50/100, Batch: 312/432, W1: 0.7645, W2: 0.5382, W3: -0.0223, W4: -0.3716, b: 0.6305\n",
      "loss: 0.1733\n",
      "Epoch: 50/100, Batch: 313/432, W1: 0.7645, W2: 0.5383, W3: -0.0223, W4: -0.3716, b: 0.6305\n",
      "loss: 0.1895\n",
      "Epoch: 50/100, Batch: 314/432, W1: 0.7645, W2: 0.5383, W3: -0.0223, W4: -0.3716, b: 0.6305\n",
      "loss: 0.1457\n",
      "Epoch: 50/100, Batch: 315/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3716, b: 0.6305\n",
      "loss: 0.1262\n",
      "Epoch: 50/100, Batch: 316/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3716, b: 0.6305\n",
      "loss: 0.1939\n",
      "Epoch: 50/100, Batch: 317/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3717, b: 0.6306\n",
      "loss: 0.1366\n",
      "Epoch: 50/100, Batch: 318/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3717, b: 0.6306\n",
      "loss: 0.1672\n",
      "Epoch: 50/100, Batch: 319/432, W1: 0.7645, W2: 0.5383, W3: -0.0222, W4: -0.3717, b: 0.6306\n",
      "loss: 0.2115\n",
      "Epoch: 50/100, Batch: 320/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3717, b: 0.6306\n",
      "loss: 0.1957\n",
      "Epoch: 50/100, Batch: 321/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3717, b: 0.6306\n",
      "loss: 0.1349\n",
      "Epoch: 50/100, Batch: 322/432, W1: 0.7647, W2: 0.5384, W3: -0.0222, W4: -0.3717, b: 0.6306\n",
      "loss: 0.1522\n",
      "Epoch: 50/100, Batch: 323/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3718, b: 0.6306\n",
      "loss: 0.148\n",
      "Epoch: 50/100, Batch: 324/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.3718, b: 0.6307\n",
      "loss: 0.1284\n",
      "Epoch: 50/100, Batch: 325/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.3718, b: 0.6307\n",
      "loss: 0.19\n",
      "Epoch: 50/100, Batch: 326/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.3718, b: 0.6307\n",
      "loss: 0.1508\n",
      "Epoch: 50/100, Batch: 327/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.3718, b: 0.6307\n",
      "loss: 0.14\n",
      "Epoch: 50/100, Batch: 328/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.3718, b: 0.6307\n",
      "loss: 0.1271\n",
      "Epoch: 50/100, Batch: 329/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3719, b: 0.6307\n",
      "loss: 0.1847\n",
      "Epoch: 50/100, Batch: 330/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.3719, b: 0.6307\n",
      "loss: 0.1232\n",
      "Epoch: 50/100, Batch: 331/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.3719, b: 0.6307\n",
      "loss: 0.1377\n",
      "Epoch: 50/100, Batch: 332/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3719, b: 0.6307\n",
      "loss: 0.1543\n",
      "Epoch: 50/100, Batch: 333/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.3719, b: 0.6308\n",
      "loss: 0.1825\n",
      "Epoch: 50/100, Batch: 334/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.372, b: 0.6308\n",
      "loss: 0.152\n",
      "Epoch: 50/100, Batch: 335/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.372, b: 0.6308\n",
      "loss: 0.2008\n",
      "Epoch: 50/100, Batch: 336/432, W1: 0.7647, W2: 0.5385, W3: -0.0221, W4: -0.372, b: 0.6308\n",
      "loss: 0.2071\n",
      "Epoch: 50/100, Batch: 337/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.372, b: 0.6308\n",
      "loss: 0.1465\n",
      "Epoch: 50/100, Batch: 338/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3721, b: 0.6308\n",
      "loss: 0.1948\n",
      "Epoch: 50/100, Batch: 339/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3721, b: 0.6308\n",
      "loss: 0.1429\n",
      "Epoch: 50/100, Batch: 340/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3721, b: 0.6308\n",
      "loss: 0.1803\n",
      "Epoch: 50/100, Batch: 341/432, W1: 0.7646, W2: 0.5384, W3: -0.0222, W4: -0.3721, b: 0.6309\n",
      "loss: 0.1632\n",
      "Epoch: 50/100, Batch: 342/432, W1: 0.7647, W2: 0.5386, W3: -0.0221, W4: -0.3721, b: 0.6309\n",
      "loss: 0.3846\n",
      "Epoch: 50/100, Batch: 343/432, W1: 0.7645, W2: 0.5376, W3: -0.0223, W4: -0.3722, b: 0.6309\n",
      "loss: 2.0705\n",
      "Epoch: 50/100, Batch: 344/432, W1: 0.7645, W2: 0.5376, W3: -0.0223, W4: -0.3722, b: 0.6309\n",
      "loss: 0.1989\n",
      "Epoch: 50/100, Batch: 345/432, W1: 0.7645, W2: 0.5376, W3: -0.0223, W4: -0.3722, b: 0.6309\n",
      "loss: 0.1858\n",
      "Epoch: 50/100, Batch: 346/432, W1: 0.7646, W2: 0.5377, W3: -0.0222, W4: -0.3722, b: 0.6309\n",
      "loss: 0.1662\n",
      "Epoch: 50/100, Batch: 347/432, W1: 0.7646, W2: 0.5378, W3: -0.0222, W4: -0.3722, b: 0.6309\n",
      "loss: 0.2016\n",
      "Epoch: 50/100, Batch: 348/432, W1: 0.7647, W2: 0.5379, W3: -0.0221, W4: -0.3722, b: 0.631\n",
      "loss: 0.1445\n",
      "Epoch: 50/100, Batch: 349/432, W1: 0.7647, W2: 0.5378, W3: -0.0222, W4: -0.3723, b: 0.631\n",
      "loss: 0.1594\n",
      "Epoch: 50/100, Batch: 350/432, W1: 0.7646, W2: 0.5378, W3: -0.0222, W4: -0.3723, b: 0.631\n",
      "loss: 0.1685\n",
      "Epoch: 50/100, Batch: 351/432, W1: 0.7646, W2: 0.5378, W3: -0.0222, W4: -0.3723, b: 0.631\n",
      "loss: 0.1619\n",
      "Epoch: 50/100, Batch: 352/432, W1: 0.7647, W2: 0.5379, W3: -0.0221, W4: -0.3723, b: 0.631\n",
      "loss: 0.1586\n",
      "Epoch: 50/100, Batch: 353/432, W1: 0.7647, W2: 0.5378, W3: -0.0221, W4: -0.3723, b: 0.631\n",
      "loss: 0.1516\n",
      "Epoch: 50/100, Batch: 354/432, W1: 0.7648, W2: 0.5379, W3: -0.0221, W4: -0.3723, b: 0.631\n",
      "loss: 0.1281\n",
      "Epoch: 50/100, Batch: 355/432, W1: 0.7648, W2: 0.538, W3: -0.0221, W4: -0.3723, b: 0.6311\n",
      "loss: 0.1329\n",
      "Epoch: 50/100, Batch: 356/432, W1: 0.7649, W2: 0.5381, W3: -0.022, W4: -0.3724, b: 0.6311\n",
      "loss: 0.1394\n",
      "Epoch: 50/100, Batch: 357/432, W1: 0.7649, W2: 0.538, W3: -0.022, W4: -0.3724, b: 0.6311\n",
      "loss: 0.1875\n",
      "Epoch: 50/100, Batch: 358/432, W1: 0.7649, W2: 0.538, W3: -0.022, W4: -0.3724, b: 0.6311\n",
      "loss: 0.1371\n",
      "Epoch: 50/100, Batch: 359/432, W1: 0.7648, W2: 0.538, W3: -0.0221, W4: -0.3724, b: 0.6311\n",
      "loss: 0.2142\n",
      "Epoch: 50/100, Batch: 360/432, W1: 0.7648, W2: 0.538, W3: -0.0221, W4: -0.3724, b: 0.6311\n",
      "loss: 0.1365\n",
      "Epoch: 50/100, Batch: 361/432, W1: 0.7648, W2: 0.5379, W3: -0.0221, W4: -0.3725, b: 0.6311\n",
      "loss: 0.1762\n",
      "Epoch: 50/100, Batch: 362/432, W1: 0.7647, W2: 0.5379, W3: -0.0221, W4: -0.3725, b: 0.6311\n",
      "loss: 0.1533\n",
      "Epoch: 50/100, Batch: 363/432, W1: 0.7648, W2: 0.538, W3: -0.0221, W4: -0.3725, b: 0.6311\n",
      "loss: 0.1334\n",
      "Epoch: 50/100, Batch: 364/432, W1: 0.7648, W2: 0.5379, W3: -0.0221, W4: -0.3725, b: 0.6311\n",
      "loss: 0.1716\n",
      "Epoch: 50/100, Batch: 365/432, W1: 0.7649, W2: 0.538, W3: -0.022, W4: -0.3725, b: 0.6312\n",
      "loss: 0.1801\n",
      "Epoch: 50/100, Batch: 366/432, W1: 0.7648, W2: 0.538, W3: -0.0221, W4: -0.3726, b: 0.6312\n",
      "loss: 0.2313\n",
      "Epoch: 50/100, Batch: 367/432, W1: 0.7648, W2: 0.5379, W3: -0.0221, W4: -0.3726, b: 0.6312\n",
      "loss: 0.1353\n",
      "Epoch: 50/100, Batch: 368/432, W1: 0.7647, W2: 0.5378, W3: -0.0221, W4: -0.3726, b: 0.6312\n",
      "loss: 0.2184\n",
      "Epoch: 50/100, Batch: 369/432, W1: 0.7647, W2: 0.5379, W3: -0.0221, W4: -0.3726, b: 0.6312\n",
      "loss: 0.1477\n",
      "Epoch: 50/100, Batch: 370/432, W1: 0.7647, W2: 0.5379, W3: -0.0221, W4: -0.3727, b: 0.6312\n",
      "loss: 0.1325\n",
      "Epoch: 50/100, Batch: 371/432, W1: 0.7647, W2: 0.5379, W3: -0.0221, W4: -0.3727, b: 0.6312\n",
      "loss: 0.1734\n",
      "Epoch: 50/100, Batch: 372/432, W1: 0.7648, W2: 0.538, W3: -0.0221, W4: -0.3727, b: 0.6312\n",
      "loss: 0.1529\n",
      "Epoch: 50/100, Batch: 373/432, W1: 0.7648, W2: 0.538, W3: -0.022, W4: -0.3727, b: 0.6313\n",
      "loss: 0.1348\n",
      "Epoch: 50/100, Batch: 374/432, W1: 0.7649, W2: 0.5381, W3: -0.022, W4: -0.3727, b: 0.6313\n",
      "loss: 0.1529\n",
      "Epoch: 50/100, Batch: 375/432, W1: 0.765, W2: 0.5381, W3: -0.022, W4: -0.3727, b: 0.6313\n",
      "loss: 0.6809\n",
      "Epoch: 50/100, Batch: 376/432, W1: 0.7649, W2: 0.5381, W3: -0.022, W4: -0.3727, b: 0.6313\n",
      "loss: 0.228\n",
      "Epoch: 50/100, Batch: 377/432, W1: 0.7649, W2: 0.5381, W3: -0.022, W4: -0.3728, b: 0.6313\n",
      "loss: 0.1791\n",
      "Epoch: 50/100, Batch: 378/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.3728, b: 0.6314\n",
      "loss: 0.1381\n",
      "Epoch: 50/100, Batch: 379/432, W1: 0.765, W2: 0.5381, W3: -0.0219, W4: -0.3728, b: 0.6314\n",
      "loss: 0.1442\n",
      "Epoch: 50/100, Batch: 380/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.3728, b: 0.6314\n",
      "loss: 0.1227\n",
      "Epoch: 50/100, Batch: 381/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.3728, b: 0.6314\n",
      "loss: 0.1519\n",
      "Epoch: 50/100, Batch: 382/432, W1: 0.7651, W2: 0.5383, W3: -0.0219, W4: -0.3728, b: 0.6314\n",
      "loss: 0.1423\n",
      "Epoch: 50/100, Batch: 383/432, W1: 0.7651, W2: 0.5382, W3: -0.0219, W4: -0.3728, b: 0.6314\n",
      "loss: 0.1687\n",
      "Epoch: 50/100, Batch: 384/432, W1: 0.7651, W2: 0.5382, W3: -0.0219, W4: -0.3728, b: 0.6314\n",
      "loss: 0.1425\n",
      "Epoch: 50/100, Batch: 385/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.3729, b: 0.6314\n",
      "loss: 0.1714\n",
      "Epoch: 50/100, Batch: 386/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.3729, b: 0.6315\n",
      "loss: 0.1825\n",
      "Epoch: 50/100, Batch: 387/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.3729, b: 0.6315\n",
      "loss: 0.1903\n",
      "Epoch: 50/100, Batch: 388/432, W1: 0.7651, W2: 0.5382, W3: -0.0219, W4: -0.3729, b: 0.6315\n",
      "loss: 0.1372\n",
      "Epoch: 50/100, Batch: 389/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.373, b: 0.6315\n",
      "loss: 0.1429\n",
      "Epoch: 50/100, Batch: 390/432, W1: 0.7651, W2: 0.5382, W3: -0.0219, W4: -0.373, b: 0.6315\n",
      "loss: 0.1495\n",
      "Epoch: 50/100, Batch: 391/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.373, b: 0.6315\n",
      "loss: 0.1614\n",
      "Epoch: 50/100, Batch: 392/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.373, b: 0.6315\n",
      "loss: 0.1556\n",
      "Epoch: 50/100, Batch: 393/432, W1: 0.7651, W2: 0.5382, W3: -0.0219, W4: -0.373, b: 0.6315\n",
      "loss: 0.1449\n",
      "Epoch: 50/100, Batch: 394/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.373, b: 0.6316\n",
      "loss: 0.1725\n",
      "Epoch: 50/100, Batch: 395/432, W1: 0.7651, W2: 0.5383, W3: -0.0219, W4: -0.3731, b: 0.6316\n",
      "loss: 0.1735\n",
      "Epoch: 50/100, Batch: 396/432, W1: 0.7652, W2: 0.5383, W3: -0.0218, W4: -0.3731, b: 0.6316\n",
      "loss: 0.1274\n",
      "Epoch: 50/100, Batch: 397/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3731, b: 0.6316\n",
      "loss: 0.1518\n",
      "Epoch: 50/100, Batch: 398/432, W1: 0.7652, W2: 0.5383, W3: -0.0218, W4: -0.3731, b: 0.6316\n",
      "loss: 0.1518\n",
      "Epoch: 50/100, Batch: 399/432, W1: 0.7652, W2: 0.5384, W3: -0.0218, W4: -0.3731, b: 0.6316\n",
      "loss: 0.1583\n",
      "Epoch: 50/100, Batch: 400/432, W1: 0.7652, W2: 0.5384, W3: -0.0218, W4: -0.3731, b: 0.6316\n",
      "loss: 0.1591\n",
      "Epoch: 50/100, Batch: 401/432, W1: 0.7652, W2: 0.5384, W3: -0.0218, W4: -0.3731, b: 0.6316\n",
      "loss: 0.1597\n",
      "Epoch: 50/100, Batch: 402/432, W1: 0.7652, W2: 0.5384, W3: -0.0218, W4: -0.3732, b: 0.6317\n",
      "loss: 0.1703\n",
      "Epoch: 50/100, Batch: 403/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3732, b: 0.6317\n",
      "loss: 0.1695\n",
      "Epoch: 50/100, Batch: 404/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3732, b: 0.6317\n",
      "loss: 0.161\n",
      "Epoch: 50/100, Batch: 405/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3732, b: 0.6317\n",
      "loss: 0.1365\n",
      "Epoch: 50/100, Batch: 406/432, W1: 0.7652, W2: 0.5384, W3: -0.0218, W4: -0.3732, b: 0.6317\n",
      "loss: 0.1647\n",
      "Epoch: 50/100, Batch: 407/432, W1: 0.7652, W2: 0.5384, W3: -0.0218, W4: -0.3733, b: 0.6317\n",
      "loss: 0.157\n",
      "Epoch: 50/100, Batch: 408/432, W1: 0.7652, W2: 0.5384, W3: -0.0218, W4: -0.3733, b: 0.6317\n",
      "loss: 0.1423\n",
      "Epoch: 50/100, Batch: 409/432, W1: 0.7652, W2: 0.5383, W3: -0.0218, W4: -0.3733, b: 0.6317\n",
      "loss: 0.1867\n",
      "Epoch: 50/100, Batch: 410/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3733, b: 0.6317\n",
      "loss: 0.1656\n",
      "Epoch: 50/100, Batch: 411/432, W1: 0.7652, W2: 0.5384, W3: -0.0218, W4: -0.3733, b: 0.6318\n",
      "loss: 0.1571\n",
      "Epoch: 50/100, Batch: 412/432, W1: 0.7652, W2: 0.5384, W3: -0.0217, W4: -0.3733, b: 0.6318\n",
      "loss: 0.1206\n",
      "Epoch: 50/100, Batch: 413/432, W1: 0.7653, W2: 0.5384, W3: -0.0217, W4: -0.3734, b: 0.6318\n",
      "loss: 0.1384\n",
      "Epoch: 50/100, Batch: 414/432, W1: 0.7652, W2: 0.5384, W3: -0.0217, W4: -0.3734, b: 0.6318\n",
      "loss: 0.1996\n",
      "Epoch: 50/100, Batch: 415/432, W1: 0.7653, W2: 0.5385, W3: -0.0217, W4: -0.3734, b: 0.6318\n",
      "loss: 0.1692\n",
      "Epoch: 50/100, Batch: 416/432, W1: 0.7653, W2: 0.5385, W3: -0.0217, W4: -0.3734, b: 0.6318\n",
      "loss: 0.1284\n",
      "Epoch: 50/100, Batch: 417/432, W1: 0.7653, W2: 0.5384, W3: -0.0217, W4: -0.3734, b: 0.6318\n",
      "loss: 0.1967\n",
      "Epoch: 50/100, Batch: 418/432, W1: 0.7653, W2: 0.5384, W3: -0.0217, W4: -0.3735, b: 0.6319\n",
      "loss: 0.1411\n",
      "Epoch: 50/100, Batch: 419/432, W1: 0.7653, W2: 0.5385, W3: -0.0217, W4: -0.3735, b: 0.6319\n",
      "loss: 0.1664\n",
      "Epoch: 50/100, Batch: 420/432, W1: 0.7653, W2: 0.5384, W3: -0.0217, W4: -0.3735, b: 0.6319\n",
      "loss: 0.1514\n",
      "Epoch: 50/100, Batch: 421/432, W1: 0.7652, W2: 0.5384, W3: -0.0217, W4: -0.3735, b: 0.6319\n",
      "loss: 0.1558\n",
      "Epoch: 50/100, Batch: 422/432, W1: 0.7653, W2: 0.5384, W3: -0.0217, W4: -0.3735, b: 0.6319\n",
      "loss: 0.2114\n",
      "Epoch: 50/100, Batch: 423/432, W1: 0.7652, W2: 0.5384, W3: -0.0217, W4: -0.3736, b: 0.6319\n",
      "loss: 0.1998\n",
      "Epoch: 50/100, Batch: 424/432, W1: 0.7652, W2: 0.5384, W3: -0.0218, W4: -0.3736, b: 0.6319\n",
      "loss: 0.1786\n",
      "Epoch: 50/100, Batch: 425/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3736, b: 0.6319\n",
      "loss: 0.1801\n",
      "Epoch: 50/100, Batch: 426/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3736, b: 0.6319\n",
      "loss: 0.1601\n",
      "Epoch: 50/100, Batch: 427/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3737, b: 0.6319\n",
      "loss: 0.2016\n",
      "Epoch: 50/100, Batch: 428/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3737, b: 0.6319\n",
      "loss: 0.1744\n",
      "Epoch: 50/100, Batch: 429/432, W1: 0.7651, W2: 0.5383, W3: -0.0218, W4: -0.3737, b: 0.632\n",
      "loss: 0.1342\n",
      "Epoch: 50/100, Batch: 430/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.3737, b: 0.632\n",
      "loss: 0.1671\n",
      "Epoch: 50/100, Batch: 431/432, W1: 0.765, W2: 0.5382, W3: -0.0218, W4: -0.3738, b: 0.632\n",
      "loss: 0.1946\n",
      "Epoch: 50/100, Batch: 432/432, W1: 0.765, W2: 0.5382, W3: -0.0219, W4: -0.3738, b: 0.632\n",
      "loss: 0.2041\n",
      "Epoch: 60/100, Batch: 1/432, W1: 0.7775, W2: 0.5221, W3: -0.0106, W4: -0.4416, b: 0.6775\n",
      "loss: 0.1244\n",
      "Epoch: 60/100, Batch: 2/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4416, b: 0.6775\n",
      "loss: 0.1562\n",
      "Epoch: 60/100, Batch: 3/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4416, b: 0.6775\n",
      "loss: 0.1307\n",
      "Epoch: 60/100, Batch: 4/432, W1: 0.7775, W2: 0.5221, W3: -0.0106, W4: -0.4416, b: 0.6776\n",
      "loss: 0.1178\n",
      "Epoch: 60/100, Batch: 5/432, W1: 0.7775, W2: 0.5221, W3: -0.0105, W4: -0.4416, b: 0.6776\n",
      "loss: 0.1109\n",
      "Epoch: 60/100, Batch: 6/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4417, b: 0.6776\n",
      "loss: 0.1562\n",
      "Epoch: 60/100, Batch: 7/432, W1: 0.7774, W2: 0.5219, W3: -0.0106, W4: -0.4417, b: 0.6776\n",
      "loss: 0.1601\n",
      "Epoch: 60/100, Batch: 8/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4417, b: 0.6776\n",
      "loss: 0.1893\n",
      "Epoch: 60/100, Batch: 9/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.4418, b: 0.6776\n",
      "loss: 0.1567\n",
      "Epoch: 60/100, Batch: 10/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.4418, b: 0.6776\n",
      "loss: 0.1444\n",
      "Epoch: 60/100, Batch: 11/432, W1: 0.7773, W2: 0.5219, W3: -0.0106, W4: -0.4418, b: 0.6776\n",
      "loss: 0.1586\n",
      "Epoch: 60/100, Batch: 12/432, W1: 0.7774, W2: 0.5219, W3: -0.0106, W4: -0.4418, b: 0.6776\n",
      "loss: 0.1583\n",
      "Epoch: 60/100, Batch: 13/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.4418, b: 0.6776\n",
      "loss: 0.1375\n",
      "Epoch: 60/100, Batch: 14/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.4419, b: 0.6776\n",
      "loss: 0.1475\n",
      "Epoch: 60/100, Batch: 15/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.4419, b: 0.6776\n",
      "loss: 0.119\n",
      "Epoch: 60/100, Batch: 16/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4419, b: 0.6777\n",
      "loss: 0.1472\n",
      "Epoch: 60/100, Batch: 17/432, W1: 0.7773, W2: 0.5219, W3: -0.0106, W4: -0.4419, b: 0.6777\n",
      "loss: 0.1804\n",
      "Epoch: 60/100, Batch: 18/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.4419, b: 0.6777\n",
      "loss: 0.2157\n",
      "Epoch: 60/100, Batch: 19/432, W1: 0.7772, W2: 0.5218, W3: -0.0107, W4: -0.442, b: 0.6777\n",
      "loss: 0.1589\n",
      "Epoch: 60/100, Batch: 20/432, W1: 0.7773, W2: 0.5219, W3: -0.0106, W4: -0.442, b: 0.6777\n",
      "loss: 0.1032\n",
      "Epoch: 60/100, Batch: 21/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.442, b: 0.6777\n",
      "loss: 0.147\n",
      "Epoch: 60/100, Batch: 22/432, W1: 0.7772, W2: 0.5219, W3: -0.0107, W4: -0.442, b: 0.6777\n",
      "loss: 0.125\n",
      "Epoch: 60/100, Batch: 23/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.442, b: 0.6777\n",
      "loss: 0.1344\n",
      "Epoch: 60/100, Batch: 24/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.442, b: 0.6777\n",
      "loss: 0.1832\n",
      "Epoch: 60/100, Batch: 25/432, W1: 0.7773, W2: 0.5219, W3: -0.0107, W4: -0.4421, b: 0.6777\n",
      "loss: 0.1942\n",
      "Epoch: 60/100, Batch: 26/432, W1: 0.7773, W2: 0.5219, W3: -0.0106, W4: -0.4421, b: 0.6778\n",
      "loss: 0.1165\n",
      "Epoch: 60/100, Batch: 27/432, W1: 0.7773, W2: 0.5219, W3: -0.0106, W4: -0.4421, b: 0.6778\n",
      "loss: 0.122\n",
      "Epoch: 60/100, Batch: 28/432, W1: 0.7773, W2: 0.5219, W3: -0.0106, W4: -0.4421, b: 0.6778\n",
      "loss: 0.1623\n",
      "Epoch: 60/100, Batch: 29/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4421, b: 0.6778\n",
      "loss: 0.1264\n",
      "Epoch: 60/100, Batch: 30/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4421, b: 0.6778\n",
      "loss: 0.1466\n",
      "Epoch: 60/100, Batch: 31/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4421, b: 0.6778\n",
      "loss: 0.1525\n",
      "Epoch: 60/100, Batch: 32/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4422, b: 0.6778\n",
      "loss: 0.1617\n",
      "Epoch: 60/100, Batch: 33/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4422, b: 0.6779\n",
      "loss: 0.1202\n",
      "Epoch: 60/100, Batch: 34/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4422, b: 0.6779\n",
      "loss: 0.1344\n",
      "Epoch: 60/100, Batch: 35/432, W1: 0.7775, W2: 0.5221, W3: -0.0105, W4: -0.4422, b: 0.6779\n",
      "loss: 0.1479\n",
      "Epoch: 60/100, Batch: 36/432, W1: 0.7775, W2: 0.5221, W3: -0.0105, W4: -0.4422, b: 0.6779\n",
      "loss: 0.1616\n",
      "Epoch: 60/100, Batch: 37/432, W1: 0.7775, W2: 0.5221, W3: -0.0105, W4: -0.4422, b: 0.6779\n",
      "loss: 0.1336\n",
      "Epoch: 60/100, Batch: 38/432, W1: 0.7775, W2: 0.5221, W3: -0.0105, W4: -0.4422, b: 0.6779\n",
      "loss: 0.1507\n",
      "Epoch: 60/100, Batch: 39/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4423, b: 0.6779\n",
      "loss: 0.169\n",
      "Epoch: 60/100, Batch: 40/432, W1: 0.7774, W2: 0.5221, W3: -0.0106, W4: -0.4423, b: 0.6779\n",
      "loss: 0.1045\n",
      "Epoch: 60/100, Batch: 41/432, W1: 0.7774, W2: 0.522, W3: -0.0106, W4: -0.4423, b: 0.6779\n",
      "loss: 0.1405\n",
      "Epoch: 60/100, Batch: 42/432, W1: 0.7775, W2: 0.5221, W3: -0.0105, W4: -0.4423, b: 0.678\n",
      "loss: 0.0968\n",
      "Epoch: 60/100, Batch: 43/432, W1: 0.7775, W2: 0.5221, W3: -0.0105, W4: -0.4423, b: 0.678\n",
      "loss: 0.1467\n",
      "Epoch: 60/100, Batch: 44/432, W1: 0.7775, W2: 0.5221, W3: -0.0105, W4: -0.4423, b: 0.678\n",
      "loss: 0.1395\n",
      "Epoch: 60/100, Batch: 45/432, W1: 0.7776, W2: 0.5222, W3: -0.0105, W4: -0.4423, b: 0.678\n",
      "loss: 0.1747\n",
      "Epoch: 60/100, Batch: 46/432, W1: 0.7776, W2: 0.5222, W3: -0.0104, W4: -0.4424, b: 0.678\n",
      "loss: 0.1591\n",
      "Epoch: 60/100, Batch: 47/432, W1: 0.7776, W2: 0.5222, W3: -0.0105, W4: -0.4424, b: 0.678\n",
      "loss: 0.1381\n",
      "Epoch: 60/100, Batch: 48/432, W1: 0.7776, W2: 0.5223, W3: -0.0104, W4: -0.4424, b: 0.678\n",
      "loss: 0.1733\n",
      "Epoch: 60/100, Batch: 49/432, W1: 0.7777, W2: 0.5223, W3: -0.0104, W4: -0.4424, b: 0.6781\n",
      "loss: 0.1096\n",
      "Epoch: 60/100, Batch: 50/432, W1: 0.7777, W2: 0.5223, W3: -0.0104, W4: -0.4424, b: 0.6781\n",
      "loss: 0.1169\n",
      "Epoch: 60/100, Batch: 51/432, W1: 0.7777, W2: 0.5223, W3: -0.0104, W4: -0.4424, b: 0.6781\n",
      "loss: 0.1196\n",
      "Epoch: 60/100, Batch: 52/432, W1: 0.7778, W2: 0.5224, W3: -0.0103, W4: -0.4424, b: 0.6781\n",
      "loss: 0.1188\n",
      "Epoch: 60/100, Batch: 53/432, W1: 0.7778, W2: 0.5224, W3: -0.0103, W4: -0.4424, b: 0.6781\n",
      "loss: 0.1258\n",
      "Epoch: 60/100, Batch: 54/432, W1: 0.7773, W2: 0.5192, W3: -0.0108, W4: -0.4426, b: 0.6781\n",
      "loss: 7.6141\n",
      "Epoch: 60/100, Batch: 55/432, W1: 0.7773, W2: 0.5192, W3: -0.0108, W4: -0.4426, b: 0.6781\n",
      "loss: 0.1555\n",
      "Epoch: 60/100, Batch: 56/432, W1: 0.7774, W2: 0.5193, W3: -0.0107, W4: -0.4426, b: 0.6781\n",
      "loss: 0.1925\n",
      "Epoch: 60/100, Batch: 57/432, W1: 0.7775, W2: 0.5193, W3: -0.0107, W4: -0.4426, b: 0.6781\n",
      "loss: 0.1548\n",
      "Epoch: 60/100, Batch: 58/432, W1: 0.7775, W2: 0.5193, W3: -0.0107, W4: -0.4426, b: 0.6781\n",
      "loss: 0.1206\n",
      "Epoch: 60/100, Batch: 59/432, W1: 0.7776, W2: 0.5194, W3: -0.0106, W4: -0.4426, b: 0.6781\n",
      "loss: 0.1007\n",
      "Epoch: 60/100, Batch: 60/432, W1: 0.7776, W2: 0.5195, W3: -0.0106, W4: -0.4426, b: 0.6782\n",
      "loss: 0.1298\n",
      "Epoch: 60/100, Batch: 61/432, W1: 0.7776, W2: 0.5195, W3: -0.0106, W4: -0.4426, b: 0.6782\n",
      "loss: 0.1309\n",
      "Epoch: 60/100, Batch: 62/432, W1: 0.7776, W2: 0.5194, W3: -0.0106, W4: -0.4427, b: 0.6782\n",
      "loss: 0.1931\n",
      "Epoch: 60/100, Batch: 63/432, W1: 0.7776, W2: 0.5195, W3: -0.0106, W4: -0.4427, b: 0.6782\n",
      "loss: 0.1464\n",
      "Epoch: 60/100, Batch: 64/432, W1: 0.7777, W2: 0.5195, W3: -0.0106, W4: -0.4427, b: 0.6782\n",
      "loss: 0.117\n",
      "Epoch: 60/100, Batch: 65/432, W1: 0.7777, W2: 0.5196, W3: -0.0105, W4: -0.4427, b: 0.6782\n",
      "loss: 0.1016\n",
      "Epoch: 60/100, Batch: 66/432, W1: 0.7777, W2: 0.5196, W3: -0.0105, W4: -0.4427, b: 0.6782\n",
      "loss: 0.1478\n",
      "Epoch: 60/100, Batch: 67/432, W1: 0.7777, W2: 0.5196, W3: -0.0105, W4: -0.4427, b: 0.6783\n",
      "loss: 0.1365\n",
      "Epoch: 60/100, Batch: 68/432, W1: 0.7778, W2: 0.5196, W3: -0.0105, W4: -0.4427, b: 0.6783\n",
      "loss: 0.1655\n",
      "Epoch: 60/100, Batch: 69/432, W1: 0.7778, W2: 0.5196, W3: -0.0105, W4: -0.4427, b: 0.6783\n",
      "loss: 0.1312\n",
      "Epoch: 60/100, Batch: 70/432, W1: 0.7778, W2: 0.5197, W3: -0.0105, W4: -0.4428, b: 0.6783\n",
      "loss: 0.1874\n",
      "Epoch: 60/100, Batch: 71/432, W1: 0.7778, W2: 0.5197, W3: -0.0105, W4: -0.4428, b: 0.6783\n",
      "loss: 0.1683\n",
      "Epoch: 60/100, Batch: 72/432, W1: 0.7779, W2: 0.5198, W3: -0.0104, W4: -0.4428, b: 0.6783\n",
      "loss: 0.1189\n",
      "Epoch: 60/100, Batch: 73/432, W1: 0.7779, W2: 0.5197, W3: -0.0104, W4: -0.4428, b: 0.6784\n",
      "loss: 0.1909\n",
      "Epoch: 60/100, Batch: 74/432, W1: 0.7778, W2: 0.5197, W3: -0.0104, W4: -0.4428, b: 0.6784\n",
      "loss: 0.1012\n",
      "Epoch: 60/100, Batch: 75/432, W1: 0.7779, W2: 0.5197, W3: -0.0104, W4: -0.4428, b: 0.6784\n",
      "loss: 0.1299\n",
      "Epoch: 60/100, Batch: 76/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4428, b: 0.6784\n",
      "loss: 0.1057\n",
      "Epoch: 60/100, Batch: 77/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4428, b: 0.6784\n",
      "loss: 0.114\n",
      "Epoch: 60/100, Batch: 78/432, W1: 0.778, W2: 0.5198, W3: -0.0103, W4: -0.4429, b: 0.6784\n",
      "loss: 0.1917\n",
      "Epoch: 60/100, Batch: 79/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4429, b: 0.6784\n",
      "loss: 0.1469\n",
      "Epoch: 60/100, Batch: 80/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4429, b: 0.6784\n",
      "loss: 0.1683\n",
      "Epoch: 60/100, Batch: 81/432, W1: 0.7781, W2: 0.5199, W3: -0.0103, W4: -0.4429, b: 0.6785\n",
      "loss: 0.1195\n",
      "Epoch: 60/100, Batch: 82/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4429, b: 0.6785\n",
      "loss: 0.1238\n",
      "Epoch: 60/100, Batch: 83/432, W1: 0.778, W2: 0.5198, W3: -0.0103, W4: -0.443, b: 0.6785\n",
      "loss: 0.115\n",
      "Epoch: 60/100, Batch: 84/432, W1: 0.7779, W2: 0.5198, W3: -0.0104, W4: -0.443, b: 0.6785\n",
      "loss: 0.1704\n",
      "Epoch: 60/100, Batch: 85/432, W1: 0.778, W2: 0.5198, W3: -0.0103, W4: -0.443, b: 0.6785\n",
      "loss: 0.1401\n",
      "Epoch: 60/100, Batch: 86/432, W1: 0.7779, W2: 0.5198, W3: -0.0103, W4: -0.443, b: 0.6785\n",
      "loss: 0.1838\n",
      "Epoch: 60/100, Batch: 87/432, W1: 0.7781, W2: 0.5199, W3: -0.0103, W4: -0.443, b: 0.6785\n",
      "loss: 0.1539\n",
      "Epoch: 60/100, Batch: 88/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.443, b: 0.6785\n",
      "loss: 0.1359\n",
      "Epoch: 60/100, Batch: 89/432, W1: 0.7782, W2: 0.5201, W3: -0.0102, W4: -0.443, b: 0.6786\n",
      "loss: 0.1059\n",
      "Epoch: 60/100, Batch: 90/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4431, b: 0.6786\n",
      "loss: 0.1773\n",
      "Epoch: 60/100, Batch: 91/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4431, b: 0.6786\n",
      "loss: 0.1628\n",
      "Epoch: 60/100, Batch: 92/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4431, b: 0.6786\n",
      "loss: 0.1784\n",
      "Epoch: 60/100, Batch: 93/432, W1: 0.7781, W2: 0.5199, W3: -0.0102, W4: -0.4431, b: 0.6786\n",
      "loss: 0.1757\n",
      "Epoch: 60/100, Batch: 94/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4431, b: 0.6786\n",
      "loss: 0.143\n",
      "Epoch: 60/100, Batch: 95/432, W1: 0.7782, W2: 0.52, W3: -0.0102, W4: -0.4431, b: 0.6786\n",
      "loss: 0.1386\n",
      "Epoch: 60/100, Batch: 96/432, W1: 0.7781, W2: 0.5199, W3: -0.0102, W4: -0.4432, b: 0.6786\n",
      "loss: 0.167\n",
      "Epoch: 60/100, Batch: 97/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4432, b: 0.6786\n",
      "loss: 0.1278\n",
      "Epoch: 60/100, Batch: 98/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4432, b: 0.6787\n",
      "loss: 0.1378\n",
      "Epoch: 60/100, Batch: 99/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4432, b: 0.6787\n",
      "loss: 0.1421\n",
      "Epoch: 60/100, Batch: 100/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4432, b: 0.6787\n",
      "loss: 0.1079\n",
      "Epoch: 60/100, Batch: 101/432, W1: 0.7782, W2: 0.52, W3: -0.0102, W4: -0.4432, b: 0.6787\n",
      "loss: 0.1209\n",
      "Epoch: 60/100, Batch: 102/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4433, b: 0.6787\n",
      "loss: 0.2086\n",
      "Epoch: 60/100, Batch: 103/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4433, b: 0.6787\n",
      "loss: 0.143\n",
      "Epoch: 60/100, Batch: 104/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4433, b: 0.6787\n",
      "loss: 0.1505\n",
      "Epoch: 60/100, Batch: 105/432, W1: 0.7782, W2: 0.52, W3: -0.0102, W4: -0.4433, b: 0.6787\n",
      "loss: 0.1532\n",
      "Epoch: 60/100, Batch: 106/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4433, b: 0.6787\n",
      "loss: 0.1403\n",
      "Epoch: 60/100, Batch: 107/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4434, b: 0.6787\n",
      "loss: 0.1727\n",
      "Epoch: 60/100, Batch: 108/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4434, b: 0.6788\n",
      "loss: 0.1162\n",
      "Epoch: 60/100, Batch: 109/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4434, b: 0.6788\n",
      "loss: 0.1181\n",
      "Epoch: 60/100, Batch: 110/432, W1: 0.7781, W2: 0.5199, W3: -0.0102, W4: -0.4434, b: 0.6788\n",
      "loss: 0.1841\n",
      "Epoch: 60/100, Batch: 111/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4434, b: 0.6788\n",
      "loss: 0.1535\n",
      "Epoch: 60/100, Batch: 112/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4434, b: 0.6788\n",
      "loss: 1.1521\n",
      "Epoch: 60/100, Batch: 113/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4434, b: 0.6788\n",
      "loss: 0.1182\n",
      "Epoch: 60/100, Batch: 114/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4434, b: 0.6788\n",
      "loss: 0.1411\n",
      "Epoch: 60/100, Batch: 115/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4435, b: 0.6788\n",
      "loss: 0.1548\n",
      "Epoch: 60/100, Batch: 116/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4435, b: 0.6788\n",
      "loss: 0.1939\n",
      "Epoch: 60/100, Batch: 117/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4435, b: 0.6789\n",
      "loss: 0.1842\n",
      "Epoch: 60/100, Batch: 118/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4435, b: 0.6789\n",
      "loss: 0.1125\n",
      "Epoch: 60/100, Batch: 119/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4436, b: 0.6789\n",
      "loss: 0.1525\n",
      "Epoch: 60/100, Batch: 120/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4436, b: 0.6789\n",
      "loss: 0.1682\n",
      "Epoch: 60/100, Batch: 121/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4436, b: 0.6789\n",
      "loss: 0.0971\n",
      "Epoch: 60/100, Batch: 122/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4436, b: 0.6789\n",
      "loss: 0.1317\n",
      "Epoch: 60/100, Batch: 123/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4436, b: 0.6789\n",
      "loss: 0.137\n",
      "Epoch: 60/100, Batch: 124/432, W1: 0.778, W2: 0.5199, W3: -0.0102, W4: -0.4436, b: 0.6789\n",
      "loss: 0.1518\n",
      "Epoch: 60/100, Batch: 125/432, W1: 0.7779, W2: 0.5198, W3: -0.0103, W4: -0.4437, b: 0.6789\n",
      "loss: 0.1768\n",
      "Epoch: 60/100, Batch: 126/432, W1: 0.778, W2: 0.5198, W3: -0.0103, W4: -0.4437, b: 0.6789\n",
      "loss: 0.115\n",
      "Epoch: 60/100, Batch: 127/432, W1: 0.778, W2: 0.5199, W3: -0.0102, W4: -0.4437, b: 0.679\n",
      "loss: 0.1408\n",
      "Epoch: 60/100, Batch: 128/432, W1: 0.7779, W2: 0.5198, W3: -0.0103, W4: -0.4437, b: 0.679\n",
      "loss: 0.1704\n",
      "Epoch: 60/100, Batch: 129/432, W1: 0.7779, W2: 0.5198, W3: -0.0103, W4: -0.4437, b: 0.679\n",
      "loss: 0.1693\n",
      "Epoch: 60/100, Batch: 130/432, W1: 0.7779, W2: 0.5198, W3: -0.0103, W4: -0.4438, b: 0.679\n",
      "loss: 0.1108\n",
      "Epoch: 60/100, Batch: 131/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4438, b: 0.679\n",
      "loss: 0.1411\n",
      "Epoch: 60/100, Batch: 132/432, W1: 0.778, W2: 0.5199, W3: -0.0102, W4: -0.4438, b: 0.679\n",
      "loss: 0.1536\n",
      "Epoch: 60/100, Batch: 133/432, W1: 0.778, W2: 0.5199, W3: -0.0103, W4: -0.4438, b: 0.679\n",
      "loss: 0.1308\n",
      "Epoch: 60/100, Batch: 134/432, W1: 0.778, W2: 0.5199, W3: -0.0102, W4: -0.4438, b: 0.679\n",
      "loss: 0.1721\n",
      "Epoch: 60/100, Batch: 135/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4438, b: 0.6791\n",
      "loss: 0.1371\n",
      "Epoch: 60/100, Batch: 136/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4438, b: 0.6791\n",
      "loss: 0.1128\n",
      "Epoch: 60/100, Batch: 137/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4438, b: 0.6791\n",
      "loss: 0.1436\n",
      "Epoch: 60/100, Batch: 138/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4438, b: 0.6791\n",
      "loss: 0.138\n",
      "Epoch: 60/100, Batch: 139/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4439, b: 0.6791\n",
      "loss: 0.138\n",
      "Epoch: 60/100, Batch: 140/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4439, b: 0.6791\n",
      "loss: 0.1854\n",
      "Epoch: 60/100, Batch: 141/432, W1: 0.7781, W2: 0.52, W3: -0.0102, W4: -0.4439, b: 0.6791\n",
      "loss: 0.1748\n",
      "Epoch: 60/100, Batch: 142/432, W1: 0.778, W2: 0.5199, W3: -0.0102, W4: -0.4439, b: 0.6791\n",
      "loss: 0.1821\n",
      "Epoch: 60/100, Batch: 143/432, W1: 0.7779, W2: 0.5191, W3: -0.0103, W4: -0.444, b: 0.6791\n",
      "loss: 1.8967\n",
      "Epoch: 60/100, Batch: 144/432, W1: 0.778, W2: 0.5192, W3: -0.0103, W4: -0.444, b: 0.6791\n",
      "loss: 0.1171\n",
      "Epoch: 60/100, Batch: 145/432, W1: 0.7781, W2: 0.5193, W3: -0.0102, W4: -0.444, b: 0.6792\n",
      "loss: 0.1381\n",
      "Epoch: 60/100, Batch: 146/432, W1: 0.7781, W2: 0.5193, W3: -0.0102, W4: -0.444, b: 0.6792\n",
      "loss: 0.1501\n",
      "Epoch: 60/100, Batch: 147/432, W1: 0.7782, W2: 0.5194, W3: -0.0102, W4: -0.444, b: 0.6792\n",
      "loss: 0.141\n",
      "Epoch: 60/100, Batch: 148/432, W1: 0.7781, W2: 0.5193, W3: -0.0102, W4: -0.444, b: 0.6792\n",
      "loss: 0.1631\n",
      "Epoch: 60/100, Batch: 149/432, W1: 0.7782, W2: 0.5194, W3: -0.0102, W4: -0.444, b: 0.6792\n",
      "loss: 0.1485\n",
      "Epoch: 60/100, Batch: 150/432, W1: 0.7782, W2: 0.5194, W3: -0.0101, W4: -0.444, b: 0.6792\n",
      "loss: 0.1029\n",
      "Epoch: 60/100, Batch: 151/432, W1: 0.7783, W2: 0.5195, W3: -0.0101, W4: -0.444, b: 0.6792\n",
      "loss: 0.1395\n",
      "Epoch: 60/100, Batch: 152/432, W1: 0.7784, W2: 0.5196, W3: -0.01, W4: -0.444, b: 0.6793\n",
      "loss: 0.158\n",
      "Epoch: 60/100, Batch: 153/432, W1: 0.7785, W2: 0.5197, W3: -0.01, W4: -0.444, b: 0.6793\n",
      "loss: 0.1106\n",
      "Epoch: 60/100, Batch: 154/432, W1: 0.7785, W2: 0.5197, W3: -0.01, W4: -0.4441, b: 0.6793\n",
      "loss: 0.1994\n",
      "Epoch: 60/100, Batch: 155/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4441, b: 0.6793\n",
      "loss: 0.1427\n",
      "Epoch: 60/100, Batch: 156/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4441, b: 0.6793\n",
      "loss: 0.1302\n",
      "Epoch: 60/100, Batch: 157/432, W1: 0.7785, W2: 0.5197, W3: -0.01, W4: -0.4441, b: 0.6793\n",
      "loss: 0.1378\n",
      "Epoch: 60/100, Batch: 158/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4441, b: 0.6793\n",
      "loss: 0.1347\n",
      "Epoch: 60/100, Batch: 159/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4441, b: 0.6794\n",
      "loss: 0.1199\n",
      "Epoch: 60/100, Batch: 160/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4441, b: 0.6794\n",
      "loss: 0.1519\n",
      "Epoch: 60/100, Batch: 161/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4442, b: 0.6794\n",
      "loss: 0.1393\n",
      "Epoch: 60/100, Batch: 162/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4442, b: 0.6794\n",
      "loss: 0.1132\n",
      "Epoch: 60/100, Batch: 163/432, W1: 0.7784, W2: 0.5197, W3: -0.0099, W4: -0.4442, b: 0.6794\n",
      "loss: 0.1427\n",
      "Epoch: 60/100, Batch: 164/432, W1: 0.7784, W2: 0.5197, W3: -0.0099, W4: -0.4442, b: 0.6794\n",
      "loss: 0.1146\n",
      "Epoch: 60/100, Batch: 165/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4442, b: 0.6794\n",
      "loss: 0.1655\n",
      "Epoch: 60/100, Batch: 166/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4442, b: 0.6794\n",
      "loss: 0.1323\n",
      "Epoch: 60/100, Batch: 167/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4443, b: 0.6794\n",
      "loss: 0.1159\n",
      "Epoch: 60/100, Batch: 168/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4443, b: 0.6794\n",
      "loss: 0.135\n",
      "Epoch: 60/100, Batch: 169/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4443, b: 0.6795\n",
      "loss: 0.133\n",
      "Epoch: 60/100, Batch: 170/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4443, b: 0.6795\n",
      "loss: 0.1302\n",
      "Epoch: 60/100, Batch: 171/432, W1: 0.7785, W2: 0.5198, W3: -0.0099, W4: -0.4443, b: 0.6795\n",
      "loss: 0.1084\n",
      "Epoch: 60/100, Batch: 172/432, W1: 0.7786, W2: 0.5198, W3: -0.0099, W4: -0.4443, b: 0.6795\n",
      "loss: 0.1545\n",
      "Epoch: 60/100, Batch: 173/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4444, b: 0.6795\n",
      "loss: 0.1809\n",
      "Epoch: 60/100, Batch: 174/432, W1: 0.7785, W2: 0.5198, W3: -0.0099, W4: -0.4444, b: 0.6795\n",
      "loss: 0.1197\n",
      "Epoch: 60/100, Batch: 175/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4444, b: 0.6795\n",
      "loss: 0.15\n",
      "Epoch: 60/100, Batch: 176/432, W1: 0.7784, W2: 0.5196, W3: -0.0099, W4: -0.4444, b: 0.6795\n",
      "loss: 0.1711\n",
      "Epoch: 60/100, Batch: 177/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4445, b: 0.6795\n",
      "loss: 0.1677\n",
      "Epoch: 60/100, Batch: 178/432, W1: 0.7784, W2: 0.5196, W3: -0.01, W4: -0.4445, b: 0.6795\n",
      "loss: 0.1111\n",
      "Epoch: 60/100, Batch: 179/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4445, b: 0.6795\n",
      "loss: 0.1746\n",
      "Epoch: 60/100, Batch: 180/432, W1: 0.7782, W2: 0.5195, W3: -0.01, W4: -0.4445, b: 0.6795\n",
      "loss: 0.1787\n",
      "Epoch: 60/100, Batch: 181/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4445, b: 0.6795\n",
      "loss: 0.1043\n",
      "Epoch: 60/100, Batch: 182/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4446, b: 0.6796\n",
      "loss: 0.1288\n",
      "Epoch: 60/100, Batch: 183/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4446, b: 0.6796\n",
      "loss: 0.1512\n",
      "Epoch: 60/100, Batch: 184/432, W1: 0.7784, W2: 0.5196, W3: -0.0099, W4: -0.4446, b: 0.6796\n",
      "loss: 0.1576\n",
      "Epoch: 60/100, Batch: 185/432, W1: 0.7784, W2: 0.5196, W3: -0.0099, W4: -0.4446, b: 0.6796\n",
      "loss: 0.1636\n",
      "Epoch: 60/100, Batch: 186/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4446, b: 0.6796\n",
      "loss: 0.1652\n",
      "Epoch: 60/100, Batch: 187/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4446, b: 0.6796\n",
      "loss: 0.1397\n",
      "Epoch: 60/100, Batch: 188/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4447, b: 0.6796\n",
      "loss: 0.1687\n",
      "Epoch: 60/100, Batch: 189/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4447, b: 0.6796\n",
      "loss: 0.1353\n",
      "Epoch: 60/100, Batch: 190/432, W1: 0.7783, W2: 0.5196, W3: -0.01, W4: -0.4447, b: 0.6797\n",
      "loss: 0.126\n",
      "Epoch: 60/100, Batch: 191/432, W1: 0.7784, W2: 0.5196, W3: -0.0099, W4: -0.4447, b: 0.6797\n",
      "loss: 0.1491\n",
      "Epoch: 60/100, Batch: 192/432, W1: 0.7783, W2: 0.5196, W3: -0.01, W4: -0.4447, b: 0.6797\n",
      "loss: 0.6954\n",
      "Epoch: 60/100, Batch: 193/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4447, b: 0.6797\n",
      "loss: 0.1506\n",
      "Epoch: 60/100, Batch: 194/432, W1: 0.7783, W2: 0.5196, W3: -0.01, W4: -0.4448, b: 0.6797\n",
      "loss: 0.1456\n",
      "Epoch: 60/100, Batch: 195/432, W1: 0.7784, W2: 0.5196, W3: -0.0099, W4: -0.4448, b: 0.6797\n",
      "loss: 0.1332\n",
      "Epoch: 60/100, Batch: 196/432, W1: 0.7783, W2: 0.5196, W3: -0.01, W4: -0.4448, b: 0.6797\n",
      "loss: 0.156\n",
      "Epoch: 60/100, Batch: 197/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4448, b: 0.6797\n",
      "loss: 0.1722\n",
      "Epoch: 60/100, Batch: 198/432, W1: 0.7783, W2: 0.5195, W3: -0.01, W4: -0.4448, b: 0.6797\n",
      "loss: 0.1164\n",
      "Epoch: 60/100, Batch: 199/432, W1: 0.7783, W2: 0.5196, W3: -0.01, W4: -0.4448, b: 0.6798\n",
      "loss: 0.1217\n",
      "Epoch: 60/100, Batch: 200/432, W1: 0.7784, W2: 0.5196, W3: -0.0099, W4: -0.4448, b: 0.6798\n",
      "loss: 0.1265\n",
      "Epoch: 60/100, Batch: 201/432, W1: 0.7784, W2: 0.5197, W3: -0.0099, W4: -0.4449, b: 0.6798\n",
      "loss: 0.1294\n",
      "Epoch: 60/100, Batch: 202/432, W1: 0.7784, W2: 0.5197, W3: -0.0099, W4: -0.4449, b: 0.6798\n",
      "loss: 0.1629\n",
      "Epoch: 60/100, Batch: 203/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4449, b: 0.6798\n",
      "loss: 0.1082\n",
      "Epoch: 60/100, Batch: 204/432, W1: 0.7785, W2: 0.5198, W3: -0.0098, W4: -0.4449, b: 0.6798\n",
      "loss: 0.1341\n",
      "Epoch: 60/100, Batch: 205/432, W1: 0.7785, W2: 0.5197, W3: -0.0099, W4: -0.4449, b: 0.6798\n",
      "loss: 0.1418\n",
      "Epoch: 60/100, Batch: 206/432, W1: 0.7785, W2: 0.5198, W3: -0.0098, W4: -0.4449, b: 0.6799\n",
      "loss: 0.1593\n",
      "Epoch: 60/100, Batch: 207/432, W1: 0.7785, W2: 0.5198, W3: -0.0098, W4: -0.4449, b: 0.6799\n",
      "loss: 0.1384\n",
      "Epoch: 60/100, Batch: 208/432, W1: 0.7786, W2: 0.5198, W3: -0.0098, W4: -0.4449, b: 0.6799\n",
      "loss: 0.1184\n",
      "Epoch: 60/100, Batch: 209/432, W1: 0.7786, W2: 0.5199, W3: -0.0098, W4: -0.445, b: 0.6799\n",
      "loss: 0.1317\n",
      "Epoch: 60/100, Batch: 210/432, W1: 0.7786, W2: 0.5199, W3: -0.0098, W4: -0.445, b: 0.6799\n",
      "loss: 0.1383\n",
      "Epoch: 60/100, Batch: 211/432, W1: 0.7787, W2: 0.5199, W3: -0.0097, W4: -0.445, b: 0.6799\n",
      "loss: 0.1724\n",
      "Epoch: 60/100, Batch: 212/432, W1: 0.7787, W2: 0.5199, W3: -0.0097, W4: -0.445, b: 0.68\n",
      "loss: 0.149\n",
      "Epoch: 60/100, Batch: 213/432, W1: 0.7787, W2: 0.5199, W3: -0.0097, W4: -0.445, b: 0.68\n",
      "loss: 0.1729\n",
      "Epoch: 60/100, Batch: 214/432, W1: 0.7787, W2: 0.5199, W3: -0.0097, W4: -0.445, b: 0.68\n",
      "loss: 0.1424\n",
      "Epoch: 60/100, Batch: 215/432, W1: 0.7786, W2: 0.5199, W3: -0.0098, W4: -0.4451, b: 0.68\n",
      "loss: 0.1194\n",
      "Epoch: 60/100, Batch: 216/432, W1: 0.7787, W2: 0.52, W3: -0.0097, W4: -0.4451, b: 0.68\n",
      "loss: 0.1125\n",
      "Epoch: 60/100, Batch: 217/432, W1: 0.7787, W2: 0.52, W3: -0.0097, W4: -0.4451, b: 0.68\n",
      "loss: 0.1469\n",
      "Epoch: 60/100, Batch: 218/432, W1: 0.7788, W2: 0.5201, W3: -0.0097, W4: -0.4451, b: 0.68\n",
      "loss: 0.1135\n",
      "Epoch: 60/100, Batch: 219/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4451, b: 0.68\n",
      "loss: 0.1211\n",
      "Epoch: 60/100, Batch: 220/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4451, b: 0.6801\n",
      "loss: 0.1805\n",
      "Epoch: 60/100, Batch: 221/432, W1: 0.7787, W2: 0.52, W3: -0.0097, W4: -0.4452, b: 0.6801\n",
      "loss: 0.1972\n",
      "Epoch: 60/100, Batch: 222/432, W1: 0.7787, W2: 0.52, W3: -0.0097, W4: -0.4452, b: 0.6801\n",
      "loss: 0.0976\n",
      "Epoch: 60/100, Batch: 223/432, W1: 0.7788, W2: 0.5201, W3: -0.0097, W4: -0.4452, b: 0.6801\n",
      "loss: 0.1363\n",
      "Epoch: 60/100, Batch: 224/432, W1: 0.7789, W2: 0.5201, W3: -0.0096, W4: -0.4452, b: 0.6801\n",
      "loss: 0.1387\n",
      "Epoch: 60/100, Batch: 225/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4452, b: 0.6801\n",
      "loss: 0.1646\n",
      "Epoch: 60/100, Batch: 226/432, W1: 0.7787, W2: 0.52, W3: -0.0097, W4: -0.4452, b: 0.6801\n",
      "loss: 0.1647\n",
      "Epoch: 60/100, Batch: 227/432, W1: 0.7788, W2: 0.52, W3: -0.0097, W4: -0.4453, b: 0.6801\n",
      "loss: 0.124\n",
      "Epoch: 60/100, Batch: 228/432, W1: 0.7788, W2: 0.52, W3: -0.0097, W4: -0.4453, b: 0.6801\n",
      "loss: 0.1624\n",
      "Epoch: 60/100, Batch: 229/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4453, b: 0.6802\n",
      "loss: 0.1326\n",
      "Epoch: 60/100, Batch: 230/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4453, b: 0.6802\n",
      "loss: 0.8124\n",
      "Epoch: 60/100, Batch: 231/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4453, b: 0.6802\n",
      "loss: 0.1228\n",
      "Epoch: 60/100, Batch: 232/432, W1: 0.779, W2: 0.5202, W3: -0.0095, W4: -0.4453, b: 0.6802\n",
      "loss: 0.1285\n",
      "Epoch: 60/100, Batch: 233/432, W1: 0.779, W2: 0.5202, W3: -0.0095, W4: -0.4453, b: 0.6802\n",
      "loss: 0.1266\n",
      "Epoch: 60/100, Batch: 234/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4453, b: 0.6802\n",
      "loss: 0.2019\n",
      "Epoch: 60/100, Batch: 235/432, W1: 0.779, W2: 0.5202, W3: -0.0095, W4: -0.4453, b: 0.6803\n",
      "loss: 0.1703\n",
      "Epoch: 60/100, Batch: 236/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4454, b: 0.6803\n",
      "loss: 0.149\n",
      "Epoch: 60/100, Batch: 237/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4454, b: 0.6803\n",
      "loss: 0.1445\n",
      "Epoch: 60/100, Batch: 238/432, W1: 0.779, W2: 0.5202, W3: -0.0095, W4: -0.4454, b: 0.6803\n",
      "loss: 0.1556\n",
      "Epoch: 60/100, Batch: 239/432, W1: 0.779, W2: 0.5202, W3: -0.0095, W4: -0.4454, b: 0.6803\n",
      "loss: 0.1306\n",
      "Epoch: 60/100, Batch: 240/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4454, b: 0.6803\n",
      "loss: 0.1391\n",
      "Epoch: 60/100, Batch: 241/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4455, b: 0.6803\n",
      "loss: 0.1684\n",
      "Epoch: 60/100, Batch: 242/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4455, b: 0.6803\n",
      "loss: 0.1482\n",
      "Epoch: 60/100, Batch: 243/432, W1: 0.779, W2: 0.5202, W3: -0.0095, W4: -0.4455, b: 0.6803\n",
      "loss: 0.1551\n",
      "Epoch: 60/100, Batch: 244/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4455, b: 0.6803\n",
      "loss: 0.1731\n",
      "Epoch: 60/100, Batch: 245/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4455, b: 0.6804\n",
      "loss: 0.1624\n",
      "Epoch: 60/100, Batch: 246/432, W1: 0.779, W2: 0.5202, W3: -0.0095, W4: -0.4455, b: 0.6804\n",
      "loss: 0.1274\n",
      "Epoch: 60/100, Batch: 247/432, W1: 0.779, W2: 0.5202, W3: -0.0095, W4: -0.4456, b: 0.6804\n",
      "loss: 0.1293\n",
      "Epoch: 60/100, Batch: 248/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4456, b: 0.6804\n",
      "loss: 0.1503\n",
      "Epoch: 60/100, Batch: 249/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4456, b: 0.6804\n",
      "loss: 0.1347\n",
      "Epoch: 60/100, Batch: 250/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4456, b: 0.6804\n",
      "loss: 0.1548\n",
      "Epoch: 60/100, Batch: 251/432, W1: 0.7789, W2: 0.5202, W3: -0.0096, W4: -0.4456, b: 0.6804\n",
      "loss: 0.156\n",
      "Epoch: 60/100, Batch: 252/432, W1: 0.779, W2: 0.5202, W3: -0.0095, W4: -0.4456, b: 0.6804\n",
      "loss: 0.1041\n",
      "Epoch: 60/100, Batch: 253/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4456, b: 0.6805\n",
      "loss: 0.1312\n",
      "Epoch: 60/100, Batch: 254/432, W1: 0.7791, W2: 0.5204, W3: -0.0094, W4: -0.4456, b: 0.6805\n",
      "loss: 0.1053\n",
      "Epoch: 60/100, Batch: 255/432, W1: 0.7791, W2: 0.5204, W3: -0.0094, W4: -0.4457, b: 0.6805\n",
      "loss: 0.1633\n",
      "Epoch: 60/100, Batch: 256/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4457, b: 0.6805\n",
      "loss: 0.1686\n",
      "Epoch: 60/100, Batch: 257/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4457, b: 0.6805\n",
      "loss: 0.1145\n",
      "Epoch: 60/100, Batch: 258/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4457, b: 0.6805\n",
      "loss: 0.111\n",
      "Epoch: 60/100, Batch: 259/432, W1: 0.7791, W2: 0.5204, W3: -0.0094, W4: -0.4457, b: 0.6805\n",
      "loss: 0.1197\n",
      "Epoch: 60/100, Batch: 260/432, W1: 0.7791, W2: 0.5204, W3: -0.0094, W4: -0.4458, b: 0.6805\n",
      "loss: 0.1585\n",
      "Epoch: 60/100, Batch: 261/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4458, b: 0.6805\n",
      "loss: 0.1496\n",
      "Epoch: 60/100, Batch: 262/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4458, b: 0.6805\n",
      "loss: 0.1465\n",
      "Epoch: 60/100, Batch: 263/432, W1: 0.7791, W2: 0.5204, W3: -0.0094, W4: -0.4458, b: 0.6806\n",
      "loss: 0.119\n",
      "Epoch: 60/100, Batch: 264/432, W1: 0.7791, W2: 0.5204, W3: -0.0094, W4: -0.4458, b: 0.6806\n",
      "loss: 0.1042\n",
      "Epoch: 60/100, Batch: 265/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4458, b: 0.6806\n",
      "loss: 0.1577\n",
      "Epoch: 60/100, Batch: 266/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4459, b: 0.6806\n",
      "loss: 0.1364\n",
      "Epoch: 60/100, Batch: 267/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4459, b: 0.6806\n",
      "loss: 0.1419\n",
      "Epoch: 60/100, Batch: 268/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4459, b: 0.6806\n",
      "loss: 0.1213\n",
      "Epoch: 60/100, Batch: 269/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4459, b: 0.6806\n",
      "loss: 0.1371\n",
      "Epoch: 60/100, Batch: 270/432, W1: 0.7791, W2: 0.5203, W3: -0.0094, W4: -0.4459, b: 0.6806\n",
      "loss: 0.1565\n",
      "Epoch: 60/100, Batch: 271/432, W1: 0.779, W2: 0.5203, W3: -0.0095, W4: -0.4459, b: 0.6806\n",
      "loss: 0.1249\n",
      "Epoch: 60/100, Batch: 272/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.446, b: 0.6806\n",
      "loss: 0.1403\n",
      "Epoch: 60/100, Batch: 273/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.446, b: 0.6806\n",
      "loss: 0.1443\n",
      "Epoch: 60/100, Batch: 274/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.446, b: 0.6806\n",
      "loss: 0.1624\n",
      "Epoch: 60/100, Batch: 275/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.446, b: 0.6806\n",
      "loss: 0.1497\n",
      "Epoch: 60/100, Batch: 276/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4461, b: 0.6806\n",
      "loss: 0.1372\n",
      "Epoch: 60/100, Batch: 277/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4461, b: 0.6807\n",
      "loss: 0.123\n",
      "Epoch: 60/100, Batch: 278/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4461, b: 0.6807\n",
      "loss: 0.1149\n",
      "Epoch: 60/100, Batch: 279/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4461, b: 0.6807\n",
      "loss: 0.1373\n",
      "Epoch: 60/100, Batch: 280/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4461, b: 0.6807\n",
      "loss: 0.1471\n",
      "Epoch: 60/100, Batch: 281/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4461, b: 0.6807\n",
      "loss: 0.1213\n",
      "Epoch: 60/100, Batch: 282/432, W1: 0.7787, W2: 0.52, W3: -0.0096, W4: -0.4462, b: 0.6807\n",
      "loss: 0.1892\n",
      "Epoch: 60/100, Batch: 283/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4462, b: 0.6807\n",
      "loss: 0.1403\n",
      "Epoch: 60/100, Batch: 284/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4462, b: 0.6807\n",
      "loss: 0.1267\n",
      "Epoch: 60/100, Batch: 285/432, W1: 0.7787, W2: 0.52, W3: -0.0096, W4: -0.4462, b: 0.6807\n",
      "loss: 0.1509\n",
      "Epoch: 60/100, Batch: 286/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4463, b: 0.6807\n",
      "loss: 0.2009\n",
      "Epoch: 60/100, Batch: 287/432, W1: 0.7787, W2: 0.52, W3: -0.0097, W4: -0.4463, b: 0.6807\n",
      "loss: 0.1454\n",
      "Epoch: 60/100, Batch: 288/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4463, b: 0.6808\n",
      "loss: 1.0532\n",
      "Epoch: 60/100, Batch: 289/432, W1: 0.7785, W2: 0.5198, W3: -0.0098, W4: -0.4463, b: 0.6808\n",
      "loss: 0.1286\n",
      "Epoch: 60/100, Batch: 290/432, W1: 0.7785, W2: 0.5198, W3: -0.0098, W4: -0.4463, b: 0.6808\n",
      "loss: 0.1385\n",
      "Epoch: 60/100, Batch: 291/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4463, b: 0.6808\n",
      "loss: 0.1472\n",
      "Epoch: 60/100, Batch: 292/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4463, b: 0.6808\n",
      "loss: 0.1227\n",
      "Epoch: 60/100, Batch: 293/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4463, b: 0.6808\n",
      "loss: 0.1248\n",
      "Epoch: 60/100, Batch: 294/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4464, b: 0.6808\n",
      "loss: 0.1191\n",
      "Epoch: 60/100, Batch: 295/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4464, b: 0.6808\n",
      "loss: 0.1394\n",
      "Epoch: 60/100, Batch: 296/432, W1: 0.7787, W2: 0.52, W3: -0.0097, W4: -0.4464, b: 0.6808\n",
      "loss: 0.1184\n",
      "Epoch: 60/100, Batch: 297/432, W1: 0.7787, W2: 0.52, W3: -0.0096, W4: -0.4464, b: 0.6809\n",
      "loss: 0.1534\n",
      "Epoch: 60/100, Batch: 298/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4464, b: 0.6809\n",
      "loss: 0.1352\n",
      "Epoch: 60/100, Batch: 299/432, W1: 0.7788, W2: 0.5201, W3: -0.0095, W4: -0.4464, b: 0.6809\n",
      "loss: 0.1276\n",
      "Epoch: 60/100, Batch: 300/432, W1: 0.7788, W2: 0.5201, W3: -0.0095, W4: -0.4464, b: 0.6809\n",
      "loss: 0.1325\n",
      "Epoch: 60/100, Batch: 301/432, W1: 0.7788, W2: 0.5201, W3: -0.0095, W4: -0.4464, b: 0.6809\n",
      "loss: 0.1646\n",
      "Epoch: 60/100, Batch: 302/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4464, b: 0.6809\n",
      "loss: 0.138\n",
      "Epoch: 60/100, Batch: 303/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4465, b: 0.6809\n",
      "loss: 0.1639\n",
      "Epoch: 60/100, Batch: 304/432, W1: 0.7788, W2: 0.5201, W3: -0.0095, W4: -0.4465, b: 0.6809\n",
      "loss: 0.1201\n",
      "Epoch: 60/100, Batch: 305/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4465, b: 0.681\n",
      "loss: 0.1862\n",
      "Epoch: 60/100, Batch: 306/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4465, b: 0.681\n",
      "loss: 0.1158\n",
      "Epoch: 60/100, Batch: 307/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4465, b: 0.681\n",
      "loss: 0.129\n",
      "Epoch: 60/100, Batch: 308/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4466, b: 0.681\n",
      "loss: 0.1437\n",
      "Epoch: 60/100, Batch: 309/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4466, b: 0.681\n",
      "loss: 0.1154\n",
      "Epoch: 60/100, Batch: 310/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4466, b: 0.681\n",
      "loss: 0.127\n",
      "Epoch: 60/100, Batch: 311/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4466, b: 0.681\n",
      "loss: 0.1356\n",
      "Epoch: 60/100, Batch: 312/432, W1: 0.7789, W2: 0.5202, W3: -0.0095, W4: -0.4466, b: 0.681\n",
      "loss: 0.1354\n",
      "Epoch: 60/100, Batch: 313/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4467, b: 0.681\n",
      "loss: 0.1511\n",
      "Epoch: 60/100, Batch: 314/432, W1: 0.7788, W2: 0.5201, W3: -0.0096, W4: -0.4467, b: 0.681\n",
      "loss: 0.189\n",
      "Epoch: 60/100, Batch: 315/432, W1: 0.7787, W2: 0.5201, W3: -0.0096, W4: -0.4467, b: 0.681\n",
      "loss: 0.1502\n",
      "Epoch: 60/100, Batch: 316/432, W1: 0.7787, W2: 0.52, W3: -0.0096, W4: -0.4467, b: 0.6811\n",
      "loss: 0.1541\n",
      "Epoch: 60/100, Batch: 317/432, W1: 0.7787, W2: 0.52, W3: -0.0096, W4: -0.4467, b: 0.6811\n",
      "loss: 0.1597\n",
      "Epoch: 60/100, Batch: 318/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4468, b: 0.6811\n",
      "loss: 0.1447\n",
      "Epoch: 60/100, Batch: 319/432, W1: 0.7787, W2: 0.52, W3: -0.0096, W4: -0.4468, b: 0.6811\n",
      "loss: 0.1552\n",
      "Epoch: 60/100, Batch: 320/432, W1: 0.7787, W2: 0.52, W3: -0.0096, W4: -0.4468, b: 0.6811\n",
      "loss: 0.626\n",
      "Epoch: 60/100, Batch: 321/432, W1: 0.7785, W2: 0.5198, W3: -0.0097, W4: -0.4468, b: 0.6811\n",
      "loss: 0.163\n",
      "Epoch: 60/100, Batch: 322/432, W1: 0.7785, W2: 0.5198, W3: -0.0097, W4: -0.4468, b: 0.6811\n",
      "loss: 1.0513\n",
      "Epoch: 60/100, Batch: 323/432, W1: 0.7785, W2: 0.5198, W3: -0.0097, W4: -0.4468, b: 0.6811\n",
      "loss: 0.1569\n",
      "Epoch: 60/100, Batch: 324/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4468, b: 0.6812\n",
      "loss: 0.1101\n",
      "Epoch: 60/100, Batch: 325/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4469, b: 0.6812\n",
      "loss: 0.1375\n",
      "Epoch: 60/100, Batch: 326/432, W1: 0.7786, W2: 0.5199, W3: -0.0097, W4: -0.4469, b: 0.6812\n",
      "loss: 0.1702\n",
      "Epoch: 60/100, Batch: 327/432, W1: 0.7785, W2: 0.5199, W3: -0.0097, W4: -0.4469, b: 0.6812\n",
      "loss: 0.1254\n",
      "Epoch: 60/100, Batch: 328/432, W1: 0.7785, W2: 0.5198, W3: -0.0097, W4: -0.4469, b: 0.6812\n",
      "loss: 0.1572\n",
      "Epoch: 60/100, Batch: 329/432, W1: 0.7784, W2: 0.5198, W3: -0.0098, W4: -0.4469, b: 0.6812\n",
      "loss: 0.1326\n",
      "Epoch: 60/100, Batch: 330/432, W1: 0.7784, W2: 0.5197, W3: -0.0098, W4: -0.447, b: 0.6812\n",
      "loss: 0.1199\n",
      "Epoch: 60/100, Batch: 331/432, W1: 0.7784, W2: 0.5197, W3: -0.0098, W4: -0.447, b: 0.6812\n",
      "loss: 0.1575\n",
      "Epoch: 60/100, Batch: 332/432, W1: 0.7784, W2: 0.5197, W3: -0.0098, W4: -0.447, b: 0.6812\n",
      "loss: 0.1193\n",
      "Epoch: 60/100, Batch: 333/432, W1: 0.7784, W2: 0.5197, W3: -0.0098, W4: -0.447, b: 0.6812\n",
      "loss: 0.1465\n",
      "Epoch: 60/100, Batch: 334/432, W1: 0.7784, W2: 0.5197, W3: -0.0098, W4: -0.447, b: 0.6812\n",
      "loss: 0.1412\n",
      "Epoch: 60/100, Batch: 335/432, W1: 0.7784, W2: 0.5197, W3: -0.0098, W4: -0.4471, b: 0.6812\n",
      "loss: 0.144\n",
      "Epoch: 60/100, Batch: 336/432, W1: 0.7783, W2: 0.5196, W3: -0.0098, W4: -0.4471, b: 0.6812\n",
      "loss: 0.1786\n",
      "Epoch: 60/100, Batch: 337/432, W1: 0.7783, W2: 0.5196, W3: -0.0098, W4: -0.4471, b: 0.6812\n",
      "loss: 0.1148\n",
      "Epoch: 60/100, Batch: 338/432, W1: 0.7783, W2: 0.5196, W3: -0.0098, W4: -0.4471, b: 0.6812\n",
      "loss: 0.1353\n",
      "Epoch: 60/100, Batch: 339/432, W1: 0.7784, W2: 0.5197, W3: -0.0098, W4: -0.4471, b: 0.6813\n",
      "loss: 0.1796\n",
      "Epoch: 60/100, Batch: 340/432, W1: 0.7784, W2: 0.5197, W3: -0.0098, W4: -0.4471, b: 0.6813\n",
      "loss: 0.1392\n",
      "Epoch: 60/100, Batch: 341/432, W1: 0.7784, W2: 0.5198, W3: -0.0098, W4: -0.4471, b: 0.6813\n",
      "loss: 0.3408\n",
      "Epoch: 60/100, Batch: 342/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4472, b: 0.6813\n",
      "loss: 0.1808\n",
      "Epoch: 60/100, Batch: 343/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4472, b: 0.6813\n",
      "loss: 0.187\n",
      "Epoch: 60/100, Batch: 344/432, W1: 0.7782, W2: 0.5196, W3: -0.0098, W4: -0.4472, b: 0.6813\n",
      "loss: 0.1284\n",
      "Epoch: 60/100, Batch: 345/432, W1: 0.7781, W2: 0.5195, W3: -0.0099, W4: -0.4473, b: 0.6813\n",
      "loss: 0.1357\n",
      "Epoch: 60/100, Batch: 346/432, W1: 0.7781, W2: 0.5195, W3: -0.0099, W4: -0.4473, b: 0.6813\n",
      "loss: 0.1623\n",
      "Epoch: 60/100, Batch: 347/432, W1: 0.7782, W2: 0.5196, W3: -0.0099, W4: -0.4473, b: 0.6813\n",
      "loss: 0.1391\n",
      "Epoch: 60/100, Batch: 348/432, W1: 0.7782, W2: 0.5196, W3: -0.0098, W4: -0.4473, b: 0.6813\n",
      "loss: 0.1622\n",
      "Epoch: 60/100, Batch: 349/432, W1: 0.7782, W2: 0.5196, W3: -0.0099, W4: -0.4473, b: 0.6813\n",
      "loss: 0.1678\n",
      "Epoch: 60/100, Batch: 350/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4473, b: 0.6814\n",
      "loss: 0.1155\n",
      "Epoch: 60/100, Batch: 351/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4473, b: 0.6814\n",
      "loss: 0.1131\n",
      "Epoch: 60/100, Batch: 352/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4474, b: 0.6814\n",
      "loss: 0.1453\n",
      "Epoch: 60/100, Batch: 353/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4474, b: 0.6814\n",
      "loss: 0.1719\n",
      "Epoch: 60/100, Batch: 354/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4474, b: 0.6814\n",
      "loss: 0.1528\n",
      "Epoch: 60/100, Batch: 355/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4474, b: 0.6814\n",
      "loss: 0.1687\n",
      "Epoch: 60/100, Batch: 356/432, W1: 0.7784, W2: 0.5198, W3: -0.0098, W4: -0.4474, b: 0.6814\n",
      "loss: 0.1206\n",
      "Epoch: 60/100, Batch: 357/432, W1: 0.7785, W2: 0.5199, W3: -0.0097, W4: -0.4474, b: 0.6815\n",
      "loss: 0.1338\n",
      "Epoch: 60/100, Batch: 358/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4474, b: 0.6815\n",
      "loss: 0.1716\n",
      "Epoch: 60/100, Batch: 359/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4475, b: 0.6815\n",
      "loss: 0.1184\n",
      "Epoch: 60/100, Batch: 360/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4475, b: 0.6815\n",
      "loss: 0.1229\n",
      "Epoch: 60/100, Batch: 361/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4475, b: 0.6815\n",
      "loss: 0.1774\n",
      "Epoch: 60/100, Batch: 362/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4475, b: 0.6815\n",
      "loss: 0.1531\n",
      "Epoch: 60/100, Batch: 363/432, W1: 0.7784, W2: 0.5198, W3: -0.0098, W4: -0.4475, b: 0.6815\n",
      "loss: 0.1189\n",
      "Epoch: 60/100, Batch: 364/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4476, b: 0.6815\n",
      "loss: 0.158\n",
      "Epoch: 60/100, Batch: 365/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4476, b: 0.6815\n",
      "loss: 0.1156\n",
      "Epoch: 60/100, Batch: 366/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4476, b: 0.6815\n",
      "loss: 0.1557\n",
      "Epoch: 60/100, Batch: 367/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4476, b: 0.6816\n",
      "loss: 0.1188\n",
      "Epoch: 60/100, Batch: 368/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4476, b: 0.6816\n",
      "loss: 0.1668\n",
      "Epoch: 60/100, Batch: 369/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4476, b: 0.6816\n",
      "loss: 0.111\n",
      "Epoch: 60/100, Batch: 370/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4477, b: 0.6816\n",
      "loss: 0.1139\n",
      "Epoch: 60/100, Batch: 371/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4477, b: 0.6816\n",
      "loss: 0.1258\n",
      "Epoch: 60/100, Batch: 372/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4477, b: 0.6816\n",
      "loss: 0.1217\n",
      "Epoch: 60/100, Batch: 373/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4477, b: 0.6816\n",
      "loss: 0.132\n",
      "Epoch: 60/100, Batch: 374/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4477, b: 0.6816\n",
      "loss: 0.1373\n",
      "Epoch: 60/100, Batch: 375/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4477, b: 0.6816\n",
      "loss: 0.1607\n",
      "Epoch: 60/100, Batch: 376/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4478, b: 0.6816\n",
      "loss: 0.1486\n",
      "Epoch: 60/100, Batch: 377/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4478, b: 0.6816\n",
      "loss: 0.1184\n",
      "Epoch: 60/100, Batch: 378/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4478, b: 0.6817\n",
      "loss: 0.1378\n",
      "Epoch: 60/100, Batch: 379/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4478, b: 0.6817\n",
      "loss: 0.0918\n",
      "Epoch: 60/100, Batch: 380/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4478, b: 0.6817\n",
      "loss: 0.1799\n",
      "Epoch: 60/100, Batch: 381/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4478, b: 0.6817\n",
      "loss: 0.1889\n",
      "Epoch: 60/100, Batch: 382/432, W1: 0.7783, W2: 0.5198, W3: -0.0097, W4: -0.4479, b: 0.6817\n",
      "loss: 0.1346\n",
      "Epoch: 60/100, Batch: 383/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4479, b: 0.6817\n",
      "loss: 0.1199\n",
      "Epoch: 60/100, Batch: 384/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4479, b: 0.6817\n",
      "loss: 0.1783\n",
      "Epoch: 60/100, Batch: 385/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4479, b: 0.6817\n",
      "loss: 0.168\n",
      "Epoch: 60/100, Batch: 386/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4479, b: 0.6817\n",
      "loss: 0.1418\n",
      "Epoch: 60/100, Batch: 387/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4479, b: 0.6818\n",
      "loss: 0.1014\n",
      "Epoch: 60/100, Batch: 388/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.448, b: 0.6818\n",
      "loss: 0.1601\n",
      "Epoch: 60/100, Batch: 389/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.448, b: 0.6818\n",
      "loss: 0.1377\n",
      "Epoch: 60/100, Batch: 390/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.448, b: 0.6818\n",
      "loss: 0.1414\n",
      "Epoch: 60/100, Batch: 391/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.448, b: 0.6818\n",
      "loss: 0.109\n",
      "Epoch: 60/100, Batch: 392/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.448, b: 0.6818\n",
      "loss: 0.1441\n",
      "Epoch: 60/100, Batch: 393/432, W1: 0.7784, W2: 0.5199, W3: -0.0097, W4: -0.448, b: 0.6818\n",
      "loss: 0.1085\n",
      "Epoch: 60/100, Batch: 394/432, W1: 0.7785, W2: 0.5199, W3: -0.0097, W4: -0.448, b: 0.6818\n",
      "loss: 0.1473\n",
      "Epoch: 60/100, Batch: 395/432, W1: 0.7785, W2: 0.5199, W3: -0.0096, W4: -0.4481, b: 0.6819\n",
      "loss: 0.1307\n",
      "Epoch: 60/100, Batch: 396/432, W1: 0.7785, W2: 0.5199, W3: -0.0096, W4: -0.4481, b: 0.6819\n",
      "loss: 0.1412\n",
      "Epoch: 60/100, Batch: 397/432, W1: 0.7785, W2: 0.52, W3: -0.0096, W4: -0.4481, b: 0.6819\n",
      "loss: 0.1444\n",
      "Epoch: 60/100, Batch: 398/432, W1: 0.7784, W2: 0.5199, W3: -0.0097, W4: -0.4481, b: 0.6819\n",
      "loss: 0.2063\n",
      "Epoch: 60/100, Batch: 399/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4482, b: 0.6819\n",
      "loss: 0.1817\n",
      "Epoch: 60/100, Batch: 400/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4482, b: 0.6819\n",
      "loss: 0.1333\n",
      "Epoch: 60/100, Batch: 401/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4482, b: 0.6819\n",
      "loss: 0.1395\n",
      "Epoch: 60/100, Batch: 402/432, W1: 0.7783, W2: 0.5198, W3: -0.0097, W4: -0.4482, b: 0.6819\n",
      "loss: 0.1628\n",
      "Epoch: 60/100, Batch: 403/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4482, b: 0.6819\n",
      "loss: 0.1247\n",
      "Epoch: 60/100, Batch: 404/432, W1: 0.7784, W2: 0.5199, W3: -0.0097, W4: -0.4482, b: 0.682\n",
      "loss: 0.1335\n",
      "Epoch: 60/100, Batch: 405/432, W1: 0.7785, W2: 0.5199, W3: -0.0096, W4: -0.4483, b: 0.682\n",
      "loss: 0.1451\n",
      "Epoch: 60/100, Batch: 406/432, W1: 0.7783, W2: 0.5198, W3: -0.0097, W4: -0.4483, b: 0.682\n",
      "loss: 0.1264\n",
      "Epoch: 60/100, Batch: 407/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4483, b: 0.682\n",
      "loss: 0.1556\n",
      "Epoch: 60/100, Batch: 408/432, W1: 0.7783, W2: 0.5197, W3: -0.0098, W4: -0.4483, b: 0.682\n",
      "loss: 0.1256\n",
      "Epoch: 60/100, Batch: 409/432, W1: 0.7783, W2: 0.5197, W3: -0.0097, W4: -0.4483, b: 0.682\n",
      "loss: 0.1114\n",
      "Epoch: 60/100, Batch: 410/432, W1: 0.7783, W2: 0.5197, W3: -0.0097, W4: -0.4484, b: 0.682\n",
      "loss: 0.1404\n",
      "Epoch: 60/100, Batch: 411/432, W1: 0.7783, W2: 0.5197, W3: -0.0097, W4: -0.4484, b: 0.682\n",
      "loss: 0.1224\n",
      "Epoch: 60/100, Batch: 412/432, W1: 0.7783, W2: 0.5198, W3: -0.0097, W4: -0.4484, b: 0.682\n",
      "loss: 0.1564\n",
      "Epoch: 60/100, Batch: 413/432, W1: 0.7783, W2: 0.5198, W3: -0.0097, W4: -0.4484, b: 0.682\n",
      "loss: 0.1277\n",
      "Epoch: 60/100, Batch: 414/432, W1: 0.7783, W2: 0.5198, W3: -0.0097, W4: -0.4484, b: 0.682\n",
      "loss: 0.163\n",
      "Epoch: 60/100, Batch: 415/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4484, b: 0.6821\n",
      "loss: 0.1149\n",
      "Epoch: 60/100, Batch: 416/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4484, b: 0.6821\n",
      "loss: 0.1615\n",
      "Epoch: 60/100, Batch: 417/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4485, b: 0.6821\n",
      "loss: 0.1415\n",
      "Epoch: 60/100, Batch: 418/432, W1: 0.7783, W2: 0.5198, W3: -0.0097, W4: -0.4485, b: 0.6821\n",
      "loss: 0.1868\n",
      "Epoch: 60/100, Batch: 419/432, W1: 0.7784, W2: 0.5198, W3: -0.0097, W4: -0.4485, b: 0.6821\n",
      "loss: 0.1331\n",
      "Epoch: 60/100, Batch: 420/432, W1: 0.7785, W2: 0.5199, W3: -0.0096, W4: -0.4485, b: 0.6821\n",
      "loss: 0.1318\n",
      "Epoch: 60/100, Batch: 421/432, W1: 0.7786, W2: 0.52, W3: -0.0096, W4: -0.4485, b: 0.6821\n",
      "loss: 0.1454\n",
      "Epoch: 60/100, Batch: 422/432, W1: 0.7786, W2: 0.52, W3: -0.0096, W4: -0.4485, b: 0.6822\n",
      "loss: 0.1747\n",
      "Epoch: 60/100, Batch: 423/432, W1: 0.7785, W2: 0.52, W3: -0.0096, W4: -0.4486, b: 0.6822\n",
      "loss: 0.1453\n",
      "Epoch: 60/100, Batch: 424/432, W1: 0.7785, W2: 0.5199, W3: -0.0096, W4: -0.4486, b: 0.6822\n",
      "loss: 0.1527\n",
      "Epoch: 60/100, Batch: 425/432, W1: 0.7785, W2: 0.52, W3: -0.0095, W4: -0.4486, b: 0.6822\n",
      "loss: 0.1508\n",
      "Epoch: 60/100, Batch: 426/432, W1: 0.7786, W2: 0.5201, W3: -0.0095, W4: -0.4486, b: 0.6822\n",
      "loss: 0.1209\n",
      "Epoch: 60/100, Batch: 427/432, W1: 0.7786, W2: 0.52, W3: -0.0095, W4: -0.4486, b: 0.6822\n",
      "loss: 0.182\n",
      "Epoch: 60/100, Batch: 428/432, W1: 0.7785, W2: 0.52, W3: -0.0095, W4: -0.4486, b: 0.6822\n",
      "loss: 0.1385\n",
      "Epoch: 60/100, Batch: 429/432, W1: 0.7785, W2: 0.52, W3: -0.0096, W4: -0.4487, b: 0.6822\n",
      "loss: 0.153\n",
      "Epoch: 60/100, Batch: 430/432, W1: 0.7785, W2: 0.52, W3: -0.0095, W4: -0.4487, b: 0.6822\n",
      "loss: 0.1475\n",
      "Epoch: 60/100, Batch: 431/432, W1: 0.7786, W2: 0.52, W3: -0.0095, W4: -0.4487, b: 0.6823\n",
      "loss: 0.1978\n",
      "Epoch: 60/100, Batch: 432/432, W1: 0.7787, W2: 0.5201, W3: -0.0095, W4: -0.4487, b: 0.6823\n",
      "loss: 0.1639\n",
      "Epoch: 70/100, Batch: 1/432, W1: 0.79, W2: 0.5037, W3: 0.0008, W4: -0.5089, b: 0.7231\n",
      "loss: 0.1412\n",
      "Epoch: 70/100, Batch: 2/432, W1: 0.79, W2: 0.5037, W3: 0.0008, W4: -0.5089, b: 0.7231\n",
      "loss: 0.1118\n",
      "Epoch: 70/100, Batch: 3/432, W1: 0.79, W2: 0.5038, W3: 0.0008, W4: -0.509, b: 0.7231\n",
      "loss: 0.146\n",
      "Epoch: 70/100, Batch: 4/432, W1: 0.79, W2: 0.5038, W3: 0.0008, W4: -0.509, b: 0.7231\n",
      "loss: 0.1587\n",
      "Epoch: 70/100, Batch: 5/432, W1: 0.79, W2: 0.5038, W3: 0.0008, W4: -0.509, b: 0.7231\n",
      "loss: 0.1418\n",
      "Epoch: 70/100, Batch: 6/432, W1: 0.7899, W2: 0.5037, W3: 0.0008, W4: -0.509, b: 0.7231\n",
      "loss: 0.1054\n",
      "Epoch: 70/100, Batch: 7/432, W1: 0.7899, W2: 0.5037, W3: 0.0008, W4: -0.509, b: 0.7231\n",
      "loss: 0.1345\n",
      "Epoch: 70/100, Batch: 8/432, W1: 0.7899, W2: 0.5037, W3: 0.0008, W4: -0.509, b: 0.7231\n",
      "loss: 0.1448\n",
      "Epoch: 70/100, Batch: 9/432, W1: 0.79, W2: 0.5038, W3: 0.0008, W4: -0.509, b: 0.7232\n",
      "loss: 0.1079\n",
      "Epoch: 70/100, Batch: 10/432, W1: 0.7901, W2: 0.5039, W3: 0.0009, W4: -0.5091, b: 0.7232\n",
      "loss: 0.1153\n",
      "Epoch: 70/100, Batch: 11/432, W1: 0.7902, W2: 0.504, W3: 0.0009, W4: -0.5091, b: 0.7232\n",
      "loss: 0.1116\n",
      "Epoch: 70/100, Batch: 12/432, W1: 0.7902, W2: 0.5039, W3: 0.0009, W4: -0.5091, b: 0.7232\n",
      "loss: 0.1287\n",
      "Epoch: 70/100, Batch: 13/432, W1: 0.7901, W2: 0.5039, W3: 0.0009, W4: -0.5091, b: 0.7232\n",
      "loss: 0.1993\n",
      "Epoch: 70/100, Batch: 14/432, W1: 0.7901, W2: 0.5039, W3: 0.0009, W4: -0.5091, b: 0.7232\n",
      "loss: 0.1157\n",
      "Epoch: 70/100, Batch: 15/432, W1: 0.7902, W2: 0.5039, W3: 0.0009, W4: -0.5091, b: 0.7232\n",
      "loss: 0.1191\n",
      "Epoch: 70/100, Batch: 16/432, W1: 0.7902, W2: 0.5039, W3: 0.0009, W4: -0.5091, b: 0.7232\n",
      "loss: 0.1503\n",
      "Epoch: 70/100, Batch: 17/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5092, b: 0.7233\n",
      "loss: 0.1237\n",
      "Epoch: 70/100, Batch: 18/432, W1: 0.7902, W2: 0.504, W3: 0.0009, W4: -0.5092, b: 0.7233\n",
      "loss: 0.1314\n",
      "Epoch: 70/100, Batch: 19/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5092, b: 0.7233\n",
      "loss: 0.109\n",
      "Epoch: 70/100, Batch: 20/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5092, b: 0.7233\n",
      "loss: 0.0974\n",
      "Epoch: 70/100, Batch: 21/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5092, b: 0.7233\n",
      "loss: 0.0916\n",
      "Epoch: 70/100, Batch: 22/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5092, b: 0.7233\n",
      "loss: 0.1316\n",
      "Epoch: 70/100, Batch: 23/432, W1: 0.7904, W2: 0.5042, W3: 0.0011, W4: -0.5092, b: 0.7233\n",
      "loss: 0.1478\n",
      "Epoch: 70/100, Batch: 24/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5092, b: 0.7233\n",
      "loss: 0.1326\n",
      "Epoch: 70/100, Batch: 25/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5092, b: 0.7234\n",
      "loss: 0.0961\n",
      "Epoch: 70/100, Batch: 26/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5093, b: 0.7234\n",
      "loss: 0.1194\n",
      "Epoch: 70/100, Batch: 27/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5093, b: 0.7234\n",
      "loss: 0.0819\n",
      "Epoch: 70/100, Batch: 28/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5093, b: 0.7234\n",
      "loss: 0.1003\n",
      "Epoch: 70/100, Batch: 29/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5093, b: 0.7234\n",
      "loss: 0.1368\n",
      "Epoch: 70/100, Batch: 30/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5093, b: 0.7234\n",
      "loss: 0.1419\n",
      "Epoch: 70/100, Batch: 31/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5093, b: 0.7234\n",
      "loss: 0.1006\n",
      "Epoch: 70/100, Batch: 32/432, W1: 0.7903, W2: 0.504, W3: 0.001, W4: -0.5093, b: 0.7234\n",
      "loss: 1.0695\n",
      "Epoch: 70/100, Batch: 33/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5093, b: 0.7234\n",
      "loss: 0.1289\n",
      "Epoch: 70/100, Batch: 34/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5093, b: 0.7235\n",
      "loss: 0.133\n",
      "Epoch: 70/100, Batch: 35/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5094, b: 0.7235\n",
      "loss: 0.1266\n",
      "Epoch: 70/100, Batch: 36/432, W1: 0.7903, W2: 0.5041, W3: 0.001, W4: -0.5094, b: 0.7235\n",
      "loss: 0.116\n",
      "Epoch: 70/100, Batch: 37/432, W1: 0.7903, W2: 0.5041, W3: 0.0011, W4: -0.5094, b: 0.7235\n",
      "loss: 0.1223\n",
      "Epoch: 70/100, Batch: 38/432, W1: 0.7904, W2: 0.5042, W3: 0.0011, W4: -0.5094, b: 0.7235\n",
      "loss: 0.1572\n",
      "Epoch: 70/100, Batch: 39/432, W1: 0.7903, W2: 0.5041, W3: 0.0011, W4: -0.5094, b: 0.7235\n",
      "loss: 0.149\n",
      "Epoch: 70/100, Batch: 40/432, W1: 0.7903, W2: 0.5041, W3: 0.0011, W4: -0.5094, b: 0.7235\n",
      "loss: 0.1651\n",
      "Epoch: 70/100, Batch: 41/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5095, b: 0.7235\n",
      "loss: 0.1277\n",
      "Epoch: 70/100, Batch: 42/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5095, b: 0.7235\n",
      "loss: 0.1397\n",
      "Epoch: 70/100, Batch: 43/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5095, b: 0.7235\n",
      "loss: 0.1451\n",
      "Epoch: 70/100, Batch: 44/432, W1: 0.7901, W2: 0.504, W3: 0.001, W4: -0.5095, b: 0.7235\n",
      "loss: 0.1665\n",
      "Epoch: 70/100, Batch: 45/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5096, b: 0.7235\n",
      "loss: 0.112\n",
      "Epoch: 70/100, Batch: 46/432, W1: 0.7902, W2: 0.504, W3: 0.001, W4: -0.5096, b: 0.7236\n",
      "loss: 0.103\n",
      "Epoch: 70/100, Batch: 47/432, W1: 0.7903, W2: 0.5041, W3: 0.0011, W4: -0.5096, b: 0.7236\n",
      "loss: 0.1275\n",
      "Epoch: 70/100, Batch: 48/432, W1: 0.7902, W2: 0.5033, W3: 0.001, W4: -0.5096, b: 0.7236\n",
      "loss: 1.7923\n",
      "Epoch: 70/100, Batch: 49/432, W1: 0.7902, W2: 0.5034, W3: 0.001, W4: -0.5096, b: 0.7236\n",
      "loss: 0.0993\n",
      "Epoch: 70/100, Batch: 50/432, W1: 0.7903, W2: 0.5034, W3: 0.001, W4: -0.5096, b: 0.7236\n",
      "loss: 0.1207\n",
      "Epoch: 70/100, Batch: 51/432, W1: 0.7903, W2: 0.5034, W3: 0.0011, W4: -0.5096, b: 0.7236\n",
      "loss: 0.1118\n",
      "Epoch: 70/100, Batch: 52/432, W1: 0.7903, W2: 0.5035, W3: 0.0011, W4: -0.5096, b: 0.7236\n",
      "loss: 0.1394\n",
      "Epoch: 70/100, Batch: 53/432, W1: 0.7904, W2: 0.5035, W3: 0.0011, W4: -0.5096, b: 0.7236\n",
      "loss: 0.1171\n",
      "Epoch: 70/100, Batch: 54/432, W1: 0.7903, W2: 0.5035, W3: 0.0011, W4: -0.5097, b: 0.7236\n",
      "loss: 0.151\n",
      "Epoch: 70/100, Batch: 55/432, W1: 0.7903, W2: 0.5034, W3: 0.001, W4: -0.5097, b: 0.7236\n",
      "loss: 0.1163\n",
      "Epoch: 70/100, Batch: 56/432, W1: 0.7903, W2: 0.5034, W3: 0.001, W4: -0.5097, b: 0.7237\n",
      "loss: 0.1257\n",
      "Epoch: 70/100, Batch: 57/432, W1: 0.7904, W2: 0.5035, W3: 0.0011, W4: -0.5097, b: 0.7237\n",
      "loss: 0.112\n",
      "Epoch: 70/100, Batch: 58/432, W1: 0.7905, W2: 0.5036, W3: 0.0011, W4: -0.5097, b: 0.7237\n",
      "loss: 0.1337\n",
      "Epoch: 70/100, Batch: 59/432, W1: 0.7905, W2: 0.5036, W3: 0.0012, W4: -0.5097, b: 0.7237\n",
      "loss: 0.1617\n",
      "Epoch: 70/100, Batch: 60/432, W1: 0.7905, W2: 0.5036, W3: 0.0012, W4: -0.5097, b: 0.7237\n",
      "loss: 0.1216\n",
      "Epoch: 70/100, Batch: 61/432, W1: 0.7905, W2: 0.5036, W3: 0.0012, W4: -0.5097, b: 0.7237\n",
      "loss: 0.1168\n",
      "Epoch: 70/100, Batch: 62/432, W1: 0.7905, W2: 0.5037, W3: 0.0012, W4: -0.5097, b: 0.7237\n",
      "loss: 0.1453\n",
      "Epoch: 70/100, Batch: 63/432, W1: 0.7905, W2: 0.5037, W3: 0.0012, W4: -0.5097, b: 0.7238\n",
      "loss: 0.1414\n",
      "Epoch: 70/100, Batch: 64/432, W1: 0.7905, W2: 0.5037, W3: 0.0012, W4: -0.5098, b: 0.7238\n",
      "loss: 0.1262\n",
      "Epoch: 70/100, Batch: 65/432, W1: 0.7906, W2: 0.5037, W3: 0.0012, W4: -0.5098, b: 0.7238\n",
      "loss: 0.139\n",
      "Epoch: 70/100, Batch: 66/432, W1: 0.7905, W2: 0.5037, W3: 0.0012, W4: -0.5098, b: 0.7238\n",
      "loss: 0.1423\n",
      "Epoch: 70/100, Batch: 67/432, W1: 0.7905, W2: 0.5036, W3: 0.0012, W4: -0.5098, b: 0.7238\n",
      "loss: 0.1354\n",
      "Epoch: 70/100, Batch: 68/432, W1: 0.7904, W2: 0.5036, W3: 0.0011, W4: -0.5098, b: 0.7238\n",
      "loss: 0.145\n",
      "Epoch: 70/100, Batch: 69/432, W1: 0.7905, W2: 0.5036, W3: 0.0012, W4: -0.5099, b: 0.7238\n",
      "loss: 0.0955\n",
      "Epoch: 70/100, Batch: 70/432, W1: 0.7904, W2: 0.5036, W3: 0.0011, W4: -0.5099, b: 0.7238\n",
      "loss: 0.1198\n",
      "Epoch: 70/100, Batch: 71/432, W1: 0.7904, W2: 0.5035, W3: 0.0011, W4: -0.5099, b: 0.7238\n",
      "loss: 0.1385\n",
      "Epoch: 70/100, Batch: 72/432, W1: 0.7904, W2: 0.5035, W3: 0.0011, W4: -0.5099, b: 0.7238\n",
      "loss: 0.1315\n",
      "Epoch: 70/100, Batch: 73/432, W1: 0.7904, W2: 0.5035, W3: 0.0011, W4: -0.5099, b: 0.7238\n",
      "loss: 0.1062\n",
      "Epoch: 70/100, Batch: 74/432, W1: 0.7904, W2: 0.5036, W3: 0.0011, W4: -0.5099, b: 0.7238\n",
      "loss: 0.1223\n",
      "Epoch: 70/100, Batch: 75/432, W1: 0.7905, W2: 0.5036, W3: 0.0012, W4: -0.5099, b: 0.7239\n",
      "loss: 0.119\n",
      "Epoch: 70/100, Batch: 76/432, W1: 0.7906, W2: 0.5037, W3: 0.0012, W4: -0.5099, b: 0.7239\n",
      "loss: 0.0971\n",
      "Epoch: 70/100, Batch: 77/432, W1: 0.7905, W2: 0.5037, W3: 0.0012, W4: -0.51, b: 0.7239\n",
      "loss: 0.1266\n",
      "Epoch: 70/100, Batch: 78/432, W1: 0.7905, W2: 0.5036, W3: 0.0012, W4: -0.51, b: 0.7239\n",
      "loss: 0.1247\n",
      "Epoch: 70/100, Batch: 79/432, W1: 0.7905, W2: 0.5037, W3: 0.0012, W4: -0.51, b: 0.7239\n",
      "loss: 0.1185\n",
      "Epoch: 70/100, Batch: 80/432, W1: 0.7906, W2: 0.5038, W3: 0.0013, W4: -0.51, b: 0.7239\n",
      "loss: 0.1406\n",
      "Epoch: 70/100, Batch: 81/432, W1: 0.7907, W2: 0.5038, W3: 0.0013, W4: -0.51, b: 0.7239\n",
      "loss: 0.133\n",
      "Epoch: 70/100, Batch: 82/432, W1: 0.7907, W2: 0.5038, W3: 0.0013, W4: -0.51, b: 0.7239\n",
      "loss: 0.1099\n",
      "Epoch: 70/100, Batch: 83/432, W1: 0.7906, W2: 0.5038, W3: 0.0013, W4: -0.51, b: 0.7239\n",
      "loss: 0.1176\n",
      "Epoch: 70/100, Batch: 84/432, W1: 0.7906, W2: 0.5037, W3: 0.0013, W4: -0.51, b: 0.724\n",
      "loss: 0.1171\n",
      "Epoch: 70/100, Batch: 85/432, W1: 0.7906, W2: 0.5037, W3: 0.0012, W4: -0.5101, b: 0.724\n",
      "loss: 0.1536\n",
      "Epoch: 70/100, Batch: 86/432, W1: 0.7906, W2: 0.5037, W3: 0.0013, W4: -0.5101, b: 0.724\n",
      "loss: 0.1218\n",
      "Epoch: 70/100, Batch: 87/432, W1: 0.7901, W2: 0.5006, W3: 0.0008, W4: -0.5102, b: 0.7239\n",
      "loss: 7.0683\n",
      "Epoch: 70/100, Batch: 88/432, W1: 0.7901, W2: 0.5005, W3: 0.0008, W4: -0.5102, b: 0.7239\n",
      "loss: 0.1244\n",
      "Epoch: 70/100, Batch: 89/432, W1: 0.7901, W2: 0.5006, W3: 0.0008, W4: -0.5102, b: 0.7239\n",
      "loss: 0.1271\n",
      "Epoch: 70/100, Batch: 90/432, W1: 0.7901, W2: 0.5006, W3: 0.0008, W4: -0.5103, b: 0.7239\n",
      "loss: 0.1652\n",
      "Epoch: 70/100, Batch: 91/432, W1: 0.7901, W2: 0.5006, W3: 0.0008, W4: -0.5103, b: 0.7239\n",
      "loss: 0.1181\n",
      "Epoch: 70/100, Batch: 92/432, W1: 0.7902, W2: 0.5007, W3: 0.0009, W4: -0.5103, b: 0.724\n",
      "loss: 0.1204\n",
      "Epoch: 70/100, Batch: 93/432, W1: 0.7902, W2: 0.5007, W3: 0.0009, W4: -0.5103, b: 0.724\n",
      "loss: 0.1084\n",
      "Epoch: 70/100, Batch: 94/432, W1: 0.7903, W2: 0.5007, W3: 0.0009, W4: -0.5103, b: 0.724\n",
      "loss: 0.1192\n",
      "Epoch: 70/100, Batch: 95/432, W1: 0.7903, W2: 0.5007, W3: 0.0009, W4: -0.5103, b: 0.724\n",
      "loss: 0.1265\n",
      "Epoch: 70/100, Batch: 96/432, W1: 0.7903, W2: 0.5007, W3: 0.0009, W4: -0.5103, b: 0.724\n",
      "loss: 0.1117\n",
      "Epoch: 70/100, Batch: 97/432, W1: 0.7903, W2: 0.5008, W3: 0.0009, W4: -0.5103, b: 0.724\n",
      "loss: 0.1304\n",
      "Epoch: 70/100, Batch: 98/432, W1: 0.7903, W2: 0.5007, W3: 0.0009, W4: -0.5103, b: 0.724\n",
      "loss: 0.1508\n",
      "Epoch: 70/100, Batch: 99/432, W1: 0.7903, W2: 0.5008, W3: 0.0009, W4: -0.5104, b: 0.7241\n",
      "loss: 0.1254\n",
      "Epoch: 70/100, Batch: 100/432, W1: 0.7903, W2: 0.5007, W3: 0.0009, W4: -0.5104, b: 0.7241\n",
      "loss: 0.1021\n",
      "Epoch: 70/100, Batch: 101/432, W1: 0.7903, W2: 0.5008, W3: 0.0009, W4: -0.5104, b: 0.7241\n",
      "loss: 0.1001\n",
      "Epoch: 70/100, Batch: 102/432, W1: 0.7904, W2: 0.5008, W3: 0.001, W4: -0.5104, b: 0.7241\n",
      "loss: 0.1466\n",
      "Epoch: 70/100, Batch: 103/432, W1: 0.7903, W2: 0.5008, W3: 0.001, W4: -0.5104, b: 0.7241\n",
      "loss: 0.1218\n",
      "Epoch: 70/100, Batch: 104/432, W1: 0.7904, W2: 0.5009, W3: 0.001, W4: -0.5104, b: 0.7241\n",
      "loss: 0.0974\n",
      "Epoch: 70/100, Batch: 105/432, W1: 0.7904, W2: 0.5009, W3: 0.001, W4: -0.5104, b: 0.7241\n",
      "loss: 0.1151\n",
      "Epoch: 70/100, Batch: 106/432, W1: 0.7904, W2: 0.5009, W3: 0.001, W4: -0.5104, b: 0.7241\n",
      "loss: 0.1345\n",
      "Epoch: 70/100, Batch: 107/432, W1: 0.7905, W2: 0.501, W3: 0.0011, W4: -0.5104, b: 0.7242\n",
      "loss: 0.1338\n",
      "Epoch: 70/100, Batch: 108/432, W1: 0.7905, W2: 0.501, W3: 0.0011, W4: -0.5105, b: 0.7242\n",
      "loss: 0.1454\n",
      "Epoch: 70/100, Batch: 109/432, W1: 0.7905, W2: 0.501, W3: 0.0011, W4: -0.5105, b: 0.7242\n",
      "loss: 0.1467\n",
      "Epoch: 70/100, Batch: 110/432, W1: 0.7906, W2: 0.501, W3: 0.0011, W4: -0.5105, b: 0.7242\n",
      "loss: 0.1184\n",
      "Epoch: 70/100, Batch: 111/432, W1: 0.7907, W2: 0.5011, W3: 0.0012, W4: -0.5105, b: 0.7242\n",
      "loss: 0.1087\n",
      "Epoch: 70/100, Batch: 112/432, W1: 0.7907, W2: 0.5012, W3: 0.0012, W4: -0.5105, b: 0.7242\n",
      "loss: 0.1316\n",
      "Epoch: 70/100, Batch: 113/432, W1: 0.7907, W2: 0.5012, W3: 0.0012, W4: -0.5105, b: 0.7242\n",
      "loss: 0.1444\n",
      "Epoch: 70/100, Batch: 114/432, W1: 0.7907, W2: 0.5012, W3: 0.0012, W4: -0.5105, b: 0.7243\n",
      "loss: 0.1427\n",
      "Epoch: 70/100, Batch: 115/432, W1: 0.7907, W2: 0.5012, W3: 0.0012, W4: -0.5105, b: 0.7243\n",
      "loss: 0.1282\n",
      "Epoch: 70/100, Batch: 116/432, W1: 0.7907, W2: 0.5012, W3: 0.0012, W4: -0.5105, b: 0.7243\n",
      "loss: 0.1368\n",
      "Epoch: 70/100, Batch: 117/432, W1: 0.7907, W2: 0.5012, W3: 0.0012, W4: -0.5106, b: 0.7243\n",
      "loss: 0.1561\n",
      "Epoch: 70/100, Batch: 118/432, W1: 0.7907, W2: 0.5012, W3: 0.0012, W4: -0.5106, b: 0.7243\n",
      "loss: 0.1325\n",
      "Epoch: 70/100, Batch: 119/432, W1: 0.7906, W2: 0.5011, W3: 0.0012, W4: -0.5106, b: 0.7243\n",
      "loss: 0.148\n",
      "Epoch: 70/100, Batch: 120/432, W1: 0.7907, W2: 0.5012, W3: 0.0012, W4: -0.5106, b: 0.7243\n",
      "loss: 0.117\n",
      "Epoch: 70/100, Batch: 121/432, W1: 0.7908, W2: 0.5013, W3: 0.0013, W4: -0.5106, b: 0.7243\n",
      "loss: 0.1036\n",
      "Epoch: 70/100, Batch: 122/432, W1: 0.7908, W2: 0.5013, W3: 0.0013, W4: -0.5106, b: 0.7244\n",
      "loss: 0.1572\n",
      "Epoch: 70/100, Batch: 123/432, W1: 0.7909, W2: 0.5014, W3: 0.0013, W4: -0.5106, b: 0.7244\n",
      "loss: 0.1122\n",
      "Epoch: 70/100, Batch: 124/432, W1: 0.7909, W2: 0.5013, W3: 0.0013, W4: -0.5107, b: 0.7244\n",
      "loss: 0.1416\n",
      "Epoch: 70/100, Batch: 125/432, W1: 0.7909, W2: 0.5013, W3: 0.0013, W4: -0.5107, b: 0.7244\n",
      "loss: 0.1139\n",
      "Epoch: 70/100, Batch: 126/432, W1: 0.7909, W2: 0.5014, W3: 0.0013, W4: -0.5107, b: 0.7244\n",
      "loss: 0.1111\n",
      "Epoch: 70/100, Batch: 127/432, W1: 0.7909, W2: 0.5014, W3: 0.0013, W4: -0.5107, b: 0.7244\n",
      "loss: 0.1415\n",
      "Epoch: 70/100, Batch: 128/432, W1: 0.7909, W2: 0.5014, W3: 0.0014, W4: -0.5107, b: 0.7244\n",
      "loss: 1.1752\n",
      "Epoch: 70/100, Batch: 129/432, W1: 0.7909, W2: 0.5014, W3: 0.0013, W4: -0.5107, b: 0.7245\n",
      "loss: 0.1359\n",
      "Epoch: 70/100, Batch: 130/432, W1: 0.7909, W2: 0.5014, W3: 0.0014, W4: -0.5107, b: 0.7245\n",
      "loss: 0.147\n",
      "Epoch: 70/100, Batch: 131/432, W1: 0.7909, W2: 0.5014, W3: 0.0013, W4: -0.5107, b: 0.7245\n",
      "loss: 0.1285\n",
      "Epoch: 70/100, Batch: 132/432, W1: 0.7909, W2: 0.5014, W3: 0.0013, W4: -0.5107, b: 0.7245\n",
      "loss: 0.7042\n",
      "Epoch: 70/100, Batch: 133/432, W1: 0.7909, W2: 0.5014, W3: 0.0014, W4: -0.5107, b: 0.7245\n",
      "loss: 0.0919\n",
      "Epoch: 70/100, Batch: 134/432, W1: 0.7909, W2: 0.5014, W3: 0.0014, W4: -0.5108, b: 0.7245\n",
      "loss: 0.1533\n",
      "Epoch: 70/100, Batch: 135/432, W1: 0.7909, W2: 0.5014, W3: 0.0014, W4: -0.5108, b: 0.7245\n",
      "loss: 0.1112\n",
      "Epoch: 70/100, Batch: 136/432, W1: 0.7909, W2: 0.5014, W3: 0.0014, W4: -0.5108, b: 0.7245\n",
      "loss: 0.1404\n",
      "Epoch: 70/100, Batch: 137/432, W1: 0.791, W2: 0.5015, W3: 0.0014, W4: -0.5108, b: 0.7246\n",
      "loss: 0.1104\n",
      "Epoch: 70/100, Batch: 138/432, W1: 0.7911, W2: 0.5016, W3: 0.0015, W4: -0.5108, b: 0.7246\n",
      "loss: 0.6331\n",
      "Epoch: 70/100, Batch: 139/432, W1: 0.7911, W2: 0.5016, W3: 0.0015, W4: -0.5108, b: 0.7246\n",
      "loss: 0.1297\n",
      "Epoch: 70/100, Batch: 140/432, W1: 0.7911, W2: 0.5016, W3: 0.0015, W4: -0.5108, b: 0.7246\n",
      "loss: 0.1132\n",
      "Epoch: 70/100, Batch: 141/432, W1: 0.7911, W2: 0.5016, W3: 0.0015, W4: -0.5108, b: 0.7246\n",
      "loss: 0.1316\n",
      "Epoch: 70/100, Batch: 142/432, W1: 0.7911, W2: 0.5016, W3: 0.0015, W4: -0.5108, b: 0.7246\n",
      "loss: 0.1454\n",
      "Epoch: 70/100, Batch: 143/432, W1: 0.7911, W2: 0.5016, W3: 0.0015, W4: -0.5108, b: 0.7246\n",
      "loss: 0.1226\n",
      "Epoch: 70/100, Batch: 144/432, W1: 0.7911, W2: 0.5017, W3: 0.0015, W4: -0.5108, b: 0.7247\n",
      "loss: 0.1465\n",
      "Epoch: 70/100, Batch: 145/432, W1: 0.7912, W2: 0.5017, W3: 0.0015, W4: -0.5108, b: 0.7247\n",
      "loss: 0.0832\n",
      "Epoch: 70/100, Batch: 146/432, W1: 0.7912, W2: 0.5017, W3: 0.0016, W4: -0.5108, b: 0.7247\n",
      "loss: 0.1553\n",
      "Epoch: 70/100, Batch: 147/432, W1: 0.7912, W2: 0.5017, W3: 0.0016, W4: -0.5109, b: 0.7247\n",
      "loss: 0.1297\n",
      "Epoch: 70/100, Batch: 148/432, W1: 0.7913, W2: 0.5018, W3: 0.0016, W4: -0.5109, b: 0.7247\n",
      "loss: 0.0902\n",
      "Epoch: 70/100, Batch: 149/432, W1: 0.7913, W2: 0.5018, W3: 0.0016, W4: -0.5109, b: 0.7247\n",
      "loss: 0.1381\n",
      "Epoch: 70/100, Batch: 150/432, W1: 0.7913, W2: 0.5018, W3: 0.0016, W4: -0.5109, b: 0.7247\n",
      "loss: 0.0963\n",
      "Epoch: 70/100, Batch: 151/432, W1: 0.7913, W2: 0.5018, W3: 0.0016, W4: -0.5109, b: 0.7247\n",
      "loss: 0.1202\n",
      "Epoch: 70/100, Batch: 152/432, W1: 0.7912, W2: 0.5017, W3: 0.0016, W4: -0.5109, b: 0.7247\n",
      "loss: 0.1853\n",
      "Epoch: 70/100, Batch: 153/432, W1: 0.7912, W2: 0.5018, W3: 0.0016, W4: -0.5109, b: 0.7247\n",
      "loss: 0.37\n",
      "Epoch: 70/100, Batch: 154/432, W1: 0.7913, W2: 0.5018, W3: 0.0016, W4: -0.511, b: 0.7248\n",
      "loss: 0.1089\n",
      "Epoch: 70/100, Batch: 155/432, W1: 0.7913, W2: 0.5019, W3: 0.0016, W4: -0.511, b: 0.7248\n",
      "loss: 0.1003\n",
      "Epoch: 70/100, Batch: 156/432, W1: 0.7912, W2: 0.5018, W3: 0.0016, W4: -0.511, b: 0.7248\n",
      "loss: 0.1374\n",
      "Epoch: 70/100, Batch: 157/432, W1: 0.7913, W2: 0.5018, W3: 0.0016, W4: -0.511, b: 0.7248\n",
      "loss: 0.112\n",
      "Epoch: 70/100, Batch: 158/432, W1: 0.7913, W2: 0.5018, W3: 0.0016, W4: -0.511, b: 0.7248\n",
      "loss: 0.1016\n",
      "Epoch: 70/100, Batch: 159/432, W1: 0.7913, W2: 0.5018, W3: 0.0016, W4: -0.511, b: 0.7248\n",
      "loss: 0.1375\n",
      "Epoch: 70/100, Batch: 160/432, W1: 0.7912, W2: 0.5018, W3: 0.0016, W4: -0.5111, b: 0.7248\n",
      "loss: 0.1301\n",
      "Epoch: 70/100, Batch: 161/432, W1: 0.7911, W2: 0.5017, W3: 0.0015, W4: -0.5111, b: 0.7248\n",
      "loss: 0.1067\n",
      "Epoch: 70/100, Batch: 162/432, W1: 0.7911, W2: 0.5017, W3: 0.0015, W4: -0.5111, b: 0.7248\n",
      "loss: 0.1443\n",
      "Epoch: 70/100, Batch: 163/432, W1: 0.7911, W2: 0.5017, W3: 0.0015, W4: -0.5111, b: 0.7248\n",
      "loss: 0.1034\n",
      "Epoch: 70/100, Batch: 164/432, W1: 0.7911, W2: 0.5017, W3: 0.0015, W4: -0.5111, b: 0.7248\n",
      "loss: 0.0998\n",
      "Epoch: 70/100, Batch: 165/432, W1: 0.7911, W2: 0.5017, W3: 0.0016, W4: -0.5111, b: 0.7248\n",
      "loss: 0.1522\n",
      "Epoch: 70/100, Batch: 166/432, W1: 0.7911, W2: 0.5017, W3: 0.0015, W4: -0.5111, b: 0.7249\n",
      "loss: 0.1196\n",
      "Epoch: 70/100, Batch: 167/432, W1: 0.7912, W2: 0.5017, W3: 0.0016, W4: -0.5112, b: 0.7249\n",
      "loss: 0.1111\n",
      "Epoch: 70/100, Batch: 168/432, W1: 0.7912, W2: 0.5018, W3: 0.0016, W4: -0.5112, b: 0.7249\n",
      "loss: 0.1215\n",
      "Epoch: 70/100, Batch: 169/432, W1: 0.7911, W2: 0.5017, W3: 0.0015, W4: -0.5112, b: 0.7249\n",
      "loss: 0.1441\n",
      "Epoch: 70/100, Batch: 170/432, W1: 0.7911, W2: 0.5017, W3: 0.0016, W4: -0.5112, b: 0.7249\n",
      "loss: 0.124\n",
      "Epoch: 70/100, Batch: 171/432, W1: 0.7912, W2: 0.5018, W3: 0.0016, W4: -0.5112, b: 0.7249\n",
      "loss: 0.1244\n",
      "Epoch: 70/100, Batch: 172/432, W1: 0.7912, W2: 0.5018, W3: 0.0016, W4: -0.5112, b: 0.7249\n",
      "loss: 0.1114\n",
      "Epoch: 70/100, Batch: 173/432, W1: 0.7912, W2: 0.5018, W3: 0.0016, W4: -0.5112, b: 0.7249\n",
      "loss: 0.1135\n",
      "Epoch: 70/100, Batch: 174/432, W1: 0.7912, W2: 0.5018, W3: 0.0016, W4: -0.5112, b: 0.7249\n",
      "loss: 0.0902\n",
      "Epoch: 70/100, Batch: 175/432, W1: 0.7913, W2: 0.5019, W3: 0.0016, W4: -0.5113, b: 0.725\n",
      "loss: 0.1419\n",
      "Epoch: 70/100, Batch: 176/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5113, b: 0.725\n",
      "loss: 0.1154\n",
      "Epoch: 70/100, Batch: 177/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5113, b: 0.725\n",
      "loss: 0.1083\n",
      "Epoch: 70/100, Batch: 178/432, W1: 0.7914, W2: 0.502, W3: 0.0017, W4: -0.5113, b: 0.725\n",
      "loss: 0.098\n",
      "Epoch: 70/100, Batch: 179/432, W1: 0.7914, W2: 0.502, W3: 0.0017, W4: -0.5113, b: 0.725\n",
      "loss: 0.1188\n",
      "Epoch: 70/100, Batch: 180/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5113, b: 0.725\n",
      "loss: 0.1369\n",
      "Epoch: 70/100, Batch: 181/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5113, b: 0.725\n",
      "loss: 0.1341\n",
      "Epoch: 70/100, Batch: 182/432, W1: 0.7916, W2: 0.5022, W3: 0.0018, W4: -0.5113, b: 0.7251\n",
      "loss: 0.1027\n",
      "Epoch: 70/100, Batch: 183/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5113, b: 0.7251\n",
      "loss: 0.1446\n",
      "Epoch: 70/100, Batch: 184/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5113, b: 0.7251\n",
      "loss: 0.1291\n",
      "Epoch: 70/100, Batch: 185/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5114, b: 0.7251\n",
      "loss: 0.1409\n",
      "Epoch: 70/100, Batch: 186/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5114, b: 0.7251\n",
      "loss: 0.1013\n",
      "Epoch: 70/100, Batch: 187/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5114, b: 0.7251\n",
      "loss: 0.1307\n",
      "Epoch: 70/100, Batch: 188/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5114, b: 0.7251\n",
      "loss: 0.1227\n",
      "Epoch: 70/100, Batch: 189/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5114, b: 0.7251\n",
      "loss: 0.1154\n",
      "Epoch: 70/100, Batch: 190/432, W1: 0.7916, W2: 0.5022, W3: 0.0018, W4: -0.5114, b: 0.7251\n",
      "loss: 0.1153\n",
      "Epoch: 70/100, Batch: 191/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5114, b: 0.7251\n",
      "loss: 0.137\n",
      "Epoch: 70/100, Batch: 192/432, W1: 0.7916, W2: 0.5022, W3: 0.0018, W4: -0.5115, b: 0.7252\n",
      "loss: 0.1097\n",
      "Epoch: 70/100, Batch: 193/432, W1: 0.7916, W2: 0.5022, W3: 0.0018, W4: -0.5115, b: 0.7252\n",
      "loss: 0.1207\n",
      "Epoch: 70/100, Batch: 194/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5115, b: 0.7252\n",
      "loss: 0.1188\n",
      "Epoch: 70/100, Batch: 195/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5115, b: 0.7252\n",
      "loss: 0.1514\n",
      "Epoch: 70/100, Batch: 196/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5115, b: 0.7252\n",
      "loss: 0.1424\n",
      "Epoch: 70/100, Batch: 197/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5115, b: 0.7252\n",
      "loss: 0.0923\n",
      "Epoch: 70/100, Batch: 198/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5116, b: 0.7252\n",
      "loss: 0.1501\n",
      "Epoch: 70/100, Batch: 199/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5116, b: 0.7252\n",
      "loss: 0.1404\n",
      "Epoch: 70/100, Batch: 200/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5116, b: 0.7252\n",
      "loss: 0.1383\n",
      "Epoch: 70/100, Batch: 201/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5116, b: 0.7252\n",
      "loss: 0.1222\n",
      "Epoch: 70/100, Batch: 202/432, W1: 0.7915, W2: 0.5022, W3: 0.0018, W4: -0.5116, b: 0.7252\n",
      "loss: 0.1212\n",
      "Epoch: 70/100, Batch: 203/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5116, b: 0.7253\n",
      "loss: 0.1438\n",
      "Epoch: 70/100, Batch: 204/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5116, b: 0.7253\n",
      "loss: 0.1321\n",
      "Epoch: 70/100, Batch: 205/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5117, b: 0.7253\n",
      "loss: 0.1306\n",
      "Epoch: 70/100, Batch: 206/432, W1: 0.7914, W2: 0.5021, W3: 0.0018, W4: -0.5117, b: 0.7253\n",
      "loss: 0.1202\n",
      "Epoch: 70/100, Batch: 207/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5117, b: 0.7253\n",
      "loss: 0.1256\n",
      "Epoch: 70/100, Batch: 208/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5117, b: 0.7253\n",
      "loss: 0.1232\n",
      "Epoch: 70/100, Batch: 209/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5117, b: 0.7253\n",
      "loss: 0.1519\n",
      "Epoch: 70/100, Batch: 210/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5117, b: 0.7253\n",
      "loss: 0.1119\n",
      "Epoch: 70/100, Batch: 211/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5117, b: 0.7253\n",
      "loss: 0.1077\n",
      "Epoch: 70/100, Batch: 212/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5118, b: 0.7253\n",
      "loss: 0.1652\n",
      "Epoch: 70/100, Batch: 213/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5118, b: 0.7253\n",
      "loss: 0.1338\n",
      "Epoch: 70/100, Batch: 214/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5118, b: 0.7253\n",
      "loss: 0.1458\n",
      "Epoch: 70/100, Batch: 215/432, W1: 0.7914, W2: 0.502, W3: 0.0017, W4: -0.5118, b: 0.7253\n",
      "loss: 0.1435\n",
      "Epoch: 70/100, Batch: 216/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5119, b: 0.7253\n",
      "loss: 0.1381\n",
      "Epoch: 70/100, Batch: 217/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5119, b: 0.7254\n",
      "loss: 0.1565\n",
      "Epoch: 70/100, Batch: 218/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5119, b: 0.7254\n",
      "loss: 0.0849\n",
      "Epoch: 70/100, Batch: 219/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5119, b: 0.7254\n",
      "loss: 0.12\n",
      "Epoch: 70/100, Batch: 220/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5119, b: 0.7254\n",
      "loss: 0.1443\n",
      "Epoch: 70/100, Batch: 221/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5119, b: 0.7254\n",
      "loss: 0.1205\n",
      "Epoch: 70/100, Batch: 222/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5119, b: 0.7254\n",
      "loss: 0.1216\n",
      "Epoch: 70/100, Batch: 223/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.512, b: 0.7254\n",
      "loss: 0.1243\n",
      "Epoch: 70/100, Batch: 224/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.512, b: 0.7254\n",
      "loss: 0.127\n",
      "Epoch: 70/100, Batch: 225/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.512, b: 0.7254\n",
      "loss: 0.164\n",
      "Epoch: 70/100, Batch: 226/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.512, b: 0.7254\n",
      "loss: 0.1301\n",
      "Epoch: 70/100, Batch: 227/432, W1: 0.7912, W2: 0.5019, W3: 0.0017, W4: -0.512, b: 0.7254\n",
      "loss: 0.1336\n",
      "Epoch: 70/100, Batch: 228/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5121, b: 0.7254\n",
      "loss: 0.1212\n",
      "Epoch: 70/100, Batch: 229/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5121, b: 0.7255\n",
      "loss: 0.1364\n",
      "Epoch: 70/100, Batch: 230/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5121, b: 0.7255\n",
      "loss: 0.1156\n",
      "Epoch: 70/100, Batch: 231/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5121, b: 0.7255\n",
      "loss: 0.1102\n",
      "Epoch: 70/100, Batch: 232/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5121, b: 0.7255\n",
      "loss: 0.1671\n",
      "Epoch: 70/100, Batch: 233/432, W1: 0.7911, W2: 0.5018, W3: 0.0016, W4: -0.5121, b: 0.7255\n",
      "loss: 0.1587\n",
      "Epoch: 70/100, Batch: 234/432, W1: 0.7912, W2: 0.5018, W3: 0.0017, W4: -0.5122, b: 0.7255\n",
      "loss: 0.1608\n",
      "Epoch: 70/100, Batch: 235/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5122, b: 0.7255\n",
      "loss: 0.1245\n",
      "Epoch: 70/100, Batch: 236/432, W1: 0.7912, W2: 0.5018, W3: 0.0017, W4: -0.5122, b: 0.7255\n",
      "loss: 0.147\n",
      "Epoch: 70/100, Batch: 237/432, W1: 0.7912, W2: 0.5018, W3: 0.0017, W4: -0.5122, b: 0.7255\n",
      "loss: 0.1184\n",
      "Epoch: 70/100, Batch: 238/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5122, b: 0.7255\n",
      "loss: 0.1229\n",
      "Epoch: 70/100, Batch: 239/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5122, b: 0.7256\n",
      "loss: 0.1132\n",
      "Epoch: 70/100, Batch: 240/432, W1: 0.7913, W2: 0.5019, W3: 0.0017, W4: -0.5122, b: 0.7256\n",
      "loss: 0.1504\n",
      "Epoch: 70/100, Batch: 241/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5122, b: 0.7256\n",
      "loss: 0.0997\n",
      "Epoch: 70/100, Batch: 242/432, W1: 0.7914, W2: 0.502, W3: 0.0018, W4: -0.5122, b: 0.7256\n",
      "loss: 0.1128\n",
      "Epoch: 70/100, Batch: 243/432, W1: 0.7914, W2: 0.5021, W3: 0.0018, W4: -0.5122, b: 0.7256\n",
      "loss: 0.1638\n",
      "Epoch: 70/100, Batch: 244/432, W1: 0.7914, W2: 0.5021, W3: 0.0018, W4: -0.5123, b: 0.7256\n",
      "loss: 0.1101\n",
      "Epoch: 70/100, Batch: 245/432, W1: 0.7915, W2: 0.5021, W3: 0.0018, W4: -0.5123, b: 0.7256\n",
      "loss: 0.0983\n",
      "Epoch: 70/100, Batch: 246/432, W1: 0.7915, W2: 0.5021, W3: 0.0019, W4: -0.5123, b: 0.7256\n",
      "loss: 0.1215\n",
      "Epoch: 70/100, Batch: 247/432, W1: 0.7915, W2: 0.5021, W3: 0.0019, W4: -0.5123, b: 0.7256\n",
      "loss: 0.1466\n",
      "Epoch: 70/100, Batch: 248/432, W1: 0.7915, W2: 0.5021, W3: 0.0019, W4: -0.5123, b: 0.7257\n",
      "loss: 0.1306\n",
      "Epoch: 70/100, Batch: 249/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5123, b: 0.7257\n",
      "loss: 1.0581\n",
      "Epoch: 70/100, Batch: 250/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5123, b: 0.7257\n",
      "loss: 0.1132\n",
      "Epoch: 70/100, Batch: 251/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5123, b: 0.7257\n",
      "loss: 0.1528\n",
      "Epoch: 70/100, Batch: 252/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5123, b: 0.7257\n",
      "loss: 0.1231\n",
      "Epoch: 70/100, Batch: 253/432, W1: 0.7915, W2: 0.5021, W3: 0.0019, W4: -0.5124, b: 0.7257\n",
      "loss: 0.1293\n",
      "Epoch: 70/100, Batch: 254/432, W1: 0.7915, W2: 0.5021, W3: 0.0019, W4: -0.5124, b: 0.7257\n",
      "loss: 0.1214\n",
      "Epoch: 70/100, Batch: 255/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5124, b: 0.7258\n",
      "loss: 0.1043\n",
      "Epoch: 70/100, Batch: 256/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5124, b: 0.7258\n",
      "loss: 0.1368\n",
      "Epoch: 70/100, Batch: 257/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5124, b: 0.7258\n",
      "loss: 0.1416\n",
      "Epoch: 70/100, Batch: 258/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5124, b: 0.7258\n",
      "loss: 0.1511\n",
      "Epoch: 70/100, Batch: 259/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5124, b: 0.7258\n",
      "loss: 0.1083\n",
      "Epoch: 70/100, Batch: 260/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5125, b: 0.7258\n",
      "loss: 0.1219\n",
      "Epoch: 70/100, Batch: 261/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5125, b: 0.7258\n",
      "loss: 0.1276\n",
      "Epoch: 70/100, Batch: 262/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5125, b: 0.7258\n",
      "loss: 0.1219\n",
      "Epoch: 70/100, Batch: 263/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5125, b: 0.7258\n",
      "loss: 0.1192\n",
      "Epoch: 70/100, Batch: 264/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5125, b: 0.7258\n",
      "loss: 0.1004\n",
      "Epoch: 70/100, Batch: 265/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5125, b: 0.7258\n",
      "loss: 0.1231\n",
      "Epoch: 70/100, Batch: 266/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5126, b: 0.7258\n",
      "loss: 0.1391\n",
      "Epoch: 70/100, Batch: 267/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5126, b: 0.7259\n",
      "loss: 0.1446\n",
      "Epoch: 70/100, Batch: 268/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5126, b: 0.7259\n",
      "loss: 0.1345\n",
      "Epoch: 70/100, Batch: 269/432, W1: 0.7916, W2: 0.5022, W3: 0.002, W4: -0.5126, b: 0.7259\n",
      "loss: 0.1619\n",
      "Epoch: 70/100, Batch: 270/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5126, b: 0.7259\n",
      "loss: 0.1627\n",
      "Epoch: 70/100, Batch: 271/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5126, b: 0.7259\n",
      "loss: 0.1178\n",
      "Epoch: 70/100, Batch: 272/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5127, b: 0.7259\n",
      "loss: 0.1339\n",
      "Epoch: 70/100, Batch: 273/432, W1: 0.7915, W2: 0.5022, W3: 0.0019, W4: -0.5127, b: 0.7259\n",
      "loss: 0.8134\n",
      "Epoch: 70/100, Batch: 274/432, W1: 0.7916, W2: 0.5022, W3: 0.0019, W4: -0.5127, b: 0.726\n",
      "loss: 0.1204\n",
      "Epoch: 70/100, Batch: 275/432, W1: 0.7917, W2: 0.5023, W3: 0.002, W4: -0.5127, b: 0.726\n",
      "loss: 0.1299\n",
      "Epoch: 70/100, Batch: 276/432, W1: 0.7917, W2: 0.5023, W3: 0.002, W4: -0.5127, b: 0.726\n",
      "loss: 0.1103\n",
      "Epoch: 70/100, Batch: 277/432, W1: 0.7917, W2: 0.5024, W3: 0.002, W4: -0.5127, b: 0.726\n",
      "loss: 0.1384\n",
      "Epoch: 70/100, Batch: 278/432, W1: 0.7917, W2: 0.5024, W3: 0.002, W4: -0.5127, b: 0.726\n",
      "loss: 0.1224\n",
      "Epoch: 70/100, Batch: 279/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5127, b: 0.726\n",
      "loss: 0.1072\n",
      "Epoch: 70/100, Batch: 280/432, W1: 0.7917, W2: 0.5023, W3: 0.002, W4: -0.5127, b: 0.726\n",
      "loss: 0.1295\n",
      "Epoch: 70/100, Batch: 281/432, W1: 0.7918, W2: 0.5024, W3: 0.0021, W4: -0.5127, b: 0.726\n",
      "loss: 0.1201\n",
      "Epoch: 70/100, Batch: 282/432, W1: 0.7918, W2: 0.5025, W3: 0.0021, W4: -0.5127, b: 0.7261\n",
      "loss: 0.0807\n",
      "Epoch: 70/100, Batch: 283/432, W1: 0.7918, W2: 0.5025, W3: 0.0021, W4: -0.5128, b: 0.7261\n",
      "loss: 0.0985\n",
      "Epoch: 70/100, Batch: 284/432, W1: 0.7918, W2: 0.5025, W3: 0.0021, W4: -0.5128, b: 0.7261\n",
      "loss: 0.1213\n",
      "Epoch: 70/100, Batch: 285/432, W1: 0.7918, W2: 0.5024, W3: 0.0021, W4: -0.5128, b: 0.7261\n",
      "loss: 0.1497\n",
      "Epoch: 70/100, Batch: 286/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5128, b: 0.7261\n",
      "loss: 0.1066\n",
      "Epoch: 70/100, Batch: 287/432, W1: 0.7917, W2: 0.5024, W3: 0.002, W4: -0.5128, b: 0.7261\n",
      "loss: 0.1518\n",
      "Epoch: 70/100, Batch: 288/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5129, b: 0.7261\n",
      "loss: 0.1367\n",
      "Epoch: 70/100, Batch: 289/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5129, b: 0.7261\n",
      "loss: 0.1262\n",
      "Epoch: 70/100, Batch: 290/432, W1: 0.7917, W2: 0.5024, W3: 0.002, W4: -0.5129, b: 0.7261\n",
      "loss: 0.1157\n",
      "Epoch: 70/100, Batch: 291/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5129, b: 0.7261\n",
      "loss: 0.14\n",
      "Epoch: 70/100, Batch: 292/432, W1: 0.7917, W2: 0.5024, W3: 0.002, W4: -0.5129, b: 0.7261\n",
      "loss: 0.1053\n",
      "Epoch: 70/100, Batch: 293/432, W1: 0.7917, W2: 0.5023, W3: 0.002, W4: -0.5129, b: 0.7261\n",
      "loss: 0.1602\n",
      "Epoch: 70/100, Batch: 294/432, W1: 0.7917, W2: 0.5024, W3: 0.002, W4: -0.513, b: 0.7262\n",
      "loss: 0.0883\n",
      "Epoch: 70/100, Batch: 295/432, W1: 0.7917, W2: 0.5023, W3: 0.002, W4: -0.513, b: 0.7262\n",
      "loss: 0.1408\n",
      "Epoch: 70/100, Batch: 296/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.513, b: 0.7262\n",
      "loss: 0.1624\n",
      "Epoch: 70/100, Batch: 297/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.513, b: 0.7262\n",
      "loss: 0.1557\n",
      "Epoch: 70/100, Batch: 298/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.513, b: 0.7262\n",
      "loss: 0.0987\n",
      "Epoch: 70/100, Batch: 299/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5131, b: 0.7262\n",
      "loss: 0.1453\n",
      "Epoch: 70/100, Batch: 300/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5131, b: 0.7262\n",
      "loss: 0.0985\n",
      "Epoch: 70/100, Batch: 301/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5131, b: 0.7262\n",
      "loss: 0.1207\n",
      "Epoch: 70/100, Batch: 302/432, W1: 0.7917, W2: 0.5023, W3: 0.002, W4: -0.5131, b: 0.7262\n",
      "loss: 0.1261\n",
      "Epoch: 70/100, Batch: 303/432, W1: 0.7917, W2: 0.5024, W3: 0.002, W4: -0.5131, b: 0.7262\n",
      "loss: 0.138\n",
      "Epoch: 70/100, Batch: 304/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5131, b: 0.7262\n",
      "loss: 0.2342\n",
      "Epoch: 70/100, Batch: 305/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5131, b: 0.7263\n",
      "loss: 0.1278\n",
      "Epoch: 70/100, Batch: 306/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5132, b: 0.7263\n",
      "loss: 0.1117\n",
      "Epoch: 70/100, Batch: 307/432, W1: 0.7918, W2: 0.5024, W3: 0.0021, W4: -0.5132, b: 0.7263\n",
      "loss: 0.1252\n",
      "Epoch: 70/100, Batch: 308/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5132, b: 0.7263\n",
      "loss: 0.1271\n",
      "Epoch: 70/100, Batch: 309/432, W1: 0.7917, W2: 0.5024, W3: 0.002, W4: -0.5132, b: 0.7263\n",
      "loss: 0.1289\n",
      "Epoch: 70/100, Batch: 310/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5132, b: 0.7263\n",
      "loss: 0.1127\n",
      "Epoch: 70/100, Batch: 311/432, W1: 0.7917, W2: 0.5024, W3: 0.002, W4: -0.5132, b: 0.7263\n",
      "loss: 0.1194\n",
      "Epoch: 70/100, Batch: 312/432, W1: 0.7918, W2: 0.5024, W3: 0.0021, W4: -0.5132, b: 0.7263\n",
      "loss: 0.1072\n",
      "Epoch: 70/100, Batch: 313/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5133, b: 0.7263\n",
      "loss: 0.1395\n",
      "Epoch: 70/100, Batch: 314/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5133, b: 0.7264\n",
      "loss: 0.1382\n",
      "Epoch: 70/100, Batch: 315/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5133, b: 0.7264\n",
      "loss: 0.1952\n",
      "Epoch: 70/100, Batch: 316/432, W1: 0.7916, W2: 0.5022, W3: 0.002, W4: -0.5134, b: 0.7264\n",
      "loss: 0.1353\n",
      "Epoch: 70/100, Batch: 317/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5134, b: 0.7264\n",
      "loss: 0.1583\n",
      "Epoch: 70/100, Batch: 318/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5134, b: 0.7264\n",
      "loss: 0.115\n",
      "Epoch: 70/100, Batch: 319/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5134, b: 0.7264\n",
      "loss: 0.1095\n",
      "Epoch: 70/100, Batch: 320/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5134, b: 0.7264\n",
      "loss: 0.1155\n",
      "Epoch: 70/100, Batch: 321/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5134, b: 0.7264\n",
      "loss: 0.1357\n",
      "Epoch: 70/100, Batch: 322/432, W1: 0.7916, W2: 0.5022, W3: 0.002, W4: -0.5134, b: 0.7264\n",
      "loss: 0.129\n",
      "Epoch: 70/100, Batch: 323/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5135, b: 0.7264\n",
      "loss: 0.1519\n",
      "Epoch: 70/100, Batch: 324/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5135, b: 0.7264\n",
      "loss: 0.1388\n",
      "Epoch: 70/100, Batch: 325/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5135, b: 0.7264\n",
      "loss: 0.1169\n",
      "Epoch: 70/100, Batch: 326/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5135, b: 0.7264\n",
      "loss: 0.1815\n",
      "Epoch: 70/100, Batch: 327/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5135, b: 0.7265\n",
      "loss: 0.1351\n",
      "Epoch: 70/100, Batch: 328/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5135, b: 0.7265\n",
      "loss: 0.1482\n",
      "Epoch: 70/100, Batch: 329/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5136, b: 0.7265\n",
      "loss: 0.1368\n",
      "Epoch: 70/100, Batch: 330/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.5136, b: 0.7265\n",
      "loss: 0.1178\n",
      "Epoch: 70/100, Batch: 331/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5136, b: 0.7265\n",
      "loss: 0.1305\n",
      "Epoch: 70/100, Batch: 332/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.5136, b: 0.7265\n",
      "loss: 0.1386\n",
      "Epoch: 70/100, Batch: 333/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5136, b: 0.7265\n",
      "loss: 0.1258\n",
      "Epoch: 70/100, Batch: 334/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5136, b: 0.7265\n",
      "loss: 0.1341\n",
      "Epoch: 70/100, Batch: 335/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5137, b: 0.7265\n",
      "loss: 0.1402\n",
      "Epoch: 70/100, Batch: 336/432, W1: 0.7916, W2: 0.5022, W3: 0.002, W4: -0.5137, b: 0.7265\n",
      "loss: 0.1456\n",
      "Epoch: 70/100, Batch: 337/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5137, b: 0.7265\n",
      "loss: 0.1246\n",
      "Epoch: 70/100, Batch: 338/432, W1: 0.7914, W2: 0.5021, W3: 0.0019, W4: -0.5138, b: 0.7265\n",
      "loss: 0.1681\n",
      "Epoch: 70/100, Batch: 339/432, W1: 0.7914, W2: 0.5021, W3: 0.0019, W4: -0.5138, b: 0.7266\n",
      "loss: 0.1162\n",
      "Epoch: 70/100, Batch: 340/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5138, b: 0.7266\n",
      "loss: 0.1442\n",
      "Epoch: 70/100, Batch: 341/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5138, b: 0.7266\n",
      "loss: 0.1643\n",
      "Epoch: 70/100, Batch: 342/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5138, b: 0.7266\n",
      "loss: 0.1269\n",
      "Epoch: 70/100, Batch: 343/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5138, b: 0.7266\n",
      "loss: 0.1163\n",
      "Epoch: 70/100, Batch: 344/432, W1: 0.7914, W2: 0.5021, W3: 0.0019, W4: -0.5138, b: 0.7266\n",
      "loss: 0.1031\n",
      "Epoch: 70/100, Batch: 345/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5138, b: 0.7266\n",
      "loss: 0.1099\n",
      "Epoch: 70/100, Batch: 346/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5138, b: 0.7266\n",
      "loss: 0.095\n",
      "Epoch: 70/100, Batch: 347/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5138, b: 0.7266\n",
      "loss: 0.1143\n",
      "Epoch: 70/100, Batch: 348/432, W1: 0.7916, W2: 0.5023, W3: 0.002, W4: -0.5139, b: 0.7267\n",
      "loss: 0.1429\n",
      "Epoch: 70/100, Batch: 349/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5139, b: 0.7267\n",
      "loss: 0.0997\n",
      "Epoch: 70/100, Batch: 350/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5139, b: 0.7267\n",
      "loss: 0.1042\n",
      "Epoch: 70/100, Batch: 351/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5139, b: 0.7267\n",
      "loss: 0.1286\n",
      "Epoch: 70/100, Batch: 352/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5139, b: 0.7267\n",
      "loss: 0.1245\n",
      "Epoch: 70/100, Batch: 353/432, W1: 0.7917, W2: 0.5024, W3: 0.0021, W4: -0.5139, b: 0.7267\n",
      "loss: 0.1013\n",
      "Epoch: 70/100, Batch: 354/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.5139, b: 0.7267\n",
      "loss: 0.1266\n",
      "Epoch: 70/100, Batch: 355/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.514, b: 0.7267\n",
      "loss: 0.1117\n",
      "Epoch: 70/100, Batch: 356/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.514, b: 0.7267\n",
      "loss: 0.1439\n",
      "Epoch: 70/100, Batch: 357/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.514, b: 0.7267\n",
      "loss: 0.128\n",
      "Epoch: 70/100, Batch: 358/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.514, b: 0.7267\n",
      "loss: 0.1148\n",
      "Epoch: 70/100, Batch: 359/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.514, b: 0.7268\n",
      "loss: 0.1266\n",
      "Epoch: 70/100, Batch: 360/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.514, b: 0.7268\n",
      "loss: 0.1565\n",
      "Epoch: 70/100, Batch: 361/432, W1: 0.7915, W2: 0.5022, W3: 0.002, W4: -0.5141, b: 0.7268\n",
      "loss: 0.1367\n",
      "Epoch: 70/100, Batch: 362/432, W1: 0.7914, W2: 0.5021, W3: 0.002, W4: -0.5141, b: 0.7268\n",
      "loss: 0.1892\n",
      "Epoch: 70/100, Batch: 363/432, W1: 0.7914, W2: 0.5021, W3: 0.0019, W4: -0.5141, b: 0.7268\n",
      "loss: 0.1317\n",
      "Epoch: 70/100, Batch: 364/432, W1: 0.7913, W2: 0.502, W3: 0.0019, W4: -0.5142, b: 0.7268\n",
      "loss: 0.1182\n",
      "Epoch: 70/100, Batch: 365/432, W1: 0.7913, W2: 0.502, W3: 0.0019, W4: -0.5142, b: 0.7268\n",
      "loss: 0.1406\n",
      "Epoch: 70/100, Batch: 366/432, W1: 0.7913, W2: 0.502, W3: 0.0019, W4: -0.5142, b: 0.7268\n",
      "loss: 0.1323\n",
      "Epoch: 70/100, Batch: 367/432, W1: 0.7912, W2: 0.5019, W3: 0.0019, W4: -0.5142, b: 0.7268\n",
      "loss: 0.1255\n",
      "Epoch: 70/100, Batch: 368/432, W1: 0.7912, W2: 0.5019, W3: 0.0018, W4: -0.5142, b: 0.7268\n",
      "loss: 0.1401\n",
      "Epoch: 70/100, Batch: 369/432, W1: 0.7912, W2: 0.5019, W3: 0.0018, W4: -0.5143, b: 0.7268\n",
      "loss: 0.1348\n",
      "Epoch: 70/100, Batch: 370/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5143, b: 0.7268\n",
      "loss: 0.1376\n",
      "Epoch: 70/100, Batch: 371/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5143, b: 0.7268\n",
      "loss: 0.1204\n",
      "Epoch: 70/100, Batch: 372/432, W1: 0.7912, W2: 0.5019, W3: 0.0018, W4: -0.5143, b: 0.7268\n",
      "loss: 0.1049\n",
      "Epoch: 70/100, Batch: 373/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5143, b: 0.7268\n",
      "loss: 0.1511\n",
      "Epoch: 70/100, Batch: 374/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5144, b: 0.7268\n",
      "loss: 0.1419\n",
      "Epoch: 70/100, Batch: 375/432, W1: 0.7911, W2: 0.5018, W3: 0.0018, W4: -0.5144, b: 0.7268\n",
      "loss: 0.1282\n",
      "Epoch: 70/100, Batch: 376/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5144, b: 0.7268\n",
      "loss: 0.137\n",
      "Epoch: 70/100, Batch: 377/432, W1: 0.7911, W2: 0.5018, W3: 0.0018, W4: -0.5144, b: 0.7268\n",
      "loss: 0.1331\n",
      "Epoch: 70/100, Batch: 378/432, W1: 0.7911, W2: 0.5018, W3: 0.0018, W4: -0.5144, b: 0.7269\n",
      "loss: 0.0931\n",
      "Epoch: 70/100, Batch: 379/432, W1: 0.7911, W2: 0.5018, W3: 0.0018, W4: -0.5144, b: 0.7269\n",
      "loss: 0.1297\n",
      "Epoch: 70/100, Batch: 380/432, W1: 0.791, W2: 0.5017, W3: 0.0017, W4: -0.5145, b: 0.7269\n",
      "loss: 0.1663\n",
      "Epoch: 70/100, Batch: 381/432, W1: 0.791, W2: 0.5017, W3: 0.0017, W4: -0.5145, b: 0.7269\n",
      "loss: 0.1451\n",
      "Epoch: 70/100, Batch: 382/432, W1: 0.791, W2: 0.5017, W3: 0.0017, W4: -0.5145, b: 0.7269\n",
      "loss: 0.1468\n",
      "Epoch: 70/100, Batch: 383/432, W1: 0.791, W2: 0.5017, W3: 0.0017, W4: -0.5145, b: 0.7269\n",
      "loss: 0.1295\n",
      "Epoch: 70/100, Batch: 384/432, W1: 0.7911, W2: 0.5018, W3: 0.0018, W4: -0.5145, b: 0.7269\n",
      "loss: 0.1158\n",
      "Epoch: 70/100, Batch: 385/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5145, b: 0.7269\n",
      "loss: 0.1109\n",
      "Epoch: 70/100, Batch: 386/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5145, b: 0.7269\n",
      "loss: 0.1169\n",
      "Epoch: 70/100, Batch: 387/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5146, b: 0.7269\n",
      "loss: 0.1775\n",
      "Epoch: 70/100, Batch: 388/432, W1: 0.7911, W2: 0.5018, W3: 0.0018, W4: -0.5146, b: 0.7269\n",
      "loss: 0.1388\n",
      "Epoch: 70/100, Batch: 389/432, W1: 0.791, W2: 0.5018, W3: 0.0018, W4: -0.5146, b: 0.727\n",
      "loss: 0.1118\n",
      "Epoch: 70/100, Batch: 390/432, W1: 0.7911, W2: 0.5018, W3: 0.0018, W4: -0.5146, b: 0.727\n",
      "loss: 0.133\n",
      "Epoch: 70/100, Batch: 391/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5146, b: 0.727\n",
      "loss: 0.111\n",
      "Epoch: 70/100, Batch: 392/432, W1: 0.7912, W2: 0.5019, W3: 0.0019, W4: -0.5146, b: 0.727\n",
      "loss: 0.1165\n",
      "Epoch: 70/100, Batch: 393/432, W1: 0.7912, W2: 0.5019, W3: 0.0019, W4: -0.5146, b: 0.727\n",
      "loss: 0.111\n",
      "Epoch: 70/100, Batch: 394/432, W1: 0.7912, W2: 0.5019, W3: 0.0019, W4: -0.5146, b: 0.727\n",
      "loss: 0.1359\n",
      "Epoch: 70/100, Batch: 395/432, W1: 0.7912, W2: 0.5019, W3: 0.0018, W4: -0.5147, b: 0.727\n",
      "loss: 0.1388\n",
      "Epoch: 70/100, Batch: 396/432, W1: 0.7911, W2: 0.5019, W3: 0.0018, W4: -0.5147, b: 0.727\n",
      "loss: 0.1339\n",
      "Epoch: 70/100, Batch: 397/432, W1: 0.7912, W2: 0.5019, W3: 0.0019, W4: -0.5147, b: 0.727\n",
      "loss: 0.1184\n",
      "Epoch: 70/100, Batch: 398/432, W1: 0.7912, W2: 0.5019, W3: 0.0019, W4: -0.5147, b: 0.7271\n",
      "loss: 0.1442\n",
      "Epoch: 70/100, Batch: 399/432, W1: 0.7912, W2: 0.502, W3: 0.0019, W4: -0.5147, b: 0.7271\n",
      "loss: 0.1041\n",
      "Epoch: 70/100, Batch: 400/432, W1: 0.7913, W2: 0.502, W3: 0.0019, W4: -0.5147, b: 0.7271\n",
      "loss: 0.1025\n",
      "Epoch: 70/100, Batch: 401/432, W1: 0.7913, W2: 0.502, W3: 0.0019, W4: -0.5147, b: 0.7271\n",
      "loss: 0.1312\n",
      "Epoch: 70/100, Batch: 402/432, W1: 0.7912, W2: 0.502, W3: 0.0019, W4: -0.5148, b: 0.7271\n",
      "loss: 0.1403\n",
      "Epoch: 70/100, Batch: 403/432, W1: 0.7912, W2: 0.502, W3: 0.0019, W4: -0.5148, b: 0.7271\n",
      "loss: 0.1214\n",
      "Epoch: 70/100, Batch: 404/432, W1: 0.7913, W2: 0.5021, W3: 0.002, W4: -0.5148, b: 0.7271\n",
      "loss: 0.1137\n",
      "Epoch: 70/100, Batch: 405/432, W1: 0.7913, W2: 0.5021, W3: 0.002, W4: -0.5148, b: 0.7271\n",
      "loss: 0.1465\n",
      "Epoch: 70/100, Batch: 406/432, W1: 0.7914, W2: 0.5021, W3: 0.002, W4: -0.5148, b: 0.7272\n",
      "loss: 0.1193\n",
      "Epoch: 70/100, Batch: 407/432, W1: 0.7913, W2: 0.5021, W3: 0.002, W4: -0.5148, b: 0.7272\n",
      "loss: 0.1516\n",
      "Epoch: 70/100, Batch: 408/432, W1: 0.7913, W2: 0.502, W3: 0.0019, W4: -0.5148, b: 0.7272\n",
      "loss: 0.1354\n",
      "Epoch: 70/100, Batch: 409/432, W1: 0.7913, W2: 0.502, W3: 0.002, W4: -0.5149, b: 0.7272\n",
      "loss: 0.1347\n",
      "Epoch: 70/100, Batch: 410/432, W1: 0.7913, W2: 0.5021, W3: 0.002, W4: -0.5149, b: 0.7272\n",
      "loss: 0.0959\n",
      "Epoch: 70/100, Batch: 411/432, W1: 0.7914, W2: 0.5022, W3: 0.002, W4: -0.5149, b: 0.7272\n",
      "loss: 0.1426\n",
      "Epoch: 70/100, Batch: 412/432, W1: 0.7915, W2: 0.5022, W3: 0.0021, W4: -0.5149, b: 0.7272\n",
      "loss: 0.1318\n",
      "Epoch: 70/100, Batch: 413/432, W1: 0.7915, W2: 0.5023, W3: 0.0021, W4: -0.5149, b: 0.7272\n",
      "loss: 0.103\n",
      "Epoch: 70/100, Batch: 414/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.5149, b: 0.7273\n",
      "loss: 0.1353\n",
      "Epoch: 70/100, Batch: 415/432, W1: 0.7915, W2: 0.5022, W3: 0.0021, W4: -0.5149, b: 0.7273\n",
      "loss: 0.112\n",
      "Epoch: 70/100, Batch: 416/432, W1: 0.7915, W2: 0.5023, W3: 0.0021, W4: -0.5149, b: 0.7273\n",
      "loss: 0.1224\n",
      "Epoch: 70/100, Batch: 417/432, W1: 0.7916, W2: 0.5023, W3: 0.0021, W4: -0.5149, b: 0.7273\n",
      "loss: 0.0981\n",
      "Epoch: 70/100, Batch: 418/432, W1: 0.7915, W2: 0.5022, W3: 0.0021, W4: -0.515, b: 0.7273\n",
      "loss: 0.1442\n",
      "Epoch: 70/100, Batch: 419/432, W1: 0.7915, W2: 0.5022, W3: 0.0021, W4: -0.515, b: 0.7273\n",
      "loss: 0.153\n",
      "Epoch: 70/100, Batch: 420/432, W1: 0.7915, W2: 0.5022, W3: 0.0021, W4: -0.515, b: 0.7273\n",
      "loss: 0.1212\n",
      "Epoch: 70/100, Batch: 421/432, W1: 0.7915, W2: 0.5023, W3: 0.0021, W4: -0.515, b: 0.7273\n",
      "loss: 0.1336\n",
      "Epoch: 70/100, Batch: 422/432, W1: 0.7915, W2: 0.5023, W3: 0.0021, W4: -0.515, b: 0.7273\n",
      "loss: 0.0976\n",
      "Epoch: 70/100, Batch: 423/432, W1: 0.7915, W2: 0.5023, W3: 0.0021, W4: -0.515, b: 0.7273\n",
      "loss: 0.1304\n",
      "Epoch: 70/100, Batch: 424/432, W1: 0.7915, W2: 0.5022, W3: 0.0021, W4: -0.515, b: 0.7273\n",
      "loss: 0.1096\n",
      "Epoch: 70/100, Batch: 425/432, W1: 0.7915, W2: 0.5023, W3: 0.0021, W4: -0.5151, b: 0.7273\n",
      "loss: 0.1284\n",
      "Epoch: 70/100, Batch: 426/432, W1: 0.7915, W2: 0.5023, W3: 0.0021, W4: -0.5151, b: 0.7274\n",
      "loss: 0.1275\n",
      "Epoch: 70/100, Batch: 427/432, W1: 0.7916, W2: 0.5024, W3: 0.0022, W4: -0.5151, b: 0.7274\n",
      "loss: 0.0998\n",
      "Epoch: 70/100, Batch: 428/432, W1: 0.7917, W2: 0.5025, W3: 0.0022, W4: -0.5151, b: 0.7274\n",
      "loss: 0.1269\n",
      "Epoch: 70/100, Batch: 429/432, W1: 0.7918, W2: 0.5026, W3: 0.0023, W4: -0.5151, b: 0.7274\n",
      "loss: 0.163\n",
      "Epoch: 70/100, Batch: 430/432, W1: 0.7918, W2: 0.5026, W3: 0.0023, W4: -0.5151, b: 0.7274\n",
      "loss: 0.1381\n",
      "Epoch: 70/100, Batch: 431/432, W1: 0.7917, W2: 0.5025, W3: 0.0022, W4: -0.5151, b: 0.7274\n",
      "loss: 0.1182\n",
      "Epoch: 70/100, Batch: 432/432, W1: 0.7916, W2: 0.5024, W3: 0.0022, W4: -0.5151, b: 0.7274\n",
      "loss: 0.1332\n",
      "Epoch: 80/100, Batch: 1/432, W1: 0.8025, W2: 0.4865, W3: 0.0119, W4: -0.5686, b: 0.7641\n",
      "loss: 0.1194\n",
      "Epoch: 80/100, Batch: 2/432, W1: 0.8025, W2: 0.4865, W3: 0.0119, W4: -0.5686, b: 0.7641\n",
      "loss: 0.1138\n",
      "Epoch: 80/100, Batch: 3/432, W1: 0.8025, W2: 0.4865, W3: 0.0119, W4: -0.5686, b: 0.7641\n",
      "loss: 0.118\n",
      "Epoch: 80/100, Batch: 4/432, W1: 0.8025, W2: 0.4865, W3: 0.0119, W4: -0.5686, b: 0.7641\n",
      "loss: 0.0708\n",
      "Epoch: 80/100, Batch: 5/432, W1: 0.8026, W2: 0.4866, W3: 0.012, W4: -0.5686, b: 0.7641\n",
      "loss: 0.1003\n",
      "Epoch: 80/100, Batch: 6/432, W1: 0.8025, W2: 0.4865, W3: 0.0119, W4: -0.5686, b: 0.7641\n",
      "loss: 0.1755\n",
      "Epoch: 80/100, Batch: 7/432, W1: 0.8025, W2: 0.4865, W3: 0.0119, W4: -0.5686, b: 0.7641\n",
      "loss: 0.1021\n",
      "Epoch: 80/100, Batch: 8/432, W1: 0.8026, W2: 0.4865, W3: 0.0119, W4: -0.5687, b: 0.7642\n",
      "loss: 0.0928\n",
      "Epoch: 80/100, Batch: 9/432, W1: 0.8026, W2: 0.4866, W3: 0.012, W4: -0.5687, b: 0.7642\n",
      "loss: 0.1084\n",
      "Epoch: 80/100, Batch: 10/432, W1: 0.8026, W2: 0.4866, W3: 0.012, W4: -0.5687, b: 0.7642\n",
      "loss: 0.1091\n",
      "Epoch: 80/100, Batch: 11/432, W1: 0.8027, W2: 0.4867, W3: 0.012, W4: -0.5687, b: 0.7642\n",
      "loss: 0.1026\n",
      "Epoch: 80/100, Batch: 12/432, W1: 0.8027, W2: 0.4867, W3: 0.012, W4: -0.5687, b: 0.7642\n",
      "loss: 0.1235\n",
      "Epoch: 80/100, Batch: 13/432, W1: 0.8027, W2: 0.4868, W3: 0.0121, W4: -0.5687, b: 0.7642\n",
      "loss: 0.3953\n",
      "Epoch: 80/100, Batch: 14/432, W1: 0.8027, W2: 0.4868, W3: 0.012, W4: -0.5687, b: 0.7642\n",
      "loss: 0.1126\n",
      "Epoch: 80/100, Batch: 15/432, W1: 0.8026, W2: 0.4867, W3: 0.012, W4: -0.5687, b: 0.7642\n",
      "loss: 0.1743\n",
      "Epoch: 80/100, Batch: 16/432, W1: 0.8027, W2: 0.4867, W3: 0.012, W4: -0.5687, b: 0.7643\n",
      "loss: 0.1185\n",
      "Epoch: 80/100, Batch: 17/432, W1: 0.8026, W2: 0.4867, W3: 0.012, W4: -0.5688, b: 0.7643\n",
      "loss: 0.176\n",
      "Epoch: 80/100, Batch: 18/432, W1: 0.8026, W2: 0.4867, W3: 0.012, W4: -0.5688, b: 0.7643\n",
      "loss: 0.1543\n",
      "Epoch: 80/100, Batch: 19/432, W1: 0.8026, W2: 0.4867, W3: 0.012, W4: -0.5688, b: 0.7643\n",
      "loss: 0.0917\n",
      "Epoch: 80/100, Batch: 20/432, W1: 0.8027, W2: 0.4868, W3: 0.012, W4: -0.5688, b: 0.7643\n",
      "loss: 0.1075\n",
      "Epoch: 80/100, Batch: 21/432, W1: 0.8027, W2: 0.4868, W3: 0.012, W4: -0.5688, b: 0.7643\n",
      "loss: 0.0966\n",
      "Epoch: 80/100, Batch: 22/432, W1: 0.8027, W2: 0.4868, W3: 0.0121, W4: -0.5688, b: 0.7643\n",
      "loss: 0.0904\n",
      "Epoch: 80/100, Batch: 23/432, W1: 0.8028, W2: 0.4868, W3: 0.0121, W4: -0.5688, b: 0.7643\n",
      "loss: 0.1155\n",
      "Epoch: 80/100, Batch: 24/432, W1: 0.8028, W2: 0.4868, W3: 0.0121, W4: -0.5688, b: 0.7643\n",
      "loss: 0.109\n",
      "Epoch: 80/100, Batch: 25/432, W1: 0.8028, W2: 0.4869, W3: 0.0121, W4: -0.5688, b: 0.7644\n",
      "loss: 0.1209\n",
      "Epoch: 80/100, Batch: 26/432, W1: 0.8028, W2: 0.4869, W3: 0.0121, W4: -0.5689, b: 0.7644\n",
      "loss: 0.09\n",
      "Epoch: 80/100, Batch: 27/432, W1: 0.8029, W2: 0.4869, W3: 0.0121, W4: -0.5689, b: 0.7644\n",
      "loss: 0.1087\n",
      "Epoch: 80/100, Batch: 28/432, W1: 0.8029, W2: 0.4869, W3: 0.0121, W4: -0.5689, b: 0.7644\n",
      "loss: 0.1123\n",
      "Epoch: 80/100, Batch: 29/432, W1: 0.8029, W2: 0.4869, W3: 0.0122, W4: -0.5689, b: 0.7644\n",
      "loss: 0.1024\n",
      "Epoch: 80/100, Batch: 30/432, W1: 0.8028, W2: 0.4869, W3: 0.0121, W4: -0.5689, b: 0.7644\n",
      "loss: 0.1203\n",
      "Epoch: 80/100, Batch: 31/432, W1: 0.8028, W2: 0.4869, W3: 0.0121, W4: -0.5689, b: 0.7644\n",
      "loss: 0.1046\n",
      "Epoch: 80/100, Batch: 32/432, W1: 0.8028, W2: 0.4869, W3: 0.0121, W4: -0.5689, b: 0.7644\n",
      "loss: 0.1148\n",
      "Epoch: 80/100, Batch: 33/432, W1: 0.8028, W2: 0.4868, W3: 0.0121, W4: -0.569, b: 0.7644\n",
      "loss: 0.1291\n",
      "Epoch: 80/100, Batch: 34/432, W1: 0.8028, W2: 0.4869, W3: 0.0121, W4: -0.569, b: 0.7644\n",
      "loss: 0.1124\n",
      "Epoch: 80/100, Batch: 35/432, W1: 0.8029, W2: 0.4869, W3: 0.0122, W4: -0.569, b: 0.7644\n",
      "loss: 0.0801\n",
      "Epoch: 80/100, Batch: 36/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.569, b: 0.7644\n",
      "loss: 0.0984\n",
      "Epoch: 80/100, Batch: 37/432, W1: 0.8029, W2: 0.4869, W3: 0.0122, W4: -0.569, b: 0.7645\n",
      "loss: 0.1104\n",
      "Epoch: 80/100, Batch: 38/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.569, b: 0.7645\n",
      "loss: 0.0962\n",
      "Epoch: 80/100, Batch: 39/432, W1: 0.803, W2: 0.487, W3: 0.0122, W4: -0.569, b: 0.7645\n",
      "loss: 0.1145\n",
      "Epoch: 80/100, Batch: 40/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.569, b: 0.7645\n",
      "loss: 0.1297\n",
      "Epoch: 80/100, Batch: 41/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.569, b: 0.7645\n",
      "loss: 0.1171\n",
      "Epoch: 80/100, Batch: 42/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5691, b: 0.7645\n",
      "loss: 0.1321\n",
      "Epoch: 80/100, Batch: 43/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5691, b: 0.7645\n",
      "loss: 0.1049\n",
      "Epoch: 80/100, Batch: 44/432, W1: 0.8028, W2: 0.4869, W3: 0.0121, W4: -0.5691, b: 0.7645\n",
      "loss: 0.1102\n",
      "Epoch: 80/100, Batch: 45/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5691, b: 0.7645\n",
      "loss: 0.1181\n",
      "Epoch: 80/100, Batch: 46/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5691, b: 0.7645\n",
      "loss: 0.1283\n",
      "Epoch: 80/100, Batch: 47/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5691, b: 0.7645\n",
      "loss: 0.1104\n",
      "Epoch: 80/100, Batch: 48/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5691, b: 0.7646\n",
      "loss: 0.1555\n",
      "Epoch: 80/100, Batch: 49/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5691, b: 0.7646\n",
      "loss: 0.1477\n",
      "Epoch: 80/100, Batch: 50/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5692, b: 0.7646\n",
      "loss: 0.1313\n",
      "Epoch: 80/100, Batch: 51/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5692, b: 0.7646\n",
      "loss: 0.1078\n",
      "Epoch: 80/100, Batch: 52/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5692, b: 0.7646\n",
      "loss: 0.1613\n",
      "Epoch: 80/100, Batch: 53/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5692, b: 0.7646\n",
      "loss: 0.0934\n",
      "Epoch: 80/100, Batch: 54/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5692, b: 0.7646\n",
      "loss: 0.1182\n",
      "Epoch: 80/100, Batch: 55/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5692, b: 0.7646\n",
      "loss: 0.1003\n",
      "Epoch: 80/100, Batch: 56/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5692, b: 0.7646\n",
      "loss: 0.1192\n",
      "Epoch: 80/100, Batch: 57/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5693, b: 0.7646\n",
      "loss: 0.1185\n",
      "Epoch: 80/100, Batch: 58/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5693, b: 0.7646\n",
      "loss: 0.1227\n",
      "Epoch: 80/100, Batch: 59/432, W1: 0.8029, W2: 0.4869, W3: 0.0122, W4: -0.5693, b: 0.7646\n",
      "loss: 0.1166\n",
      "Epoch: 80/100, Batch: 60/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5693, b: 0.7647\n",
      "loss: 0.1038\n",
      "Epoch: 80/100, Batch: 61/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5693, b: 0.7647\n",
      "loss: 0.0941\n",
      "Epoch: 80/100, Batch: 62/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5693, b: 0.7647\n",
      "loss: 0.131\n",
      "Epoch: 80/100, Batch: 63/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5693, b: 0.7647\n",
      "loss: 0.108\n",
      "Epoch: 80/100, Batch: 64/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5694, b: 0.7647\n",
      "loss: 0.1237\n",
      "Epoch: 80/100, Batch: 65/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5694, b: 0.7647\n",
      "loss: 0.0937\n",
      "Epoch: 80/100, Batch: 66/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5694, b: 0.7647\n",
      "loss: 0.0898\n",
      "Epoch: 80/100, Batch: 67/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5694, b: 0.7647\n",
      "loss: 0.1184\n",
      "Epoch: 80/100, Batch: 68/432, W1: 0.8029, W2: 0.4869, W3: 0.0122, W4: -0.5694, b: 0.7647\n",
      "loss: 0.081\n",
      "Epoch: 80/100, Batch: 69/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5694, b: 0.7647\n",
      "loss: 1.0199\n",
      "Epoch: 80/100, Batch: 70/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5694, b: 0.7647\n",
      "loss: 0.1416\n",
      "Epoch: 80/100, Batch: 71/432, W1: 0.8027, W2: 0.4868, W3: 0.0121, W4: -0.5694, b: 0.7647\n",
      "loss: 0.6731\n",
      "Epoch: 80/100, Batch: 72/432, W1: 0.8027, W2: 0.4868, W3: 0.0121, W4: -0.5695, b: 0.7647\n",
      "loss: 0.1317\n",
      "Epoch: 80/100, Batch: 73/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5695, b: 0.7648\n",
      "loss: 0.1392\n",
      "Epoch: 80/100, Batch: 74/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5695, b: 0.7648\n",
      "loss: 0.1031\n",
      "Epoch: 80/100, Batch: 75/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5695, b: 0.7648\n",
      "loss: 0.0966\n",
      "Epoch: 80/100, Batch: 76/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5695, b: 0.7648\n",
      "loss: 0.107\n",
      "Epoch: 80/100, Batch: 77/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5695, b: 0.7648\n",
      "loss: 0.1023\n",
      "Epoch: 80/100, Batch: 78/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5695, b: 0.7648\n",
      "loss: 0.0977\n",
      "Epoch: 80/100, Batch: 79/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5695, b: 0.7648\n",
      "loss: 0.1218\n",
      "Epoch: 80/100, Batch: 80/432, W1: 0.8029, W2: 0.4869, W3: 0.0122, W4: -0.5695, b: 0.7648\n",
      "loss: 0.1176\n",
      "Epoch: 80/100, Batch: 81/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5695, b: 0.7648\n",
      "loss: 0.1456\n",
      "Epoch: 80/100, Batch: 82/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5696, b: 0.7648\n",
      "loss: 0.1463\n",
      "Epoch: 80/100, Batch: 83/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5696, b: 0.7649\n",
      "loss: 0.0841\n",
      "Epoch: 80/100, Batch: 84/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5696, b: 0.7649\n",
      "loss: 0.0975\n",
      "Epoch: 80/100, Batch: 85/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5696, b: 0.7649\n",
      "loss: 0.1103\n",
      "Epoch: 80/100, Batch: 86/432, W1: 0.8029, W2: 0.487, W3: 0.0122, W4: -0.5696, b: 0.7649\n",
      "loss: 0.1305\n",
      "Epoch: 80/100, Batch: 87/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5696, b: 0.7649\n",
      "loss: 0.1215\n",
      "Epoch: 80/100, Batch: 88/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5696, b: 0.7649\n",
      "loss: 0.1189\n",
      "Epoch: 80/100, Batch: 89/432, W1: 0.8029, W2: 0.4871, W3: 0.0123, W4: -0.5696, b: 0.7649\n",
      "loss: 0.1199\n",
      "Epoch: 80/100, Batch: 90/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5697, b: 0.7649\n",
      "loss: 0.1194\n",
      "Epoch: 80/100, Batch: 91/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5697, b: 0.7649\n",
      "loss: 0.1243\n",
      "Epoch: 80/100, Batch: 92/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5697, b: 0.765\n",
      "loss: 0.1002\n",
      "Epoch: 80/100, Batch: 93/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5697, b: 0.765\n",
      "loss: 0.0895\n",
      "Epoch: 80/100, Batch: 94/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5697, b: 0.765\n",
      "loss: 0.1268\n",
      "Epoch: 80/100, Batch: 95/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5697, b: 0.765\n",
      "loss: 0.1114\n",
      "Epoch: 80/100, Batch: 96/432, W1: 0.8029, W2: 0.4871, W3: 0.0123, W4: -0.5697, b: 0.765\n",
      "loss: 0.1101\n",
      "Epoch: 80/100, Batch: 97/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5697, b: 0.765\n",
      "loss: 0.098\n",
      "Epoch: 80/100, Batch: 98/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5698, b: 0.765\n",
      "loss: 0.1223\n",
      "Epoch: 80/100, Batch: 99/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5698, b: 0.765\n",
      "loss: 0.0904\n",
      "Epoch: 80/100, Batch: 100/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5698, b: 0.765\n",
      "loss: 0.0988\n",
      "Epoch: 80/100, Batch: 101/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5698, b: 0.765\n",
      "loss: 0.1116\n",
      "Epoch: 80/100, Batch: 102/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5698, b: 0.765\n",
      "loss: 0.0876\n",
      "Epoch: 80/100, Batch: 103/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5698, b: 0.765\n",
      "loss: 0.1624\n",
      "Epoch: 80/100, Batch: 104/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5699, b: 0.765\n",
      "loss: 0.1127\n",
      "Epoch: 80/100, Batch: 105/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5699, b: 0.7651\n",
      "loss: 0.1326\n",
      "Epoch: 80/100, Batch: 106/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5699, b: 0.7651\n",
      "loss: 0.1108\n",
      "Epoch: 80/100, Batch: 107/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5699, b: 0.7651\n",
      "loss: 0.1201\n",
      "Epoch: 80/100, Batch: 108/432, W1: 0.8028, W2: 0.4869, W3: 0.0122, W4: -0.5699, b: 0.7651\n",
      "loss: 0.1071\n",
      "Epoch: 80/100, Batch: 109/432, W1: 0.8028, W2: 0.487, W3: 0.0122, W4: -0.5699, b: 0.7651\n",
      "loss: 0.1093\n",
      "Epoch: 80/100, Batch: 110/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5699, b: 0.7651\n",
      "loss: 0.1498\n",
      "Epoch: 80/100, Batch: 111/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5699, b: 0.7651\n",
      "loss: 0.0877\n",
      "Epoch: 80/100, Batch: 112/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.57, b: 0.7651\n",
      "loss: 0.1006\n",
      "Epoch: 80/100, Batch: 113/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.57, b: 0.7651\n",
      "loss: 0.0822\n",
      "Epoch: 80/100, Batch: 114/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.57, b: 0.7651\n",
      "loss: 0.1043\n",
      "Epoch: 80/100, Batch: 115/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.57, b: 0.7651\n",
      "loss: 0.0953\n",
      "Epoch: 80/100, Batch: 116/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.57, b: 0.7651\n",
      "loss: 0.1082\n",
      "Epoch: 80/100, Batch: 117/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.57, b: 0.7651\n",
      "loss: 0.1101\n",
      "Epoch: 80/100, Batch: 118/432, W1: 0.8029, W2: 0.4871, W3: 0.0123, W4: -0.57, b: 0.7652\n",
      "loss: 0.1023\n",
      "Epoch: 80/100, Batch: 119/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.57, b: 0.7652\n",
      "loss: 0.0885\n",
      "Epoch: 80/100, Batch: 120/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.57, b: 0.7652\n",
      "loss: 0.1179\n",
      "Epoch: 80/100, Batch: 121/432, W1: 0.8031, W2: 0.4872, W3: 0.0124, W4: -0.57, b: 0.7652\n",
      "loss: 0.129\n",
      "Epoch: 80/100, Batch: 122/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5701, b: 0.7652\n",
      "loss: 0.1505\n",
      "Epoch: 80/100, Batch: 123/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5701, b: 0.7652\n",
      "loss: 0.1035\n",
      "Epoch: 80/100, Batch: 124/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5701, b: 0.7652\n",
      "loss: 0.1263\n",
      "Epoch: 80/100, Batch: 125/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5701, b: 0.7652\n",
      "loss: 0.122\n",
      "Epoch: 80/100, Batch: 126/432, W1: 0.8029, W2: 0.4871, W3: 0.0123, W4: -0.5701, b: 0.7652\n",
      "loss: 0.1235\n",
      "Epoch: 80/100, Batch: 127/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5701, b: 0.7652\n",
      "loss: 0.0898\n",
      "Epoch: 80/100, Batch: 128/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5701, b: 0.7653\n",
      "loss: 0.0869\n",
      "Epoch: 80/100, Batch: 129/432, W1: 0.8031, W2: 0.4872, W3: 0.0124, W4: -0.5701, b: 0.7653\n",
      "loss: 0.1039\n",
      "Epoch: 80/100, Batch: 130/432, W1: 0.8031, W2: 0.4872, W3: 0.0124, W4: -0.5701, b: 0.7653\n",
      "loss: 0.1464\n",
      "Epoch: 80/100, Batch: 131/432, W1: 0.8031, W2: 0.4872, W3: 0.0124, W4: -0.5702, b: 0.7653\n",
      "loss: 0.1327\n",
      "Epoch: 80/100, Batch: 132/432, W1: 0.8031, W2: 0.4872, W3: 0.0124, W4: -0.5702, b: 0.7653\n",
      "loss: 0.12\n",
      "Epoch: 80/100, Batch: 133/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5702, b: 0.7653\n",
      "loss: 0.1458\n",
      "Epoch: 80/100, Batch: 134/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5702, b: 0.7653\n",
      "loss: 0.1269\n",
      "Epoch: 80/100, Batch: 135/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5702, b: 0.7653\n",
      "loss: 0.1164\n",
      "Epoch: 80/100, Batch: 136/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5703, b: 0.7653\n",
      "loss: 0.1164\n",
      "Epoch: 80/100, Batch: 137/432, W1: 0.8031, W2: 0.4872, W3: 0.0124, W4: -0.5703, b: 0.7653\n",
      "loss: 0.1051\n",
      "Epoch: 80/100, Batch: 138/432, W1: 0.8031, W2: 0.4872, W3: 0.0124, W4: -0.5703, b: 0.7654\n",
      "loss: 0.1217\n",
      "Epoch: 80/100, Batch: 139/432, W1: 0.8031, W2: 0.4872, W3: 0.0124, W4: -0.5703, b: 0.7654\n",
      "loss: 0.124\n",
      "Epoch: 80/100, Batch: 140/432, W1: 0.8031, W2: 0.4872, W3: 0.0124, W4: -0.5703, b: 0.7654\n",
      "loss: 0.1094\n",
      "Epoch: 80/100, Batch: 141/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5703, b: 0.7654\n",
      "loss: 0.1067\n",
      "Epoch: 80/100, Batch: 142/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5703, b: 0.7654\n",
      "loss: 0.1199\n",
      "Epoch: 80/100, Batch: 143/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5703, b: 0.7654\n",
      "loss: 0.1252\n",
      "Epoch: 80/100, Batch: 144/432, W1: 0.803, W2: 0.4871, W3: 0.0123, W4: -0.5704, b: 0.7654\n",
      "loss: 0.0895\n",
      "Epoch: 80/100, Batch: 145/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5704, b: 0.7654\n",
      "loss: 0.1164\n",
      "Epoch: 80/100, Batch: 146/432, W1: 0.8029, W2: 0.4871, W3: 0.0123, W4: -0.5704, b: 0.7654\n",
      "loss: 0.1234\n",
      "Epoch: 80/100, Batch: 147/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5704, b: 0.7654\n",
      "loss: 0.1308\n",
      "Epoch: 80/100, Batch: 148/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5704, b: 0.7654\n",
      "loss: 0.1005\n",
      "Epoch: 80/100, Batch: 149/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5704, b: 0.7654\n",
      "loss: 0.0937\n",
      "Epoch: 80/100, Batch: 150/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5705, b: 0.7654\n",
      "loss: 0.1349\n",
      "Epoch: 80/100, Batch: 151/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5705, b: 0.7654\n",
      "loss: 0.1216\n",
      "Epoch: 80/100, Batch: 152/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5705, b: 0.7654\n",
      "loss: 0.1316\n",
      "Epoch: 80/100, Batch: 153/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5705, b: 0.7654\n",
      "loss: 0.1013\n",
      "Epoch: 80/100, Batch: 154/432, W1: 0.8029, W2: 0.4871, W3: 0.0123, W4: -0.5705, b: 0.7655\n",
      "loss: 0.1014\n",
      "Epoch: 80/100, Batch: 155/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5705, b: 0.7655\n",
      "loss: 0.0855\n",
      "Epoch: 80/100, Batch: 156/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5705, b: 0.7655\n",
      "loss: 0.119\n",
      "Epoch: 80/100, Batch: 157/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5705, b: 0.7655\n",
      "loss: 0.1011\n",
      "Epoch: 80/100, Batch: 158/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5705, b: 0.7655\n",
      "loss: 0.1311\n",
      "Epoch: 80/100, Batch: 159/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5706, b: 0.7655\n",
      "loss: 0.1176\n",
      "Epoch: 80/100, Batch: 160/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5706, b: 0.7655\n",
      "loss: 0.0995\n",
      "Epoch: 80/100, Batch: 161/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5706, b: 0.7655\n",
      "loss: 0.0979\n",
      "Epoch: 80/100, Batch: 162/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5706, b: 0.7655\n",
      "loss: 0.1193\n",
      "Epoch: 80/100, Batch: 163/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5706, b: 0.7655\n",
      "loss: 0.1046\n",
      "Epoch: 80/100, Batch: 164/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5706, b: 0.7655\n",
      "loss: 0.112\n",
      "Epoch: 80/100, Batch: 165/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5707, b: 0.7655\n",
      "loss: 0.1292\n",
      "Epoch: 80/100, Batch: 166/432, W1: 0.8029, W2: 0.4871, W3: 0.0123, W4: -0.5707, b: 0.7655\n",
      "loss: 0.0998\n",
      "Epoch: 80/100, Batch: 167/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5707, b: 0.7656\n",
      "loss: 0.0749\n",
      "Epoch: 80/100, Batch: 168/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5707, b: 0.7656\n",
      "loss: 0.1019\n",
      "Epoch: 80/100, Batch: 169/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5707, b: 0.7656\n",
      "loss: 0.1052\n",
      "Epoch: 80/100, Batch: 170/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5707, b: 0.7656\n",
      "loss: 0.0863\n",
      "Epoch: 80/100, Batch: 171/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5707, b: 0.7656\n",
      "loss: 0.098\n",
      "Epoch: 80/100, Batch: 172/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5707, b: 0.7656\n",
      "loss: 0.1425\n",
      "Epoch: 80/100, Batch: 173/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5707, b: 0.7656\n",
      "loss: 0.1069\n",
      "Epoch: 80/100, Batch: 174/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5708, b: 0.7656\n",
      "loss: 0.1059\n",
      "Epoch: 80/100, Batch: 175/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5708, b: 0.7656\n",
      "loss: 0.1525\n",
      "Epoch: 80/100, Batch: 176/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5708, b: 0.7656\n",
      "loss: 0.131\n",
      "Epoch: 80/100, Batch: 177/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5708, b: 0.7656\n",
      "loss: 0.1615\n",
      "Epoch: 80/100, Batch: 178/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.5708, b: 0.7656\n",
      "loss: 0.0893\n",
      "Epoch: 80/100, Batch: 179/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5708, b: 0.7657\n",
      "loss: 0.1203\n",
      "Epoch: 80/100, Batch: 180/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5709, b: 0.7657\n",
      "loss: 0.1041\n",
      "Epoch: 80/100, Batch: 181/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5709, b: 0.7657\n",
      "loss: 0.09\n",
      "Epoch: 80/100, Batch: 182/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5709, b: 0.7657\n",
      "loss: 0.1022\n",
      "Epoch: 80/100, Batch: 183/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5709, b: 0.7657\n",
      "loss: 0.1098\n",
      "Epoch: 80/100, Batch: 184/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5709, b: 0.7657\n",
      "loss: 0.1442\n",
      "Epoch: 80/100, Batch: 185/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5709, b: 0.7657\n",
      "loss: 0.1154\n",
      "Epoch: 80/100, Batch: 186/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5709, b: 0.7657\n",
      "loss: 0.1436\n",
      "Epoch: 80/100, Batch: 187/432, W1: 0.803, W2: 0.4872, W3: 0.0124, W4: -0.5709, b: 0.7657\n",
      "loss: 0.1026\n",
      "Epoch: 80/100, Batch: 188/432, W1: 0.803, W2: 0.4871, W3: 0.0124, W4: -0.5709, b: 0.7657\n",
      "loss: 0.1106\n",
      "Epoch: 80/100, Batch: 189/432, W1: 0.8029, W2: 0.4871, W3: 0.0123, W4: -0.571, b: 0.7657\n",
      "loss: 0.8224\n",
      "Epoch: 80/100, Batch: 190/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.571, b: 0.7658\n",
      "loss: 0.109\n",
      "Epoch: 80/100, Batch: 191/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.571, b: 0.7658\n",
      "loss: 0.0774\n",
      "Epoch: 80/100, Batch: 192/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.571, b: 0.7658\n",
      "loss: 0.1283\n",
      "Epoch: 80/100, Batch: 193/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.571, b: 0.7658\n",
      "loss: 0.0981\n",
      "Epoch: 80/100, Batch: 194/432, W1: 0.8029, W2: 0.487, W3: 0.0123, W4: -0.571, b: 0.7658\n",
      "loss: 1.1798\n",
      "Epoch: 80/100, Batch: 195/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.571, b: 0.7658\n",
      "loss: 0.1336\n",
      "Epoch: 80/100, Batch: 196/432, W1: 0.8029, W2: 0.487, W3: 0.0124, W4: -0.571, b: 0.7658\n",
      "loss: 0.1795\n",
      "Epoch: 80/100, Batch: 197/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.571, b: 0.7658\n",
      "loss: 0.1125\n",
      "Epoch: 80/100, Batch: 198/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.571, b: 0.7658\n",
      "loss: 0.1032\n",
      "Epoch: 80/100, Batch: 199/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.571, b: 0.7658\n",
      "loss: 0.1114\n",
      "Epoch: 80/100, Batch: 200/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5711, b: 0.7658\n",
      "loss: 0.0932\n",
      "Epoch: 80/100, Batch: 201/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5711, b: 0.7658\n",
      "loss: 0.1393\n",
      "Epoch: 80/100, Batch: 202/432, W1: 0.8028, W2: 0.4869, W3: 0.0123, W4: -0.5711, b: 0.7659\n",
      "loss: 0.1421\n",
      "Epoch: 80/100, Batch: 203/432, W1: 0.8029, W2: 0.487, W3: 0.0124, W4: -0.5711, b: 0.7659\n",
      "loss: 0.1084\n",
      "Epoch: 80/100, Batch: 204/432, W1: 0.8029, W2: 0.487, W3: 0.0124, W4: -0.5711, b: 0.7659\n",
      "loss: 0.0953\n",
      "Epoch: 80/100, Batch: 205/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5711, b: 0.7659\n",
      "loss: 0.1209\n",
      "Epoch: 80/100, Batch: 206/432, W1: 0.8027, W2: 0.4869, W3: 0.0123, W4: -0.5712, b: 0.7659\n",
      "loss: 0.1404\n",
      "Epoch: 80/100, Batch: 207/432, W1: 0.8027, W2: 0.4869, W3: 0.0123, W4: -0.5712, b: 0.7659\n",
      "loss: 0.1955\n",
      "Epoch: 80/100, Batch: 208/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5712, b: 0.7659\n",
      "loss: 0.0961\n",
      "Epoch: 80/100, Batch: 209/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5712, b: 0.7659\n",
      "loss: 0.1088\n",
      "Epoch: 80/100, Batch: 210/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5712, b: 0.7659\n",
      "loss: 0.138\n",
      "Epoch: 80/100, Batch: 211/432, W1: 0.8028, W2: 0.4869, W3: 0.0123, W4: -0.5712, b: 0.7659\n",
      "loss: 0.1226\n",
      "Epoch: 80/100, Batch: 212/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5712, b: 0.7659\n",
      "loss: 0.0929\n",
      "Epoch: 80/100, Batch: 213/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5713, b: 0.766\n",
      "loss: 0.1419\n",
      "Epoch: 80/100, Batch: 214/432, W1: 0.8028, W2: 0.487, W3: 0.0124, W4: -0.5713, b: 0.766\n",
      "loss: 0.1235\n",
      "Epoch: 80/100, Batch: 215/432, W1: 0.8028, W2: 0.487, W3: 0.0124, W4: -0.5713, b: 0.766\n",
      "loss: 0.1164\n",
      "Epoch: 80/100, Batch: 216/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5713, b: 0.766\n",
      "loss: 0.1213\n",
      "Epoch: 80/100, Batch: 217/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5713, b: 0.766\n",
      "loss: 0.1096\n",
      "Epoch: 80/100, Batch: 218/432, W1: 0.8029, W2: 0.487, W3: 0.0124, W4: -0.5713, b: 0.766\n",
      "loss: 0.0959\n",
      "Epoch: 80/100, Batch: 219/432, W1: 0.8029, W2: 0.487, W3: 0.0124, W4: -0.5713, b: 0.766\n",
      "loss: 0.1084\n",
      "Epoch: 80/100, Batch: 220/432, W1: 0.8028, W2: 0.487, W3: 0.0124, W4: -0.5714, b: 0.766\n",
      "loss: 0.1484\n",
      "Epoch: 80/100, Batch: 221/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5714, b: 0.766\n",
      "loss: 0.1109\n",
      "Epoch: 80/100, Batch: 222/432, W1: 0.8028, W2: 0.487, W3: 0.0123, W4: -0.5714, b: 0.766\n",
      "loss: 0.1229\n",
      "Epoch: 80/100, Batch: 223/432, W1: 0.8028, W2: 0.487, W3: 0.0124, W4: -0.5714, b: 0.766\n",
      "loss: 0.0928\n",
      "Epoch: 80/100, Batch: 224/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5714, b: 0.7661\n",
      "loss: 0.1134\n",
      "Epoch: 80/100, Batch: 225/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5714, b: 0.7661\n",
      "loss: 0.1396\n",
      "Epoch: 80/100, Batch: 226/432, W1: 0.8028, W2: 0.487, W3: 0.0124, W4: -0.5715, b: 0.7661\n",
      "loss: 0.1219\n",
      "Epoch: 80/100, Batch: 227/432, W1: 0.8029, W2: 0.487, W3: 0.0124, W4: -0.5715, b: 0.7661\n",
      "loss: 0.0982\n",
      "Epoch: 80/100, Batch: 228/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5715, b: 0.7661\n",
      "loss: 0.1225\n",
      "Epoch: 80/100, Batch: 229/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5715, b: 0.7661\n",
      "loss: 0.0995\n",
      "Epoch: 80/100, Batch: 230/432, W1: 0.803, W2: 0.4871, W3: 0.0125, W4: -0.5715, b: 0.7661\n",
      "loss: 0.1802\n",
      "Epoch: 80/100, Batch: 231/432, W1: 0.8029, W2: 0.4871, W3: 0.0125, W4: -0.5715, b: 0.7661\n",
      "loss: 0.1181\n",
      "Epoch: 80/100, Batch: 232/432, W1: 0.8029, W2: 0.4871, W3: 0.0124, W4: -0.5715, b: 0.7661\n",
      "loss: 0.1264\n",
      "Epoch: 80/100, Batch: 233/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5715, b: 0.7662\n",
      "loss: 0.1308\n",
      "Epoch: 80/100, Batch: 234/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5715, b: 0.7662\n",
      "loss: 0.1384\n",
      "Epoch: 80/100, Batch: 235/432, W1: 0.8031, W2: 0.4873, W3: 0.0126, W4: -0.5715, b: 0.7662\n",
      "loss: 0.1163\n",
      "Epoch: 80/100, Batch: 236/432, W1: 0.8031, W2: 0.4873, W3: 0.0126, W4: -0.5715, b: 0.7662\n",
      "loss: 0.6346\n",
      "Epoch: 80/100, Batch: 237/432, W1: 0.8031, W2: 0.4873, W3: 0.0126, W4: -0.5716, b: 0.7662\n",
      "loss: 0.1093\n",
      "Epoch: 80/100, Batch: 238/432, W1: 0.8031, W2: 0.4873, W3: 0.0125, W4: -0.5716, b: 0.7662\n",
      "loss: 0.1118\n",
      "Epoch: 80/100, Batch: 239/432, W1: 0.8031, W2: 0.4873, W3: 0.0125, W4: -0.5716, b: 0.7662\n",
      "loss: 0.0867\n",
      "Epoch: 80/100, Batch: 240/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5716, b: 0.7662\n",
      "loss: 0.1146\n",
      "Epoch: 80/100, Batch: 241/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5716, b: 0.7662\n",
      "loss: 0.1684\n",
      "Epoch: 80/100, Batch: 242/432, W1: 0.8031, W2: 0.4872, W3: 0.0125, W4: -0.5716, b: 0.7663\n",
      "loss: 0.083\n",
      "Epoch: 80/100, Batch: 243/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5717, b: 0.7663\n",
      "loss: 0.1166\n",
      "Epoch: 80/100, Batch: 244/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5717, b: 0.7663\n",
      "loss: 0.0874\n",
      "Epoch: 80/100, Batch: 245/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5717, b: 0.7663\n",
      "loss: 0.1069\n",
      "Epoch: 80/100, Batch: 246/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5717, b: 0.7663\n",
      "loss: 0.092\n",
      "Epoch: 80/100, Batch: 247/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5717, b: 0.7663\n",
      "loss: 0.0987\n",
      "Epoch: 80/100, Batch: 248/432, W1: 0.8031, W2: 0.4873, W3: 0.0125, W4: -0.5717, b: 0.7663\n",
      "loss: 0.1231\n",
      "Epoch: 80/100, Batch: 249/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5717, b: 0.7663\n",
      "loss: 0.101\n",
      "Epoch: 80/100, Batch: 250/432, W1: 0.8029, W2: 0.4871, W3: 0.0125, W4: -0.5718, b: 0.7663\n",
      "loss: 0.1153\n",
      "Epoch: 80/100, Batch: 251/432, W1: 0.8029, W2: 0.4871, W3: 0.0125, W4: -0.5718, b: 0.7663\n",
      "loss: 0.1409\n",
      "Epoch: 80/100, Batch: 252/432, W1: 0.803, W2: 0.4872, W3: 0.0125, W4: -0.5718, b: 0.7663\n",
      "loss: 0.0896\n",
      "Epoch: 80/100, Batch: 253/432, W1: 0.8029, W2: 0.4864, W3: 0.0124, W4: -0.5718, b: 0.7663\n",
      "loss: 1.6651\n",
      "Epoch: 80/100, Batch: 254/432, W1: 0.8029, W2: 0.4864, W3: 0.0124, W4: -0.5718, b: 0.7663\n",
      "loss: 0.1286\n",
      "Epoch: 80/100, Batch: 255/432, W1: 0.8028, W2: 0.4863, W3: 0.0123, W4: -0.5719, b: 0.7663\n",
      "loss: 0.1333\n",
      "Epoch: 80/100, Batch: 256/432, W1: 0.8028, W2: 0.4863, W3: 0.0123, W4: -0.5719, b: 0.7663\n",
      "loss: 0.1138\n",
      "Epoch: 80/100, Batch: 257/432, W1: 0.8028, W2: 0.4864, W3: 0.0124, W4: -0.5719, b: 0.7663\n",
      "loss: 0.1181\n",
      "Epoch: 80/100, Batch: 258/432, W1: 0.8029, W2: 0.4864, W3: 0.0124, W4: -0.5719, b: 0.7664\n",
      "loss: 0.1264\n",
      "Epoch: 80/100, Batch: 259/432, W1: 0.8029, W2: 0.4864, W3: 0.0124, W4: -0.5719, b: 0.7664\n",
      "loss: 0.1053\n",
      "Epoch: 80/100, Batch: 260/432, W1: 0.803, W2: 0.4865, W3: 0.0125, W4: -0.5719, b: 0.7664\n",
      "loss: 0.133\n",
      "Epoch: 80/100, Batch: 261/432, W1: 0.803, W2: 0.4865, W3: 0.0125, W4: -0.5719, b: 0.7664\n",
      "loss: 0.1286\n",
      "Epoch: 80/100, Batch: 262/432, W1: 0.803, W2: 0.4866, W3: 0.0125, W4: -0.5719, b: 0.7664\n",
      "loss: 0.1597\n",
      "Epoch: 80/100, Batch: 263/432, W1: 0.803, W2: 0.4866, W3: 0.0125, W4: -0.5719, b: 0.7664\n",
      "loss: 0.1205\n",
      "Epoch: 80/100, Batch: 264/432, W1: 0.8029, W2: 0.4865, W3: 0.0124, W4: -0.572, b: 0.7664\n",
      "loss: 0.1568\n",
      "Epoch: 80/100, Batch: 265/432, W1: 0.8029, W2: 0.4865, W3: 0.0124, W4: -0.572, b: 0.7664\n",
      "loss: 0.1062\n",
      "Epoch: 80/100, Batch: 266/432, W1: 0.8029, W2: 0.4865, W3: 0.0124, W4: -0.572, b: 0.7664\n",
      "loss: 0.1012\n",
      "Epoch: 80/100, Batch: 267/432, W1: 0.803, W2: 0.4865, W3: 0.0125, W4: -0.572, b: 0.7665\n",
      "loss: 0.0921\n",
      "Epoch: 80/100, Batch: 268/432, W1: 0.803, W2: 0.4865, W3: 0.0125, W4: -0.572, b: 0.7665\n",
      "loss: 0.0927\n",
      "Epoch: 80/100, Batch: 269/432, W1: 0.8029, W2: 0.4865, W3: 0.0124, W4: -0.572, b: 0.7665\n",
      "loss: 0.1073\n",
      "Epoch: 80/100, Batch: 270/432, W1: 0.8029, W2: 0.4865, W3: 0.0124, W4: -0.572, b: 0.7665\n",
      "loss: 0.1146\n",
      "Epoch: 80/100, Batch: 271/432, W1: 0.8025, W2: 0.4835, W3: 0.012, W4: -0.5722, b: 0.7664\n",
      "loss: 6.6015\n",
      "Epoch: 80/100, Batch: 272/432, W1: 0.8026, W2: 0.4836, W3: 0.0121, W4: -0.5722, b: 0.7664\n",
      "loss: 0.1213\n",
      "Epoch: 80/100, Batch: 273/432, W1: 0.8026, W2: 0.4836, W3: 0.0121, W4: -0.5722, b: 0.7665\n",
      "loss: 0.0878\n",
      "Epoch: 80/100, Batch: 274/432, W1: 0.8027, W2: 0.4837, W3: 0.0121, W4: -0.5722, b: 0.7665\n",
      "loss: 0.1047\n",
      "Epoch: 80/100, Batch: 275/432, W1: 0.8028, W2: 0.4837, W3: 0.0122, W4: -0.5722, b: 0.7665\n",
      "loss: 0.1177\n",
      "Epoch: 80/100, Batch: 276/432, W1: 0.8028, W2: 0.4838, W3: 0.0122, W4: -0.5722, b: 0.7665\n",
      "loss: 0.1044\n",
      "Epoch: 80/100, Batch: 277/432, W1: 0.8029, W2: 0.4838, W3: 0.0123, W4: -0.5722, b: 0.7665\n",
      "loss: 0.1085\n",
      "Epoch: 80/100, Batch: 278/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5722, b: 0.7665\n",
      "loss: 0.1179\n",
      "Epoch: 80/100, Batch: 279/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5722, b: 0.7666\n",
      "loss: 0.1073\n",
      "Epoch: 80/100, Batch: 280/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5722, b: 0.7666\n",
      "loss: 0.1178\n",
      "Epoch: 80/100, Batch: 281/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5722, b: 0.7666\n",
      "loss: 0.116\n",
      "Epoch: 80/100, Batch: 282/432, W1: 0.8029, W2: 0.4838, W3: 0.0123, W4: -0.5723, b: 0.7666\n",
      "loss: 0.1427\n",
      "Epoch: 80/100, Batch: 283/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5723, b: 0.7666\n",
      "loss: 0.1301\n",
      "Epoch: 80/100, Batch: 284/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5723, b: 0.7666\n",
      "loss: 0.1196\n",
      "Epoch: 80/100, Batch: 285/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5723, b: 0.7666\n",
      "loss: 0.1239\n",
      "Epoch: 80/100, Batch: 286/432, W1: 0.8029, W2: 0.4838, W3: 0.0123, W4: -0.5723, b: 0.7666\n",
      "loss: 0.1292\n",
      "Epoch: 80/100, Batch: 287/432, W1: 0.8028, W2: 0.4838, W3: 0.0122, W4: -0.5723, b: 0.7666\n",
      "loss: 0.1442\n",
      "Epoch: 80/100, Batch: 288/432, W1: 0.8028, W2: 0.4838, W3: 0.0122, W4: -0.5724, b: 0.7666\n",
      "loss: 0.098\n",
      "Epoch: 80/100, Batch: 289/432, W1: 0.8028, W2: 0.4837, W3: 0.0122, W4: -0.5724, b: 0.7666\n",
      "loss: 0.1184\n",
      "Epoch: 80/100, Batch: 290/432, W1: 0.8028, W2: 0.4837, W3: 0.0122, W4: -0.5724, b: 0.7666\n",
      "loss: 0.1405\n",
      "Epoch: 80/100, Batch: 291/432, W1: 0.8028, W2: 0.4838, W3: 0.0122, W4: -0.5724, b: 0.7666\n",
      "loss: 0.1204\n",
      "Epoch: 80/100, Batch: 292/432, W1: 0.8028, W2: 0.4837, W3: 0.0122, W4: -0.5724, b: 0.7666\n",
      "loss: 0.128\n",
      "Epoch: 80/100, Batch: 293/432, W1: 0.8027, W2: 0.4837, W3: 0.0122, W4: -0.5725, b: 0.7667\n",
      "loss: 0.1541\n",
      "Epoch: 80/100, Batch: 294/432, W1: 0.8027, W2: 0.4837, W3: 0.0122, W4: -0.5725, b: 0.7667\n",
      "loss: 0.0865\n",
      "Epoch: 80/100, Batch: 295/432, W1: 0.8027, W2: 0.4837, W3: 0.0122, W4: -0.5725, b: 0.7667\n",
      "loss: 0.0895\n",
      "Epoch: 80/100, Batch: 296/432, W1: 0.8027, W2: 0.4836, W3: 0.0121, W4: -0.5725, b: 0.7667\n",
      "loss: 0.1594\n",
      "Epoch: 80/100, Batch: 297/432, W1: 0.8026, W2: 0.4836, W3: 0.0121, W4: -0.5725, b: 0.7667\n",
      "loss: 0.1158\n",
      "Epoch: 80/100, Batch: 298/432, W1: 0.8027, W2: 0.4836, W3: 0.0122, W4: -0.5725, b: 0.7667\n",
      "loss: 0.0816\n",
      "Epoch: 80/100, Batch: 299/432, W1: 0.8026, W2: 0.4836, W3: 0.0121, W4: -0.5726, b: 0.7667\n",
      "loss: 0.1185\n",
      "Epoch: 80/100, Batch: 300/432, W1: 0.8027, W2: 0.4837, W3: 0.0122, W4: -0.5726, b: 0.7667\n",
      "loss: 0.1277\n",
      "Epoch: 80/100, Batch: 301/432, W1: 0.8027, W2: 0.4837, W3: 0.0122, W4: -0.5726, b: 0.7667\n",
      "loss: 0.1236\n",
      "Epoch: 80/100, Batch: 302/432, W1: 0.8027, W2: 0.4837, W3: 0.0122, W4: -0.5726, b: 0.7667\n",
      "loss: 0.1186\n",
      "Epoch: 80/100, Batch: 303/432, W1: 0.8028, W2: 0.4837, W3: 0.0122, W4: -0.5726, b: 0.7667\n",
      "loss: 0.0905\n",
      "Epoch: 80/100, Batch: 304/432, W1: 0.8028, W2: 0.4838, W3: 0.0122, W4: -0.5726, b: 0.7667\n",
      "loss: 0.1236\n",
      "Epoch: 80/100, Batch: 305/432, W1: 0.8028, W2: 0.4838, W3: 0.0122, W4: -0.5726, b: 0.7668\n",
      "loss: 0.1383\n",
      "Epoch: 80/100, Batch: 306/432, W1: 0.8028, W2: 0.4838, W3: 0.0122, W4: -0.5726, b: 0.7668\n",
      "loss: 0.103\n",
      "Epoch: 80/100, Batch: 307/432, W1: 0.8027, W2: 0.4837, W3: 0.0122, W4: -0.5726, b: 0.7668\n",
      "loss: 0.1106\n",
      "Epoch: 80/100, Batch: 308/432, W1: 0.8027, W2: 0.4837, W3: 0.0122, W4: -0.5727, b: 0.7668\n",
      "loss: 0.1251\n",
      "Epoch: 80/100, Batch: 309/432, W1: 0.8027, W2: 0.4837, W3: 0.0122, W4: -0.5727, b: 0.7668\n",
      "loss: 0.1034\n",
      "Epoch: 80/100, Batch: 310/432, W1: 0.8028, W2: 0.4837, W3: 0.0122, W4: -0.5727, b: 0.7668\n",
      "loss: 0.0954\n",
      "Epoch: 80/100, Batch: 311/432, W1: 0.8028, W2: 0.4838, W3: 0.0123, W4: -0.5727, b: 0.7668\n",
      "loss: 0.1283\n",
      "Epoch: 80/100, Batch: 312/432, W1: 0.8028, W2: 0.4838, W3: 0.0123, W4: -0.5727, b: 0.7668\n",
      "loss: 0.1072\n",
      "Epoch: 80/100, Batch: 313/432, W1: 0.8029, W2: 0.4838, W3: 0.0123, W4: -0.5727, b: 0.7668\n",
      "loss: 0.1053\n",
      "Epoch: 80/100, Batch: 314/432, W1: 0.8029, W2: 0.4838, W3: 0.0123, W4: -0.5727, b: 0.7668\n",
      "loss: 0.1088\n",
      "Epoch: 80/100, Batch: 315/432, W1: 0.8028, W2: 0.4838, W3: 0.0123, W4: -0.5727, b: 0.7668\n",
      "loss: 0.1206\n",
      "Epoch: 80/100, Batch: 316/432, W1: 0.8028, W2: 0.4838, W3: 0.0122, W4: -0.5727, b: 0.7668\n",
      "loss: 0.0961\n",
      "Epoch: 80/100, Batch: 317/432, W1: 0.8028, W2: 0.4838, W3: 0.0122, W4: -0.5728, b: 0.7668\n",
      "loss: 0.0961\n",
      "Epoch: 80/100, Batch: 318/432, W1: 0.8028, W2: 0.4838, W3: 0.0123, W4: -0.5728, b: 0.7669\n",
      "loss: 0.1076\n",
      "Epoch: 80/100, Batch: 319/432, W1: 0.8028, W2: 0.4838, W3: 0.0123, W4: -0.5728, b: 0.7669\n",
      "loss: 0.0967\n",
      "Epoch: 80/100, Batch: 320/432, W1: 0.8028, W2: 0.4838, W3: 0.0123, W4: -0.5728, b: 0.7669\n",
      "loss: 0.1047\n",
      "Epoch: 80/100, Batch: 321/432, W1: 0.8028, W2: 0.4838, W3: 0.0123, W4: -0.5728, b: 0.7669\n",
      "loss: 0.0895\n",
      "Epoch: 80/100, Batch: 322/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5728, b: 0.7669\n",
      "loss: 0.1158\n",
      "Epoch: 80/100, Batch: 323/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5728, b: 0.7669\n",
      "loss: 0.1374\n",
      "Epoch: 80/100, Batch: 324/432, W1: 0.8029, W2: 0.4839, W3: 0.0123, W4: -0.5728, b: 0.7669\n",
      "loss: 0.0886\n",
      "Epoch: 80/100, Batch: 325/432, W1: 0.803, W2: 0.484, W3: 0.0124, W4: -0.5728, b: 0.7669\n",
      "loss: 0.1264\n",
      "Epoch: 80/100, Batch: 326/432, W1: 0.8029, W2: 0.4839, W3: 0.0124, W4: -0.5729, b: 0.7669\n",
      "loss: 0.1553\n",
      "Epoch: 80/100, Batch: 327/432, W1: 0.803, W2: 0.484, W3: 0.0124, W4: -0.5729, b: 0.767\n",
      "loss: 0.155\n",
      "Epoch: 80/100, Batch: 328/432, W1: 0.803, W2: 0.484, W3: 0.0124, W4: -0.5729, b: 0.767\n",
      "loss: 0.1494\n",
      "Epoch: 80/100, Batch: 329/432, W1: 0.803, W2: 0.484, W3: 0.0124, W4: -0.5729, b: 0.767\n",
      "loss: 0.1058\n",
      "Epoch: 80/100, Batch: 330/432, W1: 0.803, W2: 0.484, W3: 0.0124, W4: -0.5729, b: 0.767\n",
      "loss: 0.1005\n",
      "Epoch: 80/100, Batch: 331/432, W1: 0.803, W2: 0.484, W3: 0.0124, W4: -0.5729, b: 0.767\n",
      "loss: 0.1345\n",
      "Epoch: 80/100, Batch: 332/432, W1: 0.803, W2: 0.484, W3: 0.0124, W4: -0.5729, b: 0.767\n",
      "loss: 0.0949\n",
      "Epoch: 80/100, Batch: 333/432, W1: 0.8031, W2: 0.4841, W3: 0.0125, W4: -0.5729, b: 0.767\n",
      "loss: 1.0322\n",
      "Epoch: 80/100, Batch: 334/432, W1: 0.8031, W2: 0.4841, W3: 0.0125, W4: -0.5729, b: 0.7671\n",
      "loss: 0.1098\n",
      "Epoch: 80/100, Batch: 335/432, W1: 0.8031, W2: 0.4841, W3: 0.0125, W4: -0.5729, b: 0.7671\n",
      "loss: 0.0968\n",
      "Epoch: 80/100, Batch: 336/432, W1: 0.8031, W2: 0.4841, W3: 0.0125, W4: -0.5729, b: 0.7671\n",
      "loss: 0.117\n",
      "Epoch: 80/100, Batch: 337/432, W1: 0.8032, W2: 0.4842, W3: 0.0126, W4: -0.5729, b: 0.7671\n",
      "loss: 0.1393\n",
      "Epoch: 80/100, Batch: 338/432, W1: 0.8033, W2: 0.4843, W3: 0.0126, W4: -0.5729, b: 0.7671\n",
      "loss: 0.1443\n",
      "Epoch: 80/100, Batch: 339/432, W1: 0.8033, W2: 0.4843, W3: 0.0126, W4: -0.5729, b: 0.7671\n",
      "loss: 0.1062\n",
      "Epoch: 80/100, Batch: 340/432, W1: 0.8033, W2: 0.4843, W3: 0.0126, W4: -0.573, b: 0.7671\n",
      "loss: 0.1437\n",
      "Epoch: 80/100, Batch: 341/432, W1: 0.8034, W2: 0.4844, W3: 0.0127, W4: -0.573, b: 0.7672\n",
      "loss: 0.1198\n",
      "Epoch: 80/100, Batch: 342/432, W1: 0.8034, W2: 0.4844, W3: 0.0126, W4: -0.573, b: 0.7672\n",
      "loss: 0.1685\n",
      "Epoch: 80/100, Batch: 343/432, W1: 0.8034, W2: 0.4844, W3: 0.0127, W4: -0.573, b: 0.7672\n",
      "loss: 0.1009\n",
      "Epoch: 80/100, Batch: 344/432, W1: 0.8034, W2: 0.4844, W3: 0.0127, W4: -0.573, b: 0.7672\n",
      "loss: 0.1136\n",
      "Epoch: 80/100, Batch: 345/432, W1: 0.8034, W2: 0.4844, W3: 0.0127, W4: -0.573, b: 0.7672\n",
      "loss: 0.0903\n",
      "Epoch: 80/100, Batch: 346/432, W1: 0.8034, W2: 0.4844, W3: 0.0126, W4: -0.573, b: 0.7672\n",
      "loss: 0.1334\n",
      "Epoch: 80/100, Batch: 347/432, W1: 0.8034, W2: 0.4844, W3: 0.0127, W4: -0.573, b: 0.7672\n",
      "loss: 0.1126\n",
      "Epoch: 80/100, Batch: 348/432, W1: 0.8034, W2: 0.4844, W3: 0.0127, W4: -0.5731, b: 0.7672\n",
      "loss: 0.1171\n",
      "Epoch: 80/100, Batch: 349/432, W1: 0.8034, W2: 0.4844, W3: 0.0127, W4: -0.5731, b: 0.7672\n",
      "loss: 0.1031\n",
      "Epoch: 80/100, Batch: 350/432, W1: 0.8034, W2: 0.4844, W3: 0.0127, W4: -0.5731, b: 0.7672\n",
      "loss: 0.1187\n",
      "Epoch: 80/100, Batch: 351/432, W1: 0.8034, W2: 0.4844, W3: 0.0127, W4: -0.5731, b: 0.7673\n",
      "loss: 0.1256\n",
      "Epoch: 80/100, Batch: 352/432, W1: 0.8034, W2: 0.4845, W3: 0.0127, W4: -0.5731, b: 0.7673\n",
      "loss: 0.1205\n",
      "Epoch: 80/100, Batch: 353/432, W1: 0.8035, W2: 0.4845, W3: 0.0128, W4: -0.5731, b: 0.7673\n",
      "loss: 0.0873\n",
      "Epoch: 80/100, Batch: 354/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5731, b: 0.7673\n",
      "loss: 0.1178\n",
      "Epoch: 80/100, Batch: 355/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5731, b: 0.7673\n",
      "loss: 0.1152\n",
      "Epoch: 80/100, Batch: 356/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5731, b: 0.7673\n",
      "loss: 0.1149\n",
      "Epoch: 80/100, Batch: 357/432, W1: 0.8035, W2: 0.4846, W3: 0.0128, W4: -0.5732, b: 0.7673\n",
      "loss: 0.1521\n",
      "Epoch: 80/100, Batch: 358/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5732, b: 0.7673\n",
      "loss: 0.1118\n",
      "Epoch: 80/100, Batch: 359/432, W1: 0.8035, W2: 0.4845, W3: 0.0128, W4: -0.5732, b: 0.7673\n",
      "loss: 0.1738\n",
      "Epoch: 80/100, Batch: 360/432, W1: 0.8035, W2: 0.4845, W3: 0.0127, W4: -0.5732, b: 0.7673\n",
      "loss: 0.1404\n",
      "Epoch: 80/100, Batch: 361/432, W1: 0.8035, W2: 0.4845, W3: 0.0127, W4: -0.5732, b: 0.7673\n",
      "loss: 0.1118\n",
      "Epoch: 80/100, Batch: 362/432, W1: 0.8035, W2: 0.4845, W3: 0.0127, W4: -0.5732, b: 0.7674\n",
      "loss: 0.1381\n",
      "Epoch: 80/100, Batch: 363/432, W1: 0.8035, W2: 0.4846, W3: 0.0128, W4: -0.5732, b: 0.7674\n",
      "loss: 0.1066\n",
      "Epoch: 80/100, Batch: 364/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5732, b: 0.7674\n",
      "loss: 0.1221\n",
      "Epoch: 80/100, Batch: 365/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5733, b: 0.7674\n",
      "loss: 0.1079\n",
      "Epoch: 80/100, Batch: 366/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5733, b: 0.7674\n",
      "loss: 0.1274\n",
      "Epoch: 80/100, Batch: 367/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5733, b: 0.7674\n",
      "loss: 0.0989\n",
      "Epoch: 80/100, Batch: 368/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5733, b: 0.7674\n",
      "loss: 0.1093\n",
      "Epoch: 80/100, Batch: 369/432, W1: 0.8036, W2: 0.4847, W3: 0.0128, W4: -0.5733, b: 0.7674\n",
      "loss: 0.1156\n",
      "Epoch: 80/100, Batch: 370/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5733, b: 0.7674\n",
      "loss: 0.1238\n",
      "Epoch: 80/100, Batch: 371/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5733, b: 0.7674\n",
      "loss: 0.1407\n",
      "Epoch: 80/100, Batch: 372/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5734, b: 0.7674\n",
      "loss: 0.0876\n",
      "Epoch: 80/100, Batch: 373/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5734, b: 0.7675\n",
      "loss: 0.1243\n",
      "Epoch: 80/100, Batch: 374/432, W1: 0.8036, W2: 0.4846, W3: 0.0128, W4: -0.5734, b: 0.7675\n",
      "loss: 0.1137\n",
      "Epoch: 80/100, Batch: 375/432, W1: 0.8036, W2: 0.4847, W3: 0.0128, W4: -0.5734, b: 0.7675\n",
      "loss: 0.1018\n",
      "Epoch: 80/100, Batch: 376/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5734, b: 0.7675\n",
      "loss: 0.0894\n",
      "Epoch: 80/100, Batch: 377/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5734, b: 0.7675\n",
      "loss: 0.1132\n",
      "Epoch: 80/100, Batch: 378/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5734, b: 0.7675\n",
      "loss: 0.1126\n",
      "Epoch: 80/100, Batch: 379/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5734, b: 0.7675\n",
      "loss: 0.1178\n",
      "Epoch: 80/100, Batch: 380/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5734, b: 0.7675\n",
      "loss: 0.1057\n",
      "Epoch: 80/100, Batch: 381/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5734, b: 0.7675\n",
      "loss: 0.0964\n",
      "Epoch: 80/100, Batch: 382/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5735, b: 0.7675\n",
      "loss: 0.1486\n",
      "Epoch: 80/100, Batch: 383/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5735, b: 0.7676\n",
      "loss: 0.1349\n",
      "Epoch: 80/100, Batch: 384/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5735, b: 0.7676\n",
      "loss: 0.1575\n",
      "Epoch: 80/100, Batch: 385/432, W1: 0.8037, W2: 0.4848, W3: 0.0129, W4: -0.5735, b: 0.7676\n",
      "loss: 0.131\n",
      "Epoch: 80/100, Batch: 386/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5735, b: 0.7676\n",
      "loss: 0.1404\n",
      "Epoch: 80/100, Batch: 387/432, W1: 0.8037, W2: 0.4848, W3: 0.0129, W4: -0.5735, b: 0.7676\n",
      "loss: 0.1101\n",
      "Epoch: 80/100, Batch: 388/432, W1: 0.8038, W2: 0.4848, W3: 0.013, W4: -0.5736, b: 0.7676\n",
      "loss: 0.1366\n",
      "Epoch: 80/100, Batch: 389/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5736, b: 0.7676\n",
      "loss: 0.111\n",
      "Epoch: 80/100, Batch: 390/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5736, b: 0.7677\n",
      "loss: 0.1241\n",
      "Epoch: 80/100, Batch: 391/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5736, b: 0.7677\n",
      "loss: 0.1148\n",
      "Epoch: 80/100, Batch: 392/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5736, b: 0.7677\n",
      "loss: 0.095\n",
      "Epoch: 80/100, Batch: 393/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5736, b: 0.7677\n",
      "loss: 0.1057\n",
      "Epoch: 80/100, Batch: 394/432, W1: 0.8038, W2: 0.4849, W3: 0.013, W4: -0.5736, b: 0.7677\n",
      "loss: 0.1111\n",
      "Epoch: 80/100, Batch: 395/432, W1: 0.8038, W2: 0.4848, W3: 0.013, W4: -0.5736, b: 0.7677\n",
      "loss: 0.1265\n",
      "Epoch: 80/100, Batch: 396/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5736, b: 0.7677\n",
      "loss: 0.0936\n",
      "Epoch: 80/100, Batch: 397/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5737, b: 0.7677\n",
      "loss: 0.0969\n",
      "Epoch: 80/100, Batch: 398/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5737, b: 0.7677\n",
      "loss: 0.1257\n",
      "Epoch: 80/100, Batch: 399/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5737, b: 0.7677\n",
      "loss: 0.1264\n",
      "Epoch: 80/100, Batch: 400/432, W1: 0.8038, W2: 0.4849, W3: 0.013, W4: -0.5737, b: 0.7677\n",
      "loss: 0.0892\n",
      "Epoch: 80/100, Batch: 401/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5737, b: 0.7677\n",
      "loss: 0.0998\n",
      "Epoch: 80/100, Batch: 402/432, W1: 0.8039, W2: 0.485, W3: 0.0131, W4: -0.5737, b: 0.7678\n",
      "loss: 0.1115\n",
      "Epoch: 80/100, Batch: 403/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5737, b: 0.7678\n",
      "loss: 0.0997\n",
      "Epoch: 80/100, Batch: 404/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5738, b: 0.7678\n",
      "loss: 0.1333\n",
      "Epoch: 80/100, Batch: 405/432, W1: 0.8038, W2: 0.4849, W3: 0.013, W4: -0.5738, b: 0.7678\n",
      "loss: 0.1219\n",
      "Epoch: 80/100, Batch: 406/432, W1: 0.8038, W2: 0.4849, W3: 0.013, W4: -0.5738, b: 0.7678\n",
      "loss: 0.1015\n",
      "Epoch: 80/100, Batch: 407/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5738, b: 0.7678\n",
      "loss: 0.1395\n",
      "Epoch: 80/100, Batch: 408/432, W1: 0.8039, W2: 0.4849, W3: 0.013, W4: -0.5738, b: 0.7678\n",
      "loss: 0.0845\n",
      "Epoch: 80/100, Batch: 409/432, W1: 0.8038, W2: 0.4848, W3: 0.013, W4: -0.5738, b: 0.7678\n",
      "loss: 0.1386\n",
      "Epoch: 80/100, Batch: 410/432, W1: 0.8038, W2: 0.4848, W3: 0.013, W4: -0.5738, b: 0.7678\n",
      "loss: 0.1104\n",
      "Epoch: 80/100, Batch: 411/432, W1: 0.8038, W2: 0.4849, W3: 0.013, W4: -0.5739, b: 0.7678\n",
      "loss: 0.1037\n",
      "Epoch: 80/100, Batch: 412/432, W1: 0.8038, W2: 0.4848, W3: 0.013, W4: -0.5739, b: 0.7678\n",
      "loss: 0.1197\n",
      "Epoch: 80/100, Batch: 413/432, W1: 0.8038, W2: 0.4848, W3: 0.013, W4: -0.5739, b: 0.7678\n",
      "loss: 0.1465\n",
      "Epoch: 80/100, Batch: 414/432, W1: 0.8037, W2: 0.4848, W3: 0.013, W4: -0.5739, b: 0.7678\n",
      "loss: 0.1261\n",
      "Epoch: 80/100, Batch: 415/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5739, b: 0.7678\n",
      "loss: 0.1056\n",
      "Epoch: 80/100, Batch: 416/432, W1: 0.8036, W2: 0.4846, W3: 0.0129, W4: -0.574, b: 0.7678\n",
      "loss: 0.1457\n",
      "Epoch: 80/100, Batch: 417/432, W1: 0.8036, W2: 0.4846, W3: 0.0129, W4: -0.574, b: 0.7678\n",
      "loss: 0.112\n",
      "Epoch: 80/100, Batch: 418/432, W1: 0.8036, W2: 0.4846, W3: 0.0129, W4: -0.574, b: 0.7678\n",
      "loss: 0.1083\n",
      "Epoch: 80/100, Batch: 419/432, W1: 0.8036, W2: 0.4847, W3: 0.0129, W4: -0.574, b: 0.7678\n",
      "loss: 0.1006\n",
      "Epoch: 80/100, Batch: 420/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.574, b: 0.7679\n",
      "loss: 0.1113\n",
      "Epoch: 80/100, Batch: 421/432, W1: 0.8036, W2: 0.4847, W3: 0.0129, W4: -0.574, b: 0.7679\n",
      "loss: 0.1303\n",
      "Epoch: 80/100, Batch: 422/432, W1: 0.8036, W2: 0.4847, W3: 0.0129, W4: -0.574, b: 0.7679\n",
      "loss: 0.1014\n",
      "Epoch: 80/100, Batch: 423/432, W1: 0.8036, W2: 0.4846, W3: 0.0129, W4: -0.574, b: 0.7679\n",
      "loss: 0.1059\n",
      "Epoch: 80/100, Batch: 424/432, W1: 0.8036, W2: 0.4846, W3: 0.0129, W4: -0.5741, b: 0.7679\n",
      "loss: 0.1287\n",
      "Epoch: 80/100, Batch: 425/432, W1: 0.8036, W2: 0.4846, W3: 0.0129, W4: -0.5741, b: 0.7679\n",
      "loss: 0.0897\n",
      "Epoch: 80/100, Batch: 426/432, W1: 0.8036, W2: 0.4847, W3: 0.0129, W4: -0.5741, b: 0.7679\n",
      "loss: 0.0948\n",
      "Epoch: 80/100, Batch: 427/432, W1: 0.8036, W2: 0.4847, W3: 0.0129, W4: -0.5741, b: 0.7679\n",
      "loss: 0.0952\n",
      "Epoch: 80/100, Batch: 428/432, W1: 0.8037, W2: 0.4848, W3: 0.013, W4: -0.5741, b: 0.7679\n",
      "loss: 0.1577\n",
      "Epoch: 80/100, Batch: 429/432, W1: 0.8037, W2: 0.4848, W3: 0.013, W4: -0.5741, b: 0.7679\n",
      "loss: 0.0876\n",
      "Epoch: 80/100, Batch: 430/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5741, b: 0.7679\n",
      "loss: 0.1333\n",
      "Epoch: 80/100, Batch: 431/432, W1: 0.8037, W2: 0.4847, W3: 0.0129, W4: -0.5741, b: 0.7679\n",
      "loss: 0.1142\n",
      "Epoch: 80/100, Batch: 432/432, W1: 0.8038, W2: 0.4848, W3: 0.013, W4: -0.5741, b: 0.768\n",
      "loss: 0.1027\n",
      "Epoch: 90/100, Batch: 1/432, W1: 0.8146, W2: 0.4698, W3: 0.0225, W4: -0.6214, b: 0.801\n",
      "loss: 0.1564\n",
      "Epoch: 90/100, Batch: 2/432, W1: 0.8146, W2: 0.4698, W3: 0.0225, W4: -0.6215, b: 0.801\n",
      "loss: 0.112\n",
      "Epoch: 90/100, Batch: 3/432, W1: 0.8146, W2: 0.4697, W3: 0.0225, W4: -0.6215, b: 0.801\n",
      "loss: 0.1369\n",
      "Epoch: 90/100, Batch: 4/432, W1: 0.8146, W2: 0.4697, W3: 0.0225, W4: -0.6215, b: 0.801\n",
      "loss: 0.0753\n",
      "Epoch: 90/100, Batch: 5/432, W1: 0.8146, W2: 0.4697, W3: 0.0225, W4: -0.6215, b: 0.801\n",
      "loss: 0.1099\n",
      "Epoch: 90/100, Batch: 6/432, W1: 0.8146, W2: 0.4698, W3: 0.0225, W4: -0.6215, b: 0.8011\n",
      "loss: 0.1009\n",
      "Epoch: 90/100, Batch: 7/432, W1: 0.8146, W2: 0.4697, W3: 0.0225, W4: -0.6215, b: 0.8011\n",
      "loss: 0.1228\n",
      "Epoch: 90/100, Batch: 8/432, W1: 0.8145, W2: 0.4697, W3: 0.0224, W4: -0.6215, b: 0.8011\n",
      "loss: 0.1562\n",
      "Epoch: 90/100, Batch: 9/432, W1: 0.8145, W2: 0.4696, W3: 0.0224, W4: -0.6216, b: 0.8011\n",
      "loss: 0.093\n",
      "Epoch: 90/100, Batch: 10/432, W1: 0.8145, W2: 0.4696, W3: 0.0224, W4: -0.6216, b: 0.8011\n",
      "loss: 0.1356\n",
      "Epoch: 90/100, Batch: 11/432, W1: 0.8144, W2: 0.4696, W3: 0.0224, W4: -0.6216, b: 0.8011\n",
      "loss: 0.1132\n",
      "Epoch: 90/100, Batch: 12/432, W1: 0.8144, W2: 0.4696, W3: 0.0224, W4: -0.6216, b: 0.8011\n",
      "loss: 0.1178\n",
      "Epoch: 90/100, Batch: 13/432, W1: 0.8144, W2: 0.4696, W3: 0.0224, W4: -0.6216, b: 0.8011\n",
      "loss: 0.1154\n",
      "Epoch: 90/100, Batch: 14/432, W1: 0.8144, W2: 0.4696, W3: 0.0224, W4: -0.6216, b: 0.8011\n",
      "loss: 0.1039\n",
      "Epoch: 90/100, Batch: 15/432, W1: 0.8145, W2: 0.4696, W3: 0.0224, W4: -0.6216, b: 0.8011\n",
      "loss: 0.0906\n",
      "Epoch: 90/100, Batch: 16/432, W1: 0.8145, W2: 0.4696, W3: 0.0224, W4: -0.6216, b: 0.8011\n",
      "loss: 0.1052\n",
      "Epoch: 90/100, Batch: 17/432, W1: 0.8145, W2: 0.4697, W3: 0.0224, W4: -0.6217, b: 0.8011\n",
      "loss: 0.0808\n",
      "Epoch: 90/100, Batch: 18/432, W1: 0.8145, W2: 0.4697, W3: 0.0224, W4: -0.6217, b: 0.8011\n",
      "loss: 0.1046\n",
      "Epoch: 90/100, Batch: 19/432, W1: 0.8146, W2: 0.4697, W3: 0.0225, W4: -0.6216, b: 0.8012\n",
      "loss: 0.7804\n",
      "Epoch: 90/100, Batch: 20/432, W1: 0.8146, W2: 0.4697, W3: 0.0225, W4: -0.6217, b: 0.8012\n",
      "loss: 0.0971\n",
      "Epoch: 90/100, Batch: 21/432, W1: 0.8146, W2: 0.4698, W3: 0.0225, W4: -0.6217, b: 0.8012\n",
      "loss: 0.1037\n",
      "Epoch: 90/100, Batch: 22/432, W1: 0.8146, W2: 0.4698, W3: 0.0225, W4: -0.6217, b: 0.8012\n",
      "loss: 0.1382\n",
      "Epoch: 90/100, Batch: 23/432, W1: 0.8147, W2: 0.4698, W3: 0.0225, W4: -0.6217, b: 0.8012\n",
      "loss: 0.1016\n",
      "Epoch: 90/100, Batch: 24/432, W1: 0.8147, W2: 0.4698, W3: 0.0225, W4: -0.6217, b: 0.8012\n",
      "loss: 0.1391\n",
      "Epoch: 90/100, Batch: 25/432, W1: 0.8147, W2: 0.4698, W3: 0.0225, W4: -0.6217, b: 0.8012\n",
      "loss: 0.1063\n",
      "Epoch: 90/100, Batch: 26/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6217, b: 0.8013\n",
      "loss: 0.6249\n",
      "Epoch: 90/100, Batch: 27/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6217, b: 0.8013\n",
      "loss: 0.107\n",
      "Epoch: 90/100, Batch: 28/432, W1: 0.8146, W2: 0.4698, W3: 0.0225, W4: -0.6218, b: 0.8013\n",
      "loss: 0.1234\n",
      "Epoch: 90/100, Batch: 29/432, W1: 0.8146, W2: 0.4698, W3: 0.0225, W4: -0.6218, b: 0.8013\n",
      "loss: 0.1302\n",
      "Epoch: 90/100, Batch: 30/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6218, b: 0.8013\n",
      "loss: 0.0988\n",
      "Epoch: 90/100, Batch: 31/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6218, b: 0.8013\n",
      "loss: 0.129\n",
      "Epoch: 90/100, Batch: 32/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6218, b: 0.8013\n",
      "loss: 0.1606\n",
      "Epoch: 90/100, Batch: 33/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6218, b: 0.8013\n",
      "loss: 0.1191\n",
      "Epoch: 90/100, Batch: 34/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6218, b: 0.8013\n",
      "loss: 0.1024\n",
      "Epoch: 90/100, Batch: 35/432, W1: 0.8148, W2: 0.4699, W3: 0.0226, W4: -0.6218, b: 0.8013\n",
      "loss: 0.1023\n",
      "Epoch: 90/100, Batch: 36/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6218, b: 0.8013\n",
      "loss: 0.1011\n",
      "Epoch: 90/100, Batch: 37/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6218, b: 0.8013\n",
      "loss: 0.0921\n",
      "Epoch: 90/100, Batch: 38/432, W1: 0.8148, W2: 0.47, W3: 0.0226, W4: -0.6218, b: 0.8014\n",
      "loss: 0.0902\n",
      "Epoch: 90/100, Batch: 39/432, W1: 0.8148, W2: 0.4699, W3: 0.0226, W4: -0.6219, b: 0.8014\n",
      "loss: 0.1141\n",
      "Epoch: 90/100, Batch: 40/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6219, b: 0.8014\n",
      "loss: 0.1465\n",
      "Epoch: 90/100, Batch: 41/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6219, b: 0.8014\n",
      "loss: 1.0271\n",
      "Epoch: 90/100, Batch: 42/432, W1: 0.8147, W2: 0.4699, W3: 0.0226, W4: -0.6219, b: 0.8014\n",
      "loss: 0.1132\n",
      "Epoch: 90/100, Batch: 43/432, W1: 0.8148, W2: 0.47, W3: 0.0226, W4: -0.6219, b: 0.8014\n",
      "loss: 0.0946\n",
      "Epoch: 90/100, Batch: 44/432, W1: 0.8149, W2: 0.47, W3: 0.0227, W4: -0.6219, b: 0.8014\n",
      "loss: 0.0871\n",
      "Epoch: 90/100, Batch: 45/432, W1: 0.8148, W2: 0.47, W3: 0.0226, W4: -0.6219, b: 0.8014\n",
      "loss: 0.1082\n",
      "Epoch: 90/100, Batch: 46/432, W1: 0.8148, W2: 0.4699, W3: 0.0226, W4: -0.6219, b: 0.8014\n",
      "loss: 0.0931\n",
      "Epoch: 90/100, Batch: 47/432, W1: 0.8148, W2: 0.47, W3: 0.0226, W4: -0.6219, b: 0.8015\n",
      "loss: 0.1346\n",
      "Epoch: 90/100, Batch: 48/432, W1: 0.8148, W2: 0.47, W3: 0.0227, W4: -0.622, b: 0.8015\n",
      "loss: 0.0922\n",
      "Epoch: 90/100, Batch: 49/432, W1: 0.8148, W2: 0.47, W3: 0.0227, W4: -0.622, b: 0.8015\n",
      "loss: 0.0675\n",
      "Epoch: 90/100, Batch: 50/432, W1: 0.8148, W2: 0.47, W3: 0.0227, W4: -0.622, b: 0.8015\n",
      "loss: 0.1231\n",
      "Epoch: 90/100, Batch: 51/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.622, b: 0.8015\n",
      "loss: 0.1002\n",
      "Epoch: 90/100, Batch: 52/432, W1: 0.8148, W2: 0.47, W3: 0.0227, W4: -0.622, b: 0.8015\n",
      "loss: 0.0999\n",
      "Epoch: 90/100, Batch: 53/432, W1: 0.8149, W2: 0.47, W3: 0.0227, W4: -0.622, b: 0.8015\n",
      "loss: 0.0964\n",
      "Epoch: 90/100, Batch: 54/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.622, b: 0.8015\n",
      "loss: 0.1285\n",
      "Epoch: 90/100, Batch: 55/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.622, b: 0.8015\n",
      "loss: 0.1116\n",
      "Epoch: 90/100, Batch: 56/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.622, b: 0.8015\n",
      "loss: 0.0927\n",
      "Epoch: 90/100, Batch: 57/432, W1: 0.8149, W2: 0.47, W3: 0.0227, W4: -0.6221, b: 0.8015\n",
      "loss: 0.1394\n",
      "Epoch: 90/100, Batch: 58/432, W1: 0.8148, W2: 0.47, W3: 0.0227, W4: -0.6221, b: 0.8015\n",
      "loss: 0.0968\n",
      "Epoch: 90/100, Batch: 59/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.6221, b: 0.8016\n",
      "loss: 0.0898\n",
      "Epoch: 90/100, Batch: 60/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.6221, b: 0.8016\n",
      "loss: 0.1256\n",
      "Epoch: 90/100, Batch: 61/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.6221, b: 0.8016\n",
      "loss: 0.1032\n",
      "Epoch: 90/100, Batch: 62/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6221, b: 0.8016\n",
      "loss: 0.103\n",
      "Epoch: 90/100, Batch: 63/432, W1: 0.815, W2: 0.4701, W3: 0.0228, W4: -0.6221, b: 0.8016\n",
      "loss: 0.1097\n",
      "Epoch: 90/100, Batch: 64/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.6221, b: 0.8016\n",
      "loss: 0.1336\n",
      "Epoch: 90/100, Batch: 65/432, W1: 0.815, W2: 0.4701, W3: 0.0228, W4: -0.6221, b: 0.8016\n",
      "loss: 0.109\n",
      "Epoch: 90/100, Batch: 66/432, W1: 0.8149, W2: 0.4701, W3: 0.0228, W4: -0.6222, b: 0.8016\n",
      "loss: 0.1171\n",
      "Epoch: 90/100, Batch: 67/432, W1: 0.8149, W2: 0.4701, W3: 0.0228, W4: -0.6222, b: 0.8016\n",
      "loss: 0.0951\n",
      "Epoch: 90/100, Batch: 68/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.6222, b: 0.8016\n",
      "loss: 0.1006\n",
      "Epoch: 90/100, Batch: 69/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.6222, b: 0.8016\n",
      "loss: 0.1134\n",
      "Epoch: 90/100, Batch: 70/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.6222, b: 0.8016\n",
      "loss: 0.1192\n",
      "Epoch: 90/100, Batch: 71/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.6222, b: 0.8016\n",
      "loss: 0.0931\n",
      "Epoch: 90/100, Batch: 72/432, W1: 0.8149, W2: 0.4701, W3: 0.0227, W4: -0.6222, b: 0.8016\n",
      "loss: 0.0867\n",
      "Epoch: 90/100, Batch: 73/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6222, b: 0.8017\n",
      "loss: 0.101\n",
      "Epoch: 90/100, Batch: 74/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6222, b: 0.8017\n",
      "loss: 0.1073\n",
      "Epoch: 90/100, Batch: 75/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6222, b: 0.8017\n",
      "loss: 0.1187\n",
      "Epoch: 90/100, Batch: 76/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6223, b: 0.8017\n",
      "loss: 0.0948\n",
      "Epoch: 90/100, Batch: 77/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6223, b: 0.8017\n",
      "loss: 0.0812\n",
      "Epoch: 90/100, Batch: 78/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6223, b: 0.8017\n",
      "loss: 0.0912\n",
      "Epoch: 90/100, Batch: 79/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6223, b: 0.8017\n",
      "loss: 0.1047\n",
      "Epoch: 90/100, Batch: 80/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6223, b: 0.8017\n",
      "loss: 0.0944\n",
      "Epoch: 90/100, Batch: 81/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6223, b: 0.8017\n",
      "loss: 0.1126\n",
      "Epoch: 90/100, Batch: 82/432, W1: 0.815, W2: 0.4702, W3: 0.0228, W4: -0.6223, b: 0.8017\n",
      "loss: 0.1242\n",
      "Epoch: 90/100, Batch: 83/432, W1: 0.8151, W2: 0.4703, W3: 0.0229, W4: -0.6223, b: 0.8018\n",
      "loss: 0.1091\n",
      "Epoch: 90/100, Batch: 84/432, W1: 0.8151, W2: 0.4703, W3: 0.0229, W4: -0.6223, b: 0.8018\n",
      "loss: 0.1022\n",
      "Epoch: 90/100, Batch: 85/432, W1: 0.8152, W2: 0.4704, W3: 0.0229, W4: -0.6223, b: 0.8018\n",
      "loss: 0.0877\n",
      "Epoch: 90/100, Batch: 86/432, W1: 0.8151, W2: 0.4703, W3: 0.0229, W4: -0.6224, b: 0.8018\n",
      "loss: 0.1173\n",
      "Epoch: 90/100, Batch: 87/432, W1: 0.8152, W2: 0.4704, W3: 0.0229, W4: -0.6224, b: 0.8018\n",
      "loss: 0.111\n",
      "Epoch: 90/100, Batch: 88/432, W1: 0.8152, W2: 0.4704, W3: 0.0229, W4: -0.6224, b: 0.8018\n",
      "loss: 0.1141\n",
      "Epoch: 90/100, Batch: 89/432, W1: 0.8152, W2: 0.4704, W3: 0.0229, W4: -0.6224, b: 0.8018\n",
      "loss: 0.0869\n",
      "Epoch: 90/100, Batch: 90/432, W1: 0.8152, W2: 0.4704, W3: 0.0229, W4: -0.6224, b: 0.8018\n",
      "loss: 0.1015\n",
      "Epoch: 90/100, Batch: 91/432, W1: 0.8148, W2: 0.4675, W3: 0.0226, W4: -0.6225, b: 0.8018\n",
      "loss: 6.1703\n",
      "Epoch: 90/100, Batch: 92/432, W1: 0.8149, W2: 0.4676, W3: 0.0226, W4: -0.6225, b: 0.8018\n",
      "loss: 0.0945\n",
      "Epoch: 90/100, Batch: 93/432, W1: 0.8149, W2: 0.4676, W3: 0.0226, W4: -0.6225, b: 0.8018\n",
      "loss: 0.1117\n",
      "Epoch: 90/100, Batch: 94/432, W1: 0.8149, W2: 0.4676, W3: 0.0226, W4: -0.6225, b: 0.8018\n",
      "loss: 0.1086\n",
      "Epoch: 90/100, Batch: 95/432, W1: 0.8149, W2: 0.4676, W3: 0.0226, W4: -0.6225, b: 0.8018\n",
      "loss: 0.1045\n",
      "Epoch: 90/100, Batch: 96/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6225, b: 0.8019\n",
      "loss: 0.1153\n",
      "Epoch: 90/100, Batch: 97/432, W1: 0.8151, W2: 0.4678, W3: 0.0227, W4: -0.6225, b: 0.8019\n",
      "loss: 0.0959\n",
      "Epoch: 90/100, Batch: 98/432, W1: 0.8151, W2: 0.4678, W3: 0.0227, W4: -0.6225, b: 0.8019\n",
      "loss: 0.1055\n",
      "Epoch: 90/100, Batch: 99/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6226, b: 0.8019\n",
      "loss: 0.1215\n",
      "Epoch: 90/100, Batch: 100/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6226, b: 0.8019\n",
      "loss: 0.0901\n",
      "Epoch: 90/100, Batch: 101/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6226, b: 0.8019\n",
      "loss: 0.1302\n",
      "Epoch: 90/100, Batch: 102/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6226, b: 0.8019\n",
      "loss: 0.0918\n",
      "Epoch: 90/100, Batch: 103/432, W1: 0.8151, W2: 0.4678, W3: 0.0227, W4: -0.6226, b: 0.8019\n",
      "loss: 0.1457\n",
      "Epoch: 90/100, Batch: 104/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6226, b: 0.8019\n",
      "loss: 0.0755\n",
      "Epoch: 90/100, Batch: 105/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6226, b: 0.8019\n",
      "loss: 0.1318\n",
      "Epoch: 90/100, Batch: 106/432, W1: 0.8151, W2: 0.4678, W3: 0.0228, W4: -0.6226, b: 0.8019\n",
      "loss: 0.1025\n",
      "Epoch: 90/100, Batch: 107/432, W1: 0.8151, W2: 0.4679, W3: 0.0228, W4: -0.6226, b: 0.802\n",
      "loss: 0.1106\n",
      "Epoch: 90/100, Batch: 108/432, W1: 0.8151, W2: 0.4678, W3: 0.0228, W4: -0.6226, b: 0.802\n",
      "loss: 0.1274\n",
      "Epoch: 90/100, Batch: 109/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6227, b: 0.802\n",
      "loss: 0.1681\n",
      "Epoch: 90/100, Batch: 110/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6227, b: 0.802\n",
      "loss: 0.1264\n",
      "Epoch: 90/100, Batch: 111/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6227, b: 0.802\n",
      "loss: 0.1212\n",
      "Epoch: 90/100, Batch: 112/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6227, b: 0.802\n",
      "loss: 0.1061\n",
      "Epoch: 90/100, Batch: 113/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6228, b: 0.802\n",
      "loss: 0.0854\n",
      "Epoch: 90/100, Batch: 114/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6228, b: 0.802\n",
      "loss: 0.114\n",
      "Epoch: 90/100, Batch: 115/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6228, b: 0.802\n",
      "loss: 0.1068\n",
      "Epoch: 90/100, Batch: 116/432, W1: 0.815, W2: 0.4678, W3: 0.0227, W4: -0.6228, b: 0.802\n",
      "loss: 0.098\n",
      "Epoch: 90/100, Batch: 117/432, W1: 0.815, W2: 0.4678, W3: 0.0227, W4: -0.6228, b: 0.802\n",
      "loss: 1.0311\n",
      "Epoch: 90/100, Batch: 118/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6228, b: 0.802\n",
      "loss: 0.1387\n",
      "Epoch: 90/100, Batch: 119/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6228, b: 0.8021\n",
      "loss: 0.0706\n",
      "Epoch: 90/100, Batch: 120/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6228, b: 0.8021\n",
      "loss: 0.1104\n",
      "Epoch: 90/100, Batch: 121/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6228, b: 0.8021\n",
      "loss: 0.0948\n",
      "Epoch: 90/100, Batch: 122/432, W1: 0.815, W2: 0.4677, W3: 0.0227, W4: -0.6228, b: 0.8021\n",
      "loss: 0.0842\n",
      "Epoch: 90/100, Batch: 123/432, W1: 0.8151, W2: 0.4678, W3: 0.0227, W4: -0.6228, b: 0.8021\n",
      "loss: 0.0977\n",
      "Epoch: 90/100, Batch: 124/432, W1: 0.8151, W2: 0.4678, W3: 0.0228, W4: -0.6228, b: 0.8021\n",
      "loss: 0.099\n",
      "Epoch: 90/100, Batch: 125/432, W1: 0.8151, W2: 0.4679, W3: 0.0228, W4: -0.6228, b: 0.8021\n",
      "loss: 0.1232\n",
      "Epoch: 90/100, Batch: 126/432, W1: 0.8151, W2: 0.4678, W3: 0.0228, W4: -0.6229, b: 0.8021\n",
      "loss: 0.1007\n",
      "Epoch: 90/100, Batch: 127/432, W1: 0.8152, W2: 0.4679, W3: 0.0228, W4: -0.6229, b: 0.8021\n",
      "loss: 0.1138\n",
      "Epoch: 90/100, Batch: 128/432, W1: 0.8152, W2: 0.4679, W3: 0.0228, W4: -0.6229, b: 0.8021\n",
      "loss: 0.102\n",
      "Epoch: 90/100, Batch: 129/432, W1: 0.8152, W2: 0.4679, W3: 0.0228, W4: -0.6229, b: 0.8021\n",
      "loss: 0.0871\n",
      "Epoch: 90/100, Batch: 130/432, W1: 0.8152, W2: 0.4679, W3: 0.0228, W4: -0.6229, b: 0.8021\n",
      "loss: 0.1051\n",
      "Epoch: 90/100, Batch: 131/432, W1: 0.8152, W2: 0.4679, W3: 0.0228, W4: -0.6229, b: 0.8022\n",
      "loss: 0.1263\n",
      "Epoch: 90/100, Batch: 132/432, W1: 0.8152, W2: 0.4679, W3: 0.0228, W4: -0.6229, b: 0.8022\n",
      "loss: 0.1115\n",
      "Epoch: 90/100, Batch: 133/432, W1: 0.8152, W2: 0.4679, W3: 0.0229, W4: -0.6229, b: 0.8022\n",
      "loss: 0.1397\n",
      "Epoch: 90/100, Batch: 134/432, W1: 0.8152, W2: 0.4679, W3: 0.0229, W4: -0.6229, b: 0.8022\n",
      "loss: 0.0933\n",
      "Epoch: 90/100, Batch: 135/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.6229, b: 0.8022\n",
      "loss: 0.1033\n",
      "Epoch: 90/100, Batch: 136/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.6229, b: 0.8022\n",
      "loss: 0.137\n",
      "Epoch: 90/100, Batch: 137/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.623, b: 0.8022\n",
      "loss: 0.1175\n",
      "Epoch: 90/100, Batch: 138/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.623, b: 0.8022\n",
      "loss: 0.1066\n",
      "Epoch: 90/100, Batch: 139/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.623, b: 0.8022\n",
      "loss: 0.0807\n",
      "Epoch: 90/100, Batch: 140/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.623, b: 0.8022\n",
      "loss: 0.09\n",
      "Epoch: 90/100, Batch: 141/432, W1: 0.8154, W2: 0.4681, W3: 0.0229, W4: -0.623, b: 0.8023\n",
      "loss: 0.1138\n",
      "Epoch: 90/100, Batch: 142/432, W1: 0.8153, W2: 0.4681, W3: 0.0229, W4: -0.623, b: 0.8023\n",
      "loss: 0.1555\n",
      "Epoch: 90/100, Batch: 143/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.623, b: 0.8023\n",
      "loss: 0.1309\n",
      "Epoch: 90/100, Batch: 144/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.623, b: 0.8023\n",
      "loss: 0.1024\n",
      "Epoch: 90/100, Batch: 145/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.6231, b: 0.8023\n",
      "loss: 0.1311\n",
      "Epoch: 90/100, Batch: 146/432, W1: 0.8153, W2: 0.4681, W3: 0.0229, W4: -0.6231, b: 0.8023\n",
      "loss: 0.1102\n",
      "Epoch: 90/100, Batch: 147/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6231, b: 0.8023\n",
      "loss: 0.1333\n",
      "Epoch: 90/100, Batch: 148/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6231, b: 0.8023\n",
      "loss: 0.0992\n",
      "Epoch: 90/100, Batch: 149/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6231, b: 0.8023\n",
      "loss: 0.1122\n",
      "Epoch: 90/100, Batch: 150/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6231, b: 0.8023\n",
      "loss: 0.098\n",
      "Epoch: 90/100, Batch: 151/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.6231, b: 0.8023\n",
      "loss: 0.1223\n",
      "Epoch: 90/100, Batch: 152/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.6232, b: 0.8023\n",
      "loss: 0.1005\n",
      "Epoch: 90/100, Batch: 153/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6232, b: 0.8024\n",
      "loss: 0.0887\n",
      "Epoch: 90/100, Batch: 154/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6232, b: 0.8024\n",
      "loss: 0.1365\n",
      "Epoch: 90/100, Batch: 155/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6232, b: 0.8024\n",
      "loss: 0.0956\n",
      "Epoch: 90/100, Batch: 156/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6232, b: 0.8024\n",
      "loss: 0.1118\n",
      "Epoch: 90/100, Batch: 157/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6232, b: 0.8024\n",
      "loss: 0.1102\n",
      "Epoch: 90/100, Batch: 158/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6232, b: 0.8024\n",
      "loss: 0.1366\n",
      "Epoch: 90/100, Batch: 159/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6232, b: 0.8024\n",
      "loss: 0.0952\n",
      "Epoch: 90/100, Batch: 160/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6232, b: 0.8024\n",
      "loss: 0.133\n",
      "Epoch: 90/100, Batch: 161/432, W1: 0.8154, W2: 0.4682, W3: 0.023, W4: -0.6232, b: 0.8024\n",
      "loss: 0.0997\n",
      "Epoch: 90/100, Batch: 162/432, W1: 0.8154, W2: 0.4682, W3: 0.023, W4: -0.6233, b: 0.8024\n",
      "loss: 0.1017\n",
      "Epoch: 90/100, Batch: 163/432, W1: 0.8155, W2: 0.4682, W3: 0.023, W4: -0.6233, b: 0.8025\n",
      "loss: 0.0882\n",
      "Epoch: 90/100, Batch: 164/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6233, b: 0.8025\n",
      "loss: 0.1156\n",
      "Epoch: 90/100, Batch: 165/432, W1: 0.8154, W2: 0.4682, W3: 0.023, W4: -0.6233, b: 0.8025\n",
      "loss: 0.1221\n",
      "Epoch: 90/100, Batch: 166/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6233, b: 0.8025\n",
      "loss: 0.0854\n",
      "Epoch: 90/100, Batch: 167/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6233, b: 0.8025\n",
      "loss: 0.0984\n",
      "Epoch: 90/100, Batch: 168/432, W1: 0.8154, W2: 0.4682, W3: 0.023, W4: -0.6233, b: 0.8025\n",
      "loss: 0.1132\n",
      "Epoch: 90/100, Batch: 169/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6234, b: 0.8025\n",
      "loss: 0.129\n",
      "Epoch: 90/100, Batch: 170/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6234, b: 0.8025\n",
      "loss: 0.0696\n",
      "Epoch: 90/100, Batch: 171/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6234, b: 0.8025\n",
      "loss: 0.0862\n",
      "Epoch: 90/100, Batch: 172/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6234, b: 0.8025\n",
      "loss: 0.0713\n",
      "Epoch: 90/100, Batch: 173/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6234, b: 0.8025\n",
      "loss: 0.1189\n",
      "Epoch: 90/100, Batch: 174/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6234, b: 0.8025\n",
      "loss: 0.0961\n",
      "Epoch: 90/100, Batch: 175/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6234, b: 0.8025\n",
      "loss: 0.1152\n",
      "Epoch: 90/100, Batch: 176/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6234, b: 0.8025\n",
      "loss: 0.1129\n",
      "Epoch: 90/100, Batch: 177/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6235, b: 0.8025\n",
      "loss: 0.1125\n",
      "Epoch: 90/100, Batch: 178/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6235, b: 0.8025\n",
      "loss: 0.0958\n",
      "Epoch: 90/100, Batch: 179/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6235, b: 0.8026\n",
      "loss: 0.0828\n",
      "Epoch: 90/100, Batch: 180/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6235, b: 0.8026\n",
      "loss: 0.1323\n",
      "Epoch: 90/100, Batch: 181/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.6235, b: 0.8026\n",
      "loss: 0.108\n",
      "Epoch: 90/100, Batch: 182/432, W1: 0.8153, W2: 0.468, W3: 0.0229, W4: -0.6235, b: 0.8026\n",
      "loss: 0.0994\n",
      "Epoch: 90/100, Batch: 183/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6235, b: 0.8026\n",
      "loss: 0.1019\n",
      "Epoch: 90/100, Batch: 184/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6236, b: 0.8026\n",
      "loss: 0.0883\n",
      "Epoch: 90/100, Batch: 185/432, W1: 0.8153, W2: 0.468, W3: 0.023, W4: -0.6236, b: 0.8026\n",
      "loss: 0.0755\n",
      "Epoch: 90/100, Batch: 186/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6236, b: 0.8026\n",
      "loss: 0.0833\n",
      "Epoch: 90/100, Batch: 187/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6236, b: 0.8026\n",
      "loss: 0.1476\n",
      "Epoch: 90/100, Batch: 188/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6236, b: 0.8026\n",
      "loss: 0.1313\n",
      "Epoch: 90/100, Batch: 189/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6236, b: 0.8026\n",
      "loss: 0.1116\n",
      "Epoch: 90/100, Batch: 190/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6236, b: 0.8026\n",
      "loss: 0.1179\n",
      "Epoch: 90/100, Batch: 191/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6236, b: 0.8026\n",
      "loss: 0.116\n",
      "Epoch: 90/100, Batch: 192/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6237, b: 0.8026\n",
      "loss: 0.1264\n",
      "Epoch: 90/100, Batch: 193/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6237, b: 0.8026\n",
      "loss: 0.1559\n",
      "Epoch: 90/100, Batch: 194/432, W1: 0.8152, W2: 0.4679, W3: 0.0229, W4: -0.6237, b: 0.8026\n",
      "loss: 0.099\n",
      "Epoch: 90/100, Batch: 195/432, W1: 0.8151, W2: 0.4679, W3: 0.0229, W4: -0.6237, b: 0.8026\n",
      "loss: 0.0892\n",
      "Epoch: 90/100, Batch: 196/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6237, b: 0.8027\n",
      "loss: 0.1015\n",
      "Epoch: 90/100, Batch: 197/432, W1: 0.8152, W2: 0.468, W3: 0.0229, W4: -0.6237, b: 0.8027\n",
      "loss: 0.1062\n",
      "Epoch: 90/100, Batch: 198/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6237, b: 0.8027\n",
      "loss: 0.1044\n",
      "Epoch: 90/100, Batch: 199/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6237, b: 0.8027\n",
      "loss: 0.1253\n",
      "Epoch: 90/100, Batch: 200/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6237, b: 0.8027\n",
      "loss: 0.088\n",
      "Epoch: 90/100, Batch: 201/432, W1: 0.8153, W2: 0.468, W3: 0.023, W4: -0.6238, b: 0.8027\n",
      "loss: 0.095\n",
      "Epoch: 90/100, Batch: 202/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6238, b: 0.8027\n",
      "loss: 0.102\n",
      "Epoch: 90/100, Batch: 203/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6238, b: 0.8027\n",
      "loss: 0.1303\n",
      "Epoch: 90/100, Batch: 204/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6238, b: 0.8027\n",
      "loss: 0.0862\n",
      "Epoch: 90/100, Batch: 205/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6238, b: 0.8027\n",
      "loss: 0.0949\n",
      "Epoch: 90/100, Batch: 206/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6238, b: 0.8027\n",
      "loss: 0.1135\n",
      "Epoch: 90/100, Batch: 207/432, W1: 0.8154, W2: 0.4682, W3: 0.0231, W4: -0.6238, b: 0.8028\n",
      "loss: 0.0988\n",
      "Epoch: 90/100, Batch: 208/432, W1: 0.8154, W2: 0.4682, W3: 0.023, W4: -0.6238, b: 0.8028\n",
      "loss: 0.1229\n",
      "Epoch: 90/100, Batch: 209/432, W1: 0.8154, W2: 0.4681, W3: 0.023, W4: -0.6238, b: 0.8028\n",
      "loss: 0.092\n",
      "Epoch: 90/100, Batch: 210/432, W1: 0.8154, W2: 0.4682, W3: 0.0231, W4: -0.6239, b: 0.8028\n",
      "loss: 0.1053\n",
      "Epoch: 90/100, Batch: 211/432, W1: 0.8154, W2: 0.4682, W3: 0.0231, W4: -0.6239, b: 0.8028\n",
      "loss: 0.1244\n",
      "Epoch: 90/100, Batch: 212/432, W1: 0.8154, W2: 0.4682, W3: 0.0231, W4: -0.6239, b: 0.8028\n",
      "loss: 0.1088\n",
      "Epoch: 90/100, Batch: 213/432, W1: 0.8155, W2: 0.4682, W3: 0.0231, W4: -0.6239, b: 0.8028\n",
      "loss: 0.0767\n",
      "Epoch: 90/100, Batch: 214/432, W1: 0.8155, W2: 0.4682, W3: 0.0231, W4: -0.6239, b: 0.8028\n",
      "loss: 0.0812\n",
      "Epoch: 90/100, Batch: 215/432, W1: 0.8155, W2: 0.4682, W3: 0.0231, W4: -0.6239, b: 0.8028\n",
      "loss: 0.1386\n",
      "Epoch: 90/100, Batch: 216/432, W1: 0.8154, W2: 0.4682, W3: 0.0231, W4: -0.6239, b: 0.8028\n",
      "loss: 0.1073\n",
      "Epoch: 90/100, Batch: 217/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.624, b: 0.8028\n",
      "loss: 0.1341\n",
      "Epoch: 90/100, Batch: 218/432, W1: 0.8152, W2: 0.468, W3: 0.023, W4: -0.624, b: 0.8028\n",
      "loss: 0.1307\n",
      "Epoch: 90/100, Batch: 219/432, W1: 0.8151, W2: 0.4679, W3: 0.0229, W4: -0.624, b: 0.8028\n",
      "loss: 0.0945\n",
      "Epoch: 90/100, Batch: 220/432, W1: 0.8151, W2: 0.4679, W3: 0.0229, W4: -0.624, b: 0.8028\n",
      "loss: 0.1151\n",
      "Epoch: 90/100, Batch: 221/432, W1: 0.8151, W2: 0.4679, W3: 0.0229, W4: -0.624, b: 0.8028\n",
      "loss: 0.1054\n",
      "Epoch: 90/100, Batch: 222/432, W1: 0.8151, W2: 0.4679, W3: 0.0229, W4: -0.6241, b: 0.8028\n",
      "loss: 0.1172\n",
      "Epoch: 90/100, Batch: 223/432, W1: 0.8151, W2: 0.4679, W3: 0.0229, W4: -0.6241, b: 0.8028\n",
      "loss: 0.0887\n",
      "Epoch: 90/100, Batch: 224/432, W1: 0.8152, W2: 0.468, W3: 0.023, W4: -0.6241, b: 0.8029\n",
      "loss: 0.077\n",
      "Epoch: 90/100, Batch: 225/432, W1: 0.8153, W2: 0.468, W3: 0.023, W4: -0.6241, b: 0.8029\n",
      "loss: 0.1467\n",
      "Epoch: 90/100, Batch: 226/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6241, b: 0.8029\n",
      "loss: 0.1296\n",
      "Epoch: 90/100, Batch: 227/432, W1: 0.8153, W2: 0.4681, W3: 0.023, W4: -0.6241, b: 0.8029\n",
      "loss: 0.0931\n",
      "Epoch: 90/100, Batch: 228/432, W1: 0.8154, W2: 0.4682, W3: 0.0231, W4: -0.6241, b: 0.8029\n",
      "loss: 0.0931\n",
      "Epoch: 90/100, Batch: 229/432, W1: 0.8155, W2: 0.4682, W3: 0.0231, W4: -0.6241, b: 0.8029\n",
      "loss: 0.1099\n",
      "Epoch: 90/100, Batch: 230/432, W1: 0.8154, W2: 0.4682, W3: 0.0231, W4: -0.6241, b: 0.8029\n",
      "loss: 0.1369\n",
      "Epoch: 90/100, Batch: 231/432, W1: 0.8154, W2: 0.4682, W3: 0.0231, W4: -0.6241, b: 0.8029\n",
      "loss: 0.1378\n",
      "Epoch: 90/100, Batch: 232/432, W1: 0.8154, W2: 0.4682, W3: 0.0231, W4: -0.6241, b: 0.803\n",
      "loss: 0.0967\n",
      "Epoch: 90/100, Batch: 233/432, W1: 0.8155, W2: 0.4682, W3: 0.0231, W4: -0.6241, b: 0.803\n",
      "loss: 0.0954\n",
      "Epoch: 90/100, Batch: 234/432, W1: 0.8155, W2: 0.4683, W3: 0.0231, W4: -0.6241, b: 0.803\n",
      "loss: 0.1074\n",
      "Epoch: 90/100, Batch: 235/432, W1: 0.8155, W2: 0.4683, W3: 0.0231, W4: -0.6242, b: 0.803\n",
      "loss: 0.1383\n",
      "Epoch: 90/100, Batch: 236/432, W1: 0.8155, W2: 0.4683, W3: 0.0232, W4: -0.6242, b: 0.803\n",
      "loss: 0.075\n",
      "Epoch: 90/100, Batch: 237/432, W1: 0.8155, W2: 0.4683, W3: 0.0232, W4: -0.6242, b: 0.803\n",
      "loss: 0.0899\n",
      "Epoch: 90/100, Batch: 238/432, W1: 0.8156, W2: 0.4683, W3: 0.0232, W4: -0.6242, b: 0.803\n",
      "loss: 0.1107\n",
      "Epoch: 90/100, Batch: 239/432, W1: 0.8155, W2: 0.4683, W3: 0.0232, W4: -0.6242, b: 0.803\n",
      "loss: 0.0932\n",
      "Epoch: 90/100, Batch: 240/432, W1: 0.8155, W2: 0.4683, W3: 0.0232, W4: -0.6242, b: 0.803\n",
      "loss: 0.141\n",
      "Epoch: 90/100, Batch: 241/432, W1: 0.8156, W2: 0.4683, W3: 0.0232, W4: -0.6242, b: 0.803\n",
      "loss: 0.1096\n",
      "Epoch: 90/100, Batch: 242/432, W1: 0.8156, W2: 0.4684, W3: 0.0232, W4: -0.6242, b: 0.8031\n",
      "loss: 0.097\n",
      "Epoch: 90/100, Batch: 243/432, W1: 0.8156, W2: 0.4684, W3: 0.0232, W4: -0.6242, b: 0.8031\n",
      "loss: 0.1092\n",
      "Epoch: 90/100, Batch: 244/432, W1: 0.8155, W2: 0.4683, W3: 0.0232, W4: -0.6242, b: 0.8031\n",
      "loss: 1.1705\n",
      "Epoch: 90/100, Batch: 245/432, W1: 0.8156, W2: 0.4684, W3: 0.0232, W4: -0.6242, b: 0.8031\n",
      "loss: 0.0977\n",
      "Epoch: 90/100, Batch: 246/432, W1: 0.8156, W2: 0.4684, W3: 0.0232, W4: -0.6242, b: 0.8031\n",
      "loss: 0.1242\n",
      "Epoch: 90/100, Batch: 247/432, W1: 0.8156, W2: 0.4684, W3: 0.0232, W4: -0.6242, b: 0.8031\n",
      "loss: 0.1046\n",
      "Epoch: 90/100, Batch: 248/432, W1: 0.8156, W2: 0.4684, W3: 0.0232, W4: -0.6242, b: 0.8031\n",
      "loss: 0.0876\n",
      "Epoch: 90/100, Batch: 249/432, W1: 0.8156, W2: 0.4684, W3: 0.0232, W4: -0.6242, b: 0.8031\n",
      "loss: 0.1004\n",
      "Epoch: 90/100, Batch: 250/432, W1: 0.8157, W2: 0.4684, W3: 0.0233, W4: -0.6242, b: 0.8031\n",
      "loss: 0.1072\n",
      "Epoch: 90/100, Batch: 251/432, W1: 0.8156, W2: 0.4684, W3: 0.0232, W4: -0.6243, b: 0.8031\n",
      "loss: 0.104\n",
      "Epoch: 90/100, Batch: 252/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6243, b: 0.8032\n",
      "loss: 0.0663\n",
      "Epoch: 90/100, Batch: 253/432, W1: 0.8157, W2: 0.4684, W3: 0.0233, W4: -0.6243, b: 0.8032\n",
      "loss: 0.0807\n",
      "Epoch: 90/100, Batch: 254/432, W1: 0.8157, W2: 0.4684, W3: 0.0233, W4: -0.6243, b: 0.8032\n",
      "loss: 0.0985\n",
      "Epoch: 90/100, Batch: 255/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6243, b: 0.8032\n",
      "loss: 0.1302\n",
      "Epoch: 90/100, Batch: 256/432, W1: 0.8157, W2: 0.4684, W3: 0.0233, W4: -0.6243, b: 0.8032\n",
      "loss: 0.0974\n",
      "Epoch: 90/100, Batch: 257/432, W1: 0.8156, W2: 0.4684, W3: 0.0233, W4: -0.6243, b: 0.8032\n",
      "loss: 0.0986\n",
      "Epoch: 90/100, Batch: 258/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6243, b: 0.8032\n",
      "loss: 0.1135\n",
      "Epoch: 90/100, Batch: 259/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6243, b: 0.8032\n",
      "loss: 0.081\n",
      "Epoch: 90/100, Batch: 260/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6243, b: 0.8032\n",
      "loss: 0.1181\n",
      "Epoch: 90/100, Batch: 261/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6243, b: 0.8032\n",
      "loss: 0.087\n",
      "Epoch: 90/100, Batch: 262/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6244, b: 0.8032\n",
      "loss: 0.1034\n",
      "Epoch: 90/100, Batch: 263/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6244, b: 0.8033\n",
      "loss: 0.0987\n",
      "Epoch: 90/100, Batch: 264/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6244, b: 0.8033\n",
      "loss: 0.1175\n",
      "Epoch: 90/100, Batch: 265/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6244, b: 0.8033\n",
      "loss: 0.107\n",
      "Epoch: 90/100, Batch: 266/432, W1: 0.8159, W2: 0.4687, W3: 0.0234, W4: -0.6244, b: 0.8033\n",
      "loss: 0.1298\n",
      "Epoch: 90/100, Batch: 267/432, W1: 0.8159, W2: 0.4687, W3: 0.0234, W4: -0.6244, b: 0.8033\n",
      "loss: 0.1052\n",
      "Epoch: 90/100, Batch: 268/432, W1: 0.8159, W2: 0.4687, W3: 0.0234, W4: -0.6244, b: 0.8033\n",
      "loss: 0.1033\n",
      "Epoch: 90/100, Batch: 269/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6244, b: 0.8033\n",
      "loss: 0.0934\n",
      "Epoch: 90/100, Batch: 270/432, W1: 0.8159, W2: 0.4687, W3: 0.0234, W4: -0.6244, b: 0.8033\n",
      "loss: 0.0877\n",
      "Epoch: 90/100, Batch: 271/432, W1: 0.8159, W2: 0.4687, W3: 0.0234, W4: -0.6245, b: 0.8033\n",
      "loss: 0.1077\n",
      "Epoch: 90/100, Batch: 272/432, W1: 0.8159, W2: 0.4687, W3: 0.0234, W4: -0.6245, b: 0.8033\n",
      "loss: 0.0987\n",
      "Epoch: 90/100, Batch: 273/432, W1: 0.8159, W2: 0.4687, W3: 0.0235, W4: -0.6245, b: 0.8034\n",
      "loss: 0.0886\n",
      "Epoch: 90/100, Batch: 274/432, W1: 0.816, W2: 0.4688, W3: 0.0235, W4: -0.6245, b: 0.8034\n",
      "loss: 0.0956\n",
      "Epoch: 90/100, Batch: 275/432, W1: 0.8159, W2: 0.4687, W3: 0.0235, W4: -0.6245, b: 0.8034\n",
      "loss: 0.0866\n",
      "Epoch: 90/100, Batch: 276/432, W1: 0.8159, W2: 0.4687, W3: 0.0234, W4: -0.6245, b: 0.8034\n",
      "loss: 0.1005\n",
      "Epoch: 90/100, Batch: 277/432, W1: 0.8159, W2: 0.4687, W3: 0.0234, W4: -0.6245, b: 0.8034\n",
      "loss: 0.1115\n",
      "Epoch: 90/100, Batch: 278/432, W1: 0.8159, W2: 0.4687, W3: 0.0234, W4: -0.6245, b: 0.8034\n",
      "loss: 0.1065\n",
      "Epoch: 90/100, Batch: 279/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6246, b: 0.8034\n",
      "loss: 0.0939\n",
      "Epoch: 90/100, Batch: 280/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6246, b: 0.8034\n",
      "loss: 0.0939\n",
      "Epoch: 90/100, Batch: 281/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6246, b: 0.8034\n",
      "loss: 0.0988\n",
      "Epoch: 90/100, Batch: 282/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6246, b: 0.8034\n",
      "loss: 0.1183\n",
      "Epoch: 90/100, Batch: 283/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6246, b: 0.8034\n",
      "loss: 0.1068\n",
      "Epoch: 90/100, Batch: 284/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6246, b: 0.8034\n",
      "loss: 0.1184\n",
      "Epoch: 90/100, Batch: 285/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6247, b: 0.8034\n",
      "loss: 0.1207\n",
      "Epoch: 90/100, Batch: 286/432, W1: 0.8156, W2: 0.4684, W3: 0.0233, W4: -0.6247, b: 0.8034\n",
      "loss: 0.1189\n",
      "Epoch: 90/100, Batch: 287/432, W1: 0.8156, W2: 0.4684, W3: 0.0233, W4: -0.6247, b: 0.8034\n",
      "loss: 0.1081\n",
      "Epoch: 90/100, Batch: 288/432, W1: 0.8156, W2: 0.4684, W3: 0.0233, W4: -0.6247, b: 0.8034\n",
      "loss: 0.0831\n",
      "Epoch: 90/100, Batch: 289/432, W1: 0.8155, W2: 0.4683, W3: 0.0232, W4: -0.6247, b: 0.8034\n",
      "loss: 0.0976\n",
      "Epoch: 90/100, Batch: 290/432, W1: 0.8156, W2: 0.4684, W3: 0.0233, W4: -0.6247, b: 0.8034\n",
      "loss: 0.1004\n",
      "Epoch: 90/100, Batch: 291/432, W1: 0.8155, W2: 0.4683, W3: 0.0232, W4: -0.6247, b: 0.8034\n",
      "loss: 0.1117\n",
      "Epoch: 90/100, Batch: 292/432, W1: 0.8156, W2: 0.4684, W3: 0.0233, W4: -0.6248, b: 0.8034\n",
      "loss: 0.1194\n",
      "Epoch: 90/100, Batch: 293/432, W1: 0.8155, W2: 0.4683, W3: 0.0232, W4: -0.6248, b: 0.8034\n",
      "loss: 0.1507\n",
      "Epoch: 90/100, Batch: 294/432, W1: 0.8155, W2: 0.4683, W3: 0.0232, W4: -0.6248, b: 0.8034\n",
      "loss: 0.1282\n",
      "Epoch: 90/100, Batch: 295/432, W1: 0.8156, W2: 0.4684, W3: 0.0233, W4: -0.6248, b: 0.8035\n",
      "loss: 0.0843\n",
      "Epoch: 90/100, Batch: 296/432, W1: 0.8156, W2: 0.4684, W3: 0.0233, W4: -0.6248, b: 0.8035\n",
      "loss: 0.0856\n",
      "Epoch: 90/100, Batch: 297/432, W1: 0.8156, W2: 0.4685, W3: 0.0233, W4: -0.6248, b: 0.8035\n",
      "loss: 0.1024\n",
      "Epoch: 90/100, Batch: 298/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6248, b: 0.8035\n",
      "loss: 0.1009\n",
      "Epoch: 90/100, Batch: 299/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6248, b: 0.8035\n",
      "loss: 0.0761\n",
      "Epoch: 90/100, Batch: 300/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.6248, b: 0.8035\n",
      "loss: 0.0645\n",
      "Epoch: 90/100, Batch: 301/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6248, b: 0.8035\n",
      "loss: 0.0841\n",
      "Epoch: 90/100, Batch: 302/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6248, b: 0.8035\n",
      "loss: 0.0853\n",
      "Epoch: 90/100, Batch: 303/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6248, b: 0.8035\n",
      "loss: 0.0989\n",
      "Epoch: 90/100, Batch: 304/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6248, b: 0.8035\n",
      "loss: 0.0961\n",
      "Epoch: 90/100, Batch: 305/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6249, b: 0.8035\n",
      "loss: 0.1217\n",
      "Epoch: 90/100, Batch: 306/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6249, b: 0.8036\n",
      "loss: 0.117\n",
      "Epoch: 90/100, Batch: 307/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6249, b: 0.8036\n",
      "loss: 0.1036\n",
      "Epoch: 90/100, Batch: 308/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6249, b: 0.8036\n",
      "loss: 0.0903\n",
      "Epoch: 90/100, Batch: 309/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6249, b: 0.8036\n",
      "loss: 0.1099\n",
      "Epoch: 90/100, Batch: 310/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.6249, b: 0.8036\n",
      "loss: 0.1404\n",
      "Epoch: 90/100, Batch: 311/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6249, b: 0.8036\n",
      "loss: 0.0996\n",
      "Epoch: 90/100, Batch: 312/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.625, b: 0.8036\n",
      "loss: 0.107\n",
      "Epoch: 90/100, Batch: 313/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.625, b: 0.8036\n",
      "loss: 0.0904\n",
      "Epoch: 90/100, Batch: 314/432, W1: 0.8157, W2: 0.4685, W3: 0.0234, W4: -0.625, b: 0.8036\n",
      "loss: 0.0743\n",
      "Epoch: 90/100, Batch: 315/432, W1: 0.8157, W2: 0.4685, W3: 0.0234, W4: -0.625, b: 0.8036\n",
      "loss: 0.105\n",
      "Epoch: 90/100, Batch: 316/432, W1: 0.8157, W2: 0.4685, W3: 0.0234, W4: -0.625, b: 0.8036\n",
      "loss: 0.0944\n",
      "Epoch: 90/100, Batch: 317/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.625, b: 0.8036\n",
      "loss: 0.0913\n",
      "Epoch: 90/100, Batch: 318/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.625, b: 0.8036\n",
      "loss: 0.0866\n",
      "Epoch: 90/100, Batch: 319/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.625, b: 0.8036\n",
      "loss: 0.1016\n",
      "Epoch: 90/100, Batch: 320/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.625, b: 0.8036\n",
      "loss: 0.1017\n",
      "Epoch: 90/100, Batch: 321/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6251, b: 0.8036\n",
      "loss: 0.1065\n",
      "Epoch: 90/100, Batch: 322/432, W1: 0.8156, W2: 0.4685, W3: 0.0233, W4: -0.6251, b: 0.8036\n",
      "loss: 0.1088\n",
      "Epoch: 90/100, Batch: 323/432, W1: 0.8156, W2: 0.4685, W3: 0.0233, W4: -0.6251, b: 0.8037\n",
      "loss: 0.1172\n",
      "Epoch: 90/100, Batch: 324/432, W1: 0.8156, W2: 0.4685, W3: 0.0233, W4: -0.6251, b: 0.8037\n",
      "loss: 0.0995\n",
      "Epoch: 90/100, Batch: 325/432, W1: 0.8157, W2: 0.4685, W3: 0.0234, W4: -0.6251, b: 0.8037\n",
      "loss: 0.0993\n",
      "Epoch: 90/100, Batch: 326/432, W1: 0.8157, W2: 0.4685, W3: 0.0233, W4: -0.6251, b: 0.8037\n",
      "loss: 0.0991\n",
      "Epoch: 90/100, Batch: 327/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.6251, b: 0.8037\n",
      "loss: 0.1033\n",
      "Epoch: 90/100, Batch: 328/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6251, b: 0.8037\n",
      "loss: 0.1314\n",
      "Epoch: 90/100, Batch: 329/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.6251, b: 0.8037\n",
      "loss: 0.1018\n",
      "Epoch: 90/100, Batch: 330/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6251, b: 0.8037\n",
      "loss: 0.0926\n",
      "Epoch: 90/100, Batch: 331/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6251, b: 0.8037\n",
      "loss: 0.0983\n",
      "Epoch: 90/100, Batch: 332/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6251, b: 0.8037\n",
      "loss: 0.0939\n",
      "Epoch: 90/100, Batch: 333/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6252, b: 0.8038\n",
      "loss: 0.1173\n",
      "Epoch: 90/100, Batch: 334/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6252, b: 0.8038\n",
      "loss: 0.1016\n",
      "Epoch: 90/100, Batch: 335/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6252, b: 0.8038\n",
      "loss: 0.0938\n",
      "Epoch: 90/100, Batch: 336/432, W1: 0.8158, W2: 0.4686, W3: 0.0234, W4: -0.6252, b: 0.8038\n",
      "loss: 0.108\n",
      "Epoch: 90/100, Batch: 337/432, W1: 0.8158, W2: 0.4687, W3: 0.0234, W4: -0.6252, b: 0.8038\n",
      "loss: 0.0869\n",
      "Epoch: 90/100, Batch: 338/432, W1: 0.8159, W2: 0.4687, W3: 0.0235, W4: -0.6252, b: 0.8038\n",
      "loss: 0.1008\n",
      "Epoch: 90/100, Batch: 339/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6252, b: 0.8038\n",
      "loss: 0.103\n",
      "Epoch: 90/100, Batch: 340/432, W1: 0.8159, W2: 0.4687, W3: 0.0235, W4: -0.6252, b: 0.8038\n",
      "loss: 0.1039\n",
      "Epoch: 90/100, Batch: 341/432, W1: 0.8159, W2: 0.4687, W3: 0.0235, W4: -0.6253, b: 0.8038\n",
      "loss: 0.159\n",
      "Epoch: 90/100, Batch: 342/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6253, b: 0.8038\n",
      "loss: 0.0901\n",
      "Epoch: 90/100, Batch: 343/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6253, b: 0.8038\n",
      "loss: 0.1211\n",
      "Epoch: 90/100, Batch: 344/432, W1: 0.816, W2: 0.4688, W3: 0.0235, W4: -0.6253, b: 0.8039\n",
      "loss: 0.0911\n",
      "Epoch: 90/100, Batch: 345/432, W1: 0.816, W2: 0.4689, W3: 0.0236, W4: -0.6253, b: 0.8039\n",
      "loss: 0.0962\n",
      "Epoch: 90/100, Batch: 346/432, W1: 0.816, W2: 0.4688, W3: 0.0236, W4: -0.6253, b: 0.8039\n",
      "loss: 0.1012\n",
      "Epoch: 90/100, Batch: 347/432, W1: 0.816, W2: 0.4688, W3: 0.0236, W4: -0.6253, b: 0.8039\n",
      "loss: 0.1097\n",
      "Epoch: 90/100, Batch: 348/432, W1: 0.816, W2: 0.4688, W3: 0.0235, W4: -0.6253, b: 0.8039\n",
      "loss: 0.1482\n",
      "Epoch: 90/100, Batch: 349/432, W1: 0.816, W2: 0.4689, W3: 0.0236, W4: -0.6253, b: 0.8039\n",
      "loss: 0.1111\n",
      "Epoch: 90/100, Batch: 350/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6254, b: 0.8039\n",
      "loss: 0.1753\n",
      "Epoch: 90/100, Batch: 351/432, W1: 0.816, W2: 0.4688, W3: 0.0236, W4: -0.6254, b: 0.8039\n",
      "loss: 0.1\n",
      "Epoch: 90/100, Batch: 352/432, W1: 0.816, W2: 0.4689, W3: 0.0236, W4: -0.6254, b: 0.8039\n",
      "loss: 0.0722\n",
      "Epoch: 90/100, Batch: 353/432, W1: 0.816, W2: 0.4689, W3: 0.0236, W4: -0.6254, b: 0.8039\n",
      "loss: 0.1202\n",
      "Epoch: 90/100, Batch: 354/432, W1: 0.816, W2: 0.4689, W3: 0.0236, W4: -0.6254, b: 0.8039\n",
      "loss: 0.1307\n",
      "Epoch: 90/100, Batch: 355/432, W1: 0.8161, W2: 0.4689, W3: 0.0236, W4: -0.6254, b: 0.804\n",
      "loss: 0.0682\n",
      "Epoch: 90/100, Batch: 356/432, W1: 0.816, W2: 0.4689, W3: 0.0236, W4: -0.6254, b: 0.804\n",
      "loss: 0.0939\n",
      "Epoch: 90/100, Batch: 357/432, W1: 0.8161, W2: 0.4689, W3: 0.0236, W4: -0.6254, b: 0.804\n",
      "loss: 0.1063\n",
      "Epoch: 90/100, Batch: 358/432, W1: 0.8161, W2: 0.469, W3: 0.0237, W4: -0.6254, b: 0.804\n",
      "loss: 0.0922\n",
      "Epoch: 90/100, Batch: 359/432, W1: 0.8161, W2: 0.4689, W3: 0.0236, W4: -0.6255, b: 0.804\n",
      "loss: 0.1489\n",
      "Epoch: 90/100, Batch: 360/432, W1: 0.816, W2: 0.4689, W3: 0.0236, W4: -0.6255, b: 0.804\n",
      "loss: 0.0973\n",
      "Epoch: 90/100, Batch: 361/432, W1: 0.8161, W2: 0.4689, W3: 0.0236, W4: -0.6255, b: 0.804\n",
      "loss: 0.0973\n",
      "Epoch: 90/100, Batch: 362/432, W1: 0.816, W2: 0.4689, W3: 0.0236, W4: -0.6255, b: 0.804\n",
      "loss: 0.1141\n",
      "Epoch: 90/100, Batch: 363/432, W1: 0.8161, W2: 0.4689, W3: 0.0236, W4: -0.6255, b: 0.804\n",
      "loss: 0.0657\n",
      "Epoch: 90/100, Batch: 364/432, W1: 0.816, W2: 0.4688, W3: 0.0236, W4: -0.6255, b: 0.804\n",
      "loss: 0.1023\n",
      "Epoch: 90/100, Batch: 365/432, W1: 0.816, W2: 0.4688, W3: 0.0236, W4: -0.6256, b: 0.804\n",
      "loss: 0.102\n",
      "Epoch: 90/100, Batch: 366/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6256, b: 0.804\n",
      "loss: 0.3855\n",
      "Epoch: 90/100, Batch: 367/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6256, b: 0.804\n",
      "loss: 0.6251\n",
      "Epoch: 90/100, Batch: 368/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6256, b: 0.804\n",
      "loss: 0.0946\n",
      "Epoch: 90/100, Batch: 369/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6256, b: 0.8041\n",
      "loss: 0.1171\n",
      "Epoch: 90/100, Batch: 370/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6256, b: 0.8041\n",
      "loss: 0.0876\n",
      "Epoch: 90/100, Batch: 371/432, W1: 0.8157, W2: 0.4687, W3: 0.0234, W4: -0.6257, b: 0.804\n",
      "loss: 0.1451\n",
      "Epoch: 90/100, Batch: 372/432, W1: 0.8158, W2: 0.4687, W3: 0.0234, W4: -0.6257, b: 0.8041\n",
      "loss: 0.0893\n",
      "Epoch: 90/100, Batch: 373/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6257, b: 0.8041\n",
      "loss: 0.0867\n",
      "Epoch: 90/100, Batch: 374/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.6257, b: 0.8041\n",
      "loss: 0.1262\n",
      "Epoch: 90/100, Batch: 375/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.6257, b: 0.8041\n",
      "loss: 0.1045\n",
      "Epoch: 90/100, Batch: 376/432, W1: 0.8156, W2: 0.4686, W3: 0.0234, W4: -0.6257, b: 0.8041\n",
      "loss: 0.1094\n",
      "Epoch: 90/100, Batch: 377/432, W1: 0.8157, W2: 0.4686, W3: 0.0234, W4: -0.6257, b: 0.8041\n",
      "loss: 0.1045\n",
      "Epoch: 90/100, Batch: 378/432, W1: 0.8157, W2: 0.4687, W3: 0.0234, W4: -0.6257, b: 0.8041\n",
      "loss: 0.0986\n",
      "Epoch: 90/100, Batch: 379/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6257, b: 0.8041\n",
      "loss: 0.1149\n",
      "Epoch: 90/100, Batch: 380/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6258, b: 0.8041\n",
      "loss: 0.1044\n",
      "Epoch: 90/100, Batch: 381/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6258, b: 0.8041\n",
      "loss: 0.1113\n",
      "Epoch: 90/100, Batch: 382/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6258, b: 0.8041\n",
      "loss: 0.0941\n",
      "Epoch: 90/100, Batch: 383/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6258, b: 0.8041\n",
      "loss: 0.1377\n",
      "Epoch: 90/100, Batch: 384/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6258, b: 0.8042\n",
      "loss: 0.0841\n",
      "Epoch: 90/100, Batch: 385/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6258, b: 0.8042\n",
      "loss: 0.0926\n",
      "Epoch: 90/100, Batch: 386/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6258, b: 0.8042\n",
      "loss: 0.0945\n",
      "Epoch: 90/100, Batch: 387/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6258, b: 0.8042\n",
      "loss: 0.1217\n",
      "Epoch: 90/100, Batch: 388/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6258, b: 0.8042\n",
      "loss: 0.1065\n",
      "Epoch: 90/100, Batch: 389/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.0988\n",
      "Epoch: 90/100, Batch: 390/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.0753\n",
      "Epoch: 90/100, Batch: 391/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.0768\n",
      "Epoch: 90/100, Batch: 392/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.0803\n",
      "Epoch: 90/100, Batch: 393/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.1124\n",
      "Epoch: 90/100, Batch: 394/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.1003\n",
      "Epoch: 90/100, Batch: 395/432, W1: 0.8159, W2: 0.4688, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.1163\n",
      "Epoch: 90/100, Batch: 396/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.103\n",
      "Epoch: 90/100, Batch: 397/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.1313\n",
      "Epoch: 90/100, Batch: 398/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6259, b: 0.8042\n",
      "loss: 0.0911\n",
      "Epoch: 90/100, Batch: 399/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.626, b: 0.8043\n",
      "loss: 0.1036\n",
      "Epoch: 90/100, Batch: 400/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.626, b: 0.8043\n",
      "loss: 0.11\n",
      "Epoch: 90/100, Batch: 401/432, W1: 0.8157, W2: 0.4687, W3: 0.0235, W4: -0.626, b: 0.8043\n",
      "loss: 0.1241\n",
      "Epoch: 90/100, Batch: 402/432, W1: 0.8157, W2: 0.4687, W3: 0.0235, W4: -0.626, b: 0.8043\n",
      "loss: 0.0928\n",
      "Epoch: 90/100, Batch: 403/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.626, b: 0.8043\n",
      "loss: 0.1072\n",
      "Epoch: 90/100, Batch: 404/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.626, b: 0.8043\n",
      "loss: 0.1075\n",
      "Epoch: 90/100, Batch: 405/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.626, b: 0.8043\n",
      "loss: 0.0899\n",
      "Epoch: 90/100, Batch: 406/432, W1: 0.8158, W2: 0.4687, W3: 0.0235, W4: -0.6261, b: 0.8043\n",
      "loss: 0.1209\n",
      "Epoch: 90/100, Batch: 407/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6261, b: 0.8043\n",
      "loss: 0.0859\n",
      "Epoch: 90/100, Batch: 408/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6261, b: 0.8043\n",
      "loss: 0.0807\n",
      "Epoch: 90/100, Batch: 409/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6261, b: 0.8043\n",
      "loss: 0.1209\n",
      "Epoch: 90/100, Batch: 410/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6261, b: 0.8043\n",
      "loss: 0.1225\n",
      "Epoch: 90/100, Batch: 411/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6261, b: 0.8043\n",
      "loss: 0.1074\n",
      "Epoch: 90/100, Batch: 412/432, W1: 0.8159, W2: 0.4688, W3: 0.0236, W4: -0.6261, b: 0.8044\n",
      "loss: 0.0957\n",
      "Epoch: 90/100, Batch: 413/432, W1: 0.8159, W2: 0.4689, W3: 0.0236, W4: -0.6261, b: 0.8044\n",
      "loss: 0.118\n",
      "Epoch: 90/100, Batch: 414/432, W1: 0.8159, W2: 0.4689, W3: 0.0236, W4: -0.6261, b: 0.8044\n",
      "loss: 0.0942\n",
      "Epoch: 90/100, Batch: 415/432, W1: 0.8159, W2: 0.4688, W3: 0.0236, W4: -0.6262, b: 0.8044\n",
      "loss: 0.1178\n",
      "Epoch: 90/100, Batch: 416/432, W1: 0.8159, W2: 0.4688, W3: 0.0236, W4: -0.6262, b: 0.8044\n",
      "loss: 0.11\n",
      "Epoch: 90/100, Batch: 417/432, W1: 0.8158, W2: 0.4688, W3: 0.0235, W4: -0.6262, b: 0.8044\n",
      "loss: 0.1358\n",
      "Epoch: 90/100, Batch: 418/432, W1: 0.8158, W2: 0.4688, W3: 0.0236, W4: -0.6262, b: 0.8044\n",
      "loss: 0.0887\n",
      "Epoch: 90/100, Batch: 419/432, W1: 0.8159, W2: 0.4688, W3: 0.0236, W4: -0.6262, b: 0.8044\n",
      "loss: 0.1123\n",
      "Epoch: 90/100, Batch: 420/432, W1: 0.8159, W2: 0.4688, W3: 0.0236, W4: -0.6262, b: 0.8044\n",
      "loss: 0.0948\n",
      "Epoch: 90/100, Batch: 421/432, W1: 0.8159, W2: 0.4689, W3: 0.0236, W4: -0.6262, b: 0.8044\n",
      "loss: 0.1106\n",
      "Epoch: 90/100, Batch: 422/432, W1: 0.8159, W2: 0.4689, W3: 0.0236, W4: -0.6262, b: 0.8044\n",
      "loss: 0.1114\n",
      "Epoch: 90/100, Batch: 423/432, W1: 0.8159, W2: 0.4689, W3: 0.0236, W4: -0.6262, b: 0.8045\n",
      "loss: 0.1046\n",
      "Epoch: 90/100, Batch: 424/432, W1: 0.8159, W2: 0.4689, W3: 0.0236, W4: -0.6263, b: 0.8045\n",
      "loss: 0.083\n",
      "Epoch: 90/100, Batch: 425/432, W1: 0.8159, W2: 0.4689, W3: 0.0236, W4: -0.6263, b: 0.8045\n",
      "loss: 0.0784\n",
      "Epoch: 90/100, Batch: 426/432, W1: 0.8159, W2: 0.4689, W3: 0.0236, W4: -0.6263, b: 0.8045\n",
      "loss: 0.1131\n",
      "Epoch: 90/100, Batch: 427/432, W1: 0.8157, W2: 0.468, W3: 0.0234, W4: -0.6263, b: 0.8044\n",
      "loss: 1.552\n",
      "Epoch: 90/100, Batch: 428/432, W1: 0.8157, W2: 0.468, W3: 0.0234, W4: -0.6263, b: 0.8045\n",
      "loss: 0.0802\n",
      "Epoch: 90/100, Batch: 429/432, W1: 0.8157, W2: 0.468, W3: 0.0234, W4: -0.6264, b: 0.8045\n",
      "loss: 0.1098\n",
      "Epoch: 90/100, Batch: 430/432, W1: 0.8157, W2: 0.468, W3: 0.0234, W4: -0.6264, b: 0.8045\n",
      "loss: 0.1181\n",
      "Epoch: 90/100, Batch: 431/432, W1: 0.8158, W2: 0.4681, W3: 0.0235, W4: -0.6264, b: 0.8045\n",
      "loss: 0.1122\n",
      "Epoch: 90/100, Batch: 432/432, W1: 0.8158, W2: 0.4681, W3: 0.0235, W4: -0.6264, b: 0.8045\n",
      "loss: 0.0991\n",
      "Epoch: 100/100, Batch: 1/432, W1: 0.8249, W2: 0.4522, W3: 0.0317, W4: -0.6685, b: 0.8341\n",
      "loss: 0.0946\n",
      "Epoch: 100/100, Batch: 2/432, W1: 0.825, W2: 0.4523, W3: 0.0317, W4: -0.6685, b: 0.8341\n",
      "loss: 0.0682\n",
      "Epoch: 100/100, Batch: 3/432, W1: 0.825, W2: 0.4523, W3: 0.0317, W4: -0.6685, b: 0.8341\n",
      "loss: 0.0921\n",
      "Epoch: 100/100, Batch: 4/432, W1: 0.8251, W2: 0.4524, W3: 0.0318, W4: -0.6685, b: 0.8341\n",
      "loss: 0.0694\n",
      "Epoch: 100/100, Batch: 5/432, W1: 0.8251, W2: 0.4524, W3: 0.0318, W4: -0.6685, b: 0.8341\n",
      "loss: 0.108\n",
      "Epoch: 100/100, Batch: 6/432, W1: 0.8252, W2: 0.4525, W3: 0.0318, W4: -0.6685, b: 0.8341\n",
      "loss: 0.1005\n",
      "Epoch: 100/100, Batch: 7/432, W1: 0.8252, W2: 0.4525, W3: 0.0318, W4: -0.6685, b: 0.8341\n",
      "loss: 0.1275\n",
      "Epoch: 100/100, Batch: 8/432, W1: 0.8252, W2: 0.4525, W3: 0.0318, W4: -0.6685, b: 0.8341\n",
      "loss: 0.0594\n",
      "Epoch: 100/100, Batch: 9/432, W1: 0.8252, W2: 0.4525, W3: 0.0319, W4: -0.6685, b: 0.8342\n",
      "loss: 0.0863\n",
      "Epoch: 100/100, Batch: 10/432, W1: 0.8253, W2: 0.4526, W3: 0.0319, W4: -0.6685, b: 0.8342\n",
      "loss: 0.1275\n",
      "Epoch: 100/100, Batch: 11/432, W1: 0.8253, W2: 0.4526, W3: 0.0319, W4: -0.6685, b: 0.8342\n",
      "loss: 0.0801\n",
      "Epoch: 100/100, Batch: 12/432, W1: 0.8253, W2: 0.4526, W3: 0.0319, W4: -0.6685, b: 0.8342\n",
      "loss: 0.0806\n",
      "Epoch: 100/100, Batch: 13/432, W1: 0.8253, W2: 0.4526, W3: 0.0319, W4: -0.6685, b: 0.8342\n",
      "loss: 0.1132\n",
      "Epoch: 100/100, Batch: 14/432, W1: 0.8253, W2: 0.4526, W3: 0.0319, W4: -0.6686, b: 0.8342\n",
      "loss: 0.1155\n",
      "Epoch: 100/100, Batch: 15/432, W1: 0.8252, W2: 0.4525, W3: 0.0319, W4: -0.6686, b: 0.8342\n",
      "loss: 0.104\n",
      "Epoch: 100/100, Batch: 16/432, W1: 0.8253, W2: 0.4526, W3: 0.0319, W4: -0.6686, b: 0.8342\n",
      "loss: 0.0985\n",
      "Epoch: 100/100, Batch: 17/432, W1: 0.8253, W2: 0.4526, W3: 0.0319, W4: -0.6686, b: 0.8342\n",
      "loss: 0.11\n",
      "Epoch: 100/100, Batch: 18/432, W1: 0.8254, W2: 0.4527, W3: 0.032, W4: -0.6686, b: 0.8342\n",
      "loss: 0.0845\n",
      "Epoch: 100/100, Batch: 19/432, W1: 0.8255, W2: 0.4528, W3: 0.032, W4: -0.6686, b: 0.8343\n",
      "loss: 0.1125\n",
      "Epoch: 100/100, Batch: 20/432, W1: 0.8255, W2: 0.4528, W3: 0.032, W4: -0.6686, b: 0.8343\n",
      "loss: 0.0952\n",
      "Epoch: 100/100, Batch: 21/432, W1: 0.8255, W2: 0.4528, W3: 0.032, W4: -0.6686, b: 0.8343\n",
      "loss: 1.0419\n",
      "Epoch: 100/100, Batch: 22/432, W1: 0.8255, W2: 0.4528, W3: 0.032, W4: -0.6686, b: 0.8343\n",
      "loss: 0.0731\n",
      "Epoch: 100/100, Batch: 23/432, W1: 0.8256, W2: 0.4529, W3: 0.0321, W4: -0.6686, b: 0.8343\n",
      "loss: 0.0848\n",
      "Epoch: 100/100, Batch: 24/432, W1: 0.8252, W2: 0.4501, W3: 0.0317, W4: -0.6687, b: 0.8343\n",
      "loss: 5.7101\n",
      "Epoch: 100/100, Batch: 25/432, W1: 0.8252, W2: 0.4501, W3: 0.0317, W4: -0.6687, b: 0.8343\n",
      "loss: 0.099\n",
      "Epoch: 100/100, Batch: 26/432, W1: 0.8252, W2: 0.4501, W3: 0.0317, W4: -0.6687, b: 0.8343\n",
      "loss: 0.1168\n",
      "Epoch: 100/100, Batch: 27/432, W1: 0.8252, W2: 0.4501, W3: 0.0317, W4: -0.6687, b: 0.8343\n",
      "loss: 0.0914\n",
      "Epoch: 100/100, Batch: 28/432, W1: 0.8253, W2: 0.4502, W3: 0.0318, W4: -0.6687, b: 0.8343\n",
      "loss: 0.1351\n",
      "Epoch: 100/100, Batch: 29/432, W1: 0.8253, W2: 0.4502, W3: 0.0318, W4: -0.6687, b: 0.8343\n",
      "loss: 0.0908\n",
      "Epoch: 100/100, Batch: 30/432, W1: 0.8254, W2: 0.4503, W3: 0.0318, W4: -0.6687, b: 0.8344\n",
      "loss: 0.1224\n",
      "Epoch: 100/100, Batch: 31/432, W1: 0.8254, W2: 0.4503, W3: 0.0318, W4: -0.6687, b: 0.8344\n",
      "loss: 0.0995\n",
      "Epoch: 100/100, Batch: 32/432, W1: 0.8254, W2: 0.4503, W3: 0.0318, W4: -0.6687, b: 0.8344\n",
      "loss: 0.0778\n",
      "Epoch: 100/100, Batch: 33/432, W1: 0.8255, W2: 0.4504, W3: 0.0319, W4: -0.6687, b: 0.8344\n",
      "loss: 0.1158\n",
      "Epoch: 100/100, Batch: 34/432, W1: 0.8254, W2: 0.4503, W3: 0.0319, W4: -0.6688, b: 0.8344\n",
      "loss: 0.1019\n",
      "Epoch: 100/100, Batch: 35/432, W1: 0.8256, W2: 0.4505, W3: 0.032, W4: -0.6687, b: 0.8344\n",
      "loss: 0.1099\n",
      "Epoch: 100/100, Batch: 36/432, W1: 0.8256, W2: 0.4505, W3: 0.032, W4: -0.6688, b: 0.8344\n",
      "loss: 0.1043\n",
      "Epoch: 100/100, Batch: 37/432, W1: 0.8257, W2: 0.4506, W3: 0.032, W4: -0.6688, b: 0.8345\n",
      "loss: 0.0972\n",
      "Epoch: 100/100, Batch: 38/432, W1: 0.8258, W2: 0.4507, W3: 0.0321, W4: -0.6688, b: 0.8345\n",
      "loss: 0.1054\n",
      "Epoch: 100/100, Batch: 39/432, W1: 0.8258, W2: 0.4507, W3: 0.0321, W4: -0.6688, b: 0.8345\n",
      "loss: 0.1025\n",
      "Epoch: 100/100, Batch: 40/432, W1: 0.8258, W2: 0.4508, W3: 0.0321, W4: -0.6688, b: 0.8345\n",
      "loss: 0.0894\n",
      "Epoch: 100/100, Batch: 41/432, W1: 0.8259, W2: 0.4508, W3: 0.0321, W4: -0.6688, b: 0.8345\n",
      "loss: 0.108\n",
      "Epoch: 100/100, Batch: 42/432, W1: 0.8259, W2: 0.4508, W3: 0.0321, W4: -0.6688, b: 0.8345\n",
      "loss: 0.0865\n",
      "Epoch: 100/100, Batch: 43/432, W1: 0.8259, W2: 0.4508, W3: 0.0322, W4: -0.6688, b: 0.8345\n",
      "loss: 0.0954\n",
      "Epoch: 100/100, Batch: 44/432, W1: 0.8259, W2: 0.4508, W3: 0.0322, W4: -0.6688, b: 0.8345\n",
      "loss: 0.0821\n",
      "Epoch: 100/100, Batch: 45/432, W1: 0.8259, W2: 0.4508, W3: 0.0322, W4: -0.6688, b: 0.8345\n",
      "loss: 0.0899\n",
      "Epoch: 100/100, Batch: 46/432, W1: 0.8259, W2: 0.4509, W3: 0.0322, W4: -0.6688, b: 0.8346\n",
      "loss: 0.0649\n",
      "Epoch: 100/100, Batch: 47/432, W1: 0.826, W2: 0.4509, W3: 0.0322, W4: -0.6688, b: 0.8346\n",
      "loss: 0.0996\n",
      "Epoch: 100/100, Batch: 48/432, W1: 0.826, W2: 0.451, W3: 0.0322, W4: -0.6688, b: 0.8346\n",
      "loss: 0.0769\n",
      "Epoch: 100/100, Batch: 49/432, W1: 0.826, W2: 0.4509, W3: 0.0322, W4: -0.6688, b: 0.8346\n",
      "loss: 0.0806\n",
      "Epoch: 100/100, Batch: 50/432, W1: 0.8261, W2: 0.451, W3: 0.0323, W4: -0.6688, b: 0.8346\n",
      "loss: 0.1203\n",
      "Epoch: 100/100, Batch: 51/432, W1: 0.8261, W2: 0.451, W3: 0.0323, W4: -0.6688, b: 0.8346\n",
      "loss: 0.0907\n",
      "Epoch: 100/100, Batch: 52/432, W1: 0.8261, W2: 0.451, W3: 0.0323, W4: -0.6689, b: 0.8346\n",
      "loss: 0.0997\n",
      "Epoch: 100/100, Batch: 53/432, W1: 0.8261, W2: 0.451, W3: 0.0323, W4: -0.6689, b: 0.8346\n",
      "loss: 0.1068\n",
      "Epoch: 100/100, Batch: 54/432, W1: 0.8261, W2: 0.451, W3: 0.0323, W4: -0.6689, b: 0.8346\n",
      "loss: 0.0999\n",
      "Epoch: 100/100, Batch: 55/432, W1: 0.8262, W2: 0.4511, W3: 0.0323, W4: -0.6689, b: 0.8347\n",
      "loss: 0.111\n",
      "Epoch: 100/100, Batch: 56/432, W1: 0.8262, W2: 0.4511, W3: 0.0323, W4: -0.6689, b: 0.8347\n",
      "loss: 0.0923\n",
      "Epoch: 100/100, Batch: 57/432, W1: 0.8262, W2: 0.4511, W3: 0.0323, W4: -0.6689, b: 0.8347\n",
      "loss: 0.095\n",
      "Epoch: 100/100, Batch: 58/432, W1: 0.8262, W2: 0.4512, W3: 0.0324, W4: -0.6689, b: 0.8347\n",
      "loss: 0.0747\n",
      "Epoch: 100/100, Batch: 59/432, W1: 0.8262, W2: 0.4512, W3: 0.0324, W4: -0.6689, b: 0.8347\n",
      "loss: 0.0759\n",
      "Epoch: 100/100, Batch: 60/432, W1: 0.8262, W2: 0.4512, W3: 0.0324, W4: -0.6689, b: 0.8347\n",
      "loss: 0.0832\n",
      "Epoch: 100/100, Batch: 61/432, W1: 0.8262, W2: 0.4511, W3: 0.0324, W4: -0.6689, b: 0.8347\n",
      "loss: 0.0957\n",
      "Epoch: 100/100, Batch: 62/432, W1: 0.8263, W2: 0.4512, W3: 0.0324, W4: -0.6689, b: 0.8347\n",
      "loss: 0.0764\n",
      "Epoch: 100/100, Batch: 63/432, W1: 0.8262, W2: 0.4512, W3: 0.0324, W4: -0.669, b: 0.8347\n",
      "loss: 0.1359\n",
      "Epoch: 100/100, Batch: 64/432, W1: 0.8262, W2: 0.4511, W3: 0.0324, W4: -0.669, b: 0.8347\n",
      "loss: 0.0878\n",
      "Epoch: 100/100, Batch: 65/432, W1: 0.8262, W2: 0.4512, W3: 0.0324, W4: -0.669, b: 0.8347\n",
      "loss: 0.1192\n",
      "Epoch: 100/100, Batch: 66/432, W1: 0.8263, W2: 0.4512, W3: 0.0324, W4: -0.669, b: 0.8347\n",
      "loss: 0.0838\n",
      "Epoch: 100/100, Batch: 67/432, W1: 0.8263, W2: 0.4512, W3: 0.0324, W4: -0.669, b: 0.8348\n",
      "loss: 0.1096\n",
      "Epoch: 100/100, Batch: 68/432, W1: 0.8263, W2: 0.4512, W3: 0.0324, W4: -0.669, b: 0.8348\n",
      "loss: 0.0891\n",
      "Epoch: 100/100, Batch: 69/432, W1: 0.8263, W2: 0.4512, W3: 0.0324, W4: -0.669, b: 0.8348\n",
      "loss: 0.1059\n",
      "Epoch: 100/100, Batch: 70/432, W1: 0.8264, W2: 0.4513, W3: 0.0325, W4: -0.669, b: 0.8348\n",
      "loss: 0.0799\n",
      "Epoch: 100/100, Batch: 71/432, W1: 0.8263, W2: 0.4512, W3: 0.0324, W4: -0.669, b: 0.8348\n",
      "loss: 0.0831\n",
      "Epoch: 100/100, Batch: 72/432, W1: 0.8263, W2: 0.4512, W3: 0.0324, W4: -0.669, b: 0.8348\n",
      "loss: 0.0862\n",
      "Epoch: 100/100, Batch: 73/432, W1: 0.8263, W2: 0.4513, W3: 0.0325, W4: -0.669, b: 0.8348\n",
      "loss: 0.0931\n",
      "Epoch: 100/100, Batch: 74/432, W1: 0.8263, W2: 0.4513, W3: 0.0324, W4: -0.6691, b: 0.8348\n",
      "loss: 0.0917\n",
      "Epoch: 100/100, Batch: 75/432, W1: 0.8264, W2: 0.4513, W3: 0.0325, W4: -0.6691, b: 0.8348\n",
      "loss: 0.1206\n",
      "Epoch: 100/100, Batch: 76/432, W1: 0.8264, W2: 0.4513, W3: 0.0325, W4: -0.669, b: 0.8348\n",
      "loss: 1.2047\n",
      "Epoch: 100/100, Batch: 77/432, W1: 0.8264, W2: 0.4514, W3: 0.0325, W4: -0.669, b: 0.8349\n",
      "loss: 0.0947\n",
      "Epoch: 100/100, Batch: 78/432, W1: 0.8264, W2: 0.4514, W3: 0.0325, W4: -0.669, b: 0.8349\n",
      "loss: 0.0976\n",
      "Epoch: 100/100, Batch: 79/432, W1: 0.8265, W2: 0.4514, W3: 0.0326, W4: -0.669, b: 0.8349\n",
      "loss: 0.0874\n",
      "Epoch: 100/100, Batch: 80/432, W1: 0.8265, W2: 0.4515, W3: 0.0326, W4: -0.6691, b: 0.8349\n",
      "loss: 0.1198\n",
      "Epoch: 100/100, Batch: 81/432, W1: 0.8266, W2: 0.4515, W3: 0.0326, W4: -0.6691, b: 0.8349\n",
      "loss: 0.0875\n",
      "Epoch: 100/100, Batch: 82/432, W1: 0.8267, W2: 0.4516, W3: 0.0327, W4: -0.669, b: 0.8349\n",
      "loss: 0.0695\n",
      "Epoch: 100/100, Batch: 83/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.669, b: 0.8349\n",
      "loss: 0.0868\n",
      "Epoch: 100/100, Batch: 84/432, W1: 0.8267, W2: 0.4516, W3: 0.0327, W4: -0.6691, b: 0.8349\n",
      "loss: 0.126\n",
      "Epoch: 100/100, Batch: 85/432, W1: 0.8268, W2: 0.4517, W3: 0.0327, W4: -0.6691, b: 0.835\n",
      "loss: 0.0986\n",
      "Epoch: 100/100, Batch: 86/432, W1: 0.8268, W2: 0.4517, W3: 0.0328, W4: -0.6691, b: 0.835\n",
      "loss: 0.1017\n",
      "Epoch: 100/100, Batch: 87/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6691, b: 0.835\n",
      "loss: 0.1006\n",
      "Epoch: 100/100, Batch: 88/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6691, b: 0.835\n",
      "loss: 0.1228\n",
      "Epoch: 100/100, Batch: 89/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6691, b: 0.835\n",
      "loss: 0.0843\n",
      "Epoch: 100/100, Batch: 90/432, W1: 0.8268, W2: 0.4517, W3: 0.0327, W4: -0.6691, b: 0.835\n",
      "loss: 0.1138\n",
      "Epoch: 100/100, Batch: 91/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6691, b: 0.835\n",
      "loss: 0.1131\n",
      "Epoch: 100/100, Batch: 92/432, W1: 0.8267, W2: 0.4516, W3: 0.0327, W4: -0.6692, b: 0.835\n",
      "loss: 0.1124\n",
      "Epoch: 100/100, Batch: 93/432, W1: 0.8268, W2: 0.4517, W3: 0.0327, W4: -0.6692, b: 0.835\n",
      "loss: 0.099\n",
      "Epoch: 100/100, Batch: 94/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6692, b: 0.835\n",
      "loss: 0.0787\n",
      "Epoch: 100/100, Batch: 95/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6692, b: 0.835\n",
      "loss: 0.1032\n",
      "Epoch: 100/100, Batch: 96/432, W1: 0.8268, W2: 0.4517, W3: 0.0327, W4: -0.6692, b: 0.835\n",
      "loss: 0.1142\n",
      "Epoch: 100/100, Batch: 97/432, W1: 0.8268, W2: 0.4518, W3: 0.0328, W4: -0.6692, b: 0.835\n",
      "loss: 0.0847\n",
      "Epoch: 100/100, Batch: 98/432, W1: 0.8268, W2: 0.4518, W3: 0.0328, W4: -0.6692, b: 0.8351\n",
      "loss: 0.0989\n",
      "Epoch: 100/100, Batch: 99/432, W1: 0.8268, W2: 0.4517, W3: 0.0328, W4: -0.6692, b: 0.8351\n",
      "loss: 0.0953\n",
      "Epoch: 100/100, Batch: 100/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6692, b: 0.8351\n",
      "loss: 0.1079\n",
      "Epoch: 100/100, Batch: 101/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6692, b: 0.8351\n",
      "loss: 0.0972\n",
      "Epoch: 100/100, Batch: 102/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6692, b: 0.8351\n",
      "loss: 0.1157\n",
      "Epoch: 100/100, Batch: 103/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6692, b: 0.8351\n",
      "loss: 0.0846\n",
      "Epoch: 100/100, Batch: 104/432, W1: 0.8267, W2: 0.4517, W3: 0.0327, W4: -0.6693, b: 0.8351\n",
      "loss: 0.0917\n",
      "Epoch: 100/100, Batch: 105/432, W1: 0.8268, W2: 0.4517, W3: 0.0328, W4: -0.6693, b: 0.8351\n",
      "loss: 0.0926\n",
      "Epoch: 100/100, Batch: 106/432, W1: 0.8268, W2: 0.4518, W3: 0.0328, W4: -0.6693, b: 0.8351\n",
      "loss: 0.1134\n",
      "Epoch: 100/100, Batch: 107/432, W1: 0.8269, W2: 0.4518, W3: 0.0328, W4: -0.6693, b: 0.8351\n",
      "loss: 0.0741\n",
      "Epoch: 100/100, Batch: 108/432, W1: 0.827, W2: 0.4519, W3: 0.0329, W4: -0.6693, b: 0.8351\n",
      "loss: 0.0786\n",
      "Epoch: 100/100, Batch: 109/432, W1: 0.827, W2: 0.452, W3: 0.0329, W4: -0.6693, b: 0.8352\n",
      "loss: 0.0748\n",
      "Epoch: 100/100, Batch: 110/432, W1: 0.8271, W2: 0.452, W3: 0.0329, W4: -0.6693, b: 0.8352\n",
      "loss: 0.1104\n",
      "Epoch: 100/100, Batch: 111/432, W1: 0.8271, W2: 0.452, W3: 0.0329, W4: -0.6693, b: 0.8352\n",
      "loss: 0.0968\n",
      "Epoch: 100/100, Batch: 112/432, W1: 0.8271, W2: 0.452, W3: 0.033, W4: -0.6693, b: 0.8352\n",
      "loss: 0.0954\n",
      "Epoch: 100/100, Batch: 113/432, W1: 0.8271, W2: 0.4521, W3: 0.033, W4: -0.6693, b: 0.8352\n",
      "loss: 0.0975\n",
      "Epoch: 100/100, Batch: 114/432, W1: 0.8272, W2: 0.4521, W3: 0.033, W4: -0.6693, b: 0.8352\n",
      "loss: 0.0735\n",
      "Epoch: 100/100, Batch: 115/432, W1: 0.8271, W2: 0.4521, W3: 0.033, W4: -0.6693, b: 0.8352\n",
      "loss: 0.088\n",
      "Epoch: 100/100, Batch: 116/432, W1: 0.8272, W2: 0.4521, W3: 0.033, W4: -0.6693, b: 0.8352\n",
      "loss: 0.0839\n",
      "Epoch: 100/100, Batch: 117/432, W1: 0.8271, W2: 0.4521, W3: 0.033, W4: -0.6693, b: 0.8352\n",
      "loss: 0.0848\n",
      "Epoch: 100/100, Batch: 118/432, W1: 0.8271, W2: 0.4521, W3: 0.033, W4: -0.6693, b: 0.8352\n",
      "loss: 0.0839\n",
      "Epoch: 100/100, Batch: 119/432, W1: 0.8272, W2: 0.4521, W3: 0.033, W4: -0.6693, b: 0.8352\n",
      "loss: 0.089\n",
      "Epoch: 100/100, Batch: 120/432, W1: 0.8272, W2: 0.4521, W3: 0.033, W4: -0.6693, b: 0.8352\n",
      "loss: 0.0845\n",
      "Epoch: 100/100, Batch: 121/432, W1: 0.8272, W2: 0.4522, W3: 0.033, W4: -0.6693, b: 0.8353\n",
      "loss: 0.0814\n",
      "Epoch: 100/100, Batch: 122/432, W1: 0.8272, W2: 0.4522, W3: 0.0331, W4: -0.6694, b: 0.8353\n",
      "loss: 0.09\n",
      "Epoch: 100/100, Batch: 123/432, W1: 0.8272, W2: 0.4521, W3: 0.033, W4: -0.6694, b: 0.8353\n",
      "loss: 0.101\n",
      "Epoch: 100/100, Batch: 124/432, W1: 0.8271, W2: 0.4521, W3: 0.033, W4: -0.6694, b: 0.8353\n",
      "loss: 0.1335\n",
      "Epoch: 100/100, Batch: 125/432, W1: 0.8271, W2: 0.452, W3: 0.033, W4: -0.6694, b: 0.8353\n",
      "loss: 0.1306\n",
      "Epoch: 100/100, Batch: 126/432, W1: 0.827, W2: 0.452, W3: 0.0329, W4: -0.6694, b: 0.8353\n",
      "loss: 0.1072\n",
      "Epoch: 100/100, Batch: 127/432, W1: 0.827, W2: 0.452, W3: 0.0329, W4: -0.6695, b: 0.8353\n",
      "loss: 0.1294\n",
      "Epoch: 100/100, Batch: 128/432, W1: 0.8269, W2: 0.4512, W3: 0.0328, W4: -0.6695, b: 0.8352\n",
      "loss: 1.4778\n",
      "Epoch: 100/100, Batch: 129/432, W1: 0.827, W2: 0.4513, W3: 0.0329, W4: -0.6695, b: 0.8353\n",
      "loss: 0.1373\n",
      "Epoch: 100/100, Batch: 130/432, W1: 0.827, W2: 0.4513, W3: 0.0329, W4: -0.6695, b: 0.8353\n",
      "loss: 0.087\n",
      "Epoch: 100/100, Batch: 131/432, W1: 0.827, W2: 0.4513, W3: 0.0329, W4: -0.6695, b: 0.8353\n",
      "loss: 0.1194\n",
      "Epoch: 100/100, Batch: 132/432, W1: 0.827, W2: 0.4513, W3: 0.0329, W4: -0.6695, b: 0.8353\n",
      "loss: 0.1132\n",
      "Epoch: 100/100, Batch: 133/432, W1: 0.827, W2: 0.4514, W3: 0.0329, W4: -0.6695, b: 0.8353\n",
      "loss: 0.1122\n",
      "Epoch: 100/100, Batch: 134/432, W1: 0.827, W2: 0.4514, W3: 0.0329, W4: -0.6695, b: 0.8353\n",
      "loss: 0.0961\n",
      "Epoch: 100/100, Batch: 135/432, W1: 0.827, W2: 0.4513, W3: 0.0329, W4: -0.6695, b: 0.8353\n",
      "loss: 0.0942\n",
      "Epoch: 100/100, Batch: 136/432, W1: 0.827, W2: 0.4513, W3: 0.0329, W4: -0.6696, b: 0.8353\n",
      "loss: 0.1119\n",
      "Epoch: 100/100, Batch: 137/432, W1: 0.827, W2: 0.4514, W3: 0.0329, W4: -0.6696, b: 0.8353\n",
      "loss: 0.0746\n",
      "Epoch: 100/100, Batch: 138/432, W1: 0.8271, W2: 0.4514, W3: 0.0329, W4: -0.6696, b: 0.8353\n",
      "loss: 0.0962\n",
      "Epoch: 100/100, Batch: 139/432, W1: 0.8271, W2: 0.4514, W3: 0.0329, W4: -0.6696, b: 0.8354\n",
      "loss: 0.1008\n",
      "Epoch: 100/100, Batch: 140/432, W1: 0.8271, W2: 0.4514, W3: 0.0329, W4: -0.6696, b: 0.8354\n",
      "loss: 0.0765\n",
      "Epoch: 100/100, Batch: 141/432, W1: 0.8271, W2: 0.4514, W3: 0.0329, W4: -0.6696, b: 0.8354\n",
      "loss: 0.1011\n",
      "Epoch: 100/100, Batch: 142/432, W1: 0.8271, W2: 0.4514, W3: 0.0329, W4: -0.6696, b: 0.8354\n",
      "loss: 0.0937\n",
      "Epoch: 100/100, Batch: 143/432, W1: 0.8271, W2: 0.4514, W3: 0.033, W4: -0.6696, b: 0.8354\n",
      "loss: 0.1071\n",
      "Epoch: 100/100, Batch: 144/432, W1: 0.8272, W2: 0.4515, W3: 0.033, W4: -0.6696, b: 0.8354\n",
      "loss: 0.0803\n",
      "Epoch: 100/100, Batch: 145/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6696, b: 0.8354\n",
      "loss: 0.0971\n",
      "Epoch: 100/100, Batch: 146/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6696, b: 0.8354\n",
      "loss: 0.0894\n",
      "Epoch: 100/100, Batch: 147/432, W1: 0.8272, W2: 0.4515, W3: 0.033, W4: -0.6696, b: 0.8354\n",
      "loss: 0.0846\n",
      "Epoch: 100/100, Batch: 148/432, W1: 0.8271, W2: 0.4515, W3: 0.033, W4: -0.6697, b: 0.8354\n",
      "loss: 0.1088\n",
      "Epoch: 100/100, Batch: 149/432, W1: 0.8271, W2: 0.4515, W3: 0.033, W4: -0.6697, b: 0.8354\n",
      "loss: 0.092\n",
      "Epoch: 100/100, Batch: 150/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6697, b: 0.8354\n",
      "loss: 0.0976\n",
      "Epoch: 100/100, Batch: 151/432, W1: 0.8272, W2: 0.4515, W3: 0.033, W4: -0.6697, b: 0.8355\n",
      "loss: 0.1313\n",
      "Epoch: 100/100, Batch: 152/432, W1: 0.8272, W2: 0.4515, W3: 0.033, W4: -0.6697, b: 0.8355\n",
      "loss: 0.0929\n",
      "Epoch: 100/100, Batch: 153/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6697, b: 0.8355\n",
      "loss: 0.0922\n",
      "Epoch: 100/100, Batch: 154/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6697, b: 0.8355\n",
      "loss: 0.1012\n",
      "Epoch: 100/100, Batch: 155/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6697, b: 0.8355\n",
      "loss: 0.1064\n",
      "Epoch: 100/100, Batch: 156/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6697, b: 0.8355\n",
      "loss: 0.0645\n",
      "Epoch: 100/100, Batch: 157/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6697, b: 0.8355\n",
      "loss: 0.0852\n",
      "Epoch: 100/100, Batch: 158/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6698, b: 0.8355\n",
      "loss: 0.0882\n",
      "Epoch: 100/100, Batch: 159/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6698, b: 0.8355\n",
      "loss: 0.0765\n",
      "Epoch: 100/100, Batch: 160/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6698, b: 0.8355\n",
      "loss: 0.0955\n",
      "Epoch: 100/100, Batch: 161/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6698, b: 0.8355\n",
      "loss: 0.0801\n",
      "Epoch: 100/100, Batch: 162/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6698, b: 0.8355\n",
      "loss: 0.1128\n",
      "Epoch: 100/100, Batch: 163/432, W1: 0.8272, W2: 0.4515, W3: 0.033, W4: -0.6698, b: 0.8355\n",
      "loss: 0.0824\n",
      "Epoch: 100/100, Batch: 164/432, W1: 0.8272, W2: 0.4516, W3: 0.033, W4: -0.6698, b: 0.8355\n",
      "loss: 0.0902\n",
      "Epoch: 100/100, Batch: 165/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6698, b: 0.8355\n",
      "loss: 0.1007\n",
      "Epoch: 100/100, Batch: 166/432, W1: 0.8273, W2: 0.4516, W3: 0.0331, W4: -0.6698, b: 0.8356\n",
      "loss: 0.0739\n",
      "Epoch: 100/100, Batch: 167/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6698, b: 0.8356\n",
      "loss: 0.1108\n",
      "Epoch: 100/100, Batch: 168/432, W1: 0.8273, W2: 0.4516, W3: 0.0331, W4: -0.6698, b: 0.8356\n",
      "loss: 0.1088\n",
      "Epoch: 100/100, Batch: 169/432, W1: 0.8273, W2: 0.4516, W3: 0.0331, W4: -0.6699, b: 0.8356\n",
      "loss: 0.0802\n",
      "Epoch: 100/100, Batch: 170/432, W1: 0.8273, W2: 0.4517, W3: 0.0331, W4: -0.6699, b: 0.8356\n",
      "loss: 0.1225\n",
      "Epoch: 100/100, Batch: 171/432, W1: 0.8274, W2: 0.4517, W3: 0.0331, W4: -0.6699, b: 0.8356\n",
      "loss: 0.0798\n",
      "Epoch: 100/100, Batch: 172/432, W1: 0.8273, W2: 0.4517, W3: 0.0331, W4: -0.6699, b: 0.8356\n",
      "loss: 0.0903\n",
      "Epoch: 100/100, Batch: 173/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6699, b: 0.8356\n",
      "loss: 0.0925\n",
      "Epoch: 100/100, Batch: 174/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6699, b: 0.8356\n",
      "loss: 0.0825\n",
      "Epoch: 100/100, Batch: 175/432, W1: 0.8275, W2: 0.4519, W3: 0.0332, W4: -0.6699, b: 0.8357\n",
      "loss: 0.078\n",
      "Epoch: 100/100, Batch: 176/432, W1: 0.8275, W2: 0.4518, W3: 0.0332, W4: -0.6699, b: 0.8357\n",
      "loss: 0.0972\n",
      "Epoch: 100/100, Batch: 177/432, W1: 0.8275, W2: 0.4518, W3: 0.0332, W4: -0.6699, b: 0.8357\n",
      "loss: 0.1216\n",
      "Epoch: 100/100, Batch: 178/432, W1: 0.8275, W2: 0.4518, W3: 0.0332, W4: -0.6699, b: 0.8357\n",
      "loss: 0.0908\n",
      "Epoch: 100/100, Batch: 179/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6699, b: 0.8357\n",
      "loss: 0.1019\n",
      "Epoch: 100/100, Batch: 180/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.67, b: 0.8357\n",
      "loss: 0.0987\n",
      "Epoch: 100/100, Batch: 181/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.67, b: 0.8357\n",
      "loss: 0.1219\n",
      "Epoch: 100/100, Batch: 182/432, W1: 0.8275, W2: 0.4518, W3: 0.0332, W4: -0.67, b: 0.8357\n",
      "loss: 0.1055\n",
      "Epoch: 100/100, Batch: 183/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.67, b: 0.8357\n",
      "loss: 0.1204\n",
      "Epoch: 100/100, Batch: 184/432, W1: 0.8273, W2: 0.4517, W3: 0.0331, W4: -0.67, b: 0.8357\n",
      "loss: 0.1128\n",
      "Epoch: 100/100, Batch: 185/432, W1: 0.8273, W2: 0.4517, W3: 0.0331, W4: -0.67, b: 0.8357\n",
      "loss: 0.0805\n",
      "Epoch: 100/100, Batch: 186/432, W1: 0.8273, W2: 0.4516, W3: 0.0331, W4: -0.67, b: 0.8357\n",
      "loss: 0.0803\n",
      "Epoch: 100/100, Batch: 187/432, W1: 0.8273, W2: 0.4516, W3: 0.0331, W4: -0.6701, b: 0.8357\n",
      "loss: 0.0863\n",
      "Epoch: 100/100, Batch: 188/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6701, b: 0.8357\n",
      "loss: 0.0973\n",
      "Epoch: 100/100, Batch: 189/432, W1: 0.8272, W2: 0.4515, W3: 0.0331, W4: -0.6701, b: 0.8357\n",
      "loss: 0.0933\n",
      "Epoch: 100/100, Batch: 190/432, W1: 0.8272, W2: 0.4515, W3: 0.033, W4: -0.6701, b: 0.8357\n",
      "loss: 0.1062\n",
      "Epoch: 100/100, Batch: 191/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6701, b: 0.8357\n",
      "loss: 0.0964\n",
      "Epoch: 100/100, Batch: 192/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6701, b: 0.8357\n",
      "loss: 0.0908\n",
      "Epoch: 100/100, Batch: 193/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6701, b: 0.8357\n",
      "loss: 0.1036\n",
      "Epoch: 100/100, Batch: 194/432, W1: 0.8271, W2: 0.4515, W3: 0.033, W4: -0.6702, b: 0.8357\n",
      "loss: 0.1248\n",
      "Epoch: 100/100, Batch: 195/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6702, b: 0.8357\n",
      "loss: 0.095\n",
      "Epoch: 100/100, Batch: 196/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6702, b: 0.8358\n",
      "loss: 0.0902\n",
      "Epoch: 100/100, Batch: 197/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6702, b: 0.8358\n",
      "loss: 0.0824\n",
      "Epoch: 100/100, Batch: 198/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6702, b: 0.8358\n",
      "loss: 0.0942\n",
      "Epoch: 100/100, Batch: 199/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6702, b: 0.8358\n",
      "loss: 0.1193\n",
      "Epoch: 100/100, Batch: 200/432, W1: 0.8273, W2: 0.4516, W3: 0.0331, W4: -0.6702, b: 0.8358\n",
      "loss: 0.0909\n",
      "Epoch: 100/100, Batch: 201/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6702, b: 0.8358\n",
      "loss: 0.1076\n",
      "Epoch: 100/100, Batch: 202/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6702, b: 0.8358\n",
      "loss: 0.1211\n",
      "Epoch: 100/100, Batch: 203/432, W1: 0.8271, W2: 0.4515, W3: 0.033, W4: -0.6703, b: 0.8358\n",
      "loss: 0.1052\n",
      "Epoch: 100/100, Batch: 204/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6703, b: 0.8358\n",
      "loss: 0.0878\n",
      "Epoch: 100/100, Batch: 205/432, W1: 0.8272, W2: 0.4516, W3: 0.0331, W4: -0.6703, b: 0.8358\n",
      "loss: 0.3752\n",
      "Epoch: 100/100, Batch: 206/432, W1: 0.8272, W2: 0.4517, W3: 0.0331, W4: -0.6703, b: 0.8358\n",
      "loss: 0.0987\n",
      "Epoch: 100/100, Batch: 207/432, W1: 0.8272, W2: 0.4517, W3: 0.0331, W4: -0.6703, b: 0.8358\n",
      "loss: 0.0875\n",
      "Epoch: 100/100, Batch: 208/432, W1: 0.8273, W2: 0.4517, W3: 0.0331, W4: -0.6703, b: 0.8358\n",
      "loss: 0.0963\n",
      "Epoch: 100/100, Batch: 209/432, W1: 0.8272, W2: 0.4517, W3: 0.0331, W4: -0.6703, b: 0.8359\n",
      "loss: 0.1187\n",
      "Epoch: 100/100, Batch: 210/432, W1: 0.8272, W2: 0.4517, W3: 0.0331, W4: -0.6703, b: 0.8359\n",
      "loss: 0.0899\n",
      "Epoch: 100/100, Batch: 211/432, W1: 0.8272, W2: 0.4517, W3: 0.0331, W4: -0.6703, b: 0.8359\n",
      "loss: 0.1013\n",
      "Epoch: 100/100, Batch: 212/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6703, b: 0.8359\n",
      "loss: 0.0767\n",
      "Epoch: 100/100, Batch: 213/432, W1: 0.8273, W2: 0.4517, W3: 0.0332, W4: -0.6703, b: 0.8359\n",
      "loss: 0.0831\n",
      "Epoch: 100/100, Batch: 214/432, W1: 0.8273, W2: 0.4517, W3: 0.0331, W4: -0.6703, b: 0.8359\n",
      "loss: 0.1148\n",
      "Epoch: 100/100, Batch: 215/432, W1: 0.8273, W2: 0.4517, W3: 0.0331, W4: -0.6704, b: 0.8359\n",
      "loss: 0.0797\n",
      "Epoch: 100/100, Batch: 216/432, W1: 0.8273, W2: 0.4517, W3: 0.0331, W4: -0.6704, b: 0.8359\n",
      "loss: 0.0893\n",
      "Epoch: 100/100, Batch: 217/432, W1: 0.8273, W2: 0.4517, W3: 0.0331, W4: -0.6704, b: 0.8359\n",
      "loss: 0.0897\n",
      "Epoch: 100/100, Batch: 218/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6704, b: 0.8359\n",
      "loss: 0.0916\n",
      "Epoch: 100/100, Batch: 219/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6704, b: 0.8359\n",
      "loss: 0.1261\n",
      "Epoch: 100/100, Batch: 220/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6704, b: 0.8359\n",
      "loss: 0.0771\n",
      "Epoch: 100/100, Batch: 221/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6704, b: 0.8359\n",
      "loss: 0.0943\n",
      "Epoch: 100/100, Batch: 222/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6704, b: 0.8359\n",
      "loss: 0.1061\n",
      "Epoch: 100/100, Batch: 223/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6704, b: 0.836\n",
      "loss: 0.7936\n",
      "Epoch: 100/100, Batch: 224/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6704, b: 0.836\n",
      "loss: 0.1056\n",
      "Epoch: 100/100, Batch: 225/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6704, b: 0.836\n",
      "loss: 0.0943\n",
      "Epoch: 100/100, Batch: 226/432, W1: 0.8274, W2: 0.4519, W3: 0.0332, W4: -0.6704, b: 0.836\n",
      "loss: 0.1151\n",
      "Epoch: 100/100, Batch: 227/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6704, b: 0.836\n",
      "loss: 0.115\n",
      "Epoch: 100/100, Batch: 228/432, W1: 0.8274, W2: 0.4519, W3: 0.0332, W4: -0.6704, b: 0.836\n",
      "loss: 0.0862\n",
      "Epoch: 100/100, Batch: 229/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6704, b: 0.836\n",
      "loss: 0.0904\n",
      "Epoch: 100/100, Batch: 230/432, W1: 0.8274, W2: 0.4519, W3: 0.0332, W4: -0.6705, b: 0.836\n",
      "loss: 0.0867\n",
      "Epoch: 100/100, Batch: 231/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6705, b: 0.836\n",
      "loss: 0.1248\n",
      "Epoch: 100/100, Batch: 232/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6705, b: 0.836\n",
      "loss: 0.0905\n",
      "Epoch: 100/100, Batch: 233/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6705, b: 0.836\n",
      "loss: 0.1194\n",
      "Epoch: 100/100, Batch: 234/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6705, b: 0.836\n",
      "loss: 0.1078\n",
      "Epoch: 100/100, Batch: 235/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6705, b: 0.8361\n",
      "loss: 0.1012\n",
      "Epoch: 100/100, Batch: 236/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6705, b: 0.8361\n",
      "loss: 0.0958\n",
      "Epoch: 100/100, Batch: 237/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6705, b: 0.8361\n",
      "loss: 0.1064\n",
      "Epoch: 100/100, Batch: 238/432, W1: 0.8274, W2: 0.4518, W3: 0.0332, W4: -0.6706, b: 0.8361\n",
      "loss: 0.0809\n",
      "Epoch: 100/100, Batch: 239/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6706, b: 0.8361\n",
      "loss: 0.103\n",
      "Epoch: 100/100, Batch: 240/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6706, b: 0.8361\n",
      "loss: 0.078\n",
      "Epoch: 100/100, Batch: 241/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6706, b: 0.8361\n",
      "loss: 0.1056\n",
      "Epoch: 100/100, Batch: 242/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6706, b: 0.8361\n",
      "loss: 0.0891\n",
      "Epoch: 100/100, Batch: 243/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6706, b: 0.8361\n",
      "loss: 0.0859\n",
      "Epoch: 100/100, Batch: 244/432, W1: 0.8275, W2: 0.452, W3: 0.0333, W4: -0.6706, b: 0.8361\n",
      "loss: 0.1242\n",
      "Epoch: 100/100, Batch: 245/432, W1: 0.8276, W2: 0.452, W3: 0.0334, W4: -0.6706, b: 0.8361\n",
      "loss: 0.0998\n",
      "Epoch: 100/100, Batch: 246/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6706, b: 0.8362\n",
      "loss: 0.0938\n",
      "Epoch: 100/100, Batch: 247/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6706, b: 0.8362\n",
      "loss: 0.0777\n",
      "Epoch: 100/100, Batch: 248/432, W1: 0.8276, W2: 0.452, W3: 0.0334, W4: -0.6706, b: 0.8362\n",
      "loss: 0.092\n",
      "Epoch: 100/100, Batch: 249/432, W1: 0.8276, W2: 0.452, W3: 0.0334, W4: -0.6706, b: 0.8362\n",
      "loss: 0.0927\n",
      "Epoch: 100/100, Batch: 250/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6706, b: 0.8362\n",
      "loss: 0.0758\n",
      "Epoch: 100/100, Batch: 251/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6706, b: 0.8362\n",
      "loss: 0.0975\n",
      "Epoch: 100/100, Batch: 252/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6707, b: 0.8362\n",
      "loss: 0.1012\n",
      "Epoch: 100/100, Batch: 253/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6707, b: 0.8362\n",
      "loss: 0.1023\n",
      "Epoch: 100/100, Batch: 254/432, W1: 0.8277, W2: 0.4522, W3: 0.0335, W4: -0.6707, b: 0.8362\n",
      "loss: 0.0982\n",
      "Epoch: 100/100, Batch: 255/432, W1: 0.8277, W2: 0.4522, W3: 0.0335, W4: -0.6707, b: 0.8362\n",
      "loss: 0.109\n",
      "Epoch: 100/100, Batch: 256/432, W1: 0.8277, W2: 0.4521, W3: 0.0334, W4: -0.6707, b: 0.8362\n",
      "loss: 0.0928\n",
      "Epoch: 100/100, Batch: 257/432, W1: 0.8277, W2: 0.4522, W3: 0.0334, W4: -0.6707, b: 0.8362\n",
      "loss: 0.0882\n",
      "Epoch: 100/100, Batch: 258/432, W1: 0.8277, W2: 0.4522, W3: 0.0334, W4: -0.6707, b: 0.8363\n",
      "loss: 0.0791\n",
      "Epoch: 100/100, Batch: 259/432, W1: 0.8277, W2: 0.4522, W3: 0.0335, W4: -0.6707, b: 0.8363\n",
      "loss: 0.0909\n",
      "Epoch: 100/100, Batch: 260/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6707, b: 0.8363\n",
      "loss: 0.1053\n",
      "Epoch: 100/100, Batch: 261/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6708, b: 0.8363\n",
      "loss: 0.0921\n",
      "Epoch: 100/100, Batch: 262/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6708, b: 0.8363\n",
      "loss: 0.0866\n",
      "Epoch: 100/100, Batch: 263/432, W1: 0.8276, W2: 0.452, W3: 0.0334, W4: -0.6708, b: 0.8363\n",
      "loss: 0.1108\n",
      "Epoch: 100/100, Batch: 264/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6708, b: 0.8363\n",
      "loss: 0.0808\n",
      "Epoch: 100/100, Batch: 265/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6708, b: 0.8363\n",
      "loss: 0.1155\n",
      "Epoch: 100/100, Batch: 266/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6708, b: 0.8363\n",
      "loss: 0.0756\n",
      "Epoch: 100/100, Batch: 267/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6708, b: 0.8363\n",
      "loss: 0.1016\n",
      "Epoch: 100/100, Batch: 268/432, W1: 0.8276, W2: 0.452, W3: 0.0334, W4: -0.6708, b: 0.8363\n",
      "loss: 0.0851\n",
      "Epoch: 100/100, Batch: 269/432, W1: 0.8275, W2: 0.4519, W3: 0.0333, W4: -0.6709, b: 0.8363\n",
      "loss: 0.0951\n",
      "Epoch: 100/100, Batch: 270/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6709, b: 0.8363\n",
      "loss: 0.1025\n",
      "Epoch: 100/100, Batch: 271/432, W1: 0.8275, W2: 0.4519, W3: 0.0333, W4: -0.6709, b: 0.8363\n",
      "loss: 0.0808\n",
      "Epoch: 100/100, Batch: 272/432, W1: 0.8275, W2: 0.452, W3: 0.0333, W4: -0.6709, b: 0.8363\n",
      "loss: 0.1166\n",
      "Epoch: 100/100, Batch: 273/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6709, b: 0.8363\n",
      "loss: 0.1049\n",
      "Epoch: 100/100, Batch: 274/432, W1: 0.8273, W2: 0.4518, W3: 0.0333, W4: -0.6709, b: 0.8363\n",
      "loss: 0.1018\n",
      "Epoch: 100/100, Batch: 275/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6709, b: 0.8363\n",
      "loss: 0.0946\n",
      "Epoch: 100/100, Batch: 276/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6709, b: 0.8363\n",
      "loss: 0.0978\n",
      "Epoch: 100/100, Batch: 277/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.671, b: 0.8363\n",
      "loss: 0.0975\n",
      "Epoch: 100/100, Batch: 278/432, W1: 0.8273, W2: 0.4518, W3: 0.0333, W4: -0.671, b: 0.8363\n",
      "loss: 0.1086\n",
      "Epoch: 100/100, Batch: 279/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.671, b: 0.8363\n",
      "loss: 0.0987\n",
      "Epoch: 100/100, Batch: 280/432, W1: 0.8274, W2: 0.4518, W3: 0.0333, W4: -0.671, b: 0.8363\n",
      "loss: 0.1279\n",
      "Epoch: 100/100, Batch: 281/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.671, b: 0.8364\n",
      "loss: 0.1048\n",
      "Epoch: 100/100, Batch: 282/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.671, b: 0.8364\n",
      "loss: 0.0842\n",
      "Epoch: 100/100, Batch: 283/432, W1: 0.8274, W2: 0.4518, W3: 0.0333, W4: -0.671, b: 0.8364\n",
      "loss: 0.0866\n",
      "Epoch: 100/100, Batch: 284/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6711, b: 0.8364\n",
      "loss: 0.0948\n",
      "Epoch: 100/100, Batch: 285/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6711, b: 0.8364\n",
      "loss: 0.0903\n",
      "Epoch: 100/100, Batch: 286/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6711, b: 0.8364\n",
      "loss: 0.1097\n",
      "Epoch: 100/100, Batch: 287/432, W1: 0.8271, W2: 0.4516, W3: 0.0331, W4: -0.6711, b: 0.8364\n",
      "loss: 0.1328\n",
      "Epoch: 100/100, Batch: 288/432, W1: 0.8271, W2: 0.4516, W3: 0.0331, W4: -0.6711, b: 0.8364\n",
      "loss: 0.1036\n",
      "Epoch: 100/100, Batch: 289/432, W1: 0.8271, W2: 0.4516, W3: 0.0331, W4: -0.6711, b: 0.8364\n",
      "loss: 0.093\n",
      "Epoch: 100/100, Batch: 290/432, W1: 0.8272, W2: 0.4516, W3: 0.0332, W4: -0.6711, b: 0.8364\n",
      "loss: 0.0768\n",
      "Epoch: 100/100, Batch: 291/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6712, b: 0.8364\n",
      "loss: 0.1083\n",
      "Epoch: 100/100, Batch: 292/432, W1: 0.8271, W2: 0.4516, W3: 0.0331, W4: -0.6712, b: 0.8364\n",
      "loss: 0.1215\n",
      "Epoch: 100/100, Batch: 293/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6712, b: 0.8364\n",
      "loss: 0.0996\n",
      "Epoch: 100/100, Batch: 294/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6712, b: 0.8364\n",
      "loss: 0.0776\n",
      "Epoch: 100/100, Batch: 295/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6712, b: 0.8364\n",
      "loss: 0.105\n",
      "Epoch: 100/100, Batch: 296/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6712, b: 0.8364\n",
      "loss: 0.1061\n",
      "Epoch: 100/100, Batch: 297/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6712, b: 0.8364\n",
      "loss: 0.0903\n",
      "Epoch: 100/100, Batch: 298/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6712, b: 0.8364\n",
      "loss: 0.0915\n",
      "Epoch: 100/100, Batch: 299/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6712, b: 0.8365\n",
      "loss: 0.147\n",
      "Epoch: 100/100, Batch: 300/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6713, b: 0.8365\n",
      "loss: 0.1114\n",
      "Epoch: 100/100, Batch: 301/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6713, b: 0.8365\n",
      "loss: 0.0952\n",
      "Epoch: 100/100, Batch: 302/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6713, b: 0.8365\n",
      "loss: 0.1313\n",
      "Epoch: 100/100, Batch: 303/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6713, b: 0.8365\n",
      "loss: 0.1139\n",
      "Epoch: 100/100, Batch: 304/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6713, b: 0.8365\n",
      "loss: 0.0817\n",
      "Epoch: 100/100, Batch: 305/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6713, b: 0.8365\n",
      "loss: 0.0692\n",
      "Epoch: 100/100, Batch: 306/432, W1: 0.8272, W2: 0.4517, W3: 0.0332, W4: -0.6713, b: 0.8365\n",
      "loss: 0.1031\n",
      "Epoch: 100/100, Batch: 307/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6713, b: 0.8365\n",
      "loss: 0.1018\n",
      "Epoch: 100/100, Batch: 308/432, W1: 0.8273, W2: 0.4518, W3: 0.0332, W4: -0.6713, b: 0.8365\n",
      "loss: 0.1177\n",
      "Epoch: 100/100, Batch: 309/432, W1: 0.8273, W2: 0.4518, W3: 0.0333, W4: -0.6714, b: 0.8365\n",
      "loss: 0.1272\n",
      "Epoch: 100/100, Batch: 310/432, W1: 0.8273, W2: 0.4518, W3: 0.0333, W4: -0.6714, b: 0.8365\n",
      "loss: 0.0886\n",
      "Epoch: 100/100, Batch: 311/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6714, b: 0.8366\n",
      "loss: 0.1057\n",
      "Epoch: 100/100, Batch: 312/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6714, b: 0.8366\n",
      "loss: 0.0831\n",
      "Epoch: 100/100, Batch: 313/432, W1: 0.8273, W2: 0.4518, W3: 0.0333, W4: -0.6714, b: 0.8366\n",
      "loss: 0.0897\n",
      "Epoch: 100/100, Batch: 314/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6714, b: 0.8366\n",
      "loss: 0.1328\n",
      "Epoch: 100/100, Batch: 315/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6714, b: 0.8366\n",
      "loss: 0.0945\n",
      "Epoch: 100/100, Batch: 316/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6714, b: 0.8366\n",
      "loss: 0.0854\n",
      "Epoch: 100/100, Batch: 317/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6714, b: 0.8366\n",
      "loss: 0.0961\n",
      "Epoch: 100/100, Batch: 318/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6715, b: 0.8366\n",
      "loss: 0.0851\n",
      "Epoch: 100/100, Batch: 319/432, W1: 0.8273, W2: 0.4518, W3: 0.0333, W4: -0.6715, b: 0.8366\n",
      "loss: 0.107\n",
      "Epoch: 100/100, Batch: 320/432, W1: 0.8273, W2: 0.4518, W3: 0.0333, W4: -0.6715, b: 0.8366\n",
      "loss: 0.1029\n",
      "Epoch: 100/100, Batch: 321/432, W1: 0.8273, W2: 0.4518, W3: 0.0333, W4: -0.6715, b: 0.8366\n",
      "loss: 0.0983\n",
      "Epoch: 100/100, Batch: 322/432, W1: 0.8274, W2: 0.4519, W3: 0.0333, W4: -0.6715, b: 0.8366\n",
      "loss: 0.1165\n",
      "Epoch: 100/100, Batch: 323/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6715, b: 0.8367\n",
      "loss: 0.1188\n",
      "Epoch: 100/100, Batch: 324/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6715, b: 0.8367\n",
      "loss: 0.0923\n",
      "Epoch: 100/100, Batch: 325/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6715, b: 0.8367\n",
      "loss: 0.1045\n",
      "Epoch: 100/100, Batch: 326/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6715, b: 0.8367\n",
      "loss: 0.0887\n",
      "Epoch: 100/100, Batch: 327/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6715, b: 0.8367\n",
      "loss: 0.1025\n",
      "Epoch: 100/100, Batch: 328/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6715, b: 0.8367\n",
      "loss: 0.0988\n",
      "Epoch: 100/100, Batch: 329/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6715, b: 0.8367\n",
      "loss: 0.0864\n",
      "Epoch: 100/100, Batch: 330/432, W1: 0.8276, W2: 0.4521, W3: 0.0335, W4: -0.6715, b: 0.8367\n",
      "loss: 0.1139\n",
      "Epoch: 100/100, Batch: 331/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6716, b: 0.8367\n",
      "loss: 0.0939\n",
      "Epoch: 100/100, Batch: 332/432, W1: 0.8276, W2: 0.4521, W3: 0.0335, W4: -0.6716, b: 0.8367\n",
      "loss: 0.0775\n",
      "Epoch: 100/100, Batch: 333/432, W1: 0.8276, W2: 0.4521, W3: 0.0335, W4: -0.6716, b: 0.8368\n",
      "loss: 0.0972\n",
      "Epoch: 100/100, Batch: 334/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6716, b: 0.8368\n",
      "loss: 0.625\n",
      "Epoch: 100/100, Batch: 335/432, W1: 0.8276, W2: 0.4521, W3: 0.0335, W4: -0.6716, b: 0.8368\n",
      "loss: 0.1077\n",
      "Epoch: 100/100, Batch: 336/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6716, b: 0.8368\n",
      "loss: 0.0853\n",
      "Epoch: 100/100, Batch: 337/432, W1: 0.8275, W2: 0.4521, W3: 0.0334, W4: -0.6716, b: 0.8368\n",
      "loss: 0.0919\n",
      "Epoch: 100/100, Batch: 338/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6716, b: 0.8368\n",
      "loss: 0.1049\n",
      "Epoch: 100/100, Batch: 339/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6716, b: 0.8368\n",
      "loss: 0.0987\n",
      "Epoch: 100/100, Batch: 340/432, W1: 0.8276, W2: 0.4521, W3: 0.0334, W4: -0.6717, b: 0.8368\n",
      "loss: 0.1217\n",
      "Epoch: 100/100, Batch: 341/432, W1: 0.8275, W2: 0.4521, W3: 0.0334, W4: -0.6717, b: 0.8368\n",
      "loss: 0.1098\n",
      "Epoch: 100/100, Batch: 342/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6717, b: 0.8368\n",
      "loss: 0.109\n",
      "Epoch: 100/100, Batch: 343/432, W1: 0.8275, W2: 0.4521, W3: 0.0334, W4: -0.6717, b: 0.8368\n",
      "loss: 0.0626\n",
      "Epoch: 100/100, Batch: 344/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6717, b: 0.8368\n",
      "loss: 0.0696\n",
      "Epoch: 100/100, Batch: 345/432, W1: 0.8275, W2: 0.4521, W3: 0.0334, W4: -0.6717, b: 0.8368\n",
      "loss: 0.6513\n",
      "Epoch: 100/100, Batch: 346/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6717, b: 0.8369\n",
      "loss: 0.115\n",
      "Epoch: 100/100, Batch: 347/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6717, b: 0.8369\n",
      "loss: 0.117\n",
      "Epoch: 100/100, Batch: 348/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6717, b: 0.8369\n",
      "loss: 0.1407\n",
      "Epoch: 100/100, Batch: 349/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6718, b: 0.8369\n",
      "loss: 0.0935\n",
      "Epoch: 100/100, Batch: 350/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6718, b: 0.8369\n",
      "loss: 0.1071\n",
      "Epoch: 100/100, Batch: 351/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6718, b: 0.8369\n",
      "loss: 0.0983\n",
      "Epoch: 100/100, Batch: 352/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6718, b: 0.8369\n",
      "loss: 0.0955\n",
      "Epoch: 100/100, Batch: 353/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6718, b: 0.8369\n",
      "loss: 0.1062\n",
      "Epoch: 100/100, Batch: 354/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6718, b: 0.8369\n",
      "loss: 0.0926\n",
      "Epoch: 100/100, Batch: 355/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6718, b: 0.8369\n",
      "loss: 0.1252\n",
      "Epoch: 100/100, Batch: 356/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6719, b: 0.8369\n",
      "loss: 0.0845\n",
      "Epoch: 100/100, Batch: 357/432, W1: 0.8273, W2: 0.4519, W3: 0.0333, W4: -0.6719, b: 0.8369\n",
      "loss: 0.1026\n",
      "Epoch: 100/100, Batch: 358/432, W1: 0.8273, W2: 0.4519, W3: 0.0333, W4: -0.6719, b: 0.8369\n",
      "loss: 0.0856\n",
      "Epoch: 100/100, Batch: 359/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6719, b: 0.8369\n",
      "loss: 0.1093\n",
      "Epoch: 100/100, Batch: 360/432, W1: 0.8273, W2: 0.4519, W3: 0.0333, W4: -0.6719, b: 0.8369\n",
      "loss: 0.0943\n",
      "Epoch: 100/100, Batch: 361/432, W1: 0.8273, W2: 0.4519, W3: 0.0333, W4: -0.6719, b: 0.8369\n",
      "loss: 0.1181\n",
      "Epoch: 100/100, Batch: 362/432, W1: 0.8273, W2: 0.4519, W3: 0.0333, W4: -0.6719, b: 0.8369\n",
      "loss: 0.111\n",
      "Epoch: 100/100, Batch: 363/432, W1: 0.8273, W2: 0.4519, W3: 0.0333, W4: -0.6719, b: 0.8369\n",
      "loss: 0.0996\n",
      "Epoch: 100/100, Batch: 364/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6719, b: 0.837\n",
      "loss: 0.1017\n",
      "Epoch: 100/100, Batch: 365/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6719, b: 0.837\n",
      "loss: 0.1065\n",
      "Epoch: 100/100, Batch: 366/432, W1: 0.8275, W2: 0.4521, W3: 0.0335, W4: -0.6719, b: 0.837\n",
      "loss: 0.0874\n",
      "Epoch: 100/100, Batch: 367/432, W1: 0.8276, W2: 0.4521, W3: 0.0335, W4: -0.6719, b: 0.837\n",
      "loss: 0.0966\n",
      "Epoch: 100/100, Batch: 368/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.6719, b: 0.837\n",
      "loss: 0.1006\n",
      "Epoch: 100/100, Batch: 369/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.6719, b: 0.837\n",
      "loss: 0.0798\n",
      "Epoch: 100/100, Batch: 370/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.6719, b: 0.837\n",
      "loss: 0.1043\n",
      "Epoch: 100/100, Batch: 371/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.672, b: 0.837\n",
      "loss: 0.1089\n",
      "Epoch: 100/100, Batch: 372/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.672, b: 0.8371\n",
      "loss: 0.1153\n",
      "Epoch: 100/100, Batch: 373/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.672, b: 0.8371\n",
      "loss: 0.0891\n",
      "Epoch: 100/100, Batch: 374/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.672, b: 0.8371\n",
      "loss: 0.0801\n",
      "Epoch: 100/100, Batch: 375/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.672, b: 0.8371\n",
      "loss: 0.1036\n",
      "Epoch: 100/100, Batch: 376/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.672, b: 0.8371\n",
      "loss: 0.0874\n",
      "Epoch: 100/100, Batch: 377/432, W1: 0.8277, W2: 0.4522, W3: 0.0335, W4: -0.672, b: 0.8371\n",
      "loss: 0.0908\n",
      "Epoch: 100/100, Batch: 378/432, W1: 0.8277, W2: 0.4522, W3: 0.0335, W4: -0.672, b: 0.8371\n",
      "loss: 0.0965\n",
      "Epoch: 100/100, Batch: 379/432, W1: 0.8276, W2: 0.4522, W3: 0.0335, W4: -0.6721, b: 0.8371\n",
      "loss: 0.0944\n",
      "Epoch: 100/100, Batch: 380/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.6721, b: 0.8371\n",
      "loss: 0.1139\n",
      "Epoch: 100/100, Batch: 381/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.6721, b: 0.8371\n",
      "loss: 0.1534\n",
      "Epoch: 100/100, Batch: 382/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.6721, b: 0.8371\n",
      "loss: 0.115\n",
      "Epoch: 100/100, Batch: 383/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.6721, b: 0.8371\n",
      "loss: 0.1155\n",
      "Epoch: 100/100, Batch: 384/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.6721, b: 0.8371\n",
      "loss: 0.1027\n",
      "Epoch: 100/100, Batch: 385/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.6721, b: 0.8371\n",
      "loss: 0.1254\n",
      "Epoch: 100/100, Batch: 386/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.6721, b: 0.8372\n",
      "loss: 0.0992\n",
      "Epoch: 100/100, Batch: 387/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.6721, b: 0.8372\n",
      "loss: 0.1344\n",
      "Epoch: 100/100, Batch: 388/432, W1: 0.8278, W2: 0.4523, W3: 0.0336, W4: -0.6721, b: 0.8372\n",
      "loss: 0.0817\n",
      "Epoch: 100/100, Batch: 389/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.6722, b: 0.8372\n",
      "loss: 0.0945\n",
      "Epoch: 100/100, Batch: 390/432, W1: 0.8277, W2: 0.4523, W3: 0.0336, W4: -0.6722, b: 0.8372\n",
      "loss: 0.1079\n",
      "Epoch: 100/100, Batch: 391/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.6722, b: 0.8372\n",
      "loss: 0.1124\n",
      "Epoch: 100/100, Batch: 392/432, W1: 0.8276, W2: 0.4522, W3: 0.0335, W4: -0.6722, b: 0.8372\n",
      "loss: 0.121\n",
      "Epoch: 100/100, Batch: 393/432, W1: 0.8276, W2: 0.4521, W3: 0.0335, W4: -0.6722, b: 0.8372\n",
      "loss: 0.0948\n",
      "Epoch: 100/100, Batch: 394/432, W1: 0.8275, W2: 0.4521, W3: 0.0335, W4: -0.6723, b: 0.8372\n",
      "loss: 0.1308\n",
      "Epoch: 100/100, Batch: 395/432, W1: 0.8275, W2: 0.452, W3: 0.0334, W4: -0.6723, b: 0.8372\n",
      "loss: 1.0243\n",
      "Epoch: 100/100, Batch: 396/432, W1: 0.8275, W2: 0.452, W3: 0.0335, W4: -0.6723, b: 0.8372\n",
      "loss: 0.0829\n",
      "Epoch: 100/100, Batch: 397/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6723, b: 0.8372\n",
      "loss: 0.0981\n",
      "Epoch: 100/100, Batch: 398/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6723, b: 0.8372\n",
      "loss: 0.122\n",
      "Epoch: 100/100, Batch: 399/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6723, b: 0.8372\n",
      "loss: 0.0863\n",
      "Epoch: 100/100, Batch: 400/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6723, b: 0.8372\n",
      "loss: 0.1054\n",
      "Epoch: 100/100, Batch: 401/432, W1: 0.8273, W2: 0.4519, W3: 0.0334, W4: -0.6723, b: 0.8372\n",
      "loss: 0.0835\n",
      "Epoch: 100/100, Batch: 402/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6723, b: 0.8372\n",
      "loss: 0.0715\n",
      "Epoch: 100/100, Batch: 403/432, W1: 0.8273, W2: 0.4519, W3: 0.0334, W4: -0.6724, b: 0.8372\n",
      "loss: 0.1052\n",
      "Epoch: 100/100, Batch: 404/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6724, b: 0.8372\n",
      "loss: 0.1203\n",
      "Epoch: 100/100, Batch: 405/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6724, b: 0.8372\n",
      "loss: 0.087\n",
      "Epoch: 100/100, Batch: 406/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6724, b: 0.8372\n",
      "loss: 0.0954\n",
      "Epoch: 100/100, Batch: 407/432, W1: 0.8274, W2: 0.4519, W3: 0.0334, W4: -0.6724, b: 0.8373\n",
      "loss: 0.1059\n",
      "Epoch: 100/100, Batch: 408/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6724, b: 0.8373\n",
      "loss: 0.0845\n",
      "Epoch: 100/100, Batch: 409/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6724, b: 0.8373\n",
      "loss: 0.0956\n",
      "Epoch: 100/100, Batch: 410/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6724, b: 0.8373\n",
      "loss: 0.1193\n",
      "Epoch: 100/100, Batch: 411/432, W1: 0.8274, W2: 0.452, W3: 0.0334, W4: -0.6724, b: 0.8373\n",
      "loss: 0.0975\n",
      "Epoch: 100/100, Batch: 412/432, W1: 0.8275, W2: 0.4521, W3: 0.0335, W4: -0.6724, b: 0.8373\n",
      "loss: 0.0709\n",
      "Epoch: 100/100, Batch: 413/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6724, b: 0.8373\n",
      "loss: 0.1249\n",
      "Epoch: 100/100, Batch: 414/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6724, b: 0.8373\n",
      "loss: 0.0789\n",
      "Epoch: 100/100, Batch: 415/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6724, b: 0.8373\n",
      "loss: 0.1099\n",
      "Epoch: 100/100, Batch: 416/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6724, b: 0.8374\n",
      "loss: 0.103\n",
      "Epoch: 100/100, Batch: 417/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6725, b: 0.8374\n",
      "loss: 0.1023\n",
      "Epoch: 100/100, Batch: 418/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6725, b: 0.8374\n",
      "loss: 0.1006\n",
      "Epoch: 100/100, Batch: 419/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6725, b: 0.8374\n",
      "loss: 0.1089\n",
      "Epoch: 100/100, Batch: 420/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6725, b: 0.8374\n",
      "loss: 0.1135\n",
      "Epoch: 100/100, Batch: 421/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6725, b: 0.8374\n",
      "loss: 0.1077\n",
      "Epoch: 100/100, Batch: 422/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6725, b: 0.8374\n",
      "loss: 0.1057\n",
      "Epoch: 100/100, Batch: 423/432, W1: 0.8276, W2: 0.4521, W3: 0.0335, W4: -0.6725, b: 0.8374\n",
      "loss: 0.1409\n",
      "Epoch: 100/100, Batch: 424/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6725, b: 0.8374\n",
      "loss: 0.1017\n",
      "Epoch: 100/100, Batch: 425/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.6725, b: 0.8374\n",
      "loss: 0.1042\n",
      "Epoch: 100/100, Batch: 426/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6726, b: 0.8374\n",
      "loss: 0.0941\n",
      "Epoch: 100/100, Batch: 427/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6726, b: 0.8374\n",
      "loss: 0.1004\n",
      "Epoch: 100/100, Batch: 428/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6726, b: 0.8374\n",
      "loss: 0.0841\n",
      "Epoch: 100/100, Batch: 429/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6726, b: 0.8374\n",
      "loss: 0.068\n",
      "Epoch: 100/100, Batch: 430/432, W1: 0.8277, W2: 0.4522, W3: 0.0336, W4: -0.6726, b: 0.8374\n",
      "loss: 0.1033\n",
      "Epoch: 100/100, Batch: 431/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6726, b: 0.8374\n",
      "loss: 0.1182\n",
      "Epoch: 100/100, Batch: 432/432, W1: 0.8276, W2: 0.4522, W3: 0.0336, W4: -0.6726, b: 0.8374\n",
      "loss: 0.0834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "torch.manual_seed(150)\n",
    "\n",
    "features, targets = pre_df.iloc[: , :-1] , pre_df.iloc[:, -1]\n",
    "\n",
    "X_train , X_test, y_train, y_test = \\\n",
    "train_test_split(features, targets, test_size=0.2, random_state=150)\n",
    "\n",
    "X_train = torch.FloatTensor(X_train.values)\n",
    "y_train = torch.FloatTensor(y_train.values).view(-1 , 1)\n",
    "\n",
    "X_test = torch.FloatTensor(X_test.values)\n",
    "y_test = torch.FloatTensor(y_test.values).view(-1 , 1)\n",
    "\n",
    "td = TensorDataset(X_train, y_train)\n",
    "dl = DataLoader(td, batch_size=100, shuffle=True) # 미니 배치 구축 \n",
    "\n",
    "l_r = LinearRegressionModel(4)\n",
    "\n",
    "optimizer = SGD(l_r.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for i, samples in enumerate(dl):\n",
    "        \n",
    "        X_train, y_train = samples\n",
    "\n",
    "        H = l_r(X_train)\n",
    "        loss = mse_loss(H, y_train)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch: {epoch}/{epochs}, Batch: {i + 1}/{len(dl)}, ', end='')\n",
    "            for i, w in enumerate(list(l_r.parameters())[0][0]):\n",
    "                print(f'W{i + 1}: {np.round(w.item(), 4)}, ', end='')\n",
    "            print(f'b: {np.round(list(l_r.parameters())[1].item(), 4)}')\n",
    "            print(f'loss: {np.round(loss.item(), 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2741ba3-cf31-4826-a232-e81d7a0fdfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.values>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75bb0064-a940-4158-8c46-56164659e691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "\n",
    "def get_evaluation(y_test, prediction):\n",
    "    MSE = mean_squared_error(y_test, prediction)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    MSLE = mean_squared_log_error(y_test, prediction)\n",
    "    RMSLE = np.sqrt(mean_squared_log_error(y_test, prediction))\n",
    "    print('MSE: {:.4f}, RMSE: {:.4f}, MSLE: {:.4f}, RMSLE: {:.4f}'\\\n",
    "          .format(MSE, RMSE, MSLE, RMSLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fda9cce0-564e-4fc0-b1b4-14a8fc2783c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach -> numpy 배열로 변환 \n",
    "y_test = y_test.detach().numpy()\n",
    "\n",
    "H = l_r(X_test)\n",
    "prediction = H.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43da2e6a-2ba8-4fe1-8f13-fbf6410a7ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.1027, RMSE: 0.3205, MSLE: 0.0017, RMSLE: 0.0410\n"
     ]
    }
   ],
   "source": [
    "get_evaluation(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a0e6647-da8f-4c46-b98c-2a29eb3b50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features, targets = pre_df.iloc[:, :-1], pre_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(features, targets, test_size=0.2, random_state=150)\n",
    "\n",
    "l_r = LinearRegression()\n",
    "l_r.fit(X_train, y_train)\n",
    "prediction = l_r.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77b853ba-b3ca-4650-a85b-cfea18ed1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: 0.9902, W2: 0.0322, W3: 0.1530, W4: -0.6082, b: 1.8718, MSLE: 0.0011, MSE: 0.0745\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(l_r.coef_):\n",
    "            print('W{}: {:.4f}, '\\\n",
    "                  .format(i + 1, w.item()), end='')\n",
    "print('b: {:.4f}, MSLE: {:.4f}, MSE: {:.4f}'\\\n",
    "      .format(l_r.intercept_, mean_squared_log_error(y_test, prediction), mean_squared_error(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66bd85cb-b40e-4e05-aed4-2ed835a22d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "\n",
    "def get_evaluation(y_test, prediction):\n",
    "    MSE = mean_squared_error(y_test, prediction)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    MSLE = mean_squared_log_error(y_test, prediction)\n",
    "    RMSLE = np.sqrt(mean_squared_log_error(y_test, prediction))\n",
    "    print('MSE: {:.4f}, RMSE: {:.4f}, MSLE: {:.4f}, RMSLE: {:.4f}'\\\n",
    "          .format(MSE, RMSE, MSLE, RMSLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2953b4f5-083b-41ec-92a7-392508194108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: 0.9902, W2: 0.0322, W3: 0.1530, W4: -0.6082, b: 1.8718\n",
      "MSE: 0.0745, RMSE: 0.2729, MSLE: 0.0011, RMSLE: 0.0332\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(l_r.coef_):\n",
    "            print('W{}: {:.4f}, '\\\n",
    "                  .format(i + 1, w.item()), end='')\n",
    "print('b: {:.4f}'\\\n",
    "      .format(l_r.intercept_))\n",
    "get_evaluation(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86064544-d4e1-43dc-88e9-dba370a883a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.0334],\n",
       "        [ 7.3322],\n",
       "        [ 8.6153],\n",
       "        ...,\n",
       "        [ 7.6311],\n",
       "        [ 6.0913],\n",
       "        [ 6.3060]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c47b0fd-de3d-4098-a9c2-0110f2ca3ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "('정답', '예측')\n",
      "(7.602900462204755, tensor([10.0334], grad_fn=<UnbindBackward0>))\n",
      "(8.32093496888341, tensor([7.3322], grad_fn=<UnbindBackward0>))\n",
      "(8.344980368770571, tensor([8.6153], grad_fn=<UnbindBackward0>))\n",
      "(6.747586526829315, tensor([8.6578], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([9.4077], grad_fn=<UnbindBackward0>))\n",
      "(8.929435283803425, tensor([8.5184], grad_fn=<UnbindBackward0>))\n",
      "(7.105786129481271, tensor([8.9108], grad_fn=<UnbindBackward0>))\n",
      "(8.596189197642735, tensor([8.8249], grad_fn=<UnbindBackward0>))\n",
      "(8.595264726836392, tensor([6.5050], grad_fn=<UnbindBackward0>))\n",
      "(7.592870287844818, tensor([6.0215], grad_fn=<UnbindBackward0>))\n",
      "(9.718301898127523, tensor([7.4061], grad_fn=<UnbindBackward0>))\n",
      "(7.086737934510577, tensor([7.7326], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([10.2183], grad_fn=<UnbindBackward0>))\n",
      "(7.52294091807237, tensor([6.8100], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([6.8462], grad_fn=<UnbindBackward0>))\n",
      "(6.665683717782408, tensor([6.8052], grad_fn=<UnbindBackward0>))\n",
      "(6.431331081933479, tensor([6.7031], grad_fn=<UnbindBackward0>))\n",
      "(7.769800996003896, tensor([6.0058], grad_fn=<UnbindBackward0>))\n",
      "(8.659733878198347, tensor([6.7047], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([6.8685], grad_fn=<UnbindBackward0>))\n",
      "(8.787067645218054, tensor([8.6014], grad_fn=<UnbindBackward0>))\n",
      "(9.52617248784431, tensor([6.3533], grad_fn=<UnbindBackward0>))\n",
      "(8.224967478914584, tensor([7.8433], grad_fn=<UnbindBackward0>))\n",
      "(9.491828300264636, tensor([7.4256], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([8.5975], grad_fn=<UnbindBackward0>))\n",
      "(9.001222992395064, tensor([8.5239], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.4058], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([9.4246], grad_fn=<UnbindBackward0>))\n",
      "(7.81156848934518, tensor([9.3384], grad_fn=<UnbindBackward0>))\n",
      "(8.963544291996744, tensor([6.8600], grad_fn=<UnbindBackward0>))\n",
      "(9.042749779782222, tensor([6.3216], grad_fn=<UnbindBackward0>))\n",
      "(6.606650186198215, tensor([8.5900], grad_fn=<UnbindBackward0>))\n",
      "(7.463363045520021, tensor([6.1920], grad_fn=<UnbindBackward0>))\n",
      "(6.595780513961311, tensor([8.4065], grad_fn=<UnbindBackward0>))\n",
      "(7.840312983320164, tensor([8.0085], grad_fn=<UnbindBackward0>))\n",
      "(6.665683717782408, tensor([5.9119], grad_fn=<UnbindBackward0>))\n",
      "(7.584264818389059, tensor([8.4593], grad_fn=<UnbindBackward0>))\n",
      "(9.108418382250798, tensor([8.2570], grad_fn=<UnbindBackward0>))\n",
      "(8.650499558724462, tensor([5.8723], grad_fn=<UnbindBackward0>))\n",
      "(6.2878585601617845, tensor([8.5469], grad_fn=<UnbindBackward0>))\n",
      "(8.48301573961465, tensor([8.7670], grad_fn=<UnbindBackward0>))\n",
      "(7.938445551164788, tensor([7.7812], grad_fn=<UnbindBackward0>))\n",
      "(8.351138607086154, tensor([6.4716], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([7.2718], grad_fn=<UnbindBackward0>))\n",
      "(8.196987927258897, tensor([7.8243], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([6.2294], grad_fn=<UnbindBackward0>))\n",
      "(7.065613363597717, tensor([8.8824], grad_fn=<UnbindBackward0>))\n",
      "(6.826545223556594, tensor([6.6394], grad_fn=<UnbindBackward0>))\n",
      "(7.935945103353701, tensor([9.1076], grad_fn=<UnbindBackward0>))\n",
      "(8.384347278082808, tensor([5.9177], grad_fn=<UnbindBackward0>))\n",
      "(7.07326971745971, tensor([7.3228], grad_fn=<UnbindBackward0>))\n",
      "(7.68937110752969, tensor([9.0723], grad_fn=<UnbindBackward0>))\n",
      "(8.38320455141292, tensor([8.9038], grad_fn=<UnbindBackward0>))\n",
      "(7.537962659768208, tensor([8.4700], grad_fn=<UnbindBackward0>))\n",
      "(8.663887570567042, tensor([7.4096], grad_fn=<UnbindBackward0>))\n",
      "(8.09162741160107, tensor([9.0133], grad_fn=<UnbindBackward0>))\n",
      "(7.115582126184454, tensor([8.7038], grad_fn=<UnbindBackward0>))\n",
      "(7.106606137727303, tensor([6.9403], grad_fn=<UnbindBackward0>))\n",
      "(8.619569258033104, tensor([7.9028], grad_fn=<UnbindBackward0>))\n",
      "(6.8001700683022, tensor([8.6791], grad_fn=<UnbindBackward0>))\n",
      "(7.724446645633537, tensor([8.5586], grad_fn=<UnbindBackward0>))\n",
      "(9.307376334487778, tensor([6.3442], grad_fn=<UnbindBackward0>))\n",
      "(7.275172319452771, tensor([6.1571], grad_fn=<UnbindBackward0>))\n",
      "(9.635738817791797, tensor([9.4305], grad_fn=<UnbindBackward0>))\n",
      "(9.137984709784043, tensor([7.5227], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([8.5994], grad_fn=<UnbindBackward0>))\n",
      "(6.694562058521095, tensor([8.6160], grad_fn=<UnbindBackward0>))\n",
      "(8.03365842788615, tensor([6.7210], grad_fn=<UnbindBackward0>))\n",
      "(7.592366128519796, tensor([8.6784], grad_fn=<UnbindBackward0>))\n",
      "(9.103089181229207, tensor([9.6728], grad_fn=<UnbindBackward0>))\n",
      "(6.906754778648554, tensor([9.9236], grad_fn=<UnbindBackward0>))\n",
      "(8.232440158470336, tensor([6.4085], grad_fn=<UnbindBackward0>))\n",
      "(8.411610428841172, tensor([8.8833], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([9.4240], grad_fn=<UnbindBackward0>))\n",
      "(7.77863014732581, tensor([9.5531], grad_fn=<UnbindBackward0>))\n",
      "(7.773594467360194, tensor([6.8627], grad_fn=<UnbindBackward0>))\n",
      "(7.428927194802272, tensor([6.6952], grad_fn=<UnbindBackward0>))\n",
      "(8.743372131273969, tensor([9.4852], grad_fn=<UnbindBackward0>))\n",
      "(8.656781205623291, tensor([7.7687], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([8.6317], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([6.6722], grad_fn=<UnbindBackward0>))\n",
      "(7.633853559681768, tensor([8.5383], grad_fn=<UnbindBackward0>))\n",
      "(7.798933310041217, tensor([8.3804], grad_fn=<UnbindBackward0>))\n",
      "(7.721348612617949, tensor([6.6729], grad_fn=<UnbindBackward0>))\n",
      "(9.806425839692997, tensor([6.3941], grad_fn=<UnbindBackward0>))\n",
      "(8.429235912657095, tensor([7.8337], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.2406], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([7.7054], grad_fn=<UnbindBackward0>))\n",
      "(6.651571873589727, tensor([6.3726], grad_fn=<UnbindBackward0>))\n",
      "(9.081938657171658, tensor([8.3237], grad_fn=<UnbindBackward0>))\n",
      "(9.118992332516877, tensor([7.9075], grad_fn=<UnbindBackward0>))\n",
      "(6.302618975744905, tensor([8.2039], grad_fn=<UnbindBackward0>))\n",
      "(7.017506142941256, tensor([6.5642], grad_fn=<UnbindBackward0>))\n",
      "(8.64979915596426, tensor([7.7864], grad_fn=<UnbindBackward0>))\n",
      "(7.621684998724611, tensor([8.6530], grad_fn=<UnbindBackward0>))\n",
      "(9.48189306249808, tensor([7.4153], grad_fn=<UnbindBackward0>))\n",
      "(9.197964100908127, tensor([6.3553], grad_fn=<UnbindBackward0>))\n",
      "(8.074960359115858, tensor([7.5472], grad_fn=<UnbindBackward0>))\n",
      "(8.74878098951311, tensor([7.7539], grad_fn=<UnbindBackward0>))\n",
      "(9.772239265926174, tensor([7.1598], grad_fn=<UnbindBackward0>))\n",
      "(8.872346978983032, tensor([7.2498], grad_fn=<UnbindBackward0>))\n",
      "(8.397508348470257, tensor([8.8092], grad_fn=<UnbindBackward0>))\n",
      "(7.01211529430638, tensor([9.0491], grad_fn=<UnbindBackward0>))\n",
      "(9.729372232289272, tensor([8.0227], grad_fn=<UnbindBackward0>))\n",
      "(9.40894529884324, tensor([6.1778], grad_fn=<UnbindBackward0>))\n",
      "(7.551186867296149, tensor([7.6685], grad_fn=<UnbindBackward0>))\n",
      "(9.580178302441196, tensor([8.4132], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([9.7484], grad_fn=<UnbindBackward0>))\n",
      "(8.42989086301344, tensor([7.0116], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([7.2422], grad_fn=<UnbindBackward0>))\n",
      "(9.447623486844112, tensor([7.9029], grad_fn=<UnbindBackward0>))\n",
      "(7.637716432664798, tensor([6.4097], grad_fn=<UnbindBackward0>))\n",
      "(7.612336837167746, tensor([9.4680], grad_fn=<UnbindBackward0>))\n",
      "(9.722804652410682, tensor([8.6367], grad_fn=<UnbindBackward0>))\n",
      "(7.0431599159883405, tensor([6.3416], grad_fn=<UnbindBackward0>))\n",
      "(9.188605879830176, tensor([7.1829], grad_fn=<UnbindBackward0>))\n",
      "(9.036106025364846, tensor([10.0454], grad_fn=<UnbindBackward0>))\n",
      "(9.067624069774588, tensor([7.7763], grad_fn=<UnbindBackward0>))\n",
      "(7.011213987350367, tensor([8.6487], grad_fn=<UnbindBackward0>))\n",
      "(8.39615486303918, tensor([8.7193], grad_fn=<UnbindBackward0>))\n",
      "(8.747510946478448, tensor([7.4046], grad_fn=<UnbindBackward0>))\n",
      "(9.831185640780722, tensor([7.0851], grad_fn=<UnbindBackward0>))\n",
      "(7.884952945759814, tensor([8.2771], grad_fn=<UnbindBackward0>))\n",
      "(6.9363427358340495, tensor([6.5579], grad_fn=<UnbindBackward0>))\n",
      "(8.622813673279921, tensor([6.3205], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.9850], grad_fn=<UnbindBackward0>))\n",
      "(7.01211529430638, tensor([6.0279], grad_fn=<UnbindBackward0>))\n",
      "(8.612503371220562, tensor([9.2884], grad_fn=<UnbindBackward0>))\n",
      "(6.651571873589727, tensor([6.8188], grad_fn=<UnbindBackward0>))\n",
      "(9.115590035430372, tensor([7.4568], grad_fn=<UnbindBackward0>))\n",
      "(7.170888478512505, tensor([6.6588], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([7.1940], grad_fn=<UnbindBackward0>))\n",
      "(8.318010277546872, tensor([7.4296], grad_fn=<UnbindBackward0>))\n",
      "(7.814399633804487, tensor([8.4507], grad_fn=<UnbindBackward0>))\n",
      "(8.360305435879093, tensor([6.6362], grad_fn=<UnbindBackward0>))\n",
      "(9.611395771595648, tensor([8.5585], grad_fn=<UnbindBackward0>))\n",
      "(8.35608503102148, tensor([7.3201], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([7.7655], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([7.2306], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([6.3708], grad_fn=<UnbindBackward0>))\n",
      "(6.90875477931522, tensor([8.4605], grad_fn=<UnbindBackward0>))\n",
      "(8.630700432209832, tensor([6.3542], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([7.7657], grad_fn=<UnbindBackward0>))\n",
      "(9.686139673818495, tensor([6.5257], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([7.0096], grad_fn=<UnbindBackward0>))\n",
      "(6.042632833682381, tensor([6.8193], grad_fn=<UnbindBackward0>))\n",
      "(8.821437352166997, tensor([7.6963], grad_fn=<UnbindBackward0>))\n",
      "(7.848153086199526, tensor([9.6667], grad_fn=<UnbindBackward0>))\n",
      "(6.773080375655535, tensor([6.3873], grad_fn=<UnbindBackward0>))\n",
      "(8.412943170042439, tensor([9.6169], grad_fn=<UnbindBackward0>))\n",
      "(7.555905093611346, tensor([7.7935], grad_fn=<UnbindBackward0>))\n",
      "(8.32579052588609, tensor([6.5233], grad_fn=<UnbindBackward0>))\n",
      "(7.753623546559746, tensor([6.3876], grad_fn=<UnbindBackward0>))\n",
      "(9.75452339939617, tensor([6.6076], grad_fn=<UnbindBackward0>))\n",
      "(9.326700173092378, tensor([8.8732], grad_fn=<UnbindBackward0>))\n",
      "(8.25712628599743, tensor([6.3056], grad_fn=<UnbindBackward0>))\n",
      "(8.609772372709331, tensor([8.9471], grad_fn=<UnbindBackward0>))\n",
      "(8.130059039992796, tensor([10.5309], grad_fn=<UnbindBackward0>))\n",
      "(8.430981494597171, tensor([6.4205], grad_fn=<UnbindBackward0>))\n",
      "(6.880384082186005, tensor([7.2091], grad_fn=<UnbindBackward0>))\n",
      "(7.513709247839705, tensor([6.4746], grad_fn=<UnbindBackward0>))\n",
      "(7.261927092702751, tensor([8.7227], grad_fn=<UnbindBackward0>))\n",
      "(6.582025138892826, tensor([7.4039], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.4282], grad_fn=<UnbindBackward0>))\n",
      "(7.416378479192928, tensor([9.4086], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([9.0173], grad_fn=<UnbindBackward0>))\n",
      "(7.701200180857446, tensor([10.0102], grad_fn=<UnbindBackward0>))\n",
      "(8.281723990411392, tensor([7.9902], grad_fn=<UnbindBackward0>))\n",
      "(6.635946555686647, tensor([6.3272], grad_fn=<UnbindBackward0>))\n",
      "(8.11162807830774, tensor([8.5872], grad_fn=<UnbindBackward0>))\n",
      "(7.874358824729881, tensor([7.7692], grad_fn=<UnbindBackward0>))\n",
      "(8.23217423638394, tensor([6.3113], grad_fn=<UnbindBackward0>))\n",
      "(8.938269147038548, tensor([8.1398], grad_fn=<UnbindBackward0>))\n",
      "(8.429672593886743, tensor([6.5178], grad_fn=<UnbindBackward0>))\n",
      "(9.654064192201437, tensor([6.4447], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([8.6660], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([8.5380], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([9.3006], grad_fn=<UnbindBackward0>))\n",
      "(8.444407421690585, tensor([7.4132], grad_fn=<UnbindBackward0>))\n",
      "(8.827468112520654, tensor([8.3404], grad_fn=<UnbindBackward0>))\n",
      "(7.847371836159788, tensor([6.9349], grad_fn=<UnbindBackward0>))\n",
      "(8.393894975071744, tensor([8.8261], grad_fn=<UnbindBackward0>))\n",
      "(7.495541943884256, tensor([8.5621], grad_fn=<UnbindBackward0>))\n",
      "(9.324293862052029, tensor([6.7081], grad_fn=<UnbindBackward0>))\n",
      "(9.459073694289389, tensor([5.9959], grad_fn=<UnbindBackward0>))\n",
      "(8.403128235128264, tensor([8.5987], grad_fn=<UnbindBackward0>))\n",
      "(8.457018468380168, tensor([6.5916], grad_fn=<UnbindBackward0>))\n",
      "(8.92172453036431, tensor([6.9073], grad_fn=<UnbindBackward0>))\n",
      "(8.748463629942055, tensor([8.1009], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([8.2255], grad_fn=<UnbindBackward0>))\n",
      "(8.155936337972394, tensor([7.4584], grad_fn=<UnbindBackward0>))\n",
      "(6.946975992135418, tensor([7.5991], grad_fn=<UnbindBackward0>))\n",
      "(7.459914766241105, tensor([8.1287], grad_fn=<UnbindBackward0>))\n",
      "(7.9269635448629785, tensor([8.5371], grad_fn=<UnbindBackward0>))\n",
      "(7.633853559681768, tensor([7.3535], grad_fn=<UnbindBackward0>))\n",
      "(7.409136443920128, tensor([6.4274], grad_fn=<UnbindBackward0>))\n",
      "(9.577064651767916, tensor([9.4119], grad_fn=<UnbindBackward0>))\n",
      "(7.4770384723196965, tensor([6.2445], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([7.4580], grad_fn=<UnbindBackward0>))\n",
      "(8.222553638396958, tensor([7.2581], grad_fn=<UnbindBackward0>))\n",
      "(7.971776122880628, tensor([8.0567], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([8.7926], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([8.5988], grad_fn=<UnbindBackward0>))\n",
      "(7.643003635560718, tensor([6.6999], grad_fn=<UnbindBackward0>))\n",
      "(8.140898460607852, tensor([10.0048], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([9.3434], grad_fn=<UnbindBackward0>))\n",
      "(7.778211474512493, tensor([8.3742], grad_fn=<UnbindBackward0>))\n",
      "(7.251344983372214, tensor([8.3375], grad_fn=<UnbindBackward0>))\n",
      "(7.769378609513984, tensor([5.9840], grad_fn=<UnbindBackward0>))\n",
      "(8.319229938632326, tensor([9.0699], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([7.8157], grad_fn=<UnbindBackward0>))\n",
      "(6.46302945692067, tensor([8.4910], grad_fn=<UnbindBackward0>))\n",
      "(7.215239978730097, tensor([6.6228], grad_fn=<UnbindBackward0>))\n",
      "(8.889032571874742, tensor([7.8762], grad_fn=<UnbindBackward0>))\n",
      "(8.66957087183712, tensor([7.2323], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([5.9441], grad_fn=<UnbindBackward0>))\n",
      "(9.036463061964891, tensor([7.1412], grad_fn=<UnbindBackward0>))\n",
      "(8.190077049719049, tensor([7.2198], grad_fn=<UnbindBackward0>))\n",
      "(8.007034012193408, tensor([8.3721], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([6.5422], grad_fn=<UnbindBackward0>))\n",
      "(8.725832056527565, tensor([8.3968], grad_fn=<UnbindBackward0>))\n",
      "(7.107425474110705, tensor([7.8408], grad_fn=<UnbindBackward0>))\n",
      "(6.57507584059962, tensor([8.5871], grad_fn=<UnbindBackward0>))\n",
      "(6.452048954437226, tensor([6.7682], grad_fn=<UnbindBackward0>))\n",
      "(9.024492605405669, tensor([7.7163], grad_fn=<UnbindBackward0>))\n",
      "(8.362175469149628, tensor([8.4887], grad_fn=<UnbindBackward0>))\n",
      "(6.2878585601617845, tensor([9.5024], grad_fn=<UnbindBackward0>))\n",
      "(7.994632311431825, tensor([9.6353], grad_fn=<UnbindBackward0>))\n",
      "(8.39773375137891, tensor([6.3730], grad_fn=<UnbindBackward0>))\n",
      "(6.508769136971682, tensor([8.8072], grad_fn=<UnbindBackward0>))\n",
      "(8.82893352896109, tensor([7.9051], grad_fn=<UnbindBackward0>))\n",
      "(8.13739583005665, tensor([7.7940], grad_fn=<UnbindBackward0>))\n",
      "(6.839476438228843, tensor([7.6389], grad_fn=<UnbindBackward0>))\n",
      "(8.550047528287184, tensor([8.6958], grad_fn=<UnbindBackward0>))\n",
      "(8.723882104658486, tensor([7.8324], grad_fn=<UnbindBackward0>))\n",
      "(6.932447891572509, tensor([6.7273], grad_fn=<UnbindBackward0>))\n",
      "(8.981555940771887, tensor([8.8055], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([7.7674], grad_fn=<UnbindBackward0>))\n",
      "(7.522400231387125, tensor([10.1523], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([7.3397], grad_fn=<UnbindBackward0>))\n",
      "(9.679406061493943, tensor([6.6458], grad_fn=<UnbindBackward0>))\n",
      "(9.13140538388804, tensor([8.0488], grad_fn=<UnbindBackward0>))\n",
      "(9.745604917619191, tensor([9.1165], grad_fn=<UnbindBackward0>))\n",
      "(5.993961427306569, tensor([7.7961], grad_fn=<UnbindBackward0>))\n",
      "(8.201111644442758, tensor([8.2141], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([6.2026], grad_fn=<UnbindBackward0>))\n",
      "(8.428580533059634, tensor([6.3667], grad_fn=<UnbindBackward0>))\n",
      "(6.697034247666484, tensor([6.6512], grad_fn=<UnbindBackward0>))\n",
      "(7.647786045440933, tensor([7.8896], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([6.7986], grad_fn=<UnbindBackward0>))\n",
      "(7.9665866976384025, tensor([6.2387], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([7.7662], grad_fn=<UnbindBackward0>))\n",
      "(8.473032295630468, tensor([6.4747], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([8.9697], grad_fn=<UnbindBackward0>))\n",
      "(6.376726947898627, tensor([7.1660], grad_fn=<UnbindBackward0>))\n",
      "(9.210340371976184, tensor([8.8921], grad_fn=<UnbindBackward0>))\n",
      "(8.518592212329946, tensor([6.3237], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([8.6485], grad_fn=<UnbindBackward0>))\n",
      "(8.23827262463303, tensor([8.5451], grad_fn=<UnbindBackward0>))\n",
      "(7.455298485683291, tensor([9.2648], grad_fn=<UnbindBackward0>))\n",
      "(6.466144724237619, tensor([7.2119], grad_fn=<UnbindBackward0>))\n",
      "(7.627057417018934, tensor([8.4540], grad_fn=<UnbindBackward0>))\n",
      "(9.643485495857657, tensor([7.3500], grad_fn=<UnbindBackward0>))\n",
      "(7.688455356549944, tensor([9.7763], grad_fn=<UnbindBackward0>))\n",
      "(8.343791731996841, tensor([7.2537], grad_fn=<UnbindBackward0>))\n",
      "(8.773384596776648, tensor([8.4593], grad_fn=<UnbindBackward0>))\n",
      "(7.405495663199472, tensor([8.2995], grad_fn=<UnbindBackward0>))\n",
      "(9.191667106710561, tensor([6.7261], grad_fn=<UnbindBackward0>))\n",
      "(7.5406215286571525, tensor([8.0094], grad_fn=<UnbindBackward0>))\n",
      "(8.209580483475577, tensor([6.2499], grad_fn=<UnbindBackward0>))\n",
      "(7.474204806496124, tensor([9.3611], grad_fn=<UnbindBackward0>))\n",
      "(8.594339400592892, tensor([9.3244], grad_fn=<UnbindBackward0>))\n",
      "(6.508769136971682, tensor([6.9592], grad_fn=<UnbindBackward0>))\n",
      "(6.805722553416985, tensor([9.3704], grad_fn=<UnbindBackward0>))\n",
      "(9.233959237574817, tensor([9.2491], grad_fn=<UnbindBackward0>))\n",
      "(7.183111701743281, tensor([6.8129], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([6.6901], grad_fn=<UnbindBackward0>))\n",
      "(7.1785454837637, tensor([8.1859], grad_fn=<UnbindBackward0>))\n",
      "(8.917310693197807, tensor([6.2741], grad_fn=<UnbindBackward0>))\n",
      "(7.421775793644647, tensor([6.7474], grad_fn=<UnbindBackward0>))\n",
      "(9.405495886750224, tensor([9.3167], grad_fn=<UnbindBackward0>))\n",
      "(8.128585200374497, tensor([8.4824], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([7.2056], grad_fn=<UnbindBackward0>))\n",
      "(8.789050713521046, tensor([6.2770], grad_fn=<UnbindBackward0>))\n",
      "(9.137339479091693, tensor([7.9267], grad_fn=<UnbindBackward0>))\n",
      "(7.740229524763182, tensor([7.8170], grad_fn=<UnbindBackward0>))\n",
      "(7.258412150595307, tensor([6.3181], grad_fn=<UnbindBackward0>))\n",
      "(8.033334015880062, tensor([6.7927], grad_fn=<UnbindBackward0>))\n",
      "(9.050406334059858, tensor([7.2297], grad_fn=<UnbindBackward0>))\n",
      "(8.121480374750751, tensor([6.8215], grad_fn=<UnbindBackward0>))\n",
      "(7.846980982138788, tensor([8.7819], grad_fn=<UnbindBackward0>))\n",
      "(8.4368504387337, tensor([7.1172], grad_fn=<UnbindBackward0>))\n",
      "(7.617267813628347, tensor([9.4868], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([8.5204], grad_fn=<UnbindBackward0>))\n",
      "(8.13505390861157, tensor([8.5681], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.3565], grad_fn=<UnbindBackward0>))\n",
      "(9.08851166361105, tensor([9.2980], grad_fn=<UnbindBackward0>))\n",
      "(8.000349495324684, tensor([7.7237], grad_fn=<UnbindBackward0>))\n",
      "(7.951207156472972, tensor([6.3875], grad_fn=<UnbindBackward0>))\n",
      "(9.155672970633875, tensor([6.6832], grad_fn=<UnbindBackward0>))\n",
      "(8.23323750070527, tensor([8.7602], grad_fn=<UnbindBackward0>))\n",
      "(6.658011045870748, tensor([8.0072], grad_fn=<UnbindBackward0>))\n",
      "(9.061027968789174, tensor([6.7938], grad_fn=<UnbindBackward0>))\n",
      "(8.334471554600944, tensor([8.7185], grad_fn=<UnbindBackward0>))\n",
      "(7.541683099882111, tensor([6.6568], grad_fn=<UnbindBackward0>))\n",
      "(8.944811104165534, tensor([6.0856], grad_fn=<UnbindBackward0>))\n",
      "(7.488293515159428, tensor([7.9975], grad_fn=<UnbindBackward0>))\n",
      "(8.225503097566918, tensor([8.8217], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([8.9544], grad_fn=<UnbindBackward0>))\n",
      "(8.847647355711887, tensor([6.9235], grad_fn=<UnbindBackward0>))\n",
      "(9.311180686902254, tensor([6.2915], grad_fn=<UnbindBackward0>))\n",
      "(6.70073110954781, tensor([8.6170], grad_fn=<UnbindBackward0>))\n",
      "(8.526549286340263, tensor([5.9470], grad_fn=<UnbindBackward0>))\n",
      "(7.475905969367397, tensor([8.4602], grad_fn=<UnbindBackward0>))\n",
      "(6.541029999189903, tensor([8.8069], grad_fn=<UnbindBackward0>))\n",
      "(9.246575864558277, tensor([6.1950], grad_fn=<UnbindBackward0>))\n",
      "(7.627057417018934, tensor([8.7258], grad_fn=<UnbindBackward0>))\n",
      "(7.465082736399547, tensor([8.3115], grad_fn=<UnbindBackward0>))\n",
      "(8.412277021466677, tensor([6.8078], grad_fn=<UnbindBackward0>))\n",
      "(9.666118293957936, tensor([7.8716], grad_fn=<UnbindBackward0>))\n",
      "(8.602453035367061, tensor([8.7700], grad_fn=<UnbindBackward0>))\n",
      "(8.982686665184087, tensor([5.8794], grad_fn=<UnbindBackward0>))\n",
      "(7.768533300926033, tensor([9.4463], grad_fn=<UnbindBackward0>))\n",
      "(8.503499864284235, tensor([6.3974], grad_fn=<UnbindBackward0>))\n",
      "(7.142036574706803, tensor([7.4487], grad_fn=<UnbindBackward0>))\n",
      "(7.816416983691801, tensor([7.4434], grad_fn=<UnbindBackward0>))\n",
      "(8.37309184744198, tensor([8.8223], grad_fn=<UnbindBackward0>))\n",
      "(8.262300941787448, tensor([6.7790], grad_fn=<UnbindBackward0>))\n",
      "(8.455955881945048, tensor([7.6419], grad_fn=<UnbindBackward0>))\n",
      "(8.916774356365426, tensor([6.0206], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([7.8359], grad_fn=<UnbindBackward0>))\n",
      "(7.451241684987676, tensor([8.7599], grad_fn=<UnbindBackward0>))\n",
      "(8.344267356262645, tensor([8.8247], grad_fn=<UnbindBackward0>))\n",
      "(9.057305735807121, tensor([6.2934], grad_fn=<UnbindBackward0>))\n",
      "(8.814627555310697, tensor([6.7122], grad_fn=<UnbindBackward0>))\n",
      "(8.282735880201754, tensor([8.7874], grad_fn=<UnbindBackward0>))\n",
      "(7.068172000388042, tensor([10.0640], grad_fn=<UnbindBackward0>))\n",
      "(7.029972911706386, tensor([6.6241], grad_fn=<UnbindBackward0>))\n",
      "(9.526974266377731, tensor([7.3844], grad_fn=<UnbindBackward0>))\n",
      "(9.699656312254291, tensor([8.5026], grad_fn=<UnbindBackward0>))\n",
      "(8.983063289380366, tensor([9.0172], grad_fn=<UnbindBackward0>))\n",
      "(9.275378768155404, tensor([9.5814], grad_fn=<UnbindBackward0>))\n",
      "(7.753623546559746, tensor([9.0942], grad_fn=<UnbindBackward0>))\n",
      "(7.024649030453636, tensor([7.8632], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([6.2487], grad_fn=<UnbindBackward0>))\n",
      "(6.598509028614515, tensor([8.8111], grad_fn=<UnbindBackward0>))\n",
      "(8.81744592104187, tensor([8.0087], grad_fn=<UnbindBackward0>))\n",
      "(7.411556287811163, tensor([8.2587], grad_fn=<UnbindBackward0>))\n",
      "(8.496990484098719, tensor([7.9063], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([6.3144], grad_fn=<UnbindBackward0>))\n",
      "(7.510977752014095, tensor([7.2366], grad_fn=<UnbindBackward0>))\n",
      "(8.533263371593732, tensor([6.3136], grad_fn=<UnbindBackward0>))\n",
      "(8.245121966478605, tensor([6.7339], grad_fn=<UnbindBackward0>))\n",
      "(9.153875834995056, tensor([6.3253], grad_fn=<UnbindBackward0>))\n",
      "(8.528330935826693, tensor([8.2127], grad_fn=<UnbindBackward0>))\n",
      "(8.668196064952765, tensor([7.0982], grad_fn=<UnbindBackward0>))\n",
      "(8.444407421690585, tensor([9.4171], grad_fn=<UnbindBackward0>))\n",
      "(9.60918360377546, tensor([6.7877], grad_fn=<UnbindBackward0>))\n",
      "(6.436150368369428, tensor([7.8354], grad_fn=<UnbindBackward0>))\n",
      "(7.757051142032013, tensor([7.7948], grad_fn=<UnbindBackward0>))\n",
      "(9.326522126265592, tensor([7.1708], grad_fn=<UnbindBackward0>))\n",
      "(9.517898598432753, tensor([7.2522], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([8.5957], grad_fn=<UnbindBackward0>))\n",
      "(8.315077007294104, tensor([8.4891], grad_fn=<UnbindBackward0>))\n",
      "(5.948034989180646, tensor([8.6922], grad_fn=<UnbindBackward0>))\n",
      "(6.859614903654202, tensor([7.3374], grad_fn=<UnbindBackward0>))\n",
      "(6.558197802812269, tensor([5.7103], grad_fn=<UnbindBackward0>))\n",
      "(9.36314729221994, tensor([6.2847], grad_fn=<UnbindBackward0>))\n",
      "(7.488293515159428, tensor([8.7437], grad_fn=<UnbindBackward0>))\n",
      "(7.949091499830517, tensor([8.3952], grad_fn=<UnbindBackward0>))\n",
      "(7.969357742016346, tensor([8.3846], grad_fn=<UnbindBackward0>))\n",
      "(6.964135612418245, tensor([8.7169], grad_fn=<UnbindBackward0>))\n",
      "(8.5814816812986, tensor([6.2583], grad_fn=<UnbindBackward0>))\n",
      "(7.452402451223638, tensor([7.3502], grad_fn=<UnbindBackward0>))\n",
      "(8.892336539638013, tensor([6.3035], grad_fn=<UnbindBackward0>))\n",
      "(7.613818684808629, tensor([7.3781], grad_fn=<UnbindBackward0>))\n",
      "(7.995306620290822, tensor([6.7481], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([8.7683], grad_fn=<UnbindBackward0>))\n",
      "(8.876684166614451, tensor([10.1447], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([7.1889], grad_fn=<UnbindBackward0>))\n",
      "(7.416378479192928, tensor([6.3032], grad_fn=<UnbindBackward0>))\n",
      "(8.393216011596527, tensor([6.3855], grad_fn=<UnbindBackward0>))\n",
      "(8.759825595314295, tensor([6.2696], grad_fn=<UnbindBackward0>))\n",
      "(9.411647228681218, tensor([6.2081], grad_fn=<UnbindBackward0>))\n",
      "(7.504391559161238, tensor([8.4558], grad_fn=<UnbindBackward0>))\n",
      "(8.401333305321703, tensor([9.3348], grad_fn=<UnbindBackward0>))\n",
      "(7.356279876550748, tensor([8.0747], grad_fn=<UnbindBackward0>))\n",
      "(8.693664334532016, tensor([9.1049], grad_fn=<UnbindBackward0>))\n",
      "(7.796880342783522, tensor([7.8061], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([6.2598], grad_fn=<UnbindBackward0>))\n",
      "(9.188299241099303, tensor([7.3758], grad_fn=<UnbindBackward0>))\n",
      "(8.815666824946216, tensor([6.9916], grad_fn=<UnbindBackward0>))\n",
      "(7.658227526161352, tensor([7.4845], grad_fn=<UnbindBackward0>))\n",
      "(7.451241684987676, tensor([7.3756], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([9.1584], grad_fn=<UnbindBackward0>))\n",
      "(8.52595469708481, tensor([8.0351], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([7.7732], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([8.4586], grad_fn=<UnbindBackward0>))\n",
      "(9.15302900526307, tensor([8.0763], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([9.2657], grad_fn=<UnbindBackward0>))\n",
      "(9.31271643859305, tensor([6.2681], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([9.3768], grad_fn=<UnbindBackward0>))\n",
      "(6.7357800142423265, tensor([8.7659], grad_fn=<UnbindBackward0>))\n",
      "(8.675904882571059, tensor([10.2340], grad_fn=<UnbindBackward0>))\n",
      "(8.195885391314796, tensor([9.2181], grad_fn=<UnbindBackward0>))\n",
      "(7.763446388727362, tensor([7.8640], grad_fn=<UnbindBackward0>))\n",
      "(8.450839690866216, tensor([7.8471], grad_fn=<UnbindBackward0>))\n",
      "(9.270400128403846, tensor([7.6407], grad_fn=<UnbindBackward0>))\n",
      "(8.15162164696975, tensor([7.5147], grad_fn=<UnbindBackward0>))\n",
      "(7.203405521083095, tensor([6.7308], grad_fn=<UnbindBackward0>))\n",
      "(7.117205503164344, tensor([7.1662], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([6.6635], grad_fn=<UnbindBackward0>))\n",
      "(7.3987862754199485, tensor([6.8430], grad_fn=<UnbindBackward0>))\n",
      "(7.875499292445208, tensor([9.8774], grad_fn=<UnbindBackward0>))\n",
      "(9.618003063627834, tensor([10.1909], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([6.3376], grad_fn=<UnbindBackward0>))\n",
      "(8.208764045819667, tensor([6.9460], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([7.2526], grad_fn=<UnbindBackward0>))\n",
      "(7.768110378525988, tensor([6.8104], grad_fn=<UnbindBackward0>))\n",
      "(8.338066525518801, tensor([7.9554], grad_fn=<UnbindBackward0>))\n",
      "(7.051855622955894, tensor([7.1799], grad_fn=<UnbindBackward0>))\n",
      "(8.421122722665503, tensor([10.0313], grad_fn=<UnbindBackward0>))\n",
      "(8.647870515057853, tensor([7.7197], grad_fn=<UnbindBackward0>))\n",
      "(8.740496729931813, tensor([7.7608], grad_fn=<UnbindBackward0>))\n",
      "(9.690974793577944, tensor([7.3416], grad_fn=<UnbindBackward0>))\n",
      "(8.507142855562735, tensor([7.8003], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([9.2320], grad_fn=<UnbindBackward0>))\n",
      "(6.93537044601511, tensor([6.2032], grad_fn=<UnbindBackward0>))\n",
      "(8.923857580099885, tensor([8.6462], grad_fn=<UnbindBackward0>))\n",
      "(7.759614150696903, tensor([8.9675], grad_fn=<UnbindBackward0>))\n",
      "(8.734721003944811, tensor([7.9071], grad_fn=<UnbindBackward0>))\n",
      "(9.31982238259318, tensor([9.0971], grad_fn=<UnbindBackward0>))\n",
      "(7.1316985104669115, tensor([6.7939], grad_fn=<UnbindBackward0>))\n",
      "(7.653494909661253, tensor([7.7534], grad_fn=<UnbindBackward0>))\n",
      "(8.352790135124629, tensor([7.1224], grad_fn=<UnbindBackward0>))\n",
      "(7.369600720526409, tensor([6.3096], grad_fn=<UnbindBackward0>))\n",
      "(6.532334292222349, tensor([8.8764], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([7.3065], grad_fn=<UnbindBackward0>))\n",
      "(8.641532465671846, tensor([6.2053], grad_fn=<UnbindBackward0>))\n",
      "(8.317033476492403, tensor([6.3587], grad_fn=<UnbindBackward0>))\n",
      "(9.00565049932022, tensor([6.2211], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([7.8482], grad_fn=<UnbindBackward0>))\n",
      "(9.629774129311743, tensor([6.0031], grad_fn=<UnbindBackward0>))\n",
      "(8.464425125877582, tensor([6.8754], grad_fn=<UnbindBackward0>))\n",
      "(9.271811694064283, tensor([8.7068], grad_fn=<UnbindBackward0>))\n",
      "(9.13043098874788, tensor([7.2671], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([7.8796], grad_fn=<UnbindBackward0>))\n",
      "(6.558197802812269, tensor([9.3527], grad_fn=<UnbindBackward0>))\n",
      "(6.431331081933479, tensor([9.3945], grad_fn=<UnbindBackward0>))\n",
      "(9.252824983582338, tensor([9.2823], grad_fn=<UnbindBackward0>))\n",
      "(9.305923241869912, tensor([6.4246], grad_fn=<UnbindBackward0>))\n",
      "(7.241366283322318, tensor([6.9997], grad_fn=<UnbindBackward0>))\n",
      "(9.35444071594994, tensor([8.1719], grad_fn=<UnbindBackward0>))\n",
      "(9.284798282894192, tensor([6.3600], grad_fn=<UnbindBackward0>))\n",
      "(8.908694592507015, tensor([8.0180], grad_fn=<UnbindBackward0>))\n",
      "(8.121183242078828, tensor([9.6450], grad_fn=<UnbindBackward0>))\n",
      "(8.636752426473876, tensor([8.2953], grad_fn=<UnbindBackward0>))\n",
      "(8.230843564198235, tensor([7.2574], grad_fn=<UnbindBackward0>))\n",
      "(8.112227958349724, tensor([8.8201], grad_fn=<UnbindBackward0>))\n",
      "(8.799510901368867, tensor([6.3229], grad_fn=<UnbindBackward0>))\n",
      "(8.283999304248526, tensor([6.6256], grad_fn=<UnbindBackward0>))\n",
      "(9.113609183016411, tensor([8.1866], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([8.5827], grad_fn=<UnbindBackward0>))\n",
      "(6.994849985833071, tensor([8.6032], grad_fn=<UnbindBackward0>))\n",
      "(6.98933526597456, tensor([7.8142], grad_fn=<UnbindBackward0>))\n",
      "(9.835904401212332, tensor([6.3212], grad_fn=<UnbindBackward0>))\n",
      "(8.821879862683842, tensor([8.8007], grad_fn=<UnbindBackward0>))\n",
      "(6.802394763324311, tensor([8.4199], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([7.9984], grad_fn=<UnbindBackward0>))\n",
      "(7.725330037917135, tensor([6.8539], grad_fn=<UnbindBackward0>))\n",
      "(8.719644119505357, tensor([6.2031], grad_fn=<UnbindBackward0>))\n",
      "(8.542665987389269, tensor([8.2531], grad_fn=<UnbindBackward0>))\n",
      "(6.352629396319567, tensor([8.7950], grad_fn=<UnbindBackward0>))\n",
      "(6.699500340161678, tensor([6.2639], grad_fn=<UnbindBackward0>))\n",
      "(8.695004592732307, tensor([6.2664], grad_fn=<UnbindBackward0>))\n",
      "(6.587550014824796, tensor([10.1022], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([7.3961], grad_fn=<UnbindBackward0>))\n",
      "(6.424869023905388, tensor([8.1347], grad_fn=<UnbindBackward0>))\n",
      "(6.844815479208263, tensor([8.9248], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([6.2201], grad_fn=<UnbindBackward0>))\n",
      "(7.132497551660044, tensor([7.7263], grad_fn=<UnbindBackward0>))\n",
      "(8.121480374750751, tensor([9.0689], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([7.1742], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([8.4627], grad_fn=<UnbindBackward0>))\n",
      "(8.35936910622267, tensor([9.2425], grad_fn=<UnbindBackward0>))\n",
      "(7.8528278122817445, tensor([6.7241], grad_fn=<UnbindBackward0>))\n",
      "(6.975413927455952, tensor([8.3330], grad_fn=<UnbindBackward0>))\n",
      "(7.8674885686991285, tensor([9.0521], grad_fn=<UnbindBackward0>))\n",
      "(6.943122422819428, tensor([8.9325], grad_fn=<UnbindBackward0>))\n",
      "(9.084890521258767, tensor([9.7873], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.3222], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([8.4334], grad_fn=<UnbindBackward0>))\n",
      "(8.909910726701899, tensor([7.2289], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([6.3641], grad_fn=<UnbindBackward0>))\n",
      "(6.543911845564792, tensor([6.2415], grad_fn=<UnbindBackward0>))\n",
      "(7.748028524432376, tensor([10.3140], grad_fn=<UnbindBackward0>))\n",
      "(6.19644412779452, tensor([9.7729], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([8.4661], grad_fn=<UnbindBackward0>))\n",
      "(9.108086137937427, tensor([9.5697], grad_fn=<UnbindBackward0>))\n",
      "(8.257644958208228, tensor([9.0675], grad_fn=<UnbindBackward0>))\n",
      "(6.230481447578482, tensor([7.2491], grad_fn=<UnbindBackward0>))\n",
      "(6.880384082186005, tensor([7.3365], grad_fn=<UnbindBackward0>))\n",
      "(8.385032287813898, tensor([8.4956], grad_fn=<UnbindBackward0>))\n",
      "(7.037905963447182, tensor([6.6396], grad_fn=<UnbindBackward0>))\n",
      "(8.65259782842244, tensor([7.3070], grad_fn=<UnbindBackward0>))\n",
      "(8.548110294050959, tensor([8.9021], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([6.4432], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([9.1297], grad_fn=<UnbindBackward0>))\n",
      "(8.3654396361887, tensor([7.8423], grad_fn=<UnbindBackward0>))\n",
      "(6.950814768442584, tensor([9.8883], grad_fn=<UnbindBackward0>))\n",
      "(6.037870919922137, tensor([6.6799], grad_fn=<UnbindBackward0>))\n",
      "(8.840145877949938, tensor([8.6567], grad_fn=<UnbindBackward0>))\n",
      "(7.7102051944325325, tensor([7.8103], grad_fn=<UnbindBackward0>))\n",
      "(8.492695559815838, tensor([6.3687], grad_fn=<UnbindBackward0>))\n",
      "(8.764990330169104, tensor([6.4303], grad_fn=<UnbindBackward0>))\n",
      "(9.010058489805235, tensor([9.4330], grad_fn=<UnbindBackward0>))\n",
      "(9.535679435605061, tensor([6.3941], grad_fn=<UnbindBackward0>))\n",
      "(8.640118538253535, tensor([8.5149], grad_fn=<UnbindBackward0>))\n",
      "(8.556029215201436, tensor([10.1252], grad_fn=<UnbindBackward0>))\n",
      "(6.517671272912275, tensor([6.9712], grad_fn=<UnbindBackward0>))\n",
      "(7.606387389772652, tensor([5.9401], grad_fn=<UnbindBackward0>))\n",
      "(7.755767170102998, tensor([7.6595], grad_fn=<UnbindBackward0>))\n",
      "(8.879333174786028, tensor([6.3299], grad_fn=<UnbindBackward0>))\n",
      "(8.353025845202325, tensor([7.3220], grad_fn=<UnbindBackward0>))\n",
      "(8.046229101075378, tensor([6.4979], grad_fn=<UnbindBackward0>))\n",
      "(7.418780882750794, tensor([7.2360], grad_fn=<UnbindBackward0>))\n",
      "(7.3938782901077555, tensor([7.7926], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([9.5918], grad_fn=<UnbindBackward0>))\n",
      "(8.676757761087575, tensor([6.0382], grad_fn=<UnbindBackward0>))\n",
      "(8.59877317840866, tensor([7.0314], grad_fn=<UnbindBackward0>))\n",
      "(7.165493475060845, tensor([6.1792], grad_fn=<UnbindBackward0>))\n",
      "(9.725436862592819, tensor([9.8869], grad_fn=<UnbindBackward0>))\n",
      "(7.56164174558878, tensor([8.7789], grad_fn=<UnbindBackward0>))\n",
      "(8.646465527120377, tensor([7.7582], grad_fn=<UnbindBackward0>))\n",
      "(9.499721001063545, tensor([6.4291], grad_fn=<UnbindBackward0>))\n",
      "(7.726212650507529, tensor([7.6556], grad_fn=<UnbindBackward0>))\n",
      "(7.845024417241484, tensor([6.4432], grad_fn=<UnbindBackward0>))\n",
      "(9.635869511120088, tensor([6.2588], grad_fn=<UnbindBackward0>))\n",
      "(8.038512020976814, tensor([10.2821], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([6.0139], grad_fn=<UnbindBackward0>))\n",
      "(6.928537818164665, tensor([8.4949], grad_fn=<UnbindBackward0>))\n",
      "(8.878497403738631, tensor([6.7599], grad_fn=<UnbindBackward0>))\n",
      "(7.7279755421055585, tensor([6.2076], grad_fn=<UnbindBackward0>))\n",
      "(7.643482907077201, tensor([10.0870], grad_fn=<UnbindBackward0>))\n",
      "(7.018401799069201, tensor([8.5258], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([10.0085], grad_fn=<UnbindBackward0>))\n",
      "(6.960347729101308, tensor([6.2509], grad_fn=<UnbindBackward0>))\n",
      "(8.369388996647842, tensor([8.7780], grad_fn=<UnbindBackward0>))\n",
      "(8.145549631783584, tensor([7.6661], grad_fn=<UnbindBackward0>))\n",
      "(8.629271094821588, tensor([7.9070], grad_fn=<UnbindBackward0>))\n",
      "(8.147867129923947, tensor([9.7827], grad_fn=<UnbindBackward0>))\n",
      "(7.849323818040561, tensor([6.5432], grad_fn=<UnbindBackward0>))\n",
      "(6.872128101338986, tensor([6.1716], grad_fn=<UnbindBackward0>))\n",
      "(6.593044534142437, tensor([6.2553], grad_fn=<UnbindBackward0>))\n",
      "(6.416732282512326, tensor([7.1836], grad_fn=<UnbindBackward0>))\n",
      "(6.877296071497429, tensor([6.3312], grad_fn=<UnbindBackward0>))\n",
      "(7.024649030453636, tensor([9.6776], grad_fn=<UnbindBackward0>))\n",
      "(7.793174347189205, tensor([8.1257], grad_fn=<UnbindBackward0>))\n",
      "(7.170119543449628, tensor([10.1738], grad_fn=<UnbindBackward0>))\n",
      "(7.8632667240095735, tensor([8.6590], grad_fn=<UnbindBackward0>))\n",
      "(9.52937575208596, tensor([9.7111], grad_fn=<UnbindBackward0>))\n",
      "(5.926926025970411, tensor([8.5209], grad_fn=<UnbindBackward0>))\n",
      "(6.810142450115136, tensor([7.1738], grad_fn=<UnbindBackward0>))\n",
      "(7.503840746698951, tensor([6.3916], grad_fn=<UnbindBackward0>))\n",
      "(7.802618063442671, tensor([7.1859], grad_fn=<UnbindBackward0>))\n",
      "(9.13075589263334, tensor([7.4687], grad_fn=<UnbindBackward0>))\n",
      "(9.057305735807121, tensor([7.6376], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([6.7024], grad_fn=<UnbindBackward0>))\n",
      "(7.652070746116482, tensor([8.1737], grad_fn=<UnbindBackward0>))\n",
      "(8.481980435660493, tensor([6.4663], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([7.9569], grad_fn=<UnbindBackward0>))\n",
      "(6.633318433280377, tensor([9.4295], grad_fn=<UnbindBackward0>))\n",
      "(7.857093864902493, tensor([6.2977], grad_fn=<UnbindBackward0>))\n",
      "(6.1903154058531475, tensor([6.5155], grad_fn=<UnbindBackward0>))\n",
      "(8.488999457045455, tensor([7.4132], grad_fn=<UnbindBackward0>))\n",
      "(7.616283561580385, tensor([7.3077], grad_fn=<UnbindBackward0>))\n",
      "(9.199986959896691, tensor([8.6589], grad_fn=<UnbindBackward0>))\n",
      "(8.328934041955529, tensor([8.7787], grad_fn=<UnbindBackward0>))\n",
      "(9.117896081584902, tensor([8.8615], grad_fn=<UnbindBackward0>))\n",
      "(6.882437470997847, tensor([8.4841], grad_fn=<UnbindBackward0>))\n",
      "(7.045776576879511, tensor([6.6744], grad_fn=<UnbindBackward0>))\n",
      "(7.519149957669823, tensor([8.2365], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([8.6572], grad_fn=<UnbindBackward0>))\n",
      "(7.8434564043761155, tensor([6.2634], grad_fn=<UnbindBackward0>))\n",
      "(8.550241045462437, tensor([8.2551], grad_fn=<UnbindBackward0>))\n",
      "(8.60392119492606, tensor([6.7918], grad_fn=<UnbindBackward0>))\n",
      "(6.297109319933935, tensor([7.8908], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([8.3602], grad_fn=<UnbindBackward0>))\n",
      "(7.163946684342547, tensor([8.6524], grad_fn=<UnbindBackward0>))\n",
      "(8.059276223305648, tensor([8.7186], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([6.6765], grad_fn=<UnbindBackward0>))\n",
      "(8.327000740241713, tensor([6.2501], grad_fn=<UnbindBackward0>))\n",
      "(7.455298485683291, tensor([8.1309], grad_fn=<UnbindBackward0>))\n",
      "(8.314097335405807, tensor([8.3776], grad_fn=<UnbindBackward0>))\n",
      "(7.968665700466235, tensor([6.3299], grad_fn=<UnbindBackward0>))\n",
      "(6.917705609835305, tensor([7.8426], grad_fn=<UnbindBackward0>))\n",
      "(9.33917324381915, tensor([6.7398], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([7.4318], grad_fn=<UnbindBackward0>))\n",
      "(6.12029741895095, tensor([6.3542], grad_fn=<UnbindBackward0>))\n",
      "(9.439386468943006, tensor([9.5284], grad_fn=<UnbindBackward0>))\n",
      "(8.38160253710989, tensor([8.9531], grad_fn=<UnbindBackward0>))\n",
      "(7.170888478512505, tensor([9.2667], grad_fn=<UnbindBackward0>))\n",
      "(8.967121656230923, tensor([6.3385], grad_fn=<UnbindBackward0>))\n",
      "(8.711278615130434, tensor([7.3486], grad_fn=<UnbindBackward0>))\n",
      "(8.712266432135355, tensor([5.9005], grad_fn=<UnbindBackward0>))\n",
      "(8.748463629942055, tensor([8.8987], grad_fn=<UnbindBackward0>))\n",
      "(8.90557995798965, tensor([8.4553], grad_fn=<UnbindBackward0>))\n",
      "(7.411556287811163, tensor([7.5986], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([8.1815], grad_fn=<UnbindBackward0>))\n",
      "(8.432288684325794, tensor([6.6388], grad_fn=<UnbindBackward0>))\n",
      "(7.6676260915849905, tensor([9.4876], grad_fn=<UnbindBackward0>))\n",
      "(7.1316985104669115, tensor([7.0405], grad_fn=<UnbindBackward0>))\n",
      "(7.559038255443384, tensor([6.1699], grad_fn=<UnbindBackward0>))\n",
      "(6.767343125265392, tensor([6.1906], grad_fn=<UnbindBackward0>))\n",
      "(6.61338421837956, tensor([9.4954], grad_fn=<UnbindBackward0>))\n",
      "(6.726233402358747, tensor([8.6109], grad_fn=<UnbindBackward0>))\n",
      "(7.697575346802343, tensor([7.1262], grad_fn=<UnbindBackward0>))\n",
      "(6.844815479208263, tensor([8.3985], grad_fn=<UnbindBackward0>))\n",
      "(8.618485442898109, tensor([9.7659], grad_fn=<UnbindBackward0>))\n",
      "(9.667448713236185, tensor([6.2287], grad_fn=<UnbindBackward0>))\n",
      "(8.470311205516108, tensor([6.4682], grad_fn=<UnbindBackward0>))\n",
      "(6.317164686747284, tensor([9.5466], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([7.1224], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([8.3765], grad_fn=<UnbindBackward0>))\n",
      "(9.725197856855544, tensor([9.5711], grad_fn=<UnbindBackward0>))\n",
      "(6.159095388491933, tensor([7.3784], grad_fn=<UnbindBackward0>))\n",
      "(7.6093665379542115, tensor([6.1859], grad_fn=<UnbindBackward0>))\n",
      "(7.6787889981991535, tensor([8.9603], grad_fn=<UnbindBackward0>))\n",
      "(6.493753839851686, tensor([7.1858], grad_fn=<UnbindBackward0>))\n",
      "(8.630521876723241, tensor([6.3696], grad_fn=<UnbindBackward0>))\n",
      "(6.652863029353347, tensor([6.8048], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([6.6684], grad_fn=<UnbindBackward0>))\n",
      "(8.44741429680832, tensor([8.5568], grad_fn=<UnbindBackward0>))\n",
      "(7.039660349862076, tensor([8.5475], grad_fn=<UnbindBackward0>))\n",
      "(7.845416036592485, tensor([8.4350], grad_fn=<UnbindBackward0>))\n",
      "(7.96311205897929, tensor([8.1874], grad_fn=<UnbindBackward0>))\n",
      "(9.189423123720795, tensor([10.2942], grad_fn=<UnbindBackward0>))\n",
      "(8.052296499538647, tensor([9.3958], grad_fn=<UnbindBackward0>))\n",
      "(9.202409001649903, tensor([6.3026], grad_fn=<UnbindBackward0>))\n",
      "(7.069874128458572, tensor([6.2090], grad_fn=<UnbindBackward0>))\n",
      "(7.621684998724611, tensor([7.5930], grad_fn=<UnbindBackward0>))\n",
      "(9.026657889542886, tensor([6.2247], grad_fn=<UnbindBackward0>))\n",
      "(9.117896081584902, tensor([8.4799], grad_fn=<UnbindBackward0>))\n",
      "(7.271008538280992, tensor([6.7319], grad_fn=<UnbindBackward0>))\n",
      "(8.655388690167637, tensor([7.9265], grad_fn=<UnbindBackward0>))\n",
      "(8.883501584323207, tensor([9.3381], grad_fn=<UnbindBackward0>))\n",
      "(8.433159195806228, tensor([6.2549], grad_fn=<UnbindBackward0>))\n",
      "(6.396929655216146, tensor([6.8315], grad_fn=<UnbindBackward0>))\n",
      "(7.790282380703483, tensor([6.9041], grad_fn=<UnbindBackward0>))\n",
      "(9.337941716569905, tensor([7.1974], grad_fn=<UnbindBackward0>))\n",
      "(8.937218450855731, tensor([6.3580], grad_fn=<UnbindBackward0>))\n",
      "(8.543445562560303, tensor([8.8955], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([7.9345], grad_fn=<UnbindBackward0>))\n",
      "(9.09963224999176, tensor([6.6699], grad_fn=<UnbindBackward0>))\n",
      "(8.284504227258497, tensor([6.7745], grad_fn=<UnbindBackward0>))\n",
      "(8.524962928680598, tensor([8.4193], grad_fn=<UnbindBackward0>))\n",
      "(8.912203841620542, tensor([6.0296], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([10.6003], grad_fn=<UnbindBackward0>))\n",
      "(8.64241515616962, tensor([9.3606], grad_fn=<UnbindBackward0>))\n",
      "(9.014082149443356, tensor([8.5461], grad_fn=<UnbindBackward0>))\n",
      "(8.374246182096304, tensor([6.2869], grad_fn=<UnbindBackward0>))\n",
      "(6.884486652042782, tensor([6.3364], grad_fn=<UnbindBackward0>))\n",
      "(8.45638105201948, tensor([7.2075], grad_fn=<UnbindBackward0>))\n",
      "(7.0909098220799835, tensor([5.9217], grad_fn=<UnbindBackward0>))\n",
      "(6.595780513961311, tensor([6.3745], grad_fn=<UnbindBackward0>))\n",
      "(8.181720455128108, tensor([9.4567], grad_fn=<UnbindBackward0>))\n",
      "(9.069813136839207, tensor([9.0970], grad_fn=<UnbindBackward0>))\n",
      "(8.336630087637147, tensor([7.2988], grad_fn=<UnbindBackward0>))\n",
      "(9.41156545918937, tensor([6.7550], grad_fn=<UnbindBackward0>))\n",
      "(6.651571873589727, tensor([7.4106], grad_fn=<UnbindBackward0>))\n",
      "(7.557472901614746, tensor([9.1158], grad_fn=<UnbindBackward0>))\n",
      "(6.683360945766275, tensor([8.6945], grad_fn=<UnbindBackward0>))\n",
      "(8.647694999480395, tensor([9.4922], grad_fn=<UnbindBackward0>))\n",
      "(6.835184586147301, tensor([8.9005], grad_fn=<UnbindBackward0>))\n",
      "(8.090708716083997, tensor([8.8444], grad_fn=<UnbindBackward0>))\n",
      "(7.887959336599945, tensor([6.3836], grad_fn=<UnbindBackward0>))\n",
      "(7.610357618312838, tensor([7.1904], grad_fn=<UnbindBackward0>))\n",
      "(8.98744678941718, tensor([6.3968], grad_fn=<UnbindBackward0>))\n",
      "(9.362460420037015, tensor([9.1501], grad_fn=<UnbindBackward0>))\n",
      "(6.073044534100405, tensor([8.7903], grad_fn=<UnbindBackward0>))\n",
      "(7.52131798019924, tensor([7.1545], grad_fn=<UnbindBackward0>))\n",
      "(8.419580362549237, tensor([9.9093], grad_fn=<UnbindBackward0>))\n",
      "(6.272877006546167, tensor([9.0567], grad_fn=<UnbindBackward0>))\n",
      "(8.776475789346321, tensor([7.2612], grad_fn=<UnbindBackward0>))\n",
      "(8.635153989049803, tensor([8.2981], grad_fn=<UnbindBackward0>))\n",
      "(9.801897696774494, tensor([8.7436], grad_fn=<UnbindBackward0>))\n",
      "(8.319229938632326, tensor([6.7557], grad_fn=<UnbindBackward0>))\n",
      "(7.088408778675395, tensor([8.0057], grad_fn=<UnbindBackward0>))\n",
      "(7.779885115070522, tensor([6.5084], grad_fn=<UnbindBackward0>))\n",
      "(8.86177531100083, tensor([6.1735], grad_fn=<UnbindBackward0>))\n",
      "(7.214504414151143, tensor([7.1289], grad_fn=<UnbindBackward0>))\n",
      "(6.501289670540389, tensor([8.4635], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([8.6634], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([6.6699], grad_fn=<UnbindBackward0>))\n",
      "(6.437751649736401, tensor([7.6901], grad_fn=<UnbindBackward0>))\n",
      "(6.980075940561763, tensor([7.8739], grad_fn=<UnbindBackward0>))\n",
      "(9.215725844252521, tensor([6.7869], grad_fn=<UnbindBackward0>))\n",
      "(8.547334348328224, tensor([8.5011], grad_fn=<UnbindBackward0>))\n",
      "(6.45833828334479, tensor([7.9108], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([6.6527], grad_fn=<UnbindBackward0>))\n",
      "(8.449556542700426, tensor([6.7459], grad_fn=<UnbindBackward0>))\n",
      "(6.92461239604856, tensor([6.2325], grad_fn=<UnbindBackward0>))\n",
      "(9.191565216649849, tensor([6.3205], grad_fn=<UnbindBackward0>))\n",
      "(7.628517626575055, tensor([8.4615], grad_fn=<UnbindBackward0>))\n",
      "(8.505120610181969, tensor([8.6830], grad_fn=<UnbindBackward0>))\n",
      "(8.822911626354117, tensor([6.8520], grad_fn=<UnbindBackward0>))\n",
      "(8.219056661060598, tensor([9.6052], grad_fn=<UnbindBackward0>))\n",
      "(8.173857454773621, tensor([10.0585], grad_fn=<UnbindBackward0>))\n",
      "(8.014666370464942, tensor([8.4217], grad_fn=<UnbindBackward0>))\n",
      "(7.238496840894365, tensor([8.4583], grad_fn=<UnbindBackward0>))\n",
      "(7.789454566086673, tensor([8.3105], grad_fn=<UnbindBackward0>))\n",
      "(8.146998697389993, tensor([5.9934], grad_fn=<UnbindBackward0>))\n",
      "(7.234177179749849, tensor([7.3154], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([9.3665], grad_fn=<UnbindBackward0>))\n",
      "(6.823286122355687, tensor([8.1390], grad_fn=<UnbindBackward0>))\n",
      "(8.667335849845957, tensor([8.4694], grad_fn=<UnbindBackward0>))\n",
      "(8.792245847467877, tensor([7.7070], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([7.2983], grad_fn=<UnbindBackward0>))\n",
      "(7.134093721192866, tensor([6.4737], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([8.3821], grad_fn=<UnbindBackward0>))\n",
      "(8.423102268016642, tensor([6.3205], grad_fn=<UnbindBackward0>))\n",
      "(8.822911626354117, tensor([7.2777], grad_fn=<UnbindBackward0>))\n",
      "(9.250618218474752, tensor([6.3419], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([7.1986], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([6.4726], grad_fn=<UnbindBackward0>))\n",
      "(8.497602541651233, tensor([7.1794], grad_fn=<UnbindBackward0>))\n",
      "(7.203405521083095, tensor([6.8520], grad_fn=<UnbindBackward0>))\n",
      "(8.339023005744759, tensor([6.6017], grad_fn=<UnbindBackward0>))\n",
      "(9.009691898489343, tensor([6.7903], grad_fn=<UnbindBackward0>))\n",
      "(8.448700194970938, tensor([8.5519], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([10.0761], grad_fn=<UnbindBackward0>))\n",
      "(8.883778861463485, tensor([6.9650], grad_fn=<UnbindBackward0>))\n",
      "(8.957639268419648, tensor([6.2783], grad_fn=<UnbindBackward0>))\n",
      "(8.756997183623554, tensor([10.0204], grad_fn=<UnbindBackward0>))\n",
      "(8.326032685955079, tensor([8.2597], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([7.1840], grad_fn=<UnbindBackward0>))\n",
      "(6.486160788944089, tensor([7.5313], grad_fn=<UnbindBackward0>))\n",
      "(6.9167150203536085, tensor([7.0814], grad_fn=<UnbindBackward0>))\n",
      "(8.061802274538348, tensor([7.3102], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([7.4928], grad_fn=<UnbindBackward0>))\n",
      "(7.61972421378267, tensor([6.2191], grad_fn=<UnbindBackward0>))\n",
      "(8.354674261918463, tensor([8.4323], grad_fn=<UnbindBackward0>))\n",
      "(6.663132695990803, tensor([7.3468], grad_fn=<UnbindBackward0>))\n",
      "(7.078341579557671, tensor([10.3003], grad_fn=<UnbindBackward0>))\n",
      "(8.376090350438238, tensor([7.3932], grad_fn=<UnbindBackward0>))\n",
      "(9.659056522078663, tensor([7.3330], grad_fn=<UnbindBackward0>))\n",
      "(9.799625913002037, tensor([6.8603], grad_fn=<UnbindBackward0>))\n",
      "(7.264730177929867, tensor([7.5862], grad_fn=<UnbindBackward0>))\n",
      "(8.689127655323706, tensor([9.2022], grad_fn=<UnbindBackward0>))\n",
      "(8.611593866837723, tensor([6.4112], grad_fn=<UnbindBackward0>))\n",
      "(6.76849321164863, tensor([9.3050], grad_fn=<UnbindBackward0>))\n",
      "(6.880384082186005, tensor([9.2862], grad_fn=<UnbindBackward0>))\n",
      "(6.403574197934815, tensor([7.3531], grad_fn=<UnbindBackward0>))\n",
      "(9.1503780308035, tensor([8.3825], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([6.4129], grad_fn=<UnbindBackward0>))\n",
      "(8.38091517312361, tensor([8.5776], grad_fn=<UnbindBackward0>))\n",
      "(8.984818995876008, tensor([7.2878], grad_fn=<UnbindBackward0>))\n",
      "(7.374001859350161, tensor([8.6372], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([8.6587], grad_fn=<UnbindBackward0>))\n",
      "(6.953684210870537, tensor([8.5275], grad_fn=<UnbindBackward0>))\n",
      "(7.4645098346365275, tensor([6.2370], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([7.3348], grad_fn=<UnbindBackward0>))\n",
      "(7.864419904994565, tensor([9.1863], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([8.5745], grad_fn=<UnbindBackward0>))\n",
      "(6.38856140554563, tensor([6.4023], grad_fn=<UnbindBackward0>))\n",
      "(7.781138509845015, tensor([6.5243], grad_fn=<UnbindBackward0>))\n",
      "(6.769641976852503, tensor([6.2937], grad_fn=<UnbindBackward0>))\n",
      "(8.624431942085836, tensor([7.8765], grad_fn=<UnbindBackward0>))\n",
      "(9.645946636996333, tensor([8.9649], grad_fn=<UnbindBackward0>))\n",
      "(7.784057002639929, tensor([7.3476], grad_fn=<UnbindBackward0>))\n",
      "(8.100464891029363, tensor([6.7306], grad_fn=<UnbindBackward0>))\n",
      "(7.83002808253384, tensor([8.2453], grad_fn=<UnbindBackward0>))\n",
      "(7.763446388727362, tensor([6.3916], grad_fn=<UnbindBackward0>))\n",
      "(6.947937068614969, tensor([6.6551], grad_fn=<UnbindBackward0>))\n",
      "(6.626717749249025, tensor([8.6798], grad_fn=<UnbindBackward0>))\n",
      "(6.558197802812269, tensor([8.5632], grad_fn=<UnbindBackward0>))\n",
      "(6.795705775173514, tensor([7.4130], grad_fn=<UnbindBackward0>))\n",
      "(6.762729506931879, tensor([9.1635], grad_fn=<UnbindBackward0>))\n",
      "(9.352534137679358, tensor([8.9094], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([7.5951], grad_fn=<UnbindBackward0>))\n",
      "(8.819813136232225, tensor([6.1702], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([8.2223], grad_fn=<UnbindBackward0>))\n",
      "(8.615408238913192, tensor([6.3807], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([8.5269], grad_fn=<UnbindBackward0>))\n",
      "(7.887208585813932, tensor([6.3587], grad_fn=<UnbindBackward0>))\n",
      "(7.793999089503996, tensor([8.5947], grad_fn=<UnbindBackward0>))\n",
      "(8.597666575566114, tensor([9.0929], grad_fn=<UnbindBackward0>))\n",
      "(9.738436006907127, tensor([6.6432], grad_fn=<UnbindBackward0>))\n",
      "(8.480529207044645, tensor([7.0773], grad_fn=<UnbindBackward0>))\n",
      "(6.3561076606958915, tensor([6.2908], grad_fn=<UnbindBackward0>))\n",
      "(7.553286605600419, tensor([7.3353], grad_fn=<UnbindBackward0>))\n",
      "(8.061802274538348, tensor([8.6603], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([9.0804], grad_fn=<UnbindBackward0>))\n",
      "(8.804325112562537, tensor([6.6482], grad_fn=<UnbindBackward0>))\n",
      "(6.584791392385716, tensor([8.7562], grad_fn=<UnbindBackward0>))\n",
      "(8.627302414096267, tensor([8.1137], grad_fn=<UnbindBackward0>))\n",
      "(7.652070746116482, tensor([8.5031], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.7773], grad_fn=<UnbindBackward0>))\n",
      "(7.540090320145325, tensor([7.6167], grad_fn=<UnbindBackward0>))\n",
      "(7.936660155225426, tensor([7.8554], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([7.8341], grad_fn=<UnbindBackward0>))\n",
      "(8.269756947532983, tensor([9.6632], grad_fn=<UnbindBackward0>))\n",
      "(9.470548467427209, tensor([7.2846], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([6.8150], grad_fn=<UnbindBackward0>))\n",
      "(9.692025505995757, tensor([7.2349], grad_fn=<UnbindBackward0>))\n",
      "(8.774312958285378, tensor([7.8610], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([7.3243], grad_fn=<UnbindBackward0>))\n",
      "(8.09925056179696, tensor([8.6769], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([9.0946], grad_fn=<UnbindBackward0>))\n",
      "(8.416488487294606, tensor([6.2967], grad_fn=<UnbindBackward0>))\n",
      "(8.74766979009724, tensor([6.6669], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([6.7651], grad_fn=<UnbindBackward0>))\n",
      "(8.512180649592693, tensor([6.5899], grad_fn=<UnbindBackward0>))\n",
      "(6.478509642208569, tensor([10.0449], grad_fn=<UnbindBackward0>))\n",
      "(7.427738840532894, tensor([6.1699], grad_fn=<UnbindBackward0>))\n",
      "(7.774015077250727, tensor([6.3750], grad_fn=<UnbindBackward0>))\n",
      "(8.534246945982066, tensor([6.2261], grad_fn=<UnbindBackward0>))\n",
      "(8.52674740422105, tensor([6.7979], grad_fn=<UnbindBackward0>))\n",
      "(9.4053313318708, tensor([7.2695], grad_fn=<UnbindBackward0>))\n",
      "(6.699500340161678, tensor([6.3916], grad_fn=<UnbindBackward0>))\n",
      "(9.150271845510064, tensor([8.9962], grad_fn=<UnbindBackward0>))\n",
      "(7.8804263442924, tensor([7.3330], grad_fn=<UnbindBackward0>))\n",
      "(8.975124239427704, tensor([7.6601], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([6.2295], grad_fn=<UnbindBackward0>))\n",
      "(9.490620114661201, tensor([7.2330], grad_fn=<UnbindBackward0>))\n",
      "(6.568077911411976, tensor([8.1645], grad_fn=<UnbindBackward0>))\n",
      "(6.202535517187923, tensor([10.1884], grad_fn=<UnbindBackward0>))\n",
      "(9.542445945729886, tensor([8.6410], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([7.6725], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([7.0888], grad_fn=<UnbindBackward0>))\n",
      "(8.336390480591552, tensor([7.9223], grad_fn=<UnbindBackward0>))\n",
      "(7.347299700743164, tensor([7.0044], grad_fn=<UnbindBackward0>))\n",
      "(7.919356190660617, tensor([7.8907], grad_fn=<UnbindBackward0>))\n",
      "(8.236155661683124, tensor([7.7250], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([8.3124], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([8.6503], grad_fn=<UnbindBackward0>))\n",
      "(7.150701457592526, tensor([6.7304], grad_fn=<UnbindBackward0>))\n",
      "(8.583542571957771, tensor([7.8456], grad_fn=<UnbindBackward0>))\n",
      "(6.302618975744905, tensor([10.1741], grad_fn=<UnbindBackward0>))\n",
      "(9.048056708918736, tensor([8.3377], grad_fn=<UnbindBackward0>))\n",
      "(7.85748078694253, tensor([5.8791], grad_fn=<UnbindBackward0>))\n",
      "(9.53300341332321, tensor([7.3278], grad_fn=<UnbindBackward0>))\n",
      "(9.049702026013371, tensor([6.4220], grad_fn=<UnbindBackward0>))\n",
      "(8.6652683094816, tensor([8.5962], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([9.3861], grad_fn=<UnbindBackward0>))\n",
      "(6.35088571671474, tensor([7.8192], grad_fn=<UnbindBackward0>))\n",
      "(8.028455164114252, tensor([9.1703], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([9.0658], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([7.7493], grad_fn=<UnbindBackward0>))\n",
      "(8.831565879121063, tensor([8.4812], grad_fn=<UnbindBackward0>))\n",
      "(6.831953565565855, tensor([10.4043], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([8.5525], grad_fn=<UnbindBackward0>))\n",
      "(9.261413642160184, tensor([8.7337], grad_fn=<UnbindBackward0>))\n",
      "(8.865593998902725, tensor([6.4861], grad_fn=<UnbindBackward0>))\n",
      "(9.61700476065983, tensor([6.2534], grad_fn=<UnbindBackward0>))\n",
      "(8.00469951054955, tensor([8.4622], grad_fn=<UnbindBackward0>))\n",
      "(8.278174290943738, tensor([8.2378], grad_fn=<UnbindBackward0>))\n",
      "(8.572249397164315, tensor([8.8435], grad_fn=<UnbindBackward0>))\n",
      "(9.217415285648144, tensor([9.8200], grad_fn=<UnbindBackward0>))\n",
      "(7.483806687665835, tensor([7.3311], grad_fn=<UnbindBackward0>))\n",
      "(6.841615476477592, tensor([8.0044], grad_fn=<UnbindBackward0>))\n",
      "(8.782936356349264, tensor([7.7767], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([6.3527], grad_fn=<UnbindBackward0>))\n",
      "(8.799963219506997, tensor([6.3193], grad_fn=<UnbindBackward0>))\n",
      "(8.724207360800564, tensor([8.4404], grad_fn=<UnbindBackward0>))\n",
      "(7.215239978730097, tensor([6.4047], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([7.7310], grad_fn=<UnbindBackward0>))\n",
      "(8.438366410870266, tensor([6.3801], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([7.8820], grad_fn=<UnbindBackward0>))\n",
      "(8.22764270790443, tensor([6.8319], grad_fn=<UnbindBackward0>))\n",
      "(7.244941546337007, tensor([6.6419], grad_fn=<UnbindBackward0>))\n",
      "(7.382746449738912, tensor([6.3074], grad_fn=<UnbindBackward0>))\n",
      "(8.318010277546872, tensor([7.6843], grad_fn=<UnbindBackward0>))\n",
      "(8.60922527686273, tensor([9.4703], grad_fn=<UnbindBackward0>))\n",
      "(8.36450810375059, tensor([7.7620], grad_fn=<UnbindBackward0>))\n",
      "(9.784816622818449, tensor([8.2274], grad_fn=<UnbindBackward0>))\n",
      "(9.581006974541651, tensor([8.0519], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([8.6876], grad_fn=<UnbindBackward0>))\n",
      "(8.209852481301272, tensor([6.5256], grad_fn=<UnbindBackward0>))\n",
      "(8.476162841858246, tensor([6.4057], grad_fn=<UnbindBackward0>))\n",
      "(9.240578555036793, tensor([10.0224], grad_fn=<UnbindBackward0>))\n",
      "(8.81922185757494, tensor([6.3791], grad_fn=<UnbindBackward0>))\n",
      "(9.72376262158944, tensor([6.2024], grad_fn=<UnbindBackward0>))\n",
      "(9.042513261332681, tensor([10.1458], grad_fn=<UnbindBackward0>))\n",
      "(7.244941546337007, tensor([6.8092], grad_fn=<UnbindBackward0>))\n",
      "(8.630700432209832, tensor([8.0677], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.6371], grad_fn=<UnbindBackward0>))\n",
      "(9.799625913002037, tensor([6.1736], grad_fn=<UnbindBackward0>))\n",
      "(7.473637108496206, tensor([6.1865], grad_fn=<UnbindBackward0>))\n",
      "(7.495541943884256, tensor([8.3648], grad_fn=<UnbindBackward0>))\n",
      "(7.746300662231439, tensor([7.2334], grad_fn=<UnbindBackward0>))\n",
      "(8.213381737034572, tensor([6.2482], grad_fn=<UnbindBackward0>))\n",
      "(6.692083742506628, tensor([8.9974], grad_fn=<UnbindBackward0>))\n",
      "(7.643961949002529, tensor([6.5609], grad_fn=<UnbindBackward0>))\n",
      "(6.870053411798126, tensor([7.5673], grad_fn=<UnbindBackward0>))\n",
      "(8.304742269640771, tensor([9.6016], grad_fn=<UnbindBackward0>))\n",
      "(6.882437470997847, tensor([8.2732], grad_fn=<UnbindBackward0>))\n",
      "(8.610683534503575, tensor([8.0550], grad_fn=<UnbindBackward0>))\n",
      "(7.065613363597717, tensor([8.5392], grad_fn=<UnbindBackward0>))\n",
      "(9.459619396895018, tensor([7.2841], grad_fn=<UnbindBackward0>))\n",
      "(7.3632795869630385, tensor([7.3992], grad_fn=<UnbindBackward0>))\n",
      "(7.952966790923131, tensor([6.2824], grad_fn=<UnbindBackward0>))\n",
      "(8.57376254290413, tensor([6.8158], grad_fn=<UnbindBackward0>))\n",
      "(8.62371303479391, tensor([8.7653], grad_fn=<UnbindBackward0>))\n",
      "(8.34212526333359, tensor([9.0718], grad_fn=<UnbindBackward0>))\n",
      "(7.142036574706803, tensor([6.2102], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.5210], grad_fn=<UnbindBackward0>))\n",
      "(9.840973647146885, tensor([7.2597], grad_fn=<UnbindBackward0>))\n",
      "(6.61472560020376, tensor([6.6262], grad_fn=<UnbindBackward0>))\n",
      "(7.453561871643373, tensor([9.0863], grad_fn=<UnbindBackward0>))\n",
      "(7.4377951216719325, tensor([10.2752], grad_fn=<UnbindBackward0>))\n",
      "(8.073714641109857, tensor([9.2708], grad_fn=<UnbindBackward0>))\n",
      "(8.109525659752872, tensor([6.4547], grad_fn=<UnbindBackward0>))\n",
      "(7.711101251840158, tensor([6.4890], grad_fn=<UnbindBackward0>))\n",
      "(8.737131611781498, tensor([7.4198], grad_fn=<UnbindBackward0>))\n",
      "(7.867105500316739, tensor([8.1027], grad_fn=<UnbindBackward0>))\n",
      "(9.37568530456302, tensor([9.0360], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([8.2880], grad_fn=<UnbindBackward0>))\n",
      "(7.820037989458753, tensor([6.3671], grad_fn=<UnbindBackward0>))\n",
      "(8.343077871169383, tensor([6.2831], grad_fn=<UnbindBackward0>))\n",
      "(8.183676582620658, tensor([6.3437], grad_fn=<UnbindBackward0>))\n",
      "(7.074963197966044, tensor([8.6500], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([8.4112], grad_fn=<UnbindBackward0>))\n",
      "(7.7702232041587855, tensor([7.7304], grad_fn=<UnbindBackward0>))\n",
      "(6.882437470997847, tensor([8.9022], grad_fn=<UnbindBackward0>))\n",
      "(8.918248591035702, tensor([9.5857], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([8.8164], grad_fn=<UnbindBackward0>))\n",
      "(7.77779262633883, tensor([8.9112], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([8.9218], grad_fn=<UnbindBackward0>))\n",
      "(6.590301048196686, tensor([8.4226], grad_fn=<UnbindBackward0>))\n",
      "(9.164191715950203, tensor([7.1304], grad_fn=<UnbindBackward0>))\n",
      "(8.815073088844464, tensor([9.0407], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([8.0663], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([8.3751], grad_fn=<UnbindBackward0>))\n",
      "(8.793308627496552, tensor([6.8291], grad_fn=<UnbindBackward0>))\n",
      "(8.429672593886743, tensor([7.8022], grad_fn=<UnbindBackward0>))\n",
      "(8.067149039910106, tensor([8.6123], grad_fn=<UnbindBackward0>))\n",
      "(7.7354333524996886, tensor([7.2449], grad_fn=<UnbindBackward0>))\n",
      "(5.963579343618446, tensor([7.2562], grad_fn=<UnbindBackward0>))\n",
      "(6.480044561926653, tensor([6.7521], grad_fn=<UnbindBackward0>))\n",
      "(9.659886159319521, tensor([8.5367], grad_fn=<UnbindBackward0>))\n",
      "(8.469472455204826, tensor([7.6003], grad_fn=<UnbindBackward0>))\n",
      "(6.953684210870537, tensor([6.3427], grad_fn=<UnbindBackward0>))\n",
      "(9.48918349576076, tensor([8.4837], grad_fn=<UnbindBackward0>))\n",
      "(8.471777327885755, tensor([6.7893], grad_fn=<UnbindBackward0>))\n",
      "(8.393894975071744, tensor([6.7966], grad_fn=<UnbindBackward0>))\n",
      "(9.410501846898468, tensor([8.6545], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([8.7063], grad_fn=<UnbindBackward0>))\n",
      "(8.254528881939745, tensor([8.6981], grad_fn=<UnbindBackward0>))\n",
      "(9.205830216498297, tensor([7.4214], grad_fn=<UnbindBackward0>))\n",
      "(7.575584651557793, tensor([9.4579], grad_fn=<UnbindBackward0>))\n",
      "(9.38538551979936, tensor([9.8125], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([8.7196], grad_fn=<UnbindBackward0>))\n",
      "(6.8690144506657065, tensor([6.4023], grad_fn=<UnbindBackward0>))\n",
      "(8.759668671029939, tensor([8.5658], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([8.1812], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([8.7848], grad_fn=<UnbindBackward0>))\n",
      "(9.32500744695431, tensor([9.1105], grad_fn=<UnbindBackward0>))\n",
      "(7.564757012905729, tensor([6.8230], grad_fn=<UnbindBackward0>))\n",
      "(9.252249775053393, tensor([6.3868], grad_fn=<UnbindBackward0>))\n",
      "(7.025538314638521, tensor([8.4769], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([9.6769], grad_fn=<UnbindBackward0>))\n",
      "(5.932245187448011, tensor([6.5367], grad_fn=<UnbindBackward0>))\n",
      "(9.632334782035558, tensor([6.6961], grad_fn=<UnbindBackward0>))\n",
      "(8.37516869138682, tensor([6.2873], grad_fn=<UnbindBackward0>))\n",
      "(7.116394144093465, tensor([8.8617], grad_fn=<UnbindBackward0>))\n",
      "(8.671458150427666, tensor([6.1619], grad_fn=<UnbindBackward0>))\n",
      "(6.483107351457199, tensor([8.4552], grad_fn=<UnbindBackward0>))\n",
      "(6.709304340258298, tensor([6.3568], grad_fn=<UnbindBackward0>))\n",
      "(7.6255950721324535, tensor([7.0419], grad_fn=<UnbindBackward0>))\n",
      "(7.560601162768557, tensor([6.5591], grad_fn=<UnbindBackward0>))\n",
      "(9.483796737163981, tensor([7.8461], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([8.8235], grad_fn=<UnbindBackward0>))\n",
      "(9.79929302351827, tensor([6.6403], grad_fn=<UnbindBackward0>))\n",
      "(8.334711621820917, tensor([6.4988], grad_fn=<UnbindBackward0>))\n",
      "(7.013915474810528, tensor([9.3991], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([8.2428], grad_fn=<UnbindBackward0>))\n",
      "(8.1886891244442, tensor([9.8178], grad_fn=<UnbindBackward0>))\n",
      "(6.7912214627261855, tensor([7.7443], grad_fn=<UnbindBackward0>))\n",
      "(7.537962659768208, tensor([7.2324], grad_fn=<UnbindBackward0>))\n",
      "(7.126890808898808, tensor([9.8382], grad_fn=<UnbindBackward0>))\n",
      "(7.634820677745543, tensor([6.4483], grad_fn=<UnbindBackward0>))\n",
      "(8.430981494597171, tensor([9.2929], grad_fn=<UnbindBackward0>))\n",
      "(8.4071550862073, tensor([8.6870], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([6.1382], grad_fn=<UnbindBackward0>))\n",
      "(7.741099090035366, tensor([10.1599], grad_fn=<UnbindBackward0>))\n",
      "(7.0630481633881725, tensor([8.1679], grad_fn=<UnbindBackward0>))\n",
      "(7.221105098182496, tensor([10.1774], grad_fn=<UnbindBackward0>))\n",
      "(7.107425474110705, tensor([8.6490], grad_fn=<UnbindBackward0>))\n",
      "(7.711996507047669, tensor([7.1822], grad_fn=<UnbindBackward0>))\n",
      "(7.033506484287697, tensor([8.6673], grad_fn=<UnbindBackward0>))\n",
      "(8.798907492088226, tensor([8.3336], grad_fn=<UnbindBackward0>))\n",
      "(7.615298339825815, tensor([8.4313], grad_fn=<UnbindBackward0>))\n",
      "(6.066108090103747, tensor([7.7323], grad_fn=<UnbindBackward0>))\n",
      "(9.16408698745872, tensor([8.8962], grad_fn=<UnbindBackward0>))\n",
      "(8.59452453435256, tensor([6.2614], grad_fn=<UnbindBackward0>))\n",
      "(8.756525002926972, tensor([8.4038], grad_fn=<UnbindBackward0>))\n",
      "(8.854236693405742, tensor([6.3659], grad_fn=<UnbindBackward0>))\n",
      "(7.9208096792886, tensor([7.8106], grad_fn=<UnbindBackward0>))\n",
      "(9.057305735807121, tensor([6.1823], grad_fn=<UnbindBackward0>))\n",
      "(7.606387389772652, tensor([7.2104], grad_fn=<UnbindBackward0>))\n",
      "(8.593413217327646, tensor([8.7232], grad_fn=<UnbindBackward0>))\n",
      "(7.509335266016592, tensor([6.8555], grad_fn=<UnbindBackward0>))\n",
      "(7.651595573857601, tensor([7.1540], grad_fn=<UnbindBackward0>))\n",
      "(6.558197802812269, tensor([7.2276], grad_fn=<UnbindBackward0>))\n",
      "(9.576024610072503, tensor([9.0470], grad_fn=<UnbindBackward0>))\n",
      "(8.666647144584575, tensor([6.5559], grad_fn=<UnbindBackward0>))\n",
      "(7.957527402230773, tensor([7.2049], grad_fn=<UnbindBackward0>))\n",
      "(9.694986277958789, tensor([8.7039], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.7132], grad_fn=<UnbindBackward0>))\n",
      "(6.755768921984255, tensor([8.5611], grad_fn=<UnbindBackward0>))\n",
      "(8.515591910049263, tensor([10.2537], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([6.6136], grad_fn=<UnbindBackward0>))\n",
      "(8.374476889214643, tensor([7.7457], grad_fn=<UnbindBackward0>))\n",
      "(7.392647520721623, tensor([7.7081], grad_fn=<UnbindBackward0>))\n",
      "(8.55468163582723, tensor([7.0427], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([6.3016], grad_fn=<UnbindBackward0>))\n",
      "(9.231220849555537, tensor([9.4125], grad_fn=<UnbindBackward0>))\n",
      "(7.682943169878292, tensor([9.9413], grad_fn=<UnbindBackward0>))\n",
      "(8.302761580704049, tensor([6.1728], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([6.6963], grad_fn=<UnbindBackward0>))\n",
      "(6.12029741895095, tensor([8.5743], grad_fn=<UnbindBackward0>))\n",
      "(8.332789468417959, tensor([8.3867], grad_fn=<UnbindBackward0>))\n",
      "(8.792093929503284, tensor([8.7344], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([7.7174], grad_fn=<UnbindBackward0>))\n",
      "(7.27931883541462, tensor([9.4441], grad_fn=<UnbindBackward0>))\n",
      "(9.586445324777861, tensor([7.1764], grad_fn=<UnbindBackward0>))\n",
      "(8.06211758275474, tensor([8.4685], grad_fn=<UnbindBackward0>))\n",
      "(9.181014542594355, tensor([6.3283], grad_fn=<UnbindBackward0>))\n",
      "(7.53955882930103, tensor([9.1557], grad_fn=<UnbindBackward0>))\n",
      "(7.531552381407289, tensor([6.7605], grad_fn=<UnbindBackward0>))\n",
      "(7.698029170272805, tensor([6.3844], grad_fn=<UnbindBackward0>))\n",
      "(8.69901462316851, tensor([6.3959], grad_fn=<UnbindBackward0>))\n",
      "(7.656810091480378, tensor([7.4827], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([6.2800], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([8.4386], grad_fn=<UnbindBackward0>))\n",
      "(8.904901577935142, tensor([8.9816], grad_fn=<UnbindBackward0>))\n",
      "(8.556029215201436, tensor([7.7470], grad_fn=<UnbindBackward0>))\n",
      "(9.275472466913964, tensor([7.8111], grad_fn=<UnbindBackward0>))\n",
      "(9.451402423792738, tensor([7.4649], grad_fn=<UnbindBackward0>))\n",
      "(8.910585718290132, tensor([8.5743], grad_fn=<UnbindBackward0>))\n",
      "(7.346010209913293, tensor([8.8610], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([7.0071], grad_fn=<UnbindBackward0>))\n",
      "(8.0519780789023, tensor([10.1933], grad_fn=<UnbindBackward0>))\n",
      "(8.904765846682814, tensor([8.6027], grad_fn=<UnbindBackward0>))\n",
      "(9.668650902630873, tensor([6.3470], grad_fn=<UnbindBackward0>))\n",
      "(7.181591944611865, tensor([9.1104], grad_fn=<UnbindBackward0>))\n",
      "(9.146974496354362, tensor([8.2787], grad_fn=<UnbindBackward0>))\n",
      "(8.16194579946869, tensor([8.0250], grad_fn=<UnbindBackward0>))\n",
      "(7.2115567333138015, tensor([7.3703], grad_fn=<UnbindBackward0>))\n",
      "(7.822845290279774, tensor([7.1632], grad_fn=<UnbindBackward0>))\n",
      "(6.692083742506628, tensor([8.0403], grad_fn=<UnbindBackward0>))\n",
      "(8.798907492088226, tensor([7.8471], grad_fn=<UnbindBackward0>))\n",
      "(6.634633357861686, tensor([7.9822], grad_fn=<UnbindBackward0>))\n",
      "(7.829232537543592, tensor([8.0601], grad_fn=<UnbindBackward0>))\n",
      "(8.166216268592143, tensor([6.9143], grad_fn=<UnbindBackward0>))\n",
      "(7.920083199053234, tensor([7.2205], grad_fn=<UnbindBackward0>))\n",
      "(6.0112671744041615, tensor([6.4641], grad_fn=<UnbindBackward0>))\n",
      "(6.386879319362645, tensor([8.6685], grad_fn=<UnbindBackward0>))\n",
      "(7.713784616598755, tensor([8.7836], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([8.1498], grad_fn=<UnbindBackward0>))\n",
      "(6.692083742506628, tensor([7.5879], grad_fn=<UnbindBackward0>))\n",
      "(7.632885505395133, tensor([7.6523], grad_fn=<UnbindBackward0>))\n",
      "(6.536691597591305, tensor([8.9251], grad_fn=<UnbindBackward0>))\n",
      "(7.416378479192928, tensor([8.3160], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.7083], grad_fn=<UnbindBackward0>))\n",
      "(8.669399124305569, tensor([8.6552], grad_fn=<UnbindBackward0>))\n",
      "(7.214504414151143, tensor([9.7647], grad_fn=<UnbindBackward0>))\n",
      "(9.655154390148258, tensor([8.4560], grad_fn=<UnbindBackward0>))\n",
      "(8.744488113852924, tensor([8.4427], grad_fn=<UnbindBackward0>))\n",
      "(7.049254841255837, tensor([6.2356], grad_fn=<UnbindBackward0>))\n",
      "(6.837332814685591, tensor([8.5093], grad_fn=<UnbindBackward0>))\n",
      "(7.049254841255837, tensor([8.8509], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.6586], grad_fn=<UnbindBackward0>))\n",
      "(8.42463920980563, tensor([7.8310], grad_fn=<UnbindBackward0>))\n",
      "(7.469083884921234, tensor([7.9958], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([6.2479], grad_fn=<UnbindBackward0>))\n",
      "(7.750614732770409, tensor([7.1623], grad_fn=<UnbindBackward0>))\n",
      "(8.643649615368796, tensor([9.9364], grad_fn=<UnbindBackward0>))\n",
      "(7.203405521083095, tensor([7.6788], grad_fn=<UnbindBackward0>))\n",
      "(8.482601746646619, tensor([6.3440], grad_fn=<UnbindBackward0>))\n",
      "(7.1808311990445555, tensor([8.6889], grad_fn=<UnbindBackward0>))\n",
      "(8.888342869109282, tensor([6.2212], grad_fn=<UnbindBackward0>))\n",
      "(9.575538886855792, tensor([7.4803], grad_fn=<UnbindBackward0>))\n",
      "(9.344521553442203, tensor([7.2309], grad_fn=<UnbindBackward0>))\n",
      "(9.312896960301275, tensor([8.8772], grad_fn=<UnbindBackward0>))\n",
      "(7.659171367666058, tensor([6.2498], grad_fn=<UnbindBackward0>))\n",
      "(8.397508348470257, tensor([6.5702], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([9.4984], grad_fn=<UnbindBackward0>))\n",
      "(8.402455513945814, tensor([6.3292], grad_fn=<UnbindBackward0>))\n",
      "(9.171495588152615, tensor([7.8859], grad_fn=<UnbindBackward0>))\n",
      "(9.347577390281268, tensor([8.0623], grad_fn=<UnbindBackward0>))\n",
      "(9.190341725469493, tensor([7.2978], grad_fn=<UnbindBackward0>))\n",
      "(7.557994958530806, tensor([6.1301], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([7.0386], grad_fn=<UnbindBackward0>))\n",
      "(9.16711966952162, tensor([6.2745], grad_fn=<UnbindBackward0>))\n",
      "(7.744569809354496, tensor([8.9977], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([6.3129], grad_fn=<UnbindBackward0>))\n",
      "(8.213652703029998, tensor([5.8784], grad_fn=<UnbindBackward0>))\n",
      "(9.830648006359171, tensor([10.2031], grad_fn=<UnbindBackward0>))\n",
      "(7.890582534656536, tensor([7.1866], grad_fn=<UnbindBackward0>))\n",
      "(8.420902531097951, tensor([6.6828], grad_fn=<UnbindBackward0>))\n",
      "(8.946114375560743, tensor([6.7861], grad_fn=<UnbindBackward0>))\n",
      "(9.258940041812245, tensor([10.1597], grad_fn=<UnbindBackward0>))\n",
      "(8.52615293278771, tensor([8.3891], grad_fn=<UnbindBackward0>))\n",
      "(8.946765374867635, tensor([9.3483], grad_fn=<UnbindBackward0>))\n",
      "(6.98933526597456, tensor([7.8986], grad_fn=<UnbindBackward0>))\n",
      "(9.084890521258767, tensor([6.2283], grad_fn=<UnbindBackward0>))\n",
      "(6.263398262591624, tensor([6.8870], grad_fn=<UnbindBackward0>))\n",
      "(7.983098940710892, tensor([9.6923], grad_fn=<UnbindBackward0>))\n",
      "(9.02905827475689, tensor([6.8501], grad_fn=<UnbindBackward0>))\n",
      "(7.099201743553092, tensor([9.0355], grad_fn=<UnbindBackward0>))\n",
      "(8.47407690034261, tensor([8.5273], grad_fn=<UnbindBackward0>))\n",
      "(7.057897937411856, tensor([8.5753], grad_fn=<UnbindBackward0>))\n",
      "(7.597396320212795, tensor([7.6733], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([8.6997], grad_fn=<UnbindBackward0>))\n",
      "(7.774015077250727, tensor([8.5659], grad_fn=<UnbindBackward0>))\n",
      "(8.224431573221159, tensor([8.2243], grad_fn=<UnbindBackward0>))\n",
      "(8.492900498847193, tensor([6.2278], grad_fn=<UnbindBackward0>))\n",
      "(8.268475388982598, tensor([9.7261], grad_fn=<UnbindBackward0>))\n",
      "(6.92461239604856, tensor([9.8045], grad_fn=<UnbindBackward0>))\n",
      "(8.543835122362658, tensor([6.3078], grad_fn=<UnbindBackward0>))\n",
      "(8.454253391642363, tensor([8.6766], grad_fn=<UnbindBackward0>))\n",
      "(8.194229304819817, tensor([8.7248], grad_fn=<UnbindBackward0>))\n",
      "(8.257904193465674, tensor([6.6005], grad_fn=<UnbindBackward0>))\n",
      "(7.045776576879511, tensor([6.3284], grad_fn=<UnbindBackward0>))\n",
      "(7.049254841255837, tensor([8.9743], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([9.3837], grad_fn=<UnbindBackward0>))\n",
      "(8.62299361030245, tensor([8.5092], grad_fn=<UnbindBackward0>))\n",
      "(8.914894908906568, tensor([9.3822], grad_fn=<UnbindBackward0>))\n",
      "(8.210396255104774, tensor([9.1176], grad_fn=<UnbindBackward0>))\n",
      "(6.18826412308259, tensor([8.8721], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([10.0430], grad_fn=<UnbindBackward0>))\n",
      "(8.567886305731756, tensor([6.1685], grad_fn=<UnbindBackward0>))\n",
      "(7.9102237070973445, tensor([7.7872], grad_fn=<UnbindBackward0>))\n",
      "(8.747193183526933, tensor([7.3899], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([8.8898], grad_fn=<UnbindBackward0>))\n",
      "(9.142489705068009, tensor([8.8171], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([8.9047], grad_fn=<UnbindBackward0>))\n",
      "(9.155672970633875, tensor([6.6922], grad_fn=<UnbindBackward0>))\n",
      "(8.21878715560148, tensor([5.8865], grad_fn=<UnbindBackward0>))\n",
      "(7.86403565907245, tensor([7.8056], grad_fn=<UnbindBackward0>))\n",
      "(7.249215057114389, tensor([6.3855], grad_fn=<UnbindBackward0>))\n",
      "(9.01627006814768, tensor([8.4961], grad_fn=<UnbindBackward0>))\n",
      "(6.135564891081739, tensor([8.8484], grad_fn=<UnbindBackward0>))\n",
      "(6.773080375655535, tensor([7.1974], grad_fn=<UnbindBackward0>))\n",
      "(7.536363938404511, tensor([8.4440], grad_fn=<UnbindBackward0>))\n",
      "(8.5814816812986, tensor([6.3147], grad_fn=<UnbindBackward0>))\n",
      "(7.945555428253489, tensor([6.4198], grad_fn=<UnbindBackward0>))\n",
      "(8.635687085464026, tensor([8.3185], grad_fn=<UnbindBackward0>))\n",
      "(7.19668657083435, tensor([7.8532], grad_fn=<UnbindBackward0>))\n",
      "(7.7142311448490855, tensor([6.4253], grad_fn=<UnbindBackward0>))\n",
      "(6.807934943699926, tensor([7.6627], grad_fn=<UnbindBackward0>))\n",
      "(7.383989457978509, tensor([6.3850], grad_fn=<UnbindBackward0>))\n",
      "(9.544595867727551, tensor([6.1903], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([6.7955], grad_fn=<UnbindBackward0>))\n",
      "(6.890609120147166, tensor([9.3351], grad_fn=<UnbindBackward0>))\n",
      "(9.775540538205506, tensor([8.4588], grad_fn=<UnbindBackward0>))\n",
      "(7.922261058353247, tensor([7.4441], grad_fn=<UnbindBackward0>))\n",
      "(7.790282380703483, tensor([6.7275], grad_fn=<UnbindBackward0>))\n",
      "(8.97903863296051, tensor([8.4965], grad_fn=<UnbindBackward0>))\n",
      "(7.957527402230773, tensor([7.7511], grad_fn=<UnbindBackward0>))\n",
      "(7.65491704784832, tensor([8.8826], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([6.0494], grad_fn=<UnbindBackward0>))\n",
      "(7.497207223203318, tensor([7.3982], grad_fn=<UnbindBackward0>))\n",
      "(7.838737559599282, tensor([8.9800], grad_fn=<UnbindBackward0>))\n",
      "(7.831220214604293, tensor([8.6127], grad_fn=<UnbindBackward0>))\n",
      "(7.719129840906732, tensor([8.9448], grad_fn=<UnbindBackward0>))\n",
      "(9.203617826215355, tensor([8.1077], grad_fn=<UnbindBackward0>))\n",
      "(9.389573832170905, tensor([6.1996], grad_fn=<UnbindBackward0>))\n",
      "(8.098946748943339, tensor([8.9084], grad_fn=<UnbindBackward0>))\n",
      "(7.827639546366422, tensor([7.8321], grad_fn=<UnbindBackward0>))\n",
      "(7.768110378525988, tensor([8.7238], grad_fn=<UnbindBackward0>))\n",
      "(8.283999304248526, tensor([7.8184], grad_fn=<UnbindBackward0>))\n",
      "(7.018401799069201, tensor([6.2795], grad_fn=<UnbindBackward0>))\n",
      "(7.349873704738337, tensor([8.5205], grad_fn=<UnbindBackward0>))\n",
      "(8.804475183846678, tensor([7.1052], grad_fn=<UnbindBackward0>))\n",
      "(6.814542897259958, tensor([9.3032], grad_fn=<UnbindBackward0>))\n",
      "(7.374001859350161, tensor([7.2315], grad_fn=<UnbindBackward0>))\n",
      "(8.639056779173078, tensor([6.2157], grad_fn=<UnbindBackward0>))\n",
      "(7.532088143541722, tensor([6.3245], grad_fn=<UnbindBackward0>))\n",
      "(8.898365606955357, tensor([5.9320], grad_fn=<UnbindBackward0>))\n",
      "(8.420241665339788, tensor([8.7851], grad_fn=<UnbindBackward0>))\n",
      "(9.826984406550013, tensor([7.2173], grad_fn=<UnbindBackward0>))\n",
      "(9.67495450393748, tensor([7.5273], grad_fn=<UnbindBackward0>))\n",
      "(9.099408811268901, tensor([6.7156], grad_fn=<UnbindBackward0>))\n",
      "(9.22197245620589, tensor([7.9026], grad_fn=<UnbindBackward0>))\n",
      "(8.852235835227855, tensor([9.2947], grad_fn=<UnbindBackward0>))\n",
      "(7.256297239690681, tensor([7.0720], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([6.4227], grad_fn=<UnbindBackward0>))\n",
      "(8.474703139795285, tensor([6.5648], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([8.1597], grad_fn=<UnbindBackward0>))\n",
      "(9.185330208972015, tensor([6.9050], grad_fn=<UnbindBackward0>))\n",
      "(9.272093768251665, tensor([6.8410], grad_fn=<UnbindBackward0>))\n",
      "(7.397561535524052, tensor([7.1887], grad_fn=<UnbindBackward0>))\n",
      "(7.633853559681768, tensor([10.1681], grad_fn=<UnbindBackward0>))\n",
      "(6.78332520060396, tensor([6.2007], grad_fn=<UnbindBackward0>))\n",
      "(8.705165419719007, tensor([7.1164], grad_fn=<UnbindBackward0>))\n",
      "(8.35490952835879, tensor([6.1255], grad_fn=<UnbindBackward0>))\n",
      "(7.6577552711348655, tensor([8.2175], grad_fn=<UnbindBackward0>))\n",
      "(7.452402451223638, tensor([8.7830], grad_fn=<UnbindBackward0>))\n",
      "(8.597666575566114, tensor([7.3909], grad_fn=<UnbindBackward0>))\n",
      "(9.036582045842716, tensor([7.3342], grad_fn=<UnbindBackward0>))\n",
      "(8.0040315078527, tensor([8.0028], grad_fn=<UnbindBackward0>))\n",
      "(7.424761761823209, tensor([6.8277], grad_fn=<UnbindBackward0>))\n",
      "(8.414938957377482, tensor([6.1505], grad_fn=<UnbindBackward0>))\n",
      "(7.496097345175956, tensor([6.4008], grad_fn=<UnbindBackward0>))\n",
      "(7.892452043520352, tensor([8.4969], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([7.3784], grad_fn=<UnbindBackward0>))\n",
      "(6.5638555265321274, tensor([6.2920], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.7417], grad_fn=<UnbindBackward0>))\n",
      "(7.9919305198524775, tensor([7.1732], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([9.3856], grad_fn=<UnbindBackward0>))\n",
      "(6.302618975744905, tensor([6.6771], grad_fn=<UnbindBackward0>))\n",
      "(8.89740886527095, tensor([8.9632], grad_fn=<UnbindBackward0>))\n",
      "(7.506591780070841, tensor([9.9170], grad_fn=<UnbindBackward0>))\n",
      "(6.953684210870537, tensor([8.8090], grad_fn=<UnbindBackward0>))\n",
      "(7.351158226430694, tensor([7.0751], grad_fn=<UnbindBackward0>))\n",
      "(7.554858521040676, tensor([9.3447], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([7.9152], grad_fn=<UnbindBackward0>))\n",
      "(8.140315540159985, tensor([7.4423], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([8.7138], grad_fn=<UnbindBackward0>))\n",
      "(9.448884719866415, tensor([8.0871], grad_fn=<UnbindBackward0>))\n",
      "(7.416378479192928, tensor([7.5041], grad_fn=<UnbindBackward0>))\n",
      "(6.1903154058531475, tensor([8.4326], grad_fn=<UnbindBackward0>))\n",
      "(9.366403507607771, tensor([9.4420], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([7.9080], grad_fn=<UnbindBackward0>))\n",
      "(8.741296282225147, tensor([6.5304], grad_fn=<UnbindBackward0>))\n",
      "(9.596282813595483, tensor([6.7455], grad_fn=<UnbindBackward0>))\n",
      "(8.071218539969863, tensor([9.8196], grad_fn=<UnbindBackward0>))\n",
      "(8.899048433885268, tensor([8.4999], grad_fn=<UnbindBackward0>))\n",
      "(7.0967213784947605, tensor([8.6320], grad_fn=<UnbindBackward0>))\n",
      "(8.309184527686298, tensor([9.3157], grad_fn=<UnbindBackward0>))\n",
      "(7.957527402230773, tensor([7.9499], grad_fn=<UnbindBackward0>))\n",
      "(8.85907931788153, tensor([7.6459], grad_fn=<UnbindBackward0>))\n",
      "(7.987184748233473, tensor([8.0944], grad_fn=<UnbindBackward0>))\n",
      "(8.057377488557991, tensor([6.1797], grad_fn=<UnbindBackward0>))\n",
      "(7.752335163302292, tensor([6.2964], grad_fn=<UnbindBackward0>))\n",
      "(7.223295679562314, tensor([6.6870], grad_fn=<UnbindBackward0>))\n",
      "(9.817003309314352, tensor([8.0381], grad_fn=<UnbindBackward0>))\n",
      "(8.738735461363474, tensor([9.6051], grad_fn=<UnbindBackward0>))\n",
      "(5.971261839790462, tensor([8.1767], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([9.6329], grad_fn=<UnbindBackward0>))\n",
      "(7.575071699507561, tensor([6.2656], grad_fn=<UnbindBackward0>))\n",
      "(8.164510268747042, tensor([9.4248], grad_fn=<UnbindBackward0>))\n",
      "(7.473637108496206, tensor([8.6416], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([7.9643], grad_fn=<UnbindBackward0>))\n",
      "(7.731492029245684, tensor([7.2929], grad_fn=<UnbindBackward0>))\n",
      "(8.841448244098858, tensor([6.1896], grad_fn=<UnbindBackward0>))\n",
      "(8.372398606513004, tensor([6.4722], grad_fn=<UnbindBackward0>))\n",
      "(8.571870752706934, tensor([7.1047], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([10.1695], grad_fn=<UnbindBackward0>))\n",
      "(7.5411524551363085, tensor([6.3440], grad_fn=<UnbindBackward0>))\n",
      "(7.921898411023797, tensor([6.3094], grad_fn=<UnbindBackward0>))\n",
      "(7.88193748927207, tensor([6.6955], grad_fn=<UnbindBackward0>))\n",
      "(9.075322160298095, tensor([7.2326], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([8.2266], grad_fn=<UnbindBackward0>))\n",
      "(7.861341795599989, tensor([6.1861], grad_fn=<UnbindBackward0>))\n",
      "(7.404279118037268, tensor([9.5370], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([7.6987], grad_fn=<UnbindBackward0>))\n",
      "(8.353261499733874, tensor([8.0822], grad_fn=<UnbindBackward0>))\n",
      "(9.2237500588861, tensor([8.2828], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([7.2122], grad_fn=<UnbindBackward0>))\n",
      "(8.094073148069352, tensor([9.3685], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([8.5446], grad_fn=<UnbindBackward0>))\n",
      "(8.521185212685776, tensor([8.8704], grad_fn=<UnbindBackward0>))\n",
      "(7.384610383176974, tensor([7.8152], grad_fn=<UnbindBackward0>))\n",
      "(7.853993087224244, tensor([8.9416], grad_fn=<UnbindBackward0>))\n",
      "(6.54534966033442, tensor([10.1037], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([6.3131], grad_fn=<UnbindBackward0>))\n",
      "(7.6676260915849905, tensor([6.5889], grad_fn=<UnbindBackward0>))\n",
      "(8.343791731996841, tensor([6.5655], grad_fn=<UnbindBackward0>))\n",
      "(6.504288173536645, tensor([6.2723], grad_fn=<UnbindBackward0>))\n",
      "(8.535425959677298, tensor([6.3735], grad_fn=<UnbindBackward0>))\n",
      "(7.344072850573066, tensor([8.9241], grad_fn=<UnbindBackward0>))\n",
      "(9.352534137679358, tensor([6.3524], grad_fn=<UnbindBackward0>))\n",
      "(7.395721608602045, tensor([7.8514], grad_fn=<UnbindBackward0>))\n",
      "(9.066354521447801, tensor([7.9539], grad_fn=<UnbindBackward0>))\n",
      "(7.203405521083095, tensor([10.3342], grad_fn=<UnbindBackward0>))\n",
      "(8.382289428951436, tensor([6.2933], grad_fn=<UnbindBackward0>))\n",
      "(8.106816038947052, tensor([7.8184], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([6.2479], grad_fn=<UnbindBackward0>))\n",
      "(9.63174444388585, tensor([8.4869], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([6.7251], grad_fn=<UnbindBackward0>))\n",
      "(9.400630098419315, tensor([9.4512], grad_fn=<UnbindBackward0>))\n",
      "(8.463370384718731, tensor([5.8758], grad_fn=<UnbindBackward0>))\n",
      "(8.025516386489008, tensor([9.4136], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([7.2175], grad_fn=<UnbindBackward0>))\n",
      "(7.0909098220799835, tensor([7.0894], grad_fn=<UnbindBackward0>))\n",
      "(9.003562174748238, tensor([6.8147], grad_fn=<UnbindBackward0>))\n",
      "(9.684273769557766, tensor([7.8379], grad_fn=<UnbindBackward0>))\n",
      "(8.793156870913819, tensor([7.8729], grad_fn=<UnbindBackward0>))\n",
      "(9.690912952571152, tensor([7.7168], grad_fn=<UnbindBackward0>))\n",
      "(7.4489161025442, tensor([6.7341], grad_fn=<UnbindBackward0>))\n",
      "(6.173786103901937, tensor([6.2765], grad_fn=<UnbindBackward0>))\n",
      "(9.649175779559453, tensor([7.6262], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([9.4828], grad_fn=<UnbindBackward0>))\n",
      "(8.200013648175434, tensor([7.9059], grad_fn=<UnbindBackward0>))\n",
      "(8.37539918579835, tensor([9.3572], grad_fn=<UnbindBackward0>))\n",
      "(7.919356190660617, tensor([6.5797], grad_fn=<UnbindBackward0>))\n",
      "(6.551080335043404, tensor([6.3782], grad_fn=<UnbindBackward0>))\n",
      "(8.593969030218288, tensor([6.3931], grad_fn=<UnbindBackward0>))\n",
      "(6.329720905522696, tensor([7.6127], grad_fn=<UnbindBackward0>))\n",
      "(8.28045768658256, tensor([7.7876], grad_fn=<UnbindBackward0>))\n",
      "(7.521859252201629, tensor([6.4150], grad_fn=<UnbindBackward0>))\n",
      "(8.440312147080279, tensor([6.7881], grad_fn=<UnbindBackward0>))\n",
      "(6.2878585601617845, tensor([6.2895], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([7.8912], grad_fn=<UnbindBackward0>))\n",
      "(7.80057265467065, tensor([6.2841], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([7.3257], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([8.8163], grad_fn=<UnbindBackward0>))\n",
      "(9.01942196795671, tensor([6.3567], grad_fn=<UnbindBackward0>))\n",
      "(6.652863029353347, tensor([9.4724], grad_fn=<UnbindBackward0>))\n",
      "(6.625392368007956, tensor([7.1865], grad_fn=<UnbindBackward0>))\n",
      "(7.207118856207756, tensor([7.6742], grad_fn=<UnbindBackward0>))\n",
      "(6.828712071641684, tensor([10.2359], grad_fn=<UnbindBackward0>))\n",
      "(8.200013648175434, tensor([8.9097], grad_fn=<UnbindBackward0>))\n",
      "(8.804325112562537, tensor([9.5388], grad_fn=<UnbindBackward0>))\n",
      "(9.612934696659327, tensor([6.3503], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([6.2131], grad_fn=<UnbindBackward0>))\n",
      "(8.781555458546402, tensor([8.8845], grad_fn=<UnbindBackward0>))\n",
      "(8.358431899031295, tensor([7.3441], grad_fn=<UnbindBackward0>))\n",
      "(8.36287583103188, tensor([7.8439], grad_fn=<UnbindBackward0>))\n",
      "(7.257002707092073, tensor([6.8382], grad_fn=<UnbindBackward0>))\n",
      "(7.533158807455563, tensor([6.2972], grad_fn=<UnbindBackward0>))\n",
      "(7.610357618312838, tensor([8.6584], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([7.2619], grad_fn=<UnbindBackward0>))\n",
      "(8.900276348631271, tensor([6.5883], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([8.5655], grad_fn=<UnbindBackward0>))\n",
      "(8.518592212329946, tensor([7.4639], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([7.7470], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([6.6873], grad_fn=<UnbindBackward0>))\n",
      "(8.341410211461865, tensor([8.1303], grad_fn=<UnbindBackward0>))\n",
      "(7.358830898342354, tensor([9.3999], grad_fn=<UnbindBackward0>))\n",
      "(8.952993499320474, tensor([6.4692], grad_fn=<UnbindBackward0>))\n",
      "(6.905753276311464, tensor([6.2051], grad_fn=<UnbindBackward0>))\n",
      "(8.84922702143852, tensor([6.2723], grad_fn=<UnbindBackward0>))\n",
      "(7.85979918056211, tensor([10.6751], grad_fn=<UnbindBackward0>))\n",
      "(8.293549515060345, tensor([7.8700], grad_fn=<UnbindBackward0>))\n",
      "(7.761744984658913, tensor([6.3764], grad_fn=<UnbindBackward0>))\n",
      "(9.65975856761281, tensor([6.4319], grad_fn=<UnbindBackward0>))\n",
      "(9.588023099071895, tensor([6.7093], grad_fn=<UnbindBackward0>))\n",
      "(7.001245622069476, tensor([6.3144], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([8.2787], grad_fn=<UnbindBackward0>))\n",
      "(8.84779106484485, tensor([7.2190], grad_fn=<UnbindBackward0>))\n",
      "(8.276903481267057, tensor([9.2306], grad_fn=<UnbindBackward0>))\n",
      "(8.253748343328501, tensor([8.3785], grad_fn=<UnbindBackward0>))\n",
      "(6.829793737512425, tensor([6.2831], grad_fn=<UnbindBackward0>))\n",
      "(8.208764045819667, tensor([8.8490], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([6.4325], grad_fn=<UnbindBackward0>))\n",
      "(9.668144893764921, tensor([6.3276], grad_fn=<UnbindBackward0>))\n",
      "(8.273336598504486, tensor([6.7851], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([7.2811], grad_fn=<UnbindBackward0>))\n",
      "(8.37930948405285, tensor([6.3383], grad_fn=<UnbindBackward0>))\n",
      "(9.319643106866632, tensor([9.5050], grad_fn=<UnbindBackward0>))\n",
      "(8.249052274171293, tensor([7.1575], grad_fn=<UnbindBackward0>))\n",
      "(7.53689712956617, tensor([8.0261], grad_fn=<UnbindBackward0>))\n",
      "(7.557994958530806, tensor([8.1119], grad_fn=<UnbindBackward0>))\n",
      "(8.027476530860483, tensor([8.7831], grad_fn=<UnbindBackward0>))\n",
      "(8.356554845453426, tensor([6.2470], grad_fn=<UnbindBackward0>))\n",
      "(7.518607216815252, tensor([8.6077], grad_fn=<UnbindBackward0>))\n",
      "(7.503840746698951, tensor([8.9192], grad_fn=<UnbindBackward0>))\n",
      "(6.343880434126331, tensor([9.3409], grad_fn=<UnbindBackward0>))\n",
      "(9.198571388376184, tensor([9.2565], grad_fn=<UnbindBackward0>))\n",
      "(7.088408778675395, tensor([6.3559], grad_fn=<UnbindBackward0>))\n",
      "(9.660396363412222, tensor([8.4038], grad_fn=<UnbindBackward0>))\n",
      "(9.667322082870054, tensor([10.2283], grad_fn=<UnbindBackward0>))\n",
      "(8.27052509505507, tensor([8.8641], grad_fn=<UnbindBackward0>))\n",
      "(6.066108090103747, tensor([9.4379], grad_fn=<UnbindBackward0>))\n",
      "(5.926926025970411, tensor([6.4139], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([7.5763], grad_fn=<UnbindBackward0>))\n",
      "(7.280008252884188, tensor([9.4040], grad_fn=<UnbindBackward0>))\n",
      "(8.207946941048617, tensor([7.2347], grad_fn=<UnbindBackward0>))\n",
      "(6.748759547491679, tensor([6.5492], grad_fn=<UnbindBackward0>))\n",
      "(6.875232087276577, tensor([6.3075], grad_fn=<UnbindBackward0>))\n",
      "(6.257667587882639, tensor([6.8590], grad_fn=<UnbindBackward0>))\n",
      "(8.074960359115858, tensor([8.2439], grad_fn=<UnbindBackward0>))\n",
      "(9.38915578944085, tensor([9.4257], grad_fn=<UnbindBackward0>))\n",
      "(9.496120637138368, tensor([9.6339], grad_fn=<UnbindBackward0>))\n",
      "(6.398594934535208, tensor([7.1327], grad_fn=<UnbindBackward0>))\n",
      "(8.88016824790345, tensor([7.3528], grad_fn=<UnbindBackward0>))\n",
      "(9.35218709620442, tensor([7.4121], grad_fn=<UnbindBackward0>))\n",
      "(8.474703139795285, tensor([6.7358], grad_fn=<UnbindBackward0>))\n",
      "(8.3133619511344, tensor([8.3033], grad_fn=<UnbindBackward0>))\n",
      "(9.095602694505759, tensor([7.9885], grad_fn=<UnbindBackward0>))\n",
      "(7.557472901614746, tensor([8.2617], grad_fn=<UnbindBackward0>))\n",
      "(7.71333788887187, tensor([8.0188], grad_fn=<UnbindBackward0>))\n",
      "(8.257644958208228, tensor([7.8065], grad_fn=<UnbindBackward0>))\n",
      "(6.763884908562435, tensor([6.2031], grad_fn=<UnbindBackward0>))\n",
      "(8.763740720509464, tensor([6.3427], grad_fn=<UnbindBackward0>))\n",
      "(8.374938143835367, tensor([7.8701], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([8.6691], grad_fn=<UnbindBackward0>))\n",
      "(6.0844994130751715, tensor([10.4378], grad_fn=<UnbindBackward0>))\n",
      "(7.811163385025279, tensor([6.5184], grad_fn=<UnbindBackward0>))\n",
      "(6.975413927455952, tensor([8.2299], grad_fn=<UnbindBackward0>))\n",
      "(8.982435503560263, tensor([6.5815], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([7.0635], grad_fn=<UnbindBackward0>))\n",
      "(9.801233318497372, tensor([9.6607], grad_fn=<UnbindBackward0>))\n",
      "(8.166784289056151, tensor([6.8472], grad_fn=<UnbindBackward0>))\n",
      "(7.469083884921234, tensor([9.0770], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([10.0944], grad_fn=<UnbindBackward0>))\n",
      "(7.236339342754344, tensor([9.1166], grad_fn=<UnbindBackward0>))\n",
      "(6.447305862541213, tensor([8.6732], grad_fn=<UnbindBackward0>))\n",
      "(7.759614150696903, tensor([7.2420], grad_fn=<UnbindBackward0>))\n",
      "(9.82140944150052, tensor([6.5351], grad_fn=<UnbindBackward0>))\n",
      "(7.539027055823995, tensor([7.1540], grad_fn=<UnbindBackward0>))\n",
      "(9.588639812011577, tensor([6.1965], grad_fn=<UnbindBackward0>))\n",
      "(8.994172434398399, tensor([6.5259], grad_fn=<UnbindBackward0>))\n",
      "(6.584791392385716, tensor([5.8237], grad_fn=<UnbindBackward0>))\n",
      "(9.670356794688852, tensor([9.1314], grad_fn=<UnbindBackward0>))\n",
      "(6.824373670043086, tensor([8.7121], grad_fn=<UnbindBackward0>))\n",
      "(9.327945614050446, tensor([6.9345], grad_fn=<UnbindBackward0>))\n",
      "(8.24380842366528, tensor([7.2149], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([8.5281], grad_fn=<UnbindBackward0>))\n",
      "(8.644354337032917, tensor([6.2742], grad_fn=<UnbindBackward0>))\n",
      "(8.976009475021396, tensor([6.6500], grad_fn=<UnbindBackward0>))\n",
      "(6.07073772800249, tensor([7.9942], grad_fn=<UnbindBackward0>))\n",
      "(9.47401147446646, tensor([10.4817], grad_fn=<UnbindBackward0>))\n",
      "(8.584290934948731, tensor([6.2067], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([6.2842], grad_fn=<UnbindBackward0>))\n",
      "(9.109635667854551, tensor([8.7865], grad_fn=<UnbindBackward0>))\n",
      "(8.48590890137647, tensor([7.1451], grad_fn=<UnbindBackward0>))\n",
      "(8.746716349694486, tensor([7.8640], grad_fn=<UnbindBackward0>))\n",
      "(7.637716432664798, tensor([8.3244], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([8.4246], grad_fn=<UnbindBackward0>))\n",
      "(7.149916836132109, tensor([6.7029], grad_fn=<UnbindBackward0>))\n",
      "(9.35979432514452, tensor([6.6797], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([9.0230], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([8.4306], grad_fn=<UnbindBackward0>))\n",
      "(8.68946441235669, tensor([9.7450], grad_fn=<UnbindBackward0>))\n",
      "(6.3080984415095305, tensor([5.8973], grad_fn=<UnbindBackward0>))\n",
      "(8.789812386190972, tensor([8.3018], grad_fn=<UnbindBackward0>))\n",
      "(8.970432074329242, tensor([8.6884], grad_fn=<UnbindBackward0>))\n",
      "(7.381501894506707, tensor([7.2333], grad_fn=<UnbindBackward0>))\n",
      "(9.592946185938265, tensor([6.1940], grad_fn=<UnbindBackward0>))\n",
      "(6.507277712385012, tensor([9.8675], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([7.3591], grad_fn=<UnbindBackward0>))\n",
      "(8.844336332748933, tensor([9.1078], grad_fn=<UnbindBackward0>))\n",
      "(6.93537044601511, tensor([6.7300], grad_fn=<UnbindBackward0>))\n",
      "(6.8885724595653635, tensor([7.8036], grad_fn=<UnbindBackward0>))\n",
      "(7.444833273892193, tensor([8.5486], grad_fn=<UnbindBackward0>))\n",
      "(8.525359754082631, tensor([8.4010], grad_fn=<UnbindBackward0>))\n",
      "(7.555381944240273, tensor([8.4039], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([6.7871], grad_fn=<UnbindBackward0>))\n",
      "(6.403574197934815, tensor([8.5639], grad_fn=<UnbindBackward0>))\n",
      "(6.785587645007929, tensor([9.8870], grad_fn=<UnbindBackward0>))\n",
      "(8.459564078579602, tensor([7.9140], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([9.7288], grad_fn=<UnbindBackward0>))\n",
      "(8.40737832540903, tensor([6.3132], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.0055], grad_fn=<UnbindBackward0>))\n",
      "(7.521859252201629, tensor([6.2214], grad_fn=<UnbindBackward0>))\n",
      "(6.945051063725834, tensor([7.5823], grad_fn=<UnbindBackward0>))\n",
      "(8.39728289474368, tensor([7.8570], grad_fn=<UnbindBackward0>))\n",
      "(8.216628493133443, tensor([7.9770], grad_fn=<UnbindBackward0>))\n",
      "(7.7702232041587855, tensor([8.6961], grad_fn=<UnbindBackward0>))\n",
      "(7.158513997329321, tensor([6.3276], grad_fn=<UnbindBackward0>))\n",
      "(7.533158807455563, tensor([9.9680], grad_fn=<UnbindBackward0>))\n",
      "(9.625623789021686, tensor([7.7526], grad_fn=<UnbindBackward0>))\n",
      "(9.239996256883693, tensor([6.2339], grad_fn=<UnbindBackward0>))\n",
      "(8.36450810375059, tensor([6.3730], grad_fn=<UnbindBackward0>))\n",
      "(8.711113884053544, tensor([7.4758], grad_fn=<UnbindBackward0>))\n",
      "(7.768533300926033, tensor([6.8306], grad_fn=<UnbindBackward0>))\n",
      "(6.728628613084702, tensor([6.3331], grad_fn=<UnbindBackward0>))\n",
      "(9.102532419341877, tensor([6.4820], grad_fn=<UnbindBackward0>))\n",
      "(8.00202481821611, tensor([6.7343], grad_fn=<UnbindBackward0>))\n",
      "(9.340578848899913, tensor([7.8069], grad_fn=<UnbindBackward0>))\n",
      "(9.379154445411636, tensor([7.4208], grad_fn=<UnbindBackward0>))\n",
      "(7.1372784372603855, tensor([8.9550], grad_fn=<UnbindBackward0>))\n",
      "(7.018401799069201, tensor([8.5844], grad_fn=<UnbindBackward0>))\n",
      "(8.742254901886351, tensor([8.6037], grad_fn=<UnbindBackward0>))\n",
      "(6.890609120147166, tensor([9.1996], grad_fn=<UnbindBackward0>))\n",
      "(8.699348067653093, tensor([10.1526], grad_fn=<UnbindBackward0>))\n",
      "(8.21554741194707, tensor([9.5848], grad_fn=<UnbindBackward0>))\n",
      "(8.21932609390609, tensor([8.9787], grad_fn=<UnbindBackward0>))\n",
      "(8.781094735202615, tensor([8.7431], grad_fn=<UnbindBackward0>))\n",
      "(7.583756300707112, tensor([7.1627], grad_fn=<UnbindBackward0>))\n",
      "(8.688790784847216, tensor([7.2916], grad_fn=<UnbindBackward0>))\n",
      "(8.323608442343572, tensor([8.3445], grad_fn=<UnbindBackward0>))\n",
      "(7.277247726631484, tensor([8.7699], grad_fn=<UnbindBackward0>))\n",
      "(6.375024819828097, tensor([9.3244], grad_fn=<UnbindBackward0>))\n",
      "(7.92443418488756, tensor([9.2164], grad_fn=<UnbindBackward0>))\n",
      "(8.55159461813357, tensor([9.2876], grad_fn=<UnbindBackward0>))\n",
      "(7.714677473800927, tensor([7.3504], grad_fn=<UnbindBackward0>))\n",
      "(7.963807953231451, tensor([8.6264], grad_fn=<UnbindBackward0>))\n",
      "(9.272469743441732, tensor([7.2368], grad_fn=<UnbindBackward0>))\n",
      "(8.63497622707262, tensor([6.4058], grad_fn=<UnbindBackward0>))\n",
      "(6.767343125265392, tensor([7.7348], grad_fn=<UnbindBackward0>))\n",
      "(7.8961806086154915, tensor([6.9817], grad_fn=<UnbindBackward0>))\n",
      "(6.257667587882639, tensor([6.6791], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([8.7243], grad_fn=<UnbindBackward0>))\n",
      "(6.486160788944089, tensor([8.7373], grad_fn=<UnbindBackward0>))\n",
      "(9.142382678792956, tensor([8.9363], grad_fn=<UnbindBackward0>))\n",
      "(6.013715156042802, tensor([8.6614], grad_fn=<UnbindBackward0>))\n",
      "(8.528330935826693, tensor([9.2867], grad_fn=<UnbindBackward0>))\n",
      "(9.301825062098361, tensor([8.0433], grad_fn=<UnbindBackward0>))\n",
      "(6.118097198041348, tensor([7.3980], grad_fn=<UnbindBackward0>))\n",
      "(8.54188580400661, tensor([7.9056], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([8.0440], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([7.6779], grad_fn=<UnbindBackward0>))\n",
      "(7.381501894506707, tensor([7.3714], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([6.7214], grad_fn=<UnbindBackward0>))\n",
      "(8.857799727175905, tensor([9.5710], grad_fn=<UnbindBackward0>))\n",
      "(9.276315360899716, tensor([6.3970], grad_fn=<UnbindBackward0>))\n",
      "(8.99528899055931, tensor([6.6620], grad_fn=<UnbindBackward0>))\n",
      "(8.15248607578024, tensor([8.4090], grad_fn=<UnbindBackward0>))\n",
      "(8.630700432209832, tensor([6.2290], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([6.3726], grad_fn=<UnbindBackward0>))\n",
      "(6.259581464064923, tensor([6.1951], grad_fn=<UnbindBackward0>))\n",
      "(9.72184576464693, tensor([9.1372], grad_fn=<UnbindBackward0>))\n",
      "(9.603597936976602, tensor([8.2470], grad_fn=<UnbindBackward0>))\n",
      "(9.274910142625483, tensor([6.7106], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([8.4863], grad_fn=<UnbindBackward0>))\n",
      "(7.868254265520613, tensor([7.1861], grad_fn=<UnbindBackward0>))\n",
      "(8.212568398234145, tensor([6.7745], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([8.5719], grad_fn=<UnbindBackward0>))\n",
      "(7.362010551259734, tensor([6.4545], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([7.1482], grad_fn=<UnbindBackward0>))\n",
      "(6.466144724237619, tensor([8.0375], grad_fn=<UnbindBackward0>))\n",
      "(6.472346294500901, tensor([8.5768], grad_fn=<UnbindBackward0>))\n",
      "(7.16703787691222, tensor([7.7662], grad_fn=<UnbindBackward0>))\n",
      "(8.460834457746854, tensor([6.7986], grad_fn=<UnbindBackward0>))\n",
      "(7.557994958530806, tensor([7.8652], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([7.4479], grad_fn=<UnbindBackward0>))\n",
      "(7.668093709082406, tensor([6.3065], grad_fn=<UnbindBackward0>))\n",
      "(7.822845290279774, tensor([6.4725], grad_fn=<UnbindBackward0>))\n",
      "(6.628041376179533, tensor([6.7800], grad_fn=<UnbindBackward0>))\n",
      "(7.596894438144544, tensor([6.6430], grad_fn=<UnbindBackward0>))\n",
      "(8.982309899088579, tensor([6.5158], grad_fn=<UnbindBackward0>))\n",
      "(9.760078969240771, tensor([9.1894], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([6.0929], grad_fn=<UnbindBackward0>))\n",
      "(7.475339236566737, tensor([7.7531], grad_fn=<UnbindBackward0>))\n",
      "(8.101071503119543, tensor([7.3535], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([6.8011], grad_fn=<UnbindBackward0>))\n",
      "(9.791605819888035, tensor([6.8066], grad_fn=<UnbindBackward0>))\n",
      "(7.488852955733459, tensor([7.3668], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([9.4298], grad_fn=<UnbindBackward0>))\n",
      "(8.630343289348893, tensor([6.2421], grad_fn=<UnbindBackward0>))\n",
      "(8.681011276645632, tensor([8.1504], grad_fn=<UnbindBackward0>))\n",
      "(8.9242570208881, tensor([7.2327], grad_fn=<UnbindBackward0>))\n",
      "(7.190676034332207, tensor([7.7506], grad_fn=<UnbindBackward0>))\n",
      "(9.344084242025778, tensor([8.4483], grad_fn=<UnbindBackward0>))\n",
      "(6.437751649736401, tensor([6.1932], grad_fn=<UnbindBackward0>))\n",
      "(8.557374981049069, tensor([6.3619], grad_fn=<UnbindBackward0>))\n",
      "(6.948897222313312, tensor([8.7351], grad_fn=<UnbindBackward0>))\n",
      "(7.989899374942939, tensor([9.7907], grad_fn=<UnbindBackward0>))\n",
      "(9.257987025443535, tensor([9.4127], grad_fn=<UnbindBackward0>))\n",
      "(6.507277712385012, tensor([9.2947], grad_fn=<UnbindBackward0>))\n",
      "(9.179984251961283, tensor([6.8425], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([9.5371], grad_fn=<UnbindBackward0>))\n",
      "(8.139148678884066, tensor([6.7917], grad_fn=<UnbindBackward0>))\n",
      "(8.388450315523512, tensor([8.8740], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([5.9584], grad_fn=<UnbindBackward0>))\n",
      "(8.760296220470051, tensor([8.7576], grad_fn=<UnbindBackward0>))\n",
      "(6.559615237493242, tensor([7.6854], grad_fn=<UnbindBackward0>))\n",
      "(8.82379514872053, tensor([10.0806], grad_fn=<UnbindBackward0>))\n",
      "(7.4770384723196965, tensor([7.3671], grad_fn=<UnbindBackward0>))\n",
      "(8.491054533806542, tensor([8.8159], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.2501], grad_fn=<UnbindBackward0>))\n",
      "(8.061802274538348, tensor([8.9135], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([6.4032], grad_fn=<UnbindBackward0>))\n",
      "(9.10242102975669, tensor([8.1059], grad_fn=<UnbindBackward0>))\n",
      "(8.180320874773685, tensor([6.8880], grad_fn=<UnbindBackward0>))\n",
      "(7.573017256052546, tensor([7.1457], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([7.7424], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.6678], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([6.2368], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([6.2044], grad_fn=<UnbindBackward0>))\n",
      "(7.07326971745971, tensor([8.3681], grad_fn=<UnbindBackward0>))\n",
      "(8.795582216956426, tensor([6.1736], grad_fn=<UnbindBackward0>))\n",
      "(8.498418036089904, tensor([10.3211], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.3083], grad_fn=<UnbindBackward0>))\n",
      "(8.116715624819111, tensor([8.3096], grad_fn=<UnbindBackward0>))\n",
      "(6.0867747269123065, tensor([7.3943], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([7.8723], grad_fn=<UnbindBackward0>))\n",
      "(8.679482094459956, tensor([9.1639], grad_fn=<UnbindBackward0>))\n",
      "(6.877296071497429, tensor([7.8428], grad_fn=<UnbindBackward0>))\n",
      "(7.579678823090456, tensor([8.1585], grad_fn=<UnbindBackward0>))\n",
      "(7.867871490396322, tensor([9.0297], grad_fn=<UnbindBackward0>))\n",
      "(8.408493774492896, tensor([6.1798], grad_fn=<UnbindBackward0>))\n",
      "(6.556778356158042, tensor([10.1212], grad_fn=<UnbindBackward0>))\n",
      "(7.997326822998097, tensor([9.4660], grad_fn=<UnbindBackward0>))\n",
      "(7.352441100243583, tensor([8.3800], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([6.3654], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([7.8464], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([6.6608], grad_fn=<UnbindBackward0>))\n",
      "(8.369620826949102, tensor([8.8629], grad_fn=<UnbindBackward0>))\n",
      "(8.422662707570003, tensor([10.0683], grad_fn=<UnbindBackward0>))\n",
      "(7.614312146452, tensor([7.2878], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([6.4064], grad_fn=<UnbindBackward0>))\n",
      "(9.346006983397716, tensor([6.7609], grad_fn=<UnbindBackward0>))\n",
      "(7.450079569807499, tensor([7.3149], grad_fn=<UnbindBackward0>))\n",
      "(7.387090235656757, tensor([8.6063], grad_fn=<UnbindBackward0>))\n",
      "(6.45833828334479, tensor([7.6637], grad_fn=<UnbindBackward0>))\n",
      "(6.78105762593618, tensor([6.3030], grad_fn=<UnbindBackward0>))\n",
      "(7.504391559161238, tensor([7.7744], grad_fn=<UnbindBackward0>))\n",
      "(6.0063531596017325, tensor([7.7305], grad_fn=<UnbindBackward0>))\n",
      "(8.345930261979017, tensor([8.3909], grad_fn=<UnbindBackward0>))\n",
      "(6.648984550024776, tensor([7.7465], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([8.1499], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([6.3611], grad_fn=<UnbindBackward0>))\n",
      "(7.649216319820633, tensor([7.7988], grad_fn=<UnbindBackward0>))\n",
      "(7.0967213784947605, tensor([7.2308], grad_fn=<UnbindBackward0>))\n",
      "(9.789142350751268, tensor([8.1108], grad_fn=<UnbindBackward0>))\n",
      "(6.440946540632921, tensor([6.1500], grad_fn=<UnbindBackward0>))\n",
      "(6.760414691083428, tensor([10.1101], grad_fn=<UnbindBackward0>))\n",
      "(9.275191344295799, tensor([8.5505], grad_fn=<UnbindBackward0>))\n",
      "(6.638567789166521, tensor([6.2263], grad_fn=<UnbindBackward0>))\n",
      "(8.116715624819111, tensor([8.3121], grad_fn=<UnbindBackward0>))\n",
      "(7.537962659768208, tensor([8.7676], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.3309], grad_fn=<UnbindBackward0>))\n",
      "(8.227375506834035, tensor([6.3073], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([9.2840], grad_fn=<UnbindBackward0>))\n",
      "(8.222016437202196, tensor([6.4057], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([6.8775], grad_fn=<UnbindBackward0>))\n",
      "(7.16703787691222, tensor([8.0578], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([6.2370], grad_fn=<UnbindBackward0>))\n",
      "(7.5652752818989315, tensor([7.8241], grad_fn=<UnbindBackward0>))\n",
      "(9.769956159911606, tensor([7.8651], grad_fn=<UnbindBackward0>))\n",
      "(8.51197962436335, tensor([6.3408], grad_fn=<UnbindBackward0>))\n",
      "(9.106534203250593, tensor([8.8170], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([6.3021], grad_fn=<UnbindBackward0>))\n",
      "(7.500529485395295, tensor([7.6809], grad_fn=<UnbindBackward0>))\n",
      "(7.415175109613295, tensor([9.5435], grad_fn=<UnbindBackward0>))\n",
      "(8.672828482947686, tensor([9.0648], grad_fn=<UnbindBackward0>))\n",
      "(7.4815557019095165, tensor([6.3932], grad_fn=<UnbindBackward0>))\n",
      "(6.9411900550683745, tensor([6.6687], grad_fn=<UnbindBackward0>))\n",
      "(8.115819701211327, tensor([9.0414], grad_fn=<UnbindBackward0>))\n",
      "(6.9782137426306985, tensor([8.7276], grad_fn=<UnbindBackward0>))\n",
      "(8.647519453091812, tensor([8.6338], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([9.0297], grad_fn=<UnbindBackward0>))\n",
      "(8.968141414126814, tensor([9.7875], grad_fn=<UnbindBackward0>))\n",
      "(6.922643891475888, tensor([6.2053], grad_fn=<UnbindBackward0>))\n",
      "(8.463792414689122, tensor([8.2863], grad_fn=<UnbindBackward0>))\n",
      "(6.336825731146441, tensor([8.9970], grad_fn=<UnbindBackward0>))\n",
      "(8.517793011488205, tensor([8.5224], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([7.4556], grad_fn=<UnbindBackward0>))\n",
      "(7.792348924113037, tensor([10.0185], grad_fn=<UnbindBackward0>))\n",
      "(8.321664807135, tensor([9.6080], grad_fn=<UnbindBackward0>))\n",
      "(7.982074875081202, tensor([6.0731], grad_fn=<UnbindBackward0>))\n",
      "(9.184201714366488, tensor([7.7858], grad_fn=<UnbindBackward0>))\n",
      "(8.07153089355666, tensor([8.5786], grad_fn=<UnbindBackward0>))\n",
      "(9.007979359844501, tensor([8.2970], grad_fn=<UnbindBackward0>))\n",
      "(6.856461984594587, tensor([8.7377], grad_fn=<UnbindBackward0>))\n",
      "(8.056426767522984, tensor([8.4792], grad_fn=<UnbindBackward0>))\n",
      "(8.695339376799712, tensor([9.2433], grad_fn=<UnbindBackward0>))\n",
      "(7.6251071482389, tensor([6.9893], grad_fn=<UnbindBackward0>))\n",
      "(8.012018239159062, tensor([9.9195], grad_fn=<UnbindBackward0>))\n",
      "(8.27410200229233, tensor([6.3020], grad_fn=<UnbindBackward0>))\n",
      "(8.62191350218664, tensor([6.0020], grad_fn=<UnbindBackward0>))\n",
      "(7.400620577371135, tensor([6.3213], grad_fn=<UnbindBackward0>))\n",
      "(8.464425125877582, tensor([7.7680], grad_fn=<UnbindBackward0>))\n",
      "(8.70334075304372, tensor([6.3652], grad_fn=<UnbindBackward0>))\n",
      "(8.786456678344, tensor([8.7897], grad_fn=<UnbindBackward0>))\n",
      "(6.86171134048073, tensor([8.6232], grad_fn=<UnbindBackward0>))\n",
      "(7.766840537085513, tensor([9.0464], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([9.3859], grad_fn=<UnbindBackward0>))\n",
      "(7.970394907191429, tensor([8.9305], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([6.8140], grad_fn=<UnbindBackward0>))\n",
      "(9.805268145029668, tensor([6.2738], grad_fn=<UnbindBackward0>))\n",
      "(8.961622569542543, tensor([8.5443], grad_fn=<UnbindBackward0>))\n",
      "(7.575584651557793, tensor([6.3091], grad_fn=<UnbindBackward0>))\n",
      "(8.491670234185152, tensor([9.4379], grad_fn=<UnbindBackward0>))\n",
      "(8.49351506406166, tensor([8.9961], grad_fn=<UnbindBackward0>))\n",
      "(7.028201432058005, tensor([8.6406], grad_fn=<UnbindBackward0>))\n",
      "(8.061486866871327, tensor([6.9906], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([9.5343], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([6.9166], grad_fn=<UnbindBackward0>))\n",
      "(7.951559331155252, tensor([8.6861], grad_fn=<UnbindBackward0>))\n",
      "(7.412160334945205, tensor([9.4912], grad_fn=<UnbindBackward0>))\n",
      "(7.673223121121708, tensor([7.9535], grad_fn=<UnbindBackward0>))\n",
      "(8.172164452111904, tensor([6.4788], grad_fn=<UnbindBackward0>))\n",
      "(8.841881989497114, tensor([6.4709], grad_fn=<UnbindBackward0>))\n",
      "(9.577341813632218, tensor([7.1744], grad_fn=<UnbindBackward0>))\n",
      "(9.743964132018151, tensor([6.7007], grad_fn=<UnbindBackward0>))\n",
      "(8.424858580213442, tensor([7.2648], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([8.7538], grad_fn=<UnbindBackward0>))\n",
      "(8.522976436171964, tensor([7.5179], grad_fn=<UnbindBackward0>))\n",
      "(7.933796874815411, tensor([9.4922], grad_fn=<UnbindBackward0>))\n",
      "(8.757154527656606, tensor([7.1792], grad_fn=<UnbindBackward0>))\n",
      "(9.222960403332285, tensor([7.9253], grad_fn=<UnbindBackward0>))\n",
      "(6.501289670540389, tensor([10.2740], grad_fn=<UnbindBackward0>))\n",
      "(7.446585099157725, tensor([9.8730], grad_fn=<UnbindBackward0>))\n",
      "(7.77779262633883, tensor([7.1359], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([6.3257], grad_fn=<UnbindBackward0>))\n",
      "(7.465655310134056, tensor([9.3509], grad_fn=<UnbindBackward0>))\n",
      "(7.486613313139955, tensor([8.5748], grad_fn=<UnbindBackward0>))\n",
      "(8.486527777105353, tensor([6.1694], grad_fn=<UnbindBackward0>))\n",
      "(8.60611940061064, tensor([6.3287], grad_fn=<UnbindBackward0>))\n",
      "(7.527793987721444, tensor([6.7442], grad_fn=<UnbindBackward0>))\n",
      "(7.772331575169614, tensor([7.2402], grad_fn=<UnbindBackward0>))\n",
      "(8.801920904041582, tensor([8.8966], grad_fn=<UnbindBackward0>))\n",
      "(8.81195017753998, tensor([6.3028], grad_fn=<UnbindBackward0>))\n",
      "(8.007034012193408, tensor([8.6918], grad_fn=<UnbindBackward0>))\n",
      "(8.802973456578423, tensor([8.4181], grad_fn=<UnbindBackward0>))\n",
      "(8.831127635012084, tensor([7.1265], grad_fn=<UnbindBackward0>))\n",
      "(7.783224016336037, tensor([6.3355], grad_fn=<UnbindBackward0>))\n",
      "(5.82600010738045, tensor([8.9043], grad_fn=<UnbindBackward0>))\n",
      "(8.407601514786142, tensor([9.2824], grad_fn=<UnbindBackward0>))\n",
      "(8.679312040892672, tensor([6.4697], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([7.7737], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([8.1356], grad_fn=<UnbindBackward0>))\n",
      "(8.196161139282902, tensor([7.9733], grad_fn=<UnbindBackward0>))\n",
      "(7.198183577101943, tensor([8.4788], grad_fn=<UnbindBackward0>))\n",
      "(8.515591910049263, tensor([7.6805], grad_fn=<UnbindBackward0>))\n",
      "(9.53488460916053, tensor([9.3459], grad_fn=<UnbindBackward0>))\n",
      "(9.497472293122483, tensor([8.6669], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([7.8466], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([7.7364], grad_fn=<UnbindBackward0>))\n",
      "(7.7437032581737535, tensor([7.2562], grad_fn=<UnbindBackward0>))\n",
      "(7.496652438168283, tensor([7.1812], grad_fn=<UnbindBackward0>))\n",
      "(7.606387389772652, tensor([7.0891], grad_fn=<UnbindBackward0>))\n",
      "(7.6783263565068856, tensor([6.7544], grad_fn=<UnbindBackward0>))\n",
      "(8.342363500380579, tensor([7.9695], grad_fn=<UnbindBackward0>))\n",
      "(6.616065185132817, tensor([8.6411], grad_fn=<UnbindBackward0>))\n",
      "(8.814924599721019, tensor([6.2443], grad_fn=<UnbindBackward0>))\n",
      "(7.778211474512493, tensor([8.9699], grad_fn=<UnbindBackward0>))\n",
      "(8.773229786032473, tensor([8.0556], grad_fn=<UnbindBackward0>))\n",
      "(7.050122520269059, tensor([8.5864], grad_fn=<UnbindBackward0>))\n",
      "(7.253470382684528, tensor([8.3071], grad_fn=<UnbindBackward0>))\n",
      "(7.088408778675395, tensor([7.3333], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([9.4891], grad_fn=<UnbindBackward0>))\n",
      "(7.346010209913293, tensor([7.8605], grad_fn=<UnbindBackward0>))\n",
      "(8.866863661202087, tensor([6.3469], grad_fn=<UnbindBackward0>))\n",
      "(8.357024439263416, tensor([9.2375], grad_fn=<UnbindBackward0>))\n",
      "(9.073030421011577, tensor([6.9947], grad_fn=<UnbindBackward0>))\n",
      "(6.154858094016418, tensor([6.7737], grad_fn=<UnbindBackward0>))\n",
      "(7.519692404116539, tensor([8.6998], grad_fn=<UnbindBackward0>))\n",
      "(8.614501373883236, tensor([6.1951], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([6.7692], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([8.6655], grad_fn=<UnbindBackward0>))\n",
      "(7.067319848653476, tensor([6.1399], grad_fn=<UnbindBackward0>))\n",
      "(7.255591274253665, tensor([8.3097], grad_fn=<UnbindBackward0>))\n",
      "(8.023879992734878, tensor([8.7108], grad_fn=<UnbindBackward0>))\n",
      "(8.13739583005665, tensor([8.4481], grad_fn=<UnbindBackward0>))\n",
      "(8.728264161496178, tensor([5.7547], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([7.8587], grad_fn=<UnbindBackward0>))\n",
      "(8.131236549696116, tensor([7.7584], grad_fn=<UnbindBackward0>))\n",
      "(5.7899601708972535, tensor([8.6631], grad_fn=<UnbindBackward0>))\n",
      "(7.664815785285735, tensor([7.2057], grad_fn=<UnbindBackward0>))\n",
      "(6.459904454377535, tensor([7.9023], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([7.3512], grad_fn=<UnbindBackward0>))\n",
      "(6.638567789166521, tensor([8.7969], grad_fn=<UnbindBackward0>))\n",
      "(7.870165946469845, tensor([6.6559], grad_fn=<UnbindBackward0>))\n",
      "(8.247220052745229, tensor([9.0764], grad_fn=<UnbindBackward0>))\n",
      "(7.911690520708339, tensor([8.4466], grad_fn=<UnbindBackward0>))\n",
      "(6.9930151229329605, tensor([7.4699], grad_fn=<UnbindBackward0>))\n",
      "(8.857657449306988, tensor([7.4236], grad_fn=<UnbindBackward0>))\n",
      "(8.430545384690566, tensor([7.9804], grad_fn=<UnbindBackward0>))\n",
      "(7.814399633804487, tensor([6.3410], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([8.8152], grad_fn=<UnbindBackward0>))\n",
      "(8.10892415597534, tensor([8.4868], grad_fn=<UnbindBackward0>))\n",
      "(6.09807428216624, tensor([6.2287], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([8.6346], grad_fn=<UnbindBackward0>))\n",
      "(8.675734219544788, tensor([6.8164], grad_fn=<UnbindBackward0>))\n",
      "(6.472346294500901, tensor([7.3584], grad_fn=<UnbindBackward0>))\n",
      "(9.743436164387901, tensor([6.7632], grad_fn=<UnbindBackward0>))\n",
      "(6.579251212010101, tensor([7.7160], grad_fn=<UnbindBackward0>))\n",
      "(6.8690144506657065, tensor([7.5101], grad_fn=<UnbindBackward0>))\n",
      "(6.898714534329988, tensor([7.6825], grad_fn=<UnbindBackward0>))\n",
      "(7.062191632286556, tensor([8.3094], grad_fn=<UnbindBackward0>))\n",
      "(7.909122183211411, tensor([8.7832], grad_fn=<UnbindBackward0>))\n",
      "(6.84587987526405, tensor([8.3538], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([7.9617], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([7.8486], grad_fn=<UnbindBackward0>))\n",
      "(8.801017833540714, tensor([9.0950], grad_fn=<UnbindBackward0>))\n",
      "(8.09040229659332, tensor([7.3191], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([8.5821], grad_fn=<UnbindBackward0>))\n",
      "(6.748759547491679, tensor([6.7957], grad_fn=<UnbindBackward0>))\n",
      "(7.520234556474628, tensor([7.0785], grad_fn=<UnbindBackward0>))\n",
      "(7.51425465281641, tensor([7.7445], grad_fn=<UnbindBackward0>))\n",
      "(7.459338895220296, tensor([7.2735], grad_fn=<UnbindBackward0>))\n",
      "(8.679482094459956, tensor([8.0251], grad_fn=<UnbindBackward0>))\n",
      "(8.505120610181969, tensor([8.4858], grad_fn=<UnbindBackward0>))\n",
      "(9.527411329321751, tensor([10.0865], grad_fn=<UnbindBackward0>))\n",
      "(9.162304922937626, tensor([6.3635], grad_fn=<UnbindBackward0>))\n",
      "(7.188412736496954, tensor([6.4446], grad_fn=<UnbindBackward0>))\n",
      "(9.745487807887239, tensor([7.6594], grad_fn=<UnbindBackward0>))\n",
      "(9.595126468875947, tensor([9.9157], grad_fn=<UnbindBackward0>))\n",
      "(8.830689198760998, tensor([9.9845], grad_fn=<UnbindBackward0>))\n",
      "(7.754481547470383, tensor([9.0299], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([9.0519], grad_fn=<UnbindBackward0>))\n",
      "(7.452402451223638, tensor([9.2152], grad_fn=<UnbindBackward0>))\n",
      "(7.5745584842024805, tensor([7.6637], grad_fn=<UnbindBackward0>))\n",
      "(6.595780513961311, tensor([6.3373], grad_fn=<UnbindBackward0>))\n",
      "(6.947937068614969, tensor([8.0635], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.6466], grad_fn=<UnbindBackward0>))\n",
      "(9.558600006313569, tensor([8.2969], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([6.3328], grad_fn=<UnbindBackward0>))\n",
      "(6.825460036255307, tensor([8.4536], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.0217], grad_fn=<UnbindBackward0>))\n",
      "(7.658227526161352, tensor([8.9810], grad_fn=<UnbindBackward0>))\n",
      "(8.102889134640868, tensor([8.5840], grad_fn=<UnbindBackward0>))\n",
      "(8.027802848370312, tensor([6.7524], grad_fn=<UnbindBackward0>))\n",
      "(9.756494528795468, tensor([7.2678], grad_fn=<UnbindBackward0>))\n",
      "(7.16703787691222, tensor([8.5432], grad_fn=<UnbindBackward0>))\n",
      "(6.784457062637643, tensor([8.1873], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([6.5818], grad_fn=<UnbindBackward0>))\n",
      "(9.603260452582052, tensor([8.1817], grad_fn=<UnbindBackward0>))\n",
      "(8.718663567048953, tensor([8.7134], grad_fn=<UnbindBackward0>))\n",
      "(6.97166860472579, tensor([6.2854], grad_fn=<UnbindBackward0>))\n",
      "(8.567125560164447, tensor([7.3305], grad_fn=<UnbindBackward0>))\n",
      "(6.823286122355687, tensor([10.1293], grad_fn=<UnbindBackward0>))\n",
      "(7.643961949002529, tensor([7.3371], grad_fn=<UnbindBackward0>))\n",
      "(8.71948076085107, tensor([8.5844], grad_fn=<UnbindBackward0>))\n",
      "(8.978786553302003, tensor([6.2720], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([7.5568], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([7.3080], grad_fn=<UnbindBackward0>))\n",
      "(7.1284959456800365, tensor([8.5976], grad_fn=<UnbindBackward0>))\n",
      "(8.267705664762426, tensor([7.3262], grad_fn=<UnbindBackward0>))\n",
      "(8.792853288640693, tensor([7.7832], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([9.4165], grad_fn=<UnbindBackward0>))\n",
      "(7.6093665379542115, tensor([7.3879], grad_fn=<UnbindBackward0>))\n",
      "(7.246368080102461, tensor([6.3167], grad_fn=<UnbindBackward0>))\n",
      "(7.746732907753622, tensor([6.4947], grad_fn=<UnbindBackward0>))\n",
      "(6.822197390620491, tensor([6.6100], grad_fn=<UnbindBackward0>))\n",
      "(8.32020459757888, tensor([8.9661], grad_fn=<UnbindBackward0>))\n",
      "(9.676775140407365, tensor([9.5216], grad_fn=<UnbindBackward0>))\n",
      "(9.635869511120088, tensor([9.4064], grad_fn=<UnbindBackward0>))\n",
      "(7.269616749608169, tensor([9.4481], grad_fn=<UnbindBackward0>))\n",
      "(8.700680734850161, tensor([8.4358], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.3577], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([6.3561], grad_fn=<UnbindBackward0>))\n",
      "(9.09504174761272, tensor([9.1902], grad_fn=<UnbindBackward0>))\n",
      "(7.040536390215956, tensor([9.3381], grad_fn=<UnbindBackward0>))\n",
      "(7.142827401161621, tensor([6.7778], grad_fn=<UnbindBackward0>))\n",
      "(6.933423025730715, tensor([9.2074], grad_fn=<UnbindBackward0>))\n",
      "(9.442879644930755, tensor([8.6515], grad_fn=<UnbindBackward0>))\n",
      "(7.149916836132109, tensor([6.3523], grad_fn=<UnbindBackward0>))\n",
      "(7.792348924113037, tensor([8.7808], grad_fn=<UnbindBackward0>))\n",
      "(9.799792316197363, tensor([8.3922], grad_fn=<UnbindBackward0>))\n",
      "(8.129764445794171, tensor([7.7804], grad_fn=<UnbindBackward0>))\n",
      "(7.925157512224703, tensor([7.1777], grad_fn=<UnbindBackward0>))\n",
      "(8.602820277383675, tensor([9.3676], grad_fn=<UnbindBackward0>))\n",
      "(7.909122183211411, tensor([6.3898], grad_fn=<UnbindBackward0>))\n",
      "(9.226804098006848, tensor([7.2245], grad_fn=<UnbindBackward0>))\n",
      "(7.365812837209472, tensor([8.5253], grad_fn=<UnbindBackward0>))\n",
      "(8.481566013773087, tensor([6.2255], grad_fn=<UnbindBackward0>))\n",
      "(8.871224644409553, tensor([8.2493], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([8.0367], grad_fn=<UnbindBackward0>))\n",
      "(8.824236617346639, tensor([9.5851], grad_fn=<UnbindBackward0>))\n",
      "(8.805824812903607, tensor([7.5103], grad_fn=<UnbindBackward0>))\n",
      "(7.938445551164788, tensor([6.5404], grad_fn=<UnbindBackward0>))\n",
      "(8.140315540159985, tensor([8.2301], grad_fn=<UnbindBackward0>))\n",
      "(8.71308886823731, tensor([6.2282], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.3084], grad_fn=<UnbindBackward0>))\n",
      "(7.5883236773352225, tensor([6.2083], grad_fn=<UnbindBackward0>))\n",
      "(6.532334292222349, tensor([8.6511], grad_fn=<UnbindBackward0>))\n",
      "(6.742880635791903, tensor([5.8962], grad_fn=<UnbindBackward0>))\n",
      "(7.660114319173928, tensor([8.1006], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([8.9140], grad_fn=<UnbindBackward0>))\n",
      "(8.343077871169383, tensor([7.4379], grad_fn=<UnbindBackward0>))\n",
      "(9.080231686629162, tensor([8.6126], grad_fn=<UnbindBackward0>))\n",
      "(8.955835144218959, tensor([9.9795], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([9.2711], grad_fn=<UnbindBackward0>))\n",
      "(6.71174039505618, tensor([9.3098], grad_fn=<UnbindBackward0>))\n",
      "(9.675519883856426, tensor([5.8735], grad_fn=<UnbindBackward0>))\n",
      "(9.153875834995056, tensor([9.7504], grad_fn=<UnbindBackward0>))\n",
      "(7.2115567333138015, tensor([8.6395], grad_fn=<UnbindBackward0>))\n",
      "(7.70796153183549, tensor([6.7490], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([7.8582], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([9.3983], grad_fn=<UnbindBackward0>))\n",
      "(8.759197750371365, tensor([8.7425], grad_fn=<UnbindBackward0>))\n",
      "(7.173958319756794, tensor([9.9783], grad_fn=<UnbindBackward0>))\n",
      "(6.340359303727752, tensor([10.0083], grad_fn=<UnbindBackward0>))\n",
      "(9.508591395777199, tensor([6.5283], grad_fn=<UnbindBackward0>))\n",
      "(7.914617709040679, tensor([7.9535], grad_fn=<UnbindBackward0>))\n",
      "(8.887928819003305, tensor([8.3337], grad_fn=<UnbindBackward0>))\n",
      "(8.292047637431354, tensor([6.3365], grad_fn=<UnbindBackward0>))\n",
      "(6.92951677076365, tensor([6.3484], grad_fn=<UnbindBackward0>))\n",
      "(8.392310009269547, tensor([6.5606], grad_fn=<UnbindBackward0>))\n",
      "(8.12533508671429, tensor([8.3067], grad_fn=<UnbindBackward0>))\n",
      "(7.921898411023797, tensor([8.3650], grad_fn=<UnbindBackward0>))\n",
      "(6.57507584059962, tensor([9.0476], grad_fn=<UnbindBackward0>))\n",
      "(9.454540665816003, tensor([6.8141], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([6.9971], grad_fn=<UnbindBackward0>))\n",
      "(6.45833828334479, tensor([8.0811], grad_fn=<UnbindBackward0>))\n",
      "(9.435322232564033, tensor([7.4962], grad_fn=<UnbindBackward0>))\n",
      "(7.3537223303996315, tensor([7.6869], grad_fn=<UnbindBackward0>))\n",
      "(9.086136768516877, tensor([6.4429], grad_fn=<UnbindBackward0>))\n",
      "(7.112327444710911, tensor([7.7998], grad_fn=<UnbindBackward0>))\n",
      "(7.875119281040293, tensor([8.4781], grad_fn=<UnbindBackward0>))\n",
      "(6.570882962339584, tensor([8.2659], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([9.0488], grad_fn=<UnbindBackward0>))\n",
      "(8.5016733797582, tensor([9.5860], grad_fn=<UnbindBackward0>))\n",
      "(8.979668554241181, tensor([6.5914], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([10.2903], grad_fn=<UnbindBackward0>))\n",
      "(8.150756470275551, tensor([9.4660], grad_fn=<UnbindBackward0>))\n",
      "(6.456769655572163, tensor([8.2909], grad_fn=<UnbindBackward0>))\n",
      "(7.43955930913332, tensor([7.8192], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([7.8199], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([7.8120], grad_fn=<UnbindBackward0>))\n",
      "(7.252053951852814, tensor([9.7129], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([7.1982], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([7.2980], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([9.5024], grad_fn=<UnbindBackward0>))\n",
      "(7.469083884921234, tensor([8.5888], grad_fn=<UnbindBackward0>))\n",
      "(9.65374331942474, tensor([6.3535], grad_fn=<UnbindBackward0>))\n",
      "(7.477604243197589, tensor([9.9094], grad_fn=<UnbindBackward0>))\n",
      "(8.182559264068665, tensor([6.6450], grad_fn=<UnbindBackward0>))\n",
      "(6.324358962381311, tensor([8.5398], grad_fn=<UnbindBackward0>))\n",
      "(6.9411900550683745, tensor([6.4468], grad_fn=<UnbindBackward0>))\n",
      "(9.056373008678756, tensor([6.5419], grad_fn=<UnbindBackward0>))\n",
      "(6.918695219020472, tensor([7.9150], grad_fn=<UnbindBackward0>))\n",
      "(7.650168700845001, tensor([10.2595], grad_fn=<UnbindBackward0>))\n",
      "(6.393590753950631, tensor([7.2639], grad_fn=<UnbindBackward0>))\n",
      "(9.380251908693456, tensor([9.9212], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([7.6843], grad_fn=<UnbindBackward0>))\n",
      "(8.535818655539403, tensor([8.3299], grad_fn=<UnbindBackward0>))\n",
      "(8.29529885950246, tensor([8.7708], grad_fn=<UnbindBackward0>))\n",
      "(8.638525476583762, tensor([7.3540], grad_fn=<UnbindBackward0>))\n",
      "(5.863631175598097, tensor([8.6031], grad_fn=<UnbindBackward0>))\n",
      "(7.920083199053234, tensor([6.5817], grad_fn=<UnbindBackward0>))\n",
      "(8.566554620953962, tensor([7.5440], grad_fn=<UnbindBackward0>))\n",
      "(7.866722285136729, tensor([6.2829], grad_fn=<UnbindBackward0>))\n",
      "(6.180016653652572, tensor([8.7754], grad_fn=<UnbindBackward0>))\n",
      "(9.402942238177694, tensor([8.5246], grad_fn=<UnbindBackward0>))\n",
      "(8.10892415597534, tensor([6.4473], grad_fn=<UnbindBackward0>))\n",
      "(8.484669999710677, tensor([8.2468], grad_fn=<UnbindBackward0>))\n",
      "(8.55159461813357, tensor([8.4573], grad_fn=<UnbindBackward0>))\n",
      "(7.421775793644647, tensor([8.3867], grad_fn=<UnbindBackward0>))\n",
      "(6.249975242259483, tensor([6.4186], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([7.9432], grad_fn=<UnbindBackward0>))\n",
      "(9.129021850798594, tensor([6.3829], grad_fn=<UnbindBackward0>))\n",
      "(7.821643126239982, tensor([8.4350], grad_fn=<UnbindBackward0>))\n",
      "(8.620651899784468, tensor([9.4436], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([8.8495], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([6.1253], grad_fn=<UnbindBackward0>))\n",
      "(8.336390480591552, tensor([6.3006], grad_fn=<UnbindBackward0>))\n",
      "(9.567665104756411, tensor([6.2511], grad_fn=<UnbindBackward0>))\n",
      "(8.534836626588833, tensor([7.3828], grad_fn=<UnbindBackward0>))\n",
      "(6.6895992691789665, tensor([9.7254], grad_fn=<UnbindBackward0>))\n",
      "(7.254884810077338, tensor([6.5454], grad_fn=<UnbindBackward0>))\n",
      "(8.764678074116606, tensor([7.2039], grad_fn=<UnbindBackward0>))\n",
      "(8.085486772102845, tensor([10.4361], grad_fn=<UnbindBackward0>))\n",
      "(7.3537223303996315, tensor([9.1760], grad_fn=<UnbindBackward0>))\n",
      "(7.450660796211539, tensor([6.1761], grad_fn=<UnbindBackward0>))\n",
      "(7.693025748417888, tensor([8.6191], grad_fn=<UnbindBackward0>))\n",
      "(8.37930948405285, tensor([9.3153], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([9.6645], grad_fn=<UnbindBackward0>))\n",
      "(7.181591944611865, tensor([8.0769], grad_fn=<UnbindBackward0>))\n",
      "(6.716594773520978, tensor([6.7620], grad_fn=<UnbindBackward0>))\n",
      "(5.908082938168931, tensor([6.2454], grad_fn=<UnbindBackward0>))\n",
      "(9.656819456174961, tensor([6.6849], grad_fn=<UnbindBackward0>))\n",
      "(7.365812837209472, tensor([7.9832], grad_fn=<UnbindBackward0>))\n",
      "(9.410992885454862, tensor([7.9642], grad_fn=<UnbindBackward0>))\n",
      "(9.054388070202297, tensor([6.2566], grad_fn=<UnbindBackward0>))\n",
      "(9.342595951096653, tensor([6.3669], grad_fn=<UnbindBackward0>))\n",
      "(6.923628628138427, tensor([7.3189], grad_fn=<UnbindBackward0>))\n",
      "(8.498214224818435, tensor([8.2538], grad_fn=<UnbindBackward0>))\n",
      "(7.861341795599989, tensor([6.2155], grad_fn=<UnbindBackward0>))\n",
      "(9.08466376388818, tensor([6.2821], grad_fn=<UnbindBackward0>))\n",
      "(7.240649694255466, tensor([9.9922], grad_fn=<UnbindBackward0>))\n",
      "(8.494538500851432, tensor([9.9940], grad_fn=<UnbindBackward0>))\n",
      "(9.068892008391808, tensor([7.3690], grad_fn=<UnbindBackward0>))\n",
      "(8.455955881945048, tensor([7.0573], grad_fn=<UnbindBackward0>))\n",
      "(7.761319180947987, tensor([6.5027], grad_fn=<UnbindBackward0>))\n",
      "(6.721425700790643, tensor([7.1515], grad_fn=<UnbindBackward0>))\n",
      "(8.406708458240965, tensor([8.3006], grad_fn=<UnbindBackward0>))\n",
      "(7.994294986415977, tensor([6.1861], grad_fn=<UnbindBackward0>))\n",
      "(8.292047637431354, tensor([7.2186], grad_fn=<UnbindBackward0>))\n",
      "(6.951772164398911, tensor([7.9107], grad_fn=<UnbindBackward0>))\n",
      "(7.470793774195062, tensor([10.0781], grad_fn=<UnbindBackward0>))\n",
      "(7.599401333415815, tensor([9.1832], grad_fn=<UnbindBackward0>))\n",
      "(8.925188429378027, tensor([8.6364], grad_fn=<UnbindBackward0>))\n",
      "(8.304742269640771, tensor([8.6251], grad_fn=<UnbindBackward0>))\n",
      "(6.742880635791903, tensor([6.2716], grad_fn=<UnbindBackward0>))\n",
      "(8.727454116899434, tensor([8.2893], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([7.5627], grad_fn=<UnbindBackward0>))\n",
      "(8.371473537066832, tensor([7.0565], grad_fn=<UnbindBackward0>))\n",
      "(8.53817159780143, tensor([8.0059], grad_fn=<UnbindBackward0>))\n",
      "(7.3453648404168685, tensor([6.5460], grad_fn=<UnbindBackward0>))\n",
      "(9.003562174748238, tensor([8.1937], grad_fn=<UnbindBackward0>))\n",
      "(7.401231264413015, tensor([7.6793], grad_fn=<UnbindBackward0>))\n",
      "(7.747164966520335, tensor([6.2534], grad_fn=<UnbindBackward0>))\n",
      "(9.573315428463395, tensor([6.2764], grad_fn=<UnbindBackward0>))\n",
      "(6.826545223556594, tensor([5.9370], grad_fn=<UnbindBackward0>))\n",
      "(8.420902531097951, tensor([8.6064], grad_fn=<UnbindBackward0>))\n",
      "(6.834108738813838, tensor([8.3726], grad_fn=<UnbindBackward0>))\n",
      "(8.307459327011946, tensor([7.8830], grad_fn=<UnbindBackward0>))\n",
      "(7.722234744709607, tensor([6.6850], grad_fn=<UnbindBackward0>))\n",
      "(9.282940064390527, tensor([9.0359], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([7.3466], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.2588], grad_fn=<UnbindBackward0>))\n",
      "(9.274159884689137, tensor([9.5832], grad_fn=<UnbindBackward0>))\n",
      "(7.524021415206125, tensor([7.1134], grad_fn=<UnbindBackward0>))\n",
      "(9.50061907027523, tensor([8.7154], grad_fn=<UnbindBackward0>))\n",
      "(6.511745329644728, tensor([8.2713], grad_fn=<UnbindBackward0>))\n",
      "(9.79059877045751, tensor([7.3611], grad_fn=<UnbindBackward0>))\n",
      "(7.065613363597717, tensor([6.3329], grad_fn=<UnbindBackward0>))\n",
      "(7.590346945602565, tensor([7.1223], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([6.2560], grad_fn=<UnbindBackward0>))\n",
      "(8.766550149546351, tensor([7.5427], grad_fn=<UnbindBackward0>))\n",
      "(8.53934599605737, tensor([8.5608], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([7.9827], grad_fn=<UnbindBackward0>))\n",
      "(8.733916174927524, tensor([8.6999], grad_fn=<UnbindBackward0>))\n",
      "(7.804251383528112, tensor([6.4761], grad_fn=<UnbindBackward0>))\n",
      "(6.07073772800249, tensor([9.0514], grad_fn=<UnbindBackward0>))\n",
      "(8.440528106480752, tensor([8.4529], grad_fn=<UnbindBackward0>))\n",
      "(8.556413904569519, tensor([7.5241], grad_fn=<UnbindBackward0>))\n",
      "(8.574140471857987, tensor([6.5242], grad_fn=<UnbindBackward0>))\n",
      "(6.486160788944089, tensor([8.5299], grad_fn=<UnbindBackward0>))\n",
      "(9.641992909837995, tensor([6.4240], grad_fn=<UnbindBackward0>))\n",
      "(8.438799123988225, tensor([8.4602], grad_fn=<UnbindBackward0>))\n",
      "(8.000349495324684, tensor([7.1667], grad_fn=<UnbindBackward0>))\n",
      "(8.357259153499912, tensor([10.2683], grad_fn=<UnbindBackward0>))\n",
      "(7.358830898342354, tensor([7.7252], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([8.4480], grad_fn=<UnbindBackward0>))\n",
      "(6.313548046277095, tensor([6.2985], grad_fn=<UnbindBackward0>))\n",
      "(7.595889917718538, tensor([7.8446], grad_fn=<UnbindBackward0>))\n",
      "(9.017604776833355, tensor([6.2959], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([5.9192], grad_fn=<UnbindBackward0>))\n",
      "(7.405495663199472, tensor([7.7615], grad_fn=<UnbindBackward0>))\n",
      "(8.455955881945048, tensor([8.8292], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([9.0905], grad_fn=<UnbindBackward0>))\n",
      "(7.247792581767846, tensor([7.0591], grad_fn=<UnbindBackward0>))\n",
      "(8.905444318789714, tensor([7.8868], grad_fn=<UnbindBackward0>))\n",
      "(8.926517509850122, tensor([8.7694], grad_fn=<UnbindBackward0>))\n",
      "(8.603554357064281, tensor([6.3885], grad_fn=<UnbindBackward0>))\n",
      "(9.243678431586693, tensor([6.6170], grad_fn=<UnbindBackward0>))\n",
      "(9.684086987500752, tensor([8.1292], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([8.1014], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([7.2804], grad_fn=<UnbindBackward0>))\n",
      "(7.085064293952548, tensor([7.2555], grad_fn=<UnbindBackward0>))\n",
      "(7.757051142032013, tensor([9.3944], grad_fn=<UnbindBackward0>))\n",
      "(7.96346006663897, tensor([6.6701], grad_fn=<UnbindBackward0>))\n",
      "(8.151044945685024, tensor([6.4185], grad_fn=<UnbindBackward0>))\n",
      "(7.924072324923417, tensor([7.9154], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([6.7345], grad_fn=<UnbindBackward0>))\n",
      "(7.409741954080923, tensor([8.8434], grad_fn=<UnbindBackward0>))\n",
      "(8.808668062106715, tensor([8.6028], grad_fn=<UnbindBackward0>))\n",
      "(7.07326971745971, tensor([6.5377], grad_fn=<UnbindBackward0>))\n",
      "(6.804614520062624, tensor([6.1032], grad_fn=<UnbindBackward0>))\n",
      "(7.6290038896529575, tensor([6.5386], grad_fn=<UnbindBackward0>))\n",
      "(8.1786387885907, tensor([6.2096], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([7.1686], grad_fn=<UnbindBackward0>))\n",
      "(8.563122123304638, tensor([7.2430], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([8.4926], grad_fn=<UnbindBackward0>))\n",
      "(6.583409222158765, tensor([8.9399], grad_fn=<UnbindBackward0>))\n",
      "(8.210668031162976, tensor([9.3885], grad_fn=<UnbindBackward0>))\n",
      "(8.143807976771484, tensor([7.3453], grad_fn=<UnbindBackward0>))\n",
      "(9.765202019731785, tensor([9.9283], grad_fn=<UnbindBackward0>))\n",
      "(7.97522083865341, tensor([8.8250], grad_fn=<UnbindBackward0>))\n",
      "(7.0825485693553, tensor([9.2892], grad_fn=<UnbindBackward0>))\n",
      "(8.361240889642351, tensor([6.2672], grad_fn=<UnbindBackward0>))\n",
      "(8.305236829492593, tensor([6.6564], grad_fn=<UnbindBackward0>))\n",
      "(7.5766097669730375, tensor([6.3097], grad_fn=<UnbindBackward0>))\n",
      "(8.848509300888079, tensor([6.3498], grad_fn=<UnbindBackward0>))\n",
      "(8.313607139317558, tensor([6.8071], grad_fn=<UnbindBackward0>))\n",
      "(9.275191344295799, tensor([6.2468], grad_fn=<UnbindBackward0>))\n",
      "(7.706162970199576, tensor([6.2997], grad_fn=<UnbindBackward0>))\n",
      "(7.628517626575055, tensor([7.7695], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([9.0970], grad_fn=<UnbindBackward0>))\n",
      "(6.721425700790643, tensor([6.3272], grad_fn=<UnbindBackward0>))\n",
      "(9.219002745054835, tensor([7.6217], grad_fn=<UnbindBackward0>))\n",
      "(8.504715669905124, tensor([7.4620], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([8.3789], grad_fn=<UnbindBackward0>))\n",
      "(8.212297138229768, tensor([6.1586], grad_fn=<UnbindBackward0>))\n",
      "(8.683215975240689, tensor([8.0221], grad_fn=<UnbindBackward0>))\n",
      "(8.882391706295847, tensor([8.6353], grad_fn=<UnbindBackward0>))\n",
      "(9.766349664317493, tensor([10.1919], grad_fn=<UnbindBackward0>))\n",
      "(9.548881894718772, tensor([7.7301], grad_fn=<UnbindBackward0>))\n",
      "(6.628041376179533, tensor([8.2361], grad_fn=<UnbindBackward0>))\n",
      "(9.549736900114642, tensor([8.4409], grad_fn=<UnbindBackward0>))\n",
      "(8.118207049405783, tensor([6.7605], grad_fn=<UnbindBackward0>))\n",
      "(8.591929537530255, tensor([7.7556], grad_fn=<UnbindBackward0>))\n",
      "(7.059617628291383, tensor([6.1708], grad_fn=<UnbindBackward0>))\n",
      "(9.032050676295599, tensor([7.7793], grad_fn=<UnbindBackward0>))\n",
      "(8.234830280442056, tensor([6.7215], grad_fn=<UnbindBackward0>))\n",
      "(7.8961806086154915, tensor([6.3201], grad_fn=<UnbindBackward0>))\n",
      "(9.444463427333401, tensor([6.2076], grad_fn=<UnbindBackward0>))\n",
      "(6.823286122355687, tensor([7.4577], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.6968], grad_fn=<UnbindBackward0>))\n",
      "(8.723882104658486, tensor([6.2172], grad_fn=<UnbindBackward0>))\n",
      "(8.948455992345542, tensor([6.2320], grad_fn=<UnbindBackward0>))\n",
      "(6.885509670034818, tensor([8.4637], grad_fn=<UnbindBackward0>))\n",
      "(8.2960476427647, tensor([6.5732], grad_fn=<UnbindBackward0>))\n",
      "(8.939056445334039, tensor([8.4474], grad_fn=<UnbindBackward0>))\n",
      "(8.652073673610056, tensor([9.4122], grad_fn=<UnbindBackward0>))\n",
      "(8.296795865770052, tensor([8.6674], grad_fn=<UnbindBackward0>))\n",
      "(8.22817689595132, tensor([9.1519], grad_fn=<UnbindBackward0>))\n",
      "(9.355825038718164, tensor([8.3222], grad_fn=<UnbindBackward0>))\n",
      "(7.943427767876373, tensor([9.2663], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([8.4192], grad_fn=<UnbindBackward0>))\n",
      "(8.207674424355282, tensor([10.5290], grad_fn=<UnbindBackward0>))\n",
      "(8.695506726812653, tensor([6.7548], grad_fn=<UnbindBackward0>))\n",
      "(8.20685642839965, tensor([9.9880], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([7.1920], grad_fn=<UnbindBackward0>))\n",
      "(7.501634457883413, tensor([9.6248], grad_fn=<UnbindBackward0>))\n",
      "(9.514215624322715, tensor([6.1283], grad_fn=<UnbindBackward0>))\n",
      "(8.81936971001847, tensor([6.4587], grad_fn=<UnbindBackward0>))\n",
      "(8.423541635334782, tensor([9.9291], grad_fn=<UnbindBackward0>))\n",
      "(9.062999784077473, tensor([6.2463], grad_fn=<UnbindBackward0>))\n",
      "(7.397561535524052, tensor([6.3501], grad_fn=<UnbindBackward0>))\n",
      "(6.2166061010848646, tensor([8.0403], grad_fn=<UnbindBackward0>))\n",
      "(8.23880116587155, tensor([6.7507], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([6.4186], grad_fn=<UnbindBackward0>))\n",
      "(6.9167150203536085, tensor([8.9405], grad_fn=<UnbindBackward0>))\n",
      "(7.865955413933502, tensor([9.1449], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([6.3626], grad_fn=<UnbindBackward0>))\n",
      "(8.462948176563842, tensor([9.9313], grad_fn=<UnbindBackward0>))\n",
      "(8.314587291319576, tensor([6.6979], grad_fn=<UnbindBackward0>))\n",
      "(7.084226422097916, tensor([6.7324], grad_fn=<UnbindBackward0>))\n",
      "(8.012018239159062, tensor([7.1940], grad_fn=<UnbindBackward0>))\n",
      "(6.52649485957079, tensor([8.5094], grad_fn=<UnbindBackward0>))\n",
      "(6.257667587882639, tensor([7.8829], grad_fn=<UnbindBackward0>))\n",
      "(8.734077192559303, tensor([6.8425], grad_fn=<UnbindBackward0>))\n",
      "(9.239510749275963, tensor([6.7444], grad_fn=<UnbindBackward0>))\n",
      "(9.359966545931995, tensor([9.2449], grad_fn=<UnbindBackward0>))\n",
      "(6.8001700683022, tensor([8.6897], grad_fn=<UnbindBackward0>))\n",
      "(5.948034989180646, tensor([6.3250], grad_fn=<UnbindBackward0>))\n",
      "(7.744136627627991, tensor([7.5636], grad_fn=<UnbindBackward0>))\n",
      "(9.587337416104821, tensor([8.4767], grad_fn=<UnbindBackward0>))\n",
      "(6.92951677076365, tensor([7.2064], grad_fn=<UnbindBackward0>))\n",
      "(8.148445666243235, tensor([7.9531], grad_fn=<UnbindBackward0>))\n",
      "(6.647688373563329, tensor([6.8860], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([6.8098], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([6.4358], grad_fn=<UnbindBackward0>))\n",
      "(7.399398083331354, tensor([6.5872], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([6.2607], grad_fn=<UnbindBackward0>))\n",
      "(8.671115273688494, tensor([8.8016], grad_fn=<UnbindBackward0>))\n",
      "(7.914983005848394, tensor([6.3404], grad_fn=<UnbindBackward0>))\n",
      "(7.349230824613334, tensor([7.1506], grad_fn=<UnbindBackward0>))\n",
      "(7.579167967396076, tensor([6.1507], grad_fn=<UnbindBackward0>))\n",
      "(8.514790306799927, tensor([6.2991], grad_fn=<UnbindBackward0>))\n",
      "(8.35983738064003, tensor([6.5286], grad_fn=<UnbindBackward0>))\n",
      "(7.933796874815411, tensor([9.9184], grad_fn=<UnbindBackward0>))\n",
      "(6.814542897259958, tensor([8.9218], grad_fn=<UnbindBackward0>))\n",
      "(8.11969625295725, tensor([7.3475], grad_fn=<UnbindBackward0>))\n",
      "(8.132706489693264, tensor([7.7205], grad_fn=<UnbindBackward0>))\n",
      "(9.72859830755947, tensor([8.0379], grad_fn=<UnbindBackward0>))\n",
      "(7.5480289699350145, tensor([10.0943], grad_fn=<UnbindBackward0>))\n",
      "(7.213031659834869, tensor([8.4272], grad_fn=<UnbindBackward0>))\n",
      "(6.961296045910167, tensor([7.7997], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([9.1325], grad_fn=<UnbindBackward0>))\n",
      "(8.056426767522984, tensor([8.5402], grad_fn=<UnbindBackward0>))\n",
      "(6.80903930604298, tensor([6.8405], grad_fn=<UnbindBackward0>))\n",
      "(9.67721410821299, tensor([8.8600], grad_fn=<UnbindBackward0>))\n",
      "(9.098514556793926, tensor([7.1028], grad_fn=<UnbindBackward0>))\n",
      "(7.554334823725748, tensor([6.4667], grad_fn=<UnbindBackward0>))\n",
      "(8.302265794873367, tensor([9.0128], grad_fn=<UnbindBackward0>))\n",
      "(6.415096959171596, tensor([8.7403], grad_fn=<UnbindBackward0>))\n",
      "(7.5883236773352225, tensor([7.6923], grad_fn=<UnbindBackward0>))\n",
      "(8.511175119090675, tensor([7.5029], grad_fn=<UnbindBackward0>))\n",
      "(9.521787797978225, tensor([8.0016], grad_fn=<UnbindBackward0>))\n",
      "(8.583542571957771, tensor([7.8324], grad_fn=<UnbindBackward0>))\n",
      "(7.417580402414544, tensor([8.7797], grad_fn=<UnbindBackward0>))\n",
      "(8.59637398929068, tensor([6.2770], grad_fn=<UnbindBackward0>))\n",
      "(8.450625947144124, tensor([6.9539], grad_fn=<UnbindBackward0>))\n",
      "(7.537962659768208, tensor([6.2319], grad_fn=<UnbindBackward0>))\n",
      "(8.994172434398399, tensor([6.6062], grad_fn=<UnbindBackward0>))\n",
      "(9.40195197567723, tensor([9.9659], grad_fn=<UnbindBackward0>))\n",
      "(7.516977224604321, tensor([8.7697], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([7.7103], grad_fn=<UnbindBackward0>))\n",
      "(7.427144133408616, tensor([6.7489], grad_fn=<UnbindBackward0>))\n",
      "(7.261225091971921, tensor([7.9805], grad_fn=<UnbindBackward0>))\n",
      "(8.059592328887545, tensor([9.3976], grad_fn=<UnbindBackward0>))\n",
      "(8.53089883847235, tensor([6.4708], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.3358], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([8.7019], grad_fn=<UnbindBackward0>))\n",
      "(7.9269635448629785, tensor([6.7730], grad_fn=<UnbindBackward0>))\n",
      "(7.683403681053826, tensor([8.6340], grad_fn=<UnbindBackward0>))\n",
      "(6.816735880594968, tensor([6.7052], grad_fn=<UnbindBackward0>))\n",
      "(9.24541789858888, tensor([7.8137], grad_fn=<UnbindBackward0>))\n",
      "(8.352554369474591, tensor([6.7550], grad_fn=<UnbindBackward0>))\n",
      "(9.787234595711878, tensor([9.0491], grad_fn=<UnbindBackward0>))\n",
      "(8.771680359010377, tensor([9.0985], grad_fn=<UnbindBackward0>))\n",
      "(9.416703923250376, tensor([6.2787], grad_fn=<UnbindBackward0>))\n",
      "(8.93260863037757, tensor([6.1170], grad_fn=<UnbindBackward0>))\n",
      "(8.451053388911692, tensor([6.9299], grad_fn=<UnbindBackward0>))\n",
      "(9.50144159247362, tensor([6.7582], grad_fn=<UnbindBackward0>))\n",
      "(8.425516402844334, tensor([6.5632], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([8.3924], grad_fn=<UnbindBackward0>))\n",
      "(8.880585523102495, tensor([6.4026], grad_fn=<UnbindBackward0>))\n",
      "(8.267192185932146, tensor([8.4384], grad_fn=<UnbindBackward0>))\n",
      "(7.60489448081162, tensor([7.4039], grad_fn=<UnbindBackward0>))\n",
      "(7.465655310134056, tensor([7.2361], grad_fn=<UnbindBackward0>))\n",
      "(8.113426639943654, tensor([8.2424], grad_fn=<UnbindBackward0>))\n",
      "(8.850947451970402, tensor([8.5664], grad_fn=<UnbindBackward0>))\n",
      "(6.890609120147166, tensor([6.7075], grad_fn=<UnbindBackward0>))\n",
      "(6.866933284461882, tensor([6.2329], grad_fn=<UnbindBackward0>))\n",
      "(6.646390514847729, tensor([7.2514], grad_fn=<UnbindBackward0>))\n",
      "(9.655218482441182, tensor([8.8654], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([8.3220], grad_fn=<UnbindBackward0>))\n",
      "(7.2682230211595655, tensor([7.2518], grad_fn=<UnbindBackward0>))\n",
      "(7.633853559681768, tensor([8.4284], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.2748], grad_fn=<UnbindBackward0>))\n",
      "(8.560252680876685, tensor([7.1418], grad_fn=<UnbindBackward0>))\n",
      "(6.003887067106539, tensor([8.5454], grad_fn=<UnbindBackward0>))\n",
      "(8.805525052709516, tensor([8.4155], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([5.9126], grad_fn=<UnbindBackward0>))\n",
      "(6.738152494595957, tensor([7.0901], grad_fn=<UnbindBackward0>))\n",
      "(6.61472560020376, tensor([6.2765], grad_fn=<UnbindBackward0>))\n",
      "(8.36985260351753, tensor([6.0599], grad_fn=<UnbindBackward0>))\n",
      "(8.66681936537205, tensor([8.5836], grad_fn=<UnbindBackward0>))\n",
      "(8.946895523888447, tensor([9.0376], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([7.6109], grad_fn=<UnbindBackward0>))\n",
      "(9.578657284448841, tensor([8.8214], grad_fn=<UnbindBackward0>))\n",
      "(7.568379267836522, tensor([6.1768], grad_fn=<UnbindBackward0>))\n",
      "(7.969011781106478, tensor([7.3924], grad_fn=<UnbindBackward0>))\n",
      "(7.201916317531627, tensor([9.5425], grad_fn=<UnbindBackward0>))\n",
      "(5.924255797414532, tensor([7.8593], grad_fn=<UnbindBackward0>))\n",
      "(6.86171134048073, tensor([7.3220], grad_fn=<UnbindBackward0>))\n",
      "(8.451694209183541, tensor([8.7007], grad_fn=<UnbindBackward0>))\n",
      "(7.095893221097532, tensor([6.6953], grad_fn=<UnbindBackward0>))\n",
      "(6.6039438246004725, tensor([8.2343], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([8.7694], grad_fn=<UnbindBackward0>))\n",
      "(9.70509758756311, tensor([8.8875], grad_fn=<UnbindBackward0>))\n",
      "(7.664815785285735, tensor([7.8663], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([6.2048], grad_fn=<UnbindBackward0>))\n",
      "(7.66105638236183, tensor([8.6295], grad_fn=<UnbindBackward0>))\n",
      "(8.343315881404946, tensor([7.9617], grad_fn=<UnbindBackward0>))\n",
      "(7.699842407396986, tensor([6.7919], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([8.3658], grad_fn=<UnbindBackward0>))\n",
      "(6.86171134048073, tensor([10.0563], grad_fn=<UnbindBackward0>))\n",
      "(7.001245622069476, tensor([6.7206], grad_fn=<UnbindBackward0>))\n",
      "(8.631057447565285, tensor([7.0410], grad_fn=<UnbindBackward0>))\n",
      "(6.248042874508429, tensor([8.3800], grad_fn=<UnbindBackward0>))\n",
      "(8.17103418920548, tensor([7.8089], grad_fn=<UnbindBackward0>))\n",
      "(6.343880434126331, tensor([9.0113], grad_fn=<UnbindBackward0>))\n",
      "(8.446126742982377, tensor([6.8562], grad_fn=<UnbindBackward0>))\n",
      "(8.004365564979574, tensor([6.2573], grad_fn=<UnbindBackward0>))\n",
      "(8.296795865770052, tensor([6.2560], grad_fn=<UnbindBackward0>))\n",
      "(6.12029741895095, tensor([7.7877], grad_fn=<UnbindBackward0>))\n",
      "(8.940105213128122, tensor([8.2938], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([9.4848], grad_fn=<UnbindBackward0>))\n",
      "(8.728426091704613, tensor([7.3359], grad_fn=<UnbindBackward0>))\n",
      "(7.444248649496705, tensor([8.4229], grad_fn=<UnbindBackward0>))\n",
      "(8.74973246437081, tensor([7.4042], grad_fn=<UnbindBackward0>))\n",
      "(6.839476438228843, tensor([5.8031], grad_fn=<UnbindBackward0>))\n",
      "(7.961370201719511, tensor([6.3001], grad_fn=<UnbindBackward0>))\n",
      "(8.195885391314796, tensor([7.8868], grad_fn=<UnbindBackward0>))\n",
      "(8.252967195000798, tensor([6.2489], grad_fn=<UnbindBackward0>))\n",
      "(7.80954132465341, tensor([8.7869], grad_fn=<UnbindBackward0>))\n",
      "(6.077642243349034, tensor([9.5900], grad_fn=<UnbindBackward0>))\n",
      "(6.3835066348840055, tensor([8.7756], grad_fn=<UnbindBackward0>))\n",
      "(7.975564658495202, tensor([8.8995], grad_fn=<UnbindBackward0>))\n",
      "(6.767343125265392, tensor([6.5690], grad_fn=<UnbindBackward0>))\n",
      "(8.580355766373877, tensor([8.1829], grad_fn=<UnbindBackward0>))\n",
      "(7.086737934510577, tensor([9.5350], grad_fn=<UnbindBackward0>))\n",
      "(6.302618975744905, tensor([8.3335], grad_fn=<UnbindBackward0>))\n",
      "(8.986071187374463, tensor([9.4128], grad_fn=<UnbindBackward0>))\n",
      "(9.410092463833022, tensor([8.6482], grad_fn=<UnbindBackward0>))\n",
      "(6.595780513961311, tensor([8.5191], grad_fn=<UnbindBackward0>))\n",
      "(7.607381425639791, tensor([9.3292], grad_fn=<UnbindBackward0>))\n",
      "(6.077642243349034, tensor([8.7644], grad_fn=<UnbindBackward0>))\n",
      "(8.56216655705897, tensor([9.4290], grad_fn=<UnbindBackward0>))\n",
      "(7.795234929002173, tensor([6.7896], grad_fn=<UnbindBackward0>))\n",
      "(6.60934924316738, tensor([8.9605], grad_fn=<UnbindBackward0>))\n",
      "(6.804614520062624, tensor([6.3085], grad_fn=<UnbindBackward0>))\n",
      "(8.960467760919949, tensor([7.9410], grad_fn=<UnbindBackward0>))\n",
      "(9.325542301664866, tensor([7.1536], grad_fn=<UnbindBackward0>))\n",
      "(7.506591780070841, tensor([8.8760], grad_fn=<UnbindBackward0>))\n",
      "(6.950814768442584, tensor([7.3028], grad_fn=<UnbindBackward0>))\n",
      "(6.928537818164665, tensor([7.8221], grad_fn=<UnbindBackward0>))\n",
      "(6.983789965258135, tensor([7.5417], grad_fn=<UnbindBackward0>))\n",
      "(9.393744675721376, tensor([8.3481], grad_fn=<UnbindBackward0>))\n",
      "(7.6236419465115715, tensor([9.5059], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([6.4754], grad_fn=<UnbindBackward0>))\n",
      "(7.8336002236611035, tensor([7.7195], grad_fn=<UnbindBackward0>))\n",
      "(8.509362612301048, tensor([6.4850], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([6.5889], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([9.6716], grad_fn=<UnbindBackward0>))\n",
      "(8.732304571033183, tensor([6.3076], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([6.9128], grad_fn=<UnbindBackward0>))\n",
      "(9.50061907027523, tensor([6.9676], grad_fn=<UnbindBackward0>))\n",
      "(9.705219523912486, tensor([6.9386], grad_fn=<UnbindBackward0>))\n",
      "(6.898714534329988, tensor([8.9474], grad_fn=<UnbindBackward0>))\n",
      "(6.7226297948554485, tensor([7.3168], grad_fn=<UnbindBackward0>))\n",
      "(9.351579483668907, tensor([6.5995], grad_fn=<UnbindBackward0>))\n",
      "(8.418918622147897, tensor([6.7948], grad_fn=<UnbindBackward0>))\n",
      "(9.799126537211386, tensor([9.9924], grad_fn=<UnbindBackward0>))\n",
      "(6.726233402358747, tensor([8.6590], grad_fn=<UnbindBackward0>))\n",
      "(6.759255270663693, tensor([7.1983], grad_fn=<UnbindBackward0>))\n",
      "(9.516942329280475, tensor([9.5218], grad_fn=<UnbindBackward0>))\n",
      "(8.492695559815838, tensor([7.8046], grad_fn=<UnbindBackward0>))\n",
      "(8.934586870389676, tensor([8.7169], grad_fn=<UnbindBackward0>))\n",
      "(8.535818655539403, tensor([9.9324], grad_fn=<UnbindBackward0>))\n",
      "(8.875566691990551, tensor([9.6607], grad_fn=<UnbindBackward0>))\n",
      "(7.046647277848756, tensor([8.8427], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([8.7163], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([8.7346], grad_fn=<UnbindBackward0>))\n",
      "(8.481773246184977, tensor([7.8273], grad_fn=<UnbindBackward0>))\n",
      "(7.993957547573565, tensor([6.3270], grad_fn=<UnbindBackward0>))\n",
      "(9.616205400105674, tensor([8.1114], grad_fn=<UnbindBackward0>))\n",
      "(9.641798060358601, tensor([8.8671], grad_fn=<UnbindBackward0>))\n",
      "(9.328834266023971, tensor([6.3798], grad_fn=<UnbindBackward0>))\n",
      "(6.2878585601617845, tensor([7.7000], grad_fn=<UnbindBackward0>))\n",
      "(8.969923491991516, tensor([7.7357], grad_fn=<UnbindBackward0>))\n",
      "(8.516392871245468, tensor([6.2016], grad_fn=<UnbindBackward0>))\n",
      "(8.52793528794814, tensor([6.2681], grad_fn=<UnbindBackward0>))\n",
      "(9.206332350578643, tensor([8.5987], grad_fn=<UnbindBackward0>))\n",
      "(9.319015388601839, tensor([7.1582], grad_fn=<UnbindBackward0>))\n",
      "(8.367532416861831, tensor([8.6946], grad_fn=<UnbindBackward0>))\n",
      "(6.61472560020376, tensor([6.2610], grad_fn=<UnbindBackward0>))\n",
      "(8.240385115516334, tensor([9.1449], grad_fn=<UnbindBackward0>))\n",
      "(9.593968785475173, tensor([7.4168], grad_fn=<UnbindBackward0>))\n",
      "(8.472823243680297, tensor([7.9523], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([9.0635], grad_fn=<UnbindBackward0>))\n",
      "(7.951207156472972, tensor([8.4919], grad_fn=<UnbindBackward0>))\n",
      "(8.241966560231802, tensor([7.5601], grad_fn=<UnbindBackward0>))\n",
      "(8.14902386805177, tensor([8.5343], grad_fn=<UnbindBackward0>))\n",
      "(8.76795190976342, tensor([8.5229], grad_fn=<UnbindBackward0>))\n",
      "(9.119540007649686, tensor([8.4449], grad_fn=<UnbindBackward0>))\n",
      "(7.1284959456800365, tensor([8.1185], grad_fn=<UnbindBackward0>))\n",
      "(7.018401799069201, tensor([8.5094], grad_fn=<UnbindBackward0>))\n",
      "(6.858565034791365, tensor([8.5916], grad_fn=<UnbindBackward0>))\n",
      "(6.824373670043086, tensor([9.3895], grad_fn=<UnbindBackward0>))\n",
      "(6.410174881966167, tensor([7.5852], grad_fn=<UnbindBackward0>))\n",
      "(7.969011781106478, tensor([8.1093], grad_fn=<UnbindBackward0>))\n",
      "(8.74957394808293, tensor([6.1225], grad_fn=<UnbindBackward0>))\n",
      "(7.9714309977693505, tensor([8.5198], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([8.7708], grad_fn=<UnbindBackward0>))\n",
      "(8.401333305321703, tensor([6.8821], grad_fn=<UnbindBackward0>))\n",
      "(8.74687531957003, tensor([8.5975], grad_fn=<UnbindBackward0>))\n",
      "(6.70196036600254, tensor([7.9614], grad_fn=<UnbindBackward0>))\n",
      "(8.729720590267258, tensor([8.6124], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([6.4397], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([7.1114], grad_fn=<UnbindBackward0>))\n",
      "(6.386879319362645, tensor([7.5450], grad_fn=<UnbindBackward0>))\n",
      "(8.320448113956559, tensor([7.0686], grad_fn=<UnbindBackward0>))\n",
      "(6.694562058521095, tensor([8.6940], grad_fn=<UnbindBackward0>))\n",
      "(8.558527054909215, tensor([7.1197], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([6.5328], grad_fn=<UnbindBackward0>))\n",
      "(7.845416036592485, tensor([7.8615], grad_fn=<UnbindBackward0>))\n",
      "(8.37309184744198, tensor([6.2793], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([8.0903], grad_fn=<UnbindBackward0>))\n",
      "(7.1276936993473985, tensor([6.4355], grad_fn=<UnbindBackward0>))\n",
      "(8.837971491357209, tensor([8.6207], grad_fn=<UnbindBackward0>))\n",
      "(7.473637108496206, tensor([9.3471], grad_fn=<UnbindBackward0>))\n",
      "(7.694392802629421, tensor([7.2798], grad_fn=<UnbindBackward0>))\n",
      "(7.173958319756794, tensor([7.1612], grad_fn=<UnbindBackward0>))\n",
      "(8.7937637591133, tensor([6.0705], grad_fn=<UnbindBackward0>))\n",
      "(6.285998094508865, tensor([8.2583], grad_fn=<UnbindBackward0>))\n",
      "(6.616065185132817, tensor([7.8706], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([8.4893], grad_fn=<UnbindBackward0>))\n",
      "(6.932447891572509, tensor([8.0553], grad_fn=<UnbindBackward0>))\n",
      "(8.158802490694002, tensor([6.6180], grad_fn=<UnbindBackward0>))\n",
      "(6.473890696352274, tensor([7.7894], grad_fn=<UnbindBackward0>))\n",
      "(6.926577033222725, tensor([7.6361], grad_fn=<UnbindBackward0>))\n",
      "(9.305377787310935, tensor([8.4532], grad_fn=<UnbindBackward0>))\n",
      "(8.47407690034261, tensor([6.2076], grad_fn=<UnbindBackward0>))\n",
      "(8.042699496897637, tensor([6.3760], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([5.9658], grad_fn=<UnbindBackward0>))\n",
      "(7.754481547470383, tensor([8.0497], grad_fn=<UnbindBackward0>))\n",
      "(9.218804450388312, tensor([6.1609], grad_fn=<UnbindBackward0>))\n",
      "(8.929435283803425, tensor([7.6848], grad_fn=<UnbindBackward0>))\n",
      "(7.198183577101943, tensor([6.2071], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([7.3835], grad_fn=<UnbindBackward0>))\n",
      "(6.633318433280377, tensor([6.7204], grad_fn=<UnbindBackward0>))\n",
      "(7.095064377287131, tensor([8.6721], grad_fn=<UnbindBackward0>))\n",
      "(8.966483779064431, tensor([8.1620], grad_fn=<UnbindBackward0>))\n",
      "(7.173958319756794, tensor([8.2106], grad_fn=<UnbindBackward0>))\n",
      "(8.322880021769905, tensor([7.1441], grad_fn=<UnbindBackward0>))\n",
      "(8.848365694942547, tensor([7.9784], grad_fn=<UnbindBackward0>))\n",
      "(8.630521876723241, tensor([6.1401], grad_fn=<UnbindBackward0>))\n",
      "(7.904703913873747, tensor([8.2963], grad_fn=<UnbindBackward0>))\n",
      "(6.368187186350492, tensor([5.9385], grad_fn=<UnbindBackward0>))\n",
      "(6.993932975223189, tensor([6.4723], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([7.9909], grad_fn=<UnbindBackward0>))\n",
      "(6.97354301952014, tensor([7.4162], grad_fn=<UnbindBackward0>))\n",
      "(7.535830462798367, tensor([6.2605], grad_fn=<UnbindBackward0>))\n",
      "(8.670257567143075, tensor([6.2418], grad_fn=<UnbindBackward0>))\n",
      "(7.646353722445999, tensor([7.2436], grad_fn=<UnbindBackward0>))\n",
      "(9.010180657052349, tensor([6.3046], grad_fn=<UnbindBackward0>))\n",
      "(8.455955881945048, tensor([8.1199], grad_fn=<UnbindBackward0>))\n",
      "(8.475328987317539, tensor([9.3054], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.2578], grad_fn=<UnbindBackward0>))\n",
      "(7.8252452914317745, tensor([6.1920], grad_fn=<UnbindBackward0>))\n",
      "(7.7354333524996886, tensor([6.1900], grad_fn=<UnbindBackward0>))\n",
      "(8.131236549696116, tensor([7.4273], grad_fn=<UnbindBackward0>))\n",
      "(8.452974619089586, tensor([10.1618], grad_fn=<UnbindBackward0>))\n",
      "(8.692993531219926, tensor([8.0623], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([7.8295], grad_fn=<UnbindBackward0>))\n",
      "(8.224431573221159, tensor([8.0802], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([8.5153], grad_fn=<UnbindBackward0>))\n",
      "(7.490529402060711, tensor([6.5196], grad_fn=<UnbindBackward0>))\n",
      "(7.3914152346753585, tensor([7.1617], grad_fn=<UnbindBackward0>))\n",
      "(9.556692215591841, tensor([8.3581], grad_fn=<UnbindBackward0>))\n",
      "(7.668093709082406, tensor([6.8791], grad_fn=<UnbindBackward0>))\n",
      "(7.584264818389059, tensor([7.3992], grad_fn=<UnbindBackward0>))\n",
      "(8.01532730902172, tensor([7.5177], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([8.1693], grad_fn=<UnbindBackward0>))\n",
      "(7.607381425639791, tensor([8.7321], grad_fn=<UnbindBackward0>))\n",
      "(6.398594934535208, tensor([7.6075], grad_fn=<UnbindBackward0>))\n",
      "(9.765546451422905, tensor([7.7911], grad_fn=<UnbindBackward0>))\n",
      "(9.236787541991031, tensor([6.5320], grad_fn=<UnbindBackward0>))\n",
      "(8.449770515098804, tensor([6.2203], grad_fn=<UnbindBackward0>))\n",
      "(6.618738983517219, tensor([8.8167], grad_fn=<UnbindBackward0>))\n",
      "(8.711278615130434, tensor([7.0702], grad_fn=<UnbindBackward0>))\n",
      "(8.68710472813351, tensor([6.8850], grad_fn=<UnbindBackward0>))\n",
      "(9.15461622320382, tensor([8.5154], grad_fn=<UnbindBackward0>))\n",
      "(9.780019638535883, tensor([6.3360], grad_fn=<UnbindBackward0>))\n",
      "(7.564757012905729, tensor([7.3400], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([9.2894], grad_fn=<UnbindBackward0>))\n",
      "(6.822197390620491, tensor([8.6933], grad_fn=<UnbindBackward0>))\n",
      "(8.746557354543503, tensor([7.1622], grad_fn=<UnbindBackward0>))\n",
      "(8.80492526261806, tensor([6.8863], grad_fn=<UnbindBackward0>))\n",
      "(8.40178233990491, tensor([6.3115], grad_fn=<UnbindBackward0>))\n",
      "(8.180320874773685, tensor([7.1569], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([7.7153], grad_fn=<UnbindBackward0>))\n",
      "(6.994849985833071, tensor([8.5677], grad_fn=<UnbindBackward0>))\n",
      "(9.112286431500795, tensor([8.9544], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([7.2747], grad_fn=<UnbindBackward0>))\n",
      "(8.524764456912555, tensor([9.6662], grad_fn=<UnbindBackward0>))\n",
      "(6.7661917146603505, tensor([8.4282], grad_fn=<UnbindBackward0>))\n",
      "(8.07246736935477, tensor([8.4492], grad_fn=<UnbindBackward0>))\n",
      "(9.321971188147367, tensor([9.6063], grad_fn=<UnbindBackward0>))\n",
      "(8.48632152774915, tensor([8.0088], grad_fn=<UnbindBackward0>))\n",
      "(6.97914527506881, tensor([7.3739], grad_fn=<UnbindBackward0>))\n",
      "(7.4815557019095165, tensor([8.8238], grad_fn=<UnbindBackward0>))\n",
      "(6.476972362889683, tensor([9.0091], grad_fn=<UnbindBackward0>))\n",
      "(7.6544432264701125, tensor([8.4768], grad_fn=<UnbindBackward0>))\n",
      "(6.9411900550683745, tensor([6.5017], grad_fn=<UnbindBackward0>))\n",
      "(7.348587530927593, tensor([6.8332], grad_fn=<UnbindBackward0>))\n",
      "(8.062747901086354, tensor([7.1327], grad_fn=<UnbindBackward0>))\n",
      "(6.131226489483141, tensor([6.4822], grad_fn=<UnbindBackward0>))\n",
      "(9.38210641483062, tensor([9.9916], grad_fn=<UnbindBackward0>))\n",
      "(8.934586870389676, tensor([6.2242], grad_fn=<UnbindBackward0>))\n",
      "(7.3777589082278725, tensor([7.2954], grad_fn=<UnbindBackward0>))\n",
      "(8.565983355585669, tensor([8.9806], grad_fn=<UnbindBackward0>))\n",
      "(8.459564078579602, tensor([10.0523], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([8.5117], grad_fn=<UnbindBackward0>))\n",
      "(6.626717749249025, tensor([7.7071], grad_fn=<UnbindBackward0>))\n",
      "(6.601230118728877, tensor([6.1955], grad_fn=<UnbindBackward0>))\n",
      "(8.84404789894249, tensor([9.8518], grad_fn=<UnbindBackward0>))\n",
      "(8.573951525234847, tensor([10.3342], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([6.7479], grad_fn=<UnbindBackward0>))\n",
      "(7.1569563646156364, tensor([6.1642], grad_fn=<UnbindBackward0>))\n",
      "(8.934323331054905, tensor([9.4420], grad_fn=<UnbindBackward0>))\n",
      "(7.701652362642226, tensor([7.2231], grad_fn=<UnbindBackward0>))\n",
      "(8.658866349732383, tensor([7.7369], grad_fn=<UnbindBackward0>))\n",
      "(8.48920515487607, tensor([7.3436], grad_fn=<UnbindBackward0>))\n",
      "(9.727346854817254, tensor([7.2212], grad_fn=<UnbindBackward0>))\n",
      "(7.749322464660356, tensor([7.1155], grad_fn=<UnbindBackward0>))\n",
      "(8.544808358449211, tensor([7.4639], grad_fn=<UnbindBackward0>))\n",
      "(9.313708904953216, tensor([7.9143], grad_fn=<UnbindBackward0>))\n",
      "(9.452815851589884, tensor([7.3742], grad_fn=<UnbindBackward0>))\n",
      "(7.087573705557973, tensor([8.0609], grad_fn=<UnbindBackward0>))\n",
      "(9.8233073354086, tensor([8.0189], grad_fn=<UnbindBackward0>))\n",
      "(7.650168700845001, tensor([8.8859], grad_fn=<UnbindBackward0>))\n",
      "(9.744843459096531, tensor([7.9677], grad_fn=<UnbindBackward0>))\n",
      "(8.644178203170727, tensor([7.6098], grad_fn=<UnbindBackward0>))\n",
      "(9.179674957665297, tensor([8.7846], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([9.4720], grad_fn=<UnbindBackward0>))\n",
      "(6.042632833682381, tensor([10.0720], grad_fn=<UnbindBackward0>))\n",
      "(8.385260520155413, tensor([8.7070], grad_fn=<UnbindBackward0>))\n",
      "(8.341648618901306, tensor([7.2844], grad_fn=<UnbindBackward0>))\n",
      "(9.103423089630105, tensor([7.1273], grad_fn=<UnbindBackward0>))\n",
      "(7.221105098182496, tensor([7.8219], grad_fn=<UnbindBackward0>))\n",
      "(8.513586695822125, tensor([6.4858], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([10.0302], grad_fn=<UnbindBackward0>))\n",
      "(7.467942332285852, tensor([6.4547], grad_fn=<UnbindBackward0>))\n",
      "(7.599401333415815, tensor([6.7493], grad_fn=<UnbindBackward0>))\n",
      "(8.101071503119543, tensor([7.8297], grad_fn=<UnbindBackward0>))\n",
      "(8.252706676567644, tensor([7.3763], grad_fn=<UnbindBackward0>))\n",
      "(7.827240901752812, tensor([7.7567], grad_fn=<UnbindBackward0>))\n",
      "(8.53346016388011, tensor([9.5266], grad_fn=<UnbindBackward0>))\n",
      "(7.805474625270857, tensor([8.6503], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([7.5745], grad_fn=<UnbindBackward0>))\n",
      "(7.429520842786462, tensor([7.2238], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([9.0009], grad_fn=<UnbindBackward0>))\n",
      "(8.22282213081366, tensor([5.9959], grad_fn=<UnbindBackward0>))\n",
      "(8.740496729931813, tensor([8.8724], grad_fn=<UnbindBackward0>))\n",
      "(8.596558746796978, tensor([7.1068], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([6.4352], grad_fn=<UnbindBackward0>))\n",
      "(7.957877358489813, tensor([6.2944], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([6.1891], grad_fn=<UnbindBackward0>))\n",
      "(7.095064377287131, tensor([6.4286], grad_fn=<UnbindBackward0>))\n",
      "(7.735870319952567, tensor([8.3696], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([6.2633], grad_fn=<UnbindBackward0>))\n",
      "(6.259581464064923, tensor([5.9689], grad_fn=<UnbindBackward0>))\n",
      "(8.927446816256198, tensor([8.4373], grad_fn=<UnbindBackward0>))\n",
      "(7.271008538280992, tensor([7.3877], grad_fn=<UnbindBackward0>))\n",
      "(6.0112671744041615, tensor([7.8797], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([10.8220], grad_fn=<UnbindBackward0>))\n",
      "(9.053569596026234, tensor([8.4877], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([6.5524], grad_fn=<UnbindBackward0>))\n",
      "(9.18049952996516, tensor([6.5166], grad_fn=<UnbindBackward0>))\n",
      "(7.400620577371135, tensor([6.3247], grad_fn=<UnbindBackward0>))\n",
      "(7.990576881743923, tensor([7.4107], grad_fn=<UnbindBackward0>))\n",
      "(8.14757773620177, tensor([7.9493], grad_fn=<UnbindBackward0>))\n",
      "(6.230481447578482, tensor([6.3670], grad_fn=<UnbindBackward0>))\n",
      "(8.138856750696325, tensor([7.8198], grad_fn=<UnbindBackward0>))\n",
      "(8.599510233905452, tensor([6.9417], grad_fn=<UnbindBackward0>))\n",
      "(9.519294579703503, tensor([6.7908], grad_fn=<UnbindBackward0>))\n",
      "(6.932447891572509, tensor([9.3010], grad_fn=<UnbindBackward0>))\n",
      "(7.827639546366422, tensor([6.7895], grad_fn=<UnbindBackward0>))\n",
      "(8.947415950650129, tensor([9.6100], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([6.3857], grad_fn=<UnbindBackward0>))\n",
      "(8.21959545417708, tensor([7.2052], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.4248], grad_fn=<UnbindBackward0>))\n",
      "(9.182763604205949, tensor([10.1109], grad_fn=<UnbindBackward0>))\n",
      "(6.037870919922137, tensor([8.9615], grad_fn=<UnbindBackward0>))\n",
      "(6.675823221634848, tensor([6.6893], grad_fn=<UnbindBackward0>))\n",
      "(9.295416544331301, tensor([9.1222], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.2159], grad_fn=<UnbindBackward0>))\n",
      "(6.542471960506805, tensor([8.2295], grad_fn=<UnbindBackward0>))\n",
      "(7.559038255443384, tensor([8.9718], grad_fn=<UnbindBackward0>))\n",
      "(8.339739766019143, tensor([9.6983], grad_fn=<UnbindBackward0>))\n",
      "(8.744966011114109, tensor([6.0539], grad_fn=<UnbindBackward0>))\n",
      "(6.881411303642535, tensor([6.4561], grad_fn=<UnbindBackward0>))\n",
      "(6.99117688712121, tensor([8.6842], grad_fn=<UnbindBackward0>))\n",
      "(8.390268497842571, tensor([6.6828], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([6.3263], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([6.7595], grad_fn=<UnbindBackward0>))\n",
      "(8.841014310483892, tensor([6.4688], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([7.5413], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([7.3943], grad_fn=<UnbindBackward0>))\n",
      "(8.406485069431817, tensor([6.7902], grad_fn=<UnbindBackward0>))\n",
      "(5.996452088619021, tensor([8.9414], grad_fn=<UnbindBackward0>))\n",
      "(8.051022208190679, tensor([8.8333], grad_fn=<UnbindBackward0>))\n",
      "(8.00736706798333, tensor([7.2470], grad_fn=<UnbindBackward0>))\n",
      "(7.543273346705446, tensor([8.3921], grad_fn=<UnbindBackward0>))\n",
      "(7.425953657077541, tensor([9.0490], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([7.4298], grad_fn=<UnbindBackward0>))\n",
      "(8.061802274538348, tensor([6.7358], grad_fn=<UnbindBackward0>))\n",
      "(6.606650186198215, tensor([6.7623], grad_fn=<UnbindBackward0>))\n",
      "(7.95892649305011, tensor([7.8325], grad_fn=<UnbindBackward0>))\n",
      "(6.240275845170769, tensor([6.6687], grad_fn=<UnbindBackward0>))\n",
      "(7.855544677915663, tensor([8.6041], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([9.0255], grad_fn=<UnbindBackward0>))\n",
      "(7.754052639035757, tensor([6.2287], grad_fn=<UnbindBackward0>))\n",
      "(6.257667587882639, tensor([8.1833], grad_fn=<UnbindBackward0>))\n",
      "(8.343553835005117, tensor([8.4618], grad_fn=<UnbindBackward0>))\n",
      "(8.40200678160712, tensor([8.5668], grad_fn=<UnbindBackward0>))\n",
      "(6.675823221634848, tensor([7.7156], grad_fn=<UnbindBackward0>))\n",
      "(6.859614903654202, tensor([6.2742], grad_fn=<UnbindBackward0>))\n",
      "(8.641532465671846, tensor([6.8149], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([7.2117], grad_fn=<UnbindBackward0>))\n",
      "(7.872073979866873, tensor([7.1122], grad_fn=<UnbindBackward0>))\n",
      "(6.396929655216146, tensor([7.9241], grad_fn=<UnbindBackward0>))\n",
      "(6.810142450115136, tensor([8.7377], grad_fn=<UnbindBackward0>))\n",
      "(8.059908334578276, tensor([8.7530], grad_fn=<UnbindBackward0>))\n",
      "(7.113142108707088, tensor([7.8277], grad_fn=<UnbindBackward0>))\n",
      "(8.28121766128665, tensor([8.7900], grad_fn=<UnbindBackward0>))\n",
      "(7.713784616598755, tensor([6.1995], grad_fn=<UnbindBackward0>))\n",
      "(7.687538766201629, tensor([8.5282], grad_fn=<UnbindBackward0>))\n",
      "(8.827614750837508, tensor([9.6640], grad_fn=<UnbindBackward0>))\n",
      "(6.975413927455952, tensor([8.6649], grad_fn=<UnbindBackward0>))\n",
      "(7.188412736496954, tensor([6.2984], grad_fn=<UnbindBackward0>))\n",
      "(8.962263554116761, tensor([8.3597], grad_fn=<UnbindBackward0>))\n",
      "(8.685246776412487, tensor([6.2489], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([9.4961], grad_fn=<UnbindBackward0>))\n",
      "(7.147559271189454, tensor([7.3751], grad_fn=<UnbindBackward0>))\n",
      "(6.408528791059498, tensor([8.4205], grad_fn=<UnbindBackward0>))\n",
      "(8.617400451833262, tensor([6.6663], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([7.6913], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([7.8376], grad_fn=<UnbindBackward0>))\n",
      "(9.655667013509477, tensor([8.6009], grad_fn=<UnbindBackward0>))\n",
      "(8.496990484098719, tensor([7.8204], grad_fn=<UnbindBackward0>))\n",
      "(9.118225083068378, tensor([8.8783], grad_fn=<UnbindBackward0>))\n",
      "(9.188605879830176, tensor([8.6801], grad_fn=<UnbindBackward0>))\n",
      "(6.951772164398911, tensor([6.9542], grad_fn=<UnbindBackward0>))\n",
      "(8.593042503699674, tensor([8.3651], grad_fn=<UnbindBackward0>))\n",
      "(8.510772623613315, tensor([9.4990], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([10.3003], grad_fn=<UnbindBackward0>))\n",
      "(8.373322820996535, tensor([9.2227], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([8.3826], grad_fn=<UnbindBackward0>))\n",
      "(6.1903154058531475, tensor([8.8952], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([8.7263], grad_fn=<UnbindBackward0>))\n",
      "(8.822322177471738, tensor([8.8843], grad_fn=<UnbindBackward0>))\n",
      "(9.14494815320913, tensor([7.8490], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([8.8136], grad_fn=<UnbindBackward0>))\n",
      "(8.55333223803211, tensor([7.9425], grad_fn=<UnbindBackward0>))\n",
      "(8.565983355585669, tensor([5.8090], grad_fn=<UnbindBackward0>))\n",
      "(8.59637398929068, tensor([8.6175], grad_fn=<UnbindBackward0>))\n",
      "(7.098375638590786, tensor([8.6096], grad_fn=<UnbindBackward0>))\n",
      "(6.208590026096629, tensor([6.5093], grad_fn=<UnbindBackward0>))\n",
      "(6.977281341630747, tensor([9.5703], grad_fn=<UnbindBackward0>))\n",
      "(6.359573868672378, tensor([6.5143], grad_fn=<UnbindBackward0>))\n",
      "(6.421622267806518, tensor([6.7972], grad_fn=<UnbindBackward0>))\n",
      "(9.465447598495786, tensor([7.8265], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([6.6780], grad_fn=<UnbindBackward0>))\n",
      "(6.6039438246004725, tensor([10.0219], grad_fn=<UnbindBackward0>))\n",
      "(6.297109319933935, tensor([7.2185], grad_fn=<UnbindBackward0>))\n",
      "(7.142036574706803, tensor([6.6192], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([6.5616], grad_fn=<UnbindBackward0>))\n",
      "(8.434463543817241, tensor([8.2287], grad_fn=<UnbindBackward0>))\n",
      "(7.999007213243955, tensor([6.4362], grad_fn=<UnbindBackward0>))\n",
      "(7.742402021815782, tensor([9.1333], grad_fn=<UnbindBackward0>))\n",
      "(6.398594934535208, tensor([7.1926], grad_fn=<UnbindBackward0>))\n",
      "(8.249052274171293, tensor([6.2137], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([8.1967], grad_fn=<UnbindBackward0>))\n",
      "(7.841492924460013, tensor([6.2573], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([6.3267], grad_fn=<UnbindBackward0>))\n",
      "(7.150701457592526, tensor([6.3277], grad_fn=<UnbindBackward0>))\n",
      "(7.418180822726788, tensor([6.4921], grad_fn=<UnbindBackward0>))\n",
      "(7.044032897274685, tensor([8.4647], grad_fn=<UnbindBackward0>))\n",
      "(6.953684210870537, tensor([8.6145], grad_fn=<UnbindBackward0>))\n",
      "(6.642486801367256, tensor([7.8599], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([8.5131], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([8.2219], grad_fn=<UnbindBackward0>))\n",
      "(9.467305471765902, tensor([6.8338], grad_fn=<UnbindBackward0>))\n",
      "(8.946114375560743, tensor([7.8261], grad_fn=<UnbindBackward0>))\n",
      "(6.045005314036012, tensor([7.8617], grad_fn=<UnbindBackward0>))\n",
      "(8.189244525735901, tensor([6.3735], grad_fn=<UnbindBackward0>))\n",
      "(8.630521876723241, tensor([9.4870], grad_fn=<UnbindBackward0>))\n",
      "(8.193400231952097, tensor([6.6642], grad_fn=<UnbindBackward0>))\n",
      "(7.016609683894219, tensor([6.4106], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([6.3292], grad_fn=<UnbindBackward0>))\n",
      "(9.734003244543665, tensor([9.1410], grad_fn=<UnbindBackward0>))\n",
      "(7.829630389150193, tensor([9.4389], grad_fn=<UnbindBackward0>))\n",
      "(8.894533135798554, tensor([9.9975], grad_fn=<UnbindBackward0>))\n",
      "(8.459564078579602, tensor([10.0300], grad_fn=<UnbindBackward0>))\n",
      "(9.244065241377804, tensor([5.8589], grad_fn=<UnbindBackward0>))\n",
      "(8.780326390946605, tensor([7.6863], grad_fn=<UnbindBackward0>))\n",
      "(9.755857235532103, tensor([7.0396], grad_fn=<UnbindBackward0>))\n",
      "(6.626717749249025, tensor([9.9367], grad_fn=<UnbindBackward0>))\n",
      "(8.521185212685776, tensor([8.2994], grad_fn=<UnbindBackward0>))\n",
      "(8.622633703874234, tensor([6.7812], grad_fn=<UnbindBackward0>))\n",
      "(7.8961806086154915, tensor([8.4181], grad_fn=<UnbindBackward0>))\n",
      "(9.132919225007598, tensor([10.5561], grad_fn=<UnbindBackward0>))\n",
      "(6.901737206656574, tensor([6.3580], grad_fn=<UnbindBackward0>))\n",
      "(8.983816112502495, tensor([7.4837], grad_fn=<UnbindBackward0>))\n",
      "(6.408528791059498, tensor([8.7893], grad_fn=<UnbindBackward0>))\n",
      "(7.087573705557973, tensor([7.1848], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([7.9346], grad_fn=<UnbindBackward0>))\n",
      "(8.084254106307318, tensor([9.1528], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([8.8853], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.3053], grad_fn=<UnbindBackward0>))\n",
      "(8.57602797713713, tensor([7.6143], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([9.0669], grad_fn=<UnbindBackward0>))\n",
      "(9.488275092973083, tensor([8.4622], grad_fn=<UnbindBackward0>))\n",
      "(7.988882253309227, tensor([6.0979], grad_fn=<UnbindBackward0>))\n",
      "(9.839482222560216, tensor([6.2310], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([6.3340], grad_fn=<UnbindBackward0>))\n",
      "(9.220191688026556, tensor([6.3658], grad_fn=<UnbindBackward0>))\n",
      "(8.127109185346375, tensor([6.3472], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([8.5079], grad_fn=<UnbindBackward0>))\n",
      "(7.371489295214277, tensor([8.5185], grad_fn=<UnbindBackward0>))\n",
      "(6.1779441140506, tensor([8.6179], grad_fn=<UnbindBackward0>))\n",
      "(6.975413927455952, tensor([8.6941], grad_fn=<UnbindBackward0>))\n",
      "(7.122866658599083, tensor([7.6916], grad_fn=<UnbindBackward0>))\n",
      "(9.266815218904162, tensor([7.8379], grad_fn=<UnbindBackward0>))\n",
      "(8.236155661683124, tensor([8.5549], grad_fn=<UnbindBackward0>))\n",
      "(9.093469420244768, tensor([6.2913], grad_fn=<UnbindBackward0>))\n",
      "(6.626717749249025, tensor([7.1824], grad_fn=<UnbindBackward0>))\n",
      "(8.161375023197486, tensor([8.3536], grad_fn=<UnbindBackward0>))\n",
      "(9.276876895776427, tensor([5.8853], grad_fn=<UnbindBackward0>))\n",
      "(8.035926369891792, tensor([6.3428], grad_fn=<UnbindBackward0>))\n",
      "(9.179056081936416, tensor([6.3286], grad_fn=<UnbindBackward0>))\n",
      "(6.633318433280377, tensor([9.4614], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([8.8677], grad_fn=<UnbindBackward0>))\n",
      "(9.574983485564092, tensor([8.7159], grad_fn=<UnbindBackward0>))\n",
      "(7.8961806086154915, tensor([10.2090], grad_fn=<UnbindBackward0>))\n",
      "(8.33302993974291, tensor([6.7339], grad_fn=<UnbindBackward0>))\n",
      "(6.652863029353347, tensor([6.4971], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([8.5153], grad_fn=<UnbindBackward0>))\n",
      "(8.704668113450987, tensor([8.4999], grad_fn=<UnbindBackward0>))\n",
      "(9.176162920174596, tensor([6.2102], grad_fn=<UnbindBackward0>))\n",
      "(7.560080465021827, tensor([8.3040], grad_fn=<UnbindBackward0>))\n",
      "(6.948897222313312, tensor([8.7509], grad_fn=<UnbindBackward0>))\n",
      "(7.014814351275545, tensor([8.0061], grad_fn=<UnbindBackward0>))\n",
      "(7.58629630715272, tensor([10.3349], grad_fn=<UnbindBackward0>))\n",
      "(8.916506080039204, tensor([6.3208], grad_fn=<UnbindBackward0>))\n",
      "(6.2285110035911835, tensor([6.4597], grad_fn=<UnbindBackward0>))\n",
      "(6.326149473155099, tensor([8.4018], grad_fn=<UnbindBackward0>))\n",
      "(7.431299675155903, tensor([7.0374], grad_fn=<UnbindBackward0>))\n",
      "(7.520234556474628, tensor([7.7187], grad_fn=<UnbindBackward0>))\n",
      "(8.721602344674197, tensor([7.4263], grad_fn=<UnbindBackward0>))\n",
      "(9.39349491507403, tensor([7.2576], grad_fn=<UnbindBackward0>))\n",
      "(8.984568369308281, tensor([6.5858], grad_fn=<UnbindBackward0>))\n",
      "(9.718903438087914, tensor([6.1582], grad_fn=<UnbindBackward0>))\n",
      "(7.775695749915245, tensor([8.6293], grad_fn=<UnbindBackward0>))\n",
      "(8.679822114864455, tensor([7.0460], grad_fn=<UnbindBackward0>))\n",
      "(8.383890344101816, tensor([5.8682], grad_fn=<UnbindBackward0>))\n",
      "(8.86177531100083, tensor([9.6352], grad_fn=<UnbindBackward0>))\n",
      "(6.38856140554563, tensor([7.7542], grad_fn=<UnbindBackward0>))\n",
      "(7.513709247839705, tensor([6.6657], grad_fn=<UnbindBackward0>))\n",
      "(8.204398418149381, tensor([9.9824], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([6.2278], grad_fn=<UnbindBackward0>))\n",
      "(7.447751280047908, tensor([8.1748], grad_fn=<UnbindBackward0>))\n",
      "(7.434847875211999, tensor([7.7810], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([8.8815], grad_fn=<UnbindBackward0>))\n",
      "(6.825460036255307, tensor([6.3622], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([7.7766], grad_fn=<UnbindBackward0>))\n",
      "(6.352629396319567, tensor([6.4370], grad_fn=<UnbindBackward0>))\n",
      "(9.42084437974393, tensor([6.3255], grad_fn=<UnbindBackward0>))\n",
      "(6.9782137426306985, tensor([7.2863], grad_fn=<UnbindBackward0>))\n",
      "(8.054204897064407, tensor([7.1389], grad_fn=<UnbindBackward0>))\n",
      "(8.664232934065552, tensor([7.2694], grad_fn=<UnbindBackward0>))\n",
      "(9.074176947163311, tensor([6.6290], grad_fn=<UnbindBackward0>))\n",
      "(8.739055922830724, tensor([7.2672], grad_fn=<UnbindBackward0>))\n",
      "(8.60557030203365, tensor([8.0453], grad_fn=<UnbindBackward0>))\n",
      "(8.76795190976342, tensor([6.3154], grad_fn=<UnbindBackward0>))\n",
      "(8.010359588919783, tensor([6.4998], grad_fn=<UnbindBackward0>))\n",
      "(6.570882962339584, tensor([10.0124], grad_fn=<UnbindBackward0>))\n",
      "(6.899723107284872, tensor([6.7268], grad_fn=<UnbindBackward0>))\n",
      "(9.841399360222706, tensor([8.3860], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([6.3386], grad_fn=<UnbindBackward0>))\n",
      "(7.847762537473608, tensor([7.2832], grad_fn=<UnbindBackward0>))\n",
      "(8.456593569287309, tensor([8.7347], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([9.5977], grad_fn=<UnbindBackward0>))\n",
      "(9.251194365984448, tensor([8.3886], grad_fn=<UnbindBackward0>))\n",
      "(7.057897937411856, tensor([7.3411], grad_fn=<UnbindBackward0>))\n",
      "(8.09376775793108, tensor([7.8512], grad_fn=<UnbindBackward0>))\n",
      "(7.945909598613133, tensor([8.2139], grad_fn=<UnbindBackward0>))\n",
      "(8.265907334155747, tensor([7.8514], grad_fn=<UnbindBackward0>))\n",
      "(7.188412736496954, tensor([9.9745], grad_fn=<UnbindBackward0>))\n",
      "(8.272315147956022, tensor([6.2356], grad_fn=<UnbindBackward0>))\n",
      "(8.260751354700513, tensor([7.2228], grad_fn=<UnbindBackward0>))\n",
      "(7.02108396428914, tensor([6.3588], grad_fn=<UnbindBackward0>))\n",
      "(7.515344571180436, tensor([8.5730], grad_fn=<UnbindBackward0>))\n",
      "(6.635946555686647, tensor([7.8069], grad_fn=<UnbindBackward0>))\n",
      "(7.933796874815411, tensor([7.4133], grad_fn=<UnbindBackward0>))\n",
      "(8.082093278178382, tensor([6.9333], grad_fn=<UnbindBackward0>))\n",
      "(9.67457740632758, tensor([9.0510], grad_fn=<UnbindBackward0>))\n",
      "(8.926118971153382, tensor([6.4480], grad_fn=<UnbindBackward0>))\n",
      "(7.19668657083435, tensor([6.5774], grad_fn=<UnbindBackward0>))\n",
      "(9.132919225007598, tensor([8.8668], grad_fn=<UnbindBackward0>))\n",
      "(7.379008127628304, tensor([6.2918], grad_fn=<UnbindBackward0>))\n",
      "(6.806829360392176, tensor([6.1253], grad_fn=<UnbindBackward0>))\n",
      "(6.747586526829315, tensor([6.8404], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([8.3739], grad_fn=<UnbindBackward0>))\n",
      "(7.681560362559537, tensor([10.0597], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([6.4173], grad_fn=<UnbindBackward0>))\n",
      "(7.249215057114389, tensor([8.9988], grad_fn=<UnbindBackward0>))\n",
      "(9.621124641561947, tensor([5.9242], grad_fn=<UnbindBackward0>))\n",
      "(9.560997243589352, tensor([8.5573], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.4478], grad_fn=<UnbindBackward0>))\n",
      "(7.051855622955894, tensor([9.2195], grad_fn=<UnbindBackward0>))\n",
      "(9.218407743053941, tensor([7.7346], grad_fn=<UnbindBackward0>))\n",
      "(9.704060527839234, tensor([8.7594], grad_fn=<UnbindBackward0>))\n",
      "(9.200189020919808, tensor([7.2196], grad_fn=<UnbindBackward0>))\n",
      "(9.215526898663482, tensor([8.9216], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([6.6453], grad_fn=<UnbindBackward0>))\n",
      "(6.690842277418564, tensor([8.6900], grad_fn=<UnbindBackward0>))\n",
      "(8.676246121270838, tensor([6.6336], grad_fn=<UnbindBackward0>))\n",
      "(8.91018077801329, tensor([9.3886], grad_fn=<UnbindBackward0>))\n",
      "(7.102499355774649, tensor([8.0062], grad_fn=<UnbindBackward0>))\n",
      "(8.473241303887054, tensor([5.8461], grad_fn=<UnbindBackward0>))\n",
      "(8.522777569710138, tensor([8.5435], grad_fn=<UnbindBackward0>))\n",
      "(6.352629396319567, tensor([9.8222], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([6.2764], grad_fn=<UnbindBackward0>))\n",
      "(8.685246776412487, tensor([9.2028], grad_fn=<UnbindBackward0>))\n",
      "(7.490529402060711, tensor([10.4670], grad_fn=<UnbindBackward0>))\n",
      "(7.645397699428633, tensor([9.9921], grad_fn=<UnbindBackward0>))\n",
      "(6.464588303689961, tensor([8.5590], grad_fn=<UnbindBackward0>))\n",
      "(6.734591659972948, tensor([8.0239], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([7.1873], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([6.3101], grad_fn=<UnbindBackward0>))\n",
      "(6.694562058521095, tensor([6.6960], grad_fn=<UnbindBackward0>))\n",
      "(7.866338923046544, tensor([6.7469], grad_fn=<UnbindBackward0>))\n",
      "(7.8434564043761155, tensor([7.8192], grad_fn=<UnbindBackward0>))\n",
      "(7.784057002639929, tensor([8.4529], grad_fn=<UnbindBackward0>))\n",
      "(6.6052979209482015, tensor([7.7983], grad_fn=<UnbindBackward0>))\n",
      "(7.849323818040561, tensor([8.3018], grad_fn=<UnbindBackward0>))\n",
      "(8.327484416188264, tensor([7.4495], grad_fn=<UnbindBackward0>))\n",
      "(7.9483852851118995, tensor([8.3084], grad_fn=<UnbindBackward0>))\n",
      "(7.71244383427499, tensor([8.5456], grad_fn=<UnbindBackward0>))\n",
      "(8.700847193443972, tensor([7.7845], grad_fn=<UnbindBackward0>))\n",
      "(7.142036574706803, tensor([9.4154], grad_fn=<UnbindBackward0>))\n",
      "(6.536691597591305, tensor([6.2276], grad_fn=<UnbindBackward0>))\n",
      "(9.670104253491726, tensor([7.4120], grad_fn=<UnbindBackward0>))\n",
      "(9.521934264474277, tensor([8.6876], grad_fn=<UnbindBackward0>))\n",
      "(7.512071245835466, tensor([10.0121], grad_fn=<UnbindBackward0>))\n",
      "(7.774015077250727, tensor([7.4889], grad_fn=<UnbindBackward0>))\n",
      "(7.969357742016346, tensor([7.2733], grad_fn=<UnbindBackward0>))\n",
      "(7.752764808851328, tensor([7.1905], grad_fn=<UnbindBackward0>))\n",
      "(7.732369222284388, tensor([6.5183], grad_fn=<UnbindBackward0>))\n",
      "(8.53070154144103, tensor([7.7938], grad_fn=<UnbindBackward0>))\n",
      "(9.500319803476646, tensor([6.2536], grad_fn=<UnbindBackward0>))\n",
      "(6.733401891837359, tensor([6.4075], grad_fn=<UnbindBackward0>))\n",
      "(7.529406457837013, tensor([8.3442], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([6.7164], grad_fn=<UnbindBackward0>))\n",
      "(7.9919305198524775, tensor([8.0272], grad_fn=<UnbindBackward0>))\n",
      "(6.115892125483034, tensor([6.8065], grad_fn=<UnbindBackward0>))\n",
      "(8.569785641535407, tensor([8.6319], grad_fn=<UnbindBackward0>))\n",
      "(7.195937226475569, tensor([7.8058], grad_fn=<UnbindBackward0>))\n",
      "(5.908082938168931, tensor([6.2805], grad_fn=<UnbindBackward0>))\n",
      "(7.443078374348516, tensor([7.5758], grad_fn=<UnbindBackward0>))\n",
      "(8.518592212329946, tensor([8.9515], grad_fn=<UnbindBackward0>))\n",
      "(9.413934064045677, tensor([7.9395], grad_fn=<UnbindBackward0>))\n",
      "(6.956545443151569, tensor([6.6605], grad_fn=<UnbindBackward0>))\n",
      "(8.35936910622267, tensor([8.9866], grad_fn=<UnbindBackward0>))\n",
      "(9.312806703520673, tensor([6.8133], grad_fn=<UnbindBackward0>))\n",
      "(6.102558594613569, tensor([8.2483], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([7.8497], grad_fn=<UnbindBackward0>))\n",
      "(9.273033441314606, tensor([7.7108], grad_fn=<UnbindBackward0>))\n",
      "(7.051855622955894, tensor([9.0690], grad_fn=<UnbindBackward0>))\n",
      "(8.302017809751204, tensor([9.2345], grad_fn=<UnbindBackward0>))\n",
      "(7.232733136177615, tensor([10.1069], grad_fn=<UnbindBackward0>))\n",
      "(7.715123603632105, tensor([7.2450], grad_fn=<UnbindBackward0>))\n",
      "(9.135185698375423, tensor([8.3521], grad_fn=<UnbindBackward0>))\n",
      "(6.543911845564792, tensor([7.1512], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([7.8394], grad_fn=<UnbindBackward0>))\n",
      "(7.782807262839695, tensor([6.2988], grad_fn=<UnbindBackward0>))\n",
      "(8.892886141190731, tensor([8.7608], grad_fn=<UnbindBackward0>))\n",
      "(7.016609683894219, tensor([6.8667], grad_fn=<UnbindBackward0>))\n",
      "(8.648922962094131, tensor([6.5498], grad_fn=<UnbindBackward0>))\n",
      "(7.787382026484701, tensor([8.3258], grad_fn=<UnbindBackward0>))\n",
      "(7.154615356913663, tensor([8.1181], grad_fn=<UnbindBackward0>))\n",
      "(8.360539381370861, tensor([7.4603], grad_fn=<UnbindBackward0>))\n",
      "(9.353141170607854, tensor([9.5277], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([6.8263], grad_fn=<UnbindBackward0>))\n",
      "(6.823286122355687, tensor([6.4093], grad_fn=<UnbindBackward0>))\n",
      "(7.947325027016463, tensor([8.4002], grad_fn=<UnbindBackward0>))\n",
      "(8.157657015196472, tensor([6.2974], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([9.2798], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([8.4026], grad_fn=<UnbindBackward0>))\n",
      "(6.9650803456014065, tensor([6.8310], grad_fn=<UnbindBackward0>))\n",
      "(6.263398262591624, tensor([9.6868], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.3879], grad_fn=<UnbindBackward0>))\n",
      "(7.679713639966372, tensor([6.3475], grad_fn=<UnbindBackward0>))\n",
      "(8.115819701211327, tensor([6.8600], grad_fn=<UnbindBackward0>))\n",
      "(7.590346945602565, tensor([8.9493], grad_fn=<UnbindBackward0>))\n",
      "(9.634888894560614, tensor([7.7813], grad_fn=<UnbindBackward0>))\n",
      "(6.947937068614969, tensor([8.7745], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([8.0312], grad_fn=<UnbindBackward0>))\n",
      "(6.97914527506881, tensor([7.8851], grad_fn=<UnbindBackward0>))\n",
      "(8.574140471857987, tensor([6.6265], grad_fn=<UnbindBackward0>))\n",
      "(8.174984532943087, tensor([6.6071], grad_fn=<UnbindBackward0>))\n",
      "(8.81640849968169, tensor([8.8801], grad_fn=<UnbindBackward0>))\n",
      "(9.131729971394272, tensor([8.8205], grad_fn=<UnbindBackward0>))\n",
      "(8.464846711044029, tensor([6.6024], grad_fn=<UnbindBackward0>))\n",
      "(6.721425700790643, tensor([7.3298], grad_fn=<UnbindBackward0>))\n",
      "(7.373374309910049, tensor([8.7089], grad_fn=<UnbindBackward0>))\n",
      "(8.725994381014571, tensor([9.5841], grad_fn=<UnbindBackward0>))\n",
      "(9.116578992161708, tensor([7.7785], grad_fn=<UnbindBackward0>))\n",
      "(8.369388996647842, tensor([6.7376], grad_fn=<UnbindBackward0>))\n",
      "(8.372860820526318, tensor([8.6613], grad_fn=<UnbindBackward0>))\n",
      "(8.786609455061125, tensor([8.0932], grad_fn=<UnbindBackward0>))\n",
      "(9.042513261332681, tensor([8.6897], grad_fn=<UnbindBackward0>))\n",
      "(7.7501841622578365, tensor([7.0822], grad_fn=<UnbindBackward0>))\n",
      "(8.263332667439967, tensor([8.9575], grad_fn=<UnbindBackward0>))\n",
      "(8.537975730598767, tensor([8.5459], grad_fn=<UnbindBackward0>))\n",
      "(6.97914527506881, tensor([8.4286], grad_fn=<UnbindBackward0>))\n",
      "(7.422373700986824, tensor([6.6779], grad_fn=<UnbindBackward0>))\n",
      "(7.582738488914411, tensor([6.4757], grad_fn=<UnbindBackward0>))\n",
      "(8.31434234336979, tensor([8.7825], grad_fn=<UnbindBackward0>))\n",
      "(8.39049553837028, tensor([9.4249], grad_fn=<UnbindBackward0>))\n",
      "(8.498418036089904, tensor([7.4755], grad_fn=<UnbindBackward0>))\n",
      "(8.491259809389733, tensor([6.2339], grad_fn=<UnbindBackward0>))\n",
      "(8.145259566516865, tensor([7.7770], grad_fn=<UnbindBackward0>))\n",
      "(8.09040229659332, tensor([6.5418], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([8.2628], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([6.8389], grad_fn=<UnbindBackward0>))\n",
      "(7.955775781534187, tensor([8.3546], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([8.3037], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([7.0448], grad_fn=<UnbindBackward0>))\n",
      "(7.454719949364001, tensor([6.4479], grad_fn=<UnbindBackward0>))\n",
      "(7.938088726896952, tensor([6.4030], grad_fn=<UnbindBackward0>))\n",
      "(6.658011045870748, tensor([7.3753], grad_fn=<UnbindBackward0>))\n",
      "(9.782110566369528, tensor([8.5730], grad_fn=<UnbindBackward0>))\n",
      "(6.70808408385307, tensor([7.8024], grad_fn=<UnbindBackward0>))\n",
      "(7.923710333969238, tensor([6.0402], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([6.2617], grad_fn=<UnbindBackward0>))\n",
      "(7.224753405767971, tensor([7.7624], grad_fn=<UnbindBackward0>))\n",
      "(7.761744984658913, tensor([10.2158], grad_fn=<UnbindBackward0>))\n",
      "(8.312135107648412, tensor([6.8001], grad_fn=<UnbindBackward0>))\n",
      "(7.941651252930556, tensor([7.2935], grad_fn=<UnbindBackward0>))\n",
      "(6.683360945766275, tensor([7.2985], grad_fn=<UnbindBackward0>))\n",
      "(8.63817111796914, tensor([8.2365], grad_fn=<UnbindBackward0>))\n",
      "(8.216088098632316, tensor([8.7911], grad_fn=<UnbindBackward0>))\n",
      "(7.618742377670413, tensor([9.8871], grad_fn=<UnbindBackward0>))\n",
      "(7.4109518755836366, tensor([9.4228], grad_fn=<UnbindBackward0>))\n",
      "(7.369600720526409, tensor([7.7492], grad_fn=<UnbindBackward0>))\n",
      "(6.376726947898627, tensor([9.4760], grad_fn=<UnbindBackward0>))\n",
      "(7.02108396428914, tensor([8.7435], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([8.2499], grad_fn=<UnbindBackward0>))\n",
      "(7.349230824613334, tensor([7.7446], grad_fn=<UnbindBackward0>))\n",
      "(7.698936199813447, tensor([6.2467], grad_fn=<UnbindBackward0>))\n",
      "(8.390949464841986, tensor([8.5885], grad_fn=<UnbindBackward0>))\n",
      "(8.6921543938039, tensor([7.5241], grad_fn=<UnbindBackward0>))\n",
      "(7.034387929915503, tensor([6.1882], grad_fn=<UnbindBackward0>))\n",
      "(8.3707791729607, tensor([8.5665], grad_fn=<UnbindBackward0>))\n",
      "(7.017506142941256, tensor([6.4409], grad_fn=<UnbindBackward0>))\n",
      "(8.894533135798554, tensor([9.9742], grad_fn=<UnbindBackward0>))\n",
      "(8.089175678837561, tensor([6.7651], grad_fn=<UnbindBackward0>))\n",
      "(6.728628613084702, tensor([5.6404], grad_fn=<UnbindBackward0>))\n",
      "(8.748939631537715, tensor([8.0490], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([7.4642], grad_fn=<UnbindBackward0>))\n",
      "(7.009408932708637, tensor([9.1357], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([9.8870], grad_fn=<UnbindBackward0>))\n",
      "(6.996681488176539, tensor([10.0146], grad_fn=<UnbindBackward0>))\n",
      "(8.712266432135355, tensor([10.0400], grad_fn=<UnbindBackward0>))\n",
      "(6.829793737512425, tensor([9.6033], grad_fn=<UnbindBackward0>))\n",
      "(9.237079669165805, tensor([7.7231], grad_fn=<UnbindBackward0>))\n",
      "(8.116715624819111, tensor([7.7542], grad_fn=<UnbindBackward0>))\n",
      "(8.259199362666282, tensor([10.6177], grad_fn=<UnbindBackward0>))\n",
      "(8.68777949199177, tensor([6.3888], grad_fn=<UnbindBackward0>))\n",
      "(6.626717749249025, tensor([8.7696], grad_fn=<UnbindBackward0>))\n",
      "(9.561630845638076, tensor([6.8087], grad_fn=<UnbindBackward0>))\n",
      "(9.566615235893575, tensor([7.4525], grad_fn=<UnbindBackward0>))\n",
      "(8.271548374755515, tensor([6.3829], grad_fn=<UnbindBackward0>))\n",
      "(7.455876687491824, tensor([6.3957], grad_fn=<UnbindBackward0>))\n",
      "(9.388904879892426, tensor([6.6357], grad_fn=<UnbindBackward0>))\n",
      "(8.517393171418904, tensor([7.2439], grad_fn=<UnbindBackward0>))\n",
      "(8.950792138768174, tensor([8.7079], grad_fn=<UnbindBackward0>))\n",
      "(7.3914152346753585, tensor([9.5002], grad_fn=<UnbindBackward0>))\n",
      "(8.773229786032473, tensor([6.8403], grad_fn=<UnbindBackward0>))\n",
      "(8.376781037699493, tensor([6.7730], grad_fn=<UnbindBackward0>))\n",
      "(6.230481447578482, tensor([6.4053], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([7.8710], grad_fn=<UnbindBackward0>))\n",
      "(8.120886021092838, tensor([7.3476], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([7.4792], grad_fn=<UnbindBackward0>))\n",
      "(6.342121418721152, tensor([7.8682], grad_fn=<UnbindBackward0>))\n",
      "(7.780720886117918, tensor([8.9921], grad_fn=<UnbindBackward0>))\n",
      "(9.748469830269519, tensor([9.4358], grad_fn=<UnbindBackward0>))\n",
      "(6.375024819828097, tensor([6.8478], grad_fn=<UnbindBackward0>))\n",
      "(8.504918160540624, tensor([9.0647], grad_fn=<UnbindBackward0>))\n",
      "(9.312355297382913, tensor([6.2615], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([7.7403], grad_fn=<UnbindBackward0>))\n",
      "(8.85366542803745, tensor([8.2684], grad_fn=<UnbindBackward0>))\n",
      "(6.651571873589727, tensor([8.6306], grad_fn=<UnbindBackward0>))\n",
      "(9.40640045488065, tensor([9.0967], grad_fn=<UnbindBackward0>))\n",
      "(6.97914527506881, tensor([7.7035], grad_fn=<UnbindBackward0>))\n",
      "(6.54534966033442, tensor([8.0207], grad_fn=<UnbindBackward0>))\n",
      "(8.920923462230686, tensor([7.9245], grad_fn=<UnbindBackward0>))\n",
      "(7.728855823852543, tensor([8.1617], grad_fn=<UnbindBackward0>))\n",
      "(8.981681639973862, tensor([6.3385], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([6.5526], grad_fn=<UnbindBackward0>))\n",
      "(7.939515260662406, tensor([7.1112], grad_fn=<UnbindBackward0>))\n",
      "(7.218176838403408, tensor([9.3088], grad_fn=<UnbindBackward0>))\n",
      "(8.722742874329398, tensor([6.7221], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([7.0204], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([8.2234], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([6.2463], grad_fn=<UnbindBackward0>))\n",
      "(8.105609402299896, tensor([6.1481], grad_fn=<UnbindBackward0>))\n",
      "(7.509335266016592, tensor([6.4185], grad_fn=<UnbindBackward0>))\n",
      "(8.653819788948063, tensor([8.2287], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([6.7590], grad_fn=<UnbindBackward0>))\n",
      "(8.893847217670283, tensor([7.1927], grad_fn=<UnbindBackward0>))\n",
      "(7.907283609426348, tensor([6.2713], grad_fn=<UnbindBackward0>))\n",
      "(8.3707791729607, tensor([7.0900], grad_fn=<UnbindBackward0>))\n",
      "(8.103796712981794, tensor([7.3803], grad_fn=<UnbindBackward0>))\n",
      "(7.093404625868766, tensor([10.0999], grad_fn=<UnbindBackward0>))\n",
      "(9.812249072101856, tensor([7.5411], grad_fn=<UnbindBackward0>))\n",
      "(9.357466436866476, tensor([6.8946], grad_fn=<UnbindBackward0>))\n",
      "(8.506940814951886, tensor([6.5800], grad_fn=<UnbindBackward0>))\n",
      "(7.236339342754344, tensor([7.8887], grad_fn=<UnbindBackward0>))\n",
      "(7.789868559054706, tensor([6.8141], grad_fn=<UnbindBackward0>))\n",
      "(6.401917196727186, tensor([6.7497], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([7.8528], grad_fn=<UnbindBackward0>))\n",
      "(9.473857817428666, tensor([8.0689], grad_fn=<UnbindBackward0>))\n",
      "(6.342121418721152, tensor([8.1159], grad_fn=<UnbindBackward0>))\n",
      "(7.644919344958857, tensor([7.5291], grad_fn=<UnbindBackward0>))\n",
      "(6.0014148779611505, tensor([6.2771], grad_fn=<UnbindBackward0>))\n",
      "(6.8679744089702925, tensor([7.3333], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([6.8205], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([7.3026], grad_fn=<UnbindBackward0>))\n",
      "(8.543445562560303, tensor([8.1660], grad_fn=<UnbindBackward0>))\n",
      "(7.173958319756794, tensor([8.9229], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.3468], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([7.6970], grad_fn=<UnbindBackward0>))\n",
      "(9.069813136839207, tensor([7.2946], grad_fn=<UnbindBackward0>))\n",
      "(7.555905093611346, tensor([6.8128], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([6.3941], grad_fn=<UnbindBackward0>))\n",
      "(8.450839690866216, tensor([8.0560], grad_fn=<UnbindBackward0>))\n",
      "(8.014335737299424, tensor([8.8530], grad_fn=<UnbindBackward0>))\n",
      "(9.496496280431916, tensor([8.5386], grad_fn=<UnbindBackward0>))\n",
      "(8.261784679514752, tensor([8.3300], grad_fn=<UnbindBackward0>))\n",
      "(8.921591063563978, tensor([6.3750], grad_fn=<UnbindBackward0>))\n",
      "(7.515344571180436, tensor([6.4978], grad_fn=<UnbindBackward0>))\n",
      "(6.774223886357614, tensor([8.7428], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([6.3087], grad_fn=<UnbindBackward0>))\n",
      "(7.630461261783627, tensor([9.0005], grad_fn=<UnbindBackward0>))\n",
      "(7.741099090035366, tensor([9.1174], grad_fn=<UnbindBackward0>))\n",
      "(8.512783482927537, tensor([6.5030], grad_fn=<UnbindBackward0>))\n",
      "(8.074337694089515, tensor([9.4458], grad_fn=<UnbindBackward0>))\n",
      "(6.8658910748834385, tensor([9.3939], grad_fn=<UnbindBackward0>))\n",
      "(8.545197387825835, tensor([7.2417], grad_fn=<UnbindBackward0>))\n",
      "(9.07566546864958, tensor([6.9020], grad_fn=<UnbindBackward0>))\n",
      "(6.956545443151569, tensor([6.8604], grad_fn=<UnbindBackward0>))\n",
      "(7.04141166379481, tensor([8.4544], grad_fn=<UnbindBackward0>))\n",
      "(7.4570320891223805, tensor([8.9743], grad_fn=<UnbindBackward0>))\n",
      "(7.733245646529795, tensor([8.4101], grad_fn=<UnbindBackward0>))\n",
      "(8.299783171949787, tensor([6.2713], grad_fn=<UnbindBackward0>))\n",
      "(7.562681246721884, tensor([8.0228], grad_fn=<UnbindBackward0>))\n",
      "(6.826545223556594, tensor([6.8749], grad_fn=<UnbindBackward0>))\n",
      "(6.853299093186078, tensor([7.1718], grad_fn=<UnbindBackward0>))\n",
      "(7.582738488914411, tensor([6.4682], grad_fn=<UnbindBackward0>))\n",
      "(7.588829878307813, tensor([9.4368], grad_fn=<UnbindBackward0>))\n",
      "(8.36450810375059, tensor([6.7720], grad_fn=<UnbindBackward0>))\n",
      "(9.716193654115616, tensor([8.5438], grad_fn=<UnbindBackward0>))\n",
      "(6.403574197934815, tensor([7.3885], grad_fn=<UnbindBackward0>))\n",
      "(7.607381425639791, tensor([6.9840], grad_fn=<UnbindBackward0>))\n",
      "(7.111512116496157, tensor([9.4946], grad_fn=<UnbindBackward0>))\n",
      "(7.781138509845015, tensor([6.4410], grad_fn=<UnbindBackward0>))\n",
      "(9.090542808520587, tensor([10.5428], grad_fn=<UnbindBackward0>))\n",
      "(7.5883236773352225, tensor([8.6132], grad_fn=<UnbindBackward0>))\n",
      "(8.687948111838727, tensor([9.2384], grad_fn=<UnbindBackward0>))\n",
      "(9.274722630905451, tensor([8.6044], grad_fn=<UnbindBackward0>))\n",
      "(9.095153962167066, tensor([7.8204], grad_fn=<UnbindBackward0>))\n",
      "(8.814033201652784, tensor([7.2904], grad_fn=<UnbindBackward0>))\n",
      "(9.415075523607037, tensor([9.5691], grad_fn=<UnbindBackward0>))\n",
      "(7.98139158158007, tensor([8.4421], grad_fn=<UnbindBackward0>))\n",
      "(6.741700694652055, tensor([7.3448], grad_fn=<UnbindBackward0>))\n",
      "(9.362803915102651, tensor([9.0122], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([6.1925], grad_fn=<UnbindBackward0>))\n",
      "(7.869019376499023, tensor([8.2053], grad_fn=<UnbindBackward0>))\n",
      "(7.77779262633883, tensor([9.0965], grad_fn=<UnbindBackward0>))\n",
      "(8.452334619067742, tensor([6.3908], grad_fn=<UnbindBackward0>))\n",
      "(8.634620608292202, tensor([6.7628], grad_fn=<UnbindBackward0>))\n",
      "(8.195057690895077, tensor([8.8792], grad_fn=<UnbindBackward0>))\n",
      "(7.983098940710892, tensor([7.8746], grad_fn=<UnbindBackward0>))\n",
      "(9.631153757031147, tensor([6.4742], grad_fn=<UnbindBackward0>))\n",
      "(9.192278229157774, tensor([8.0837], grad_fn=<UnbindBackward0>))\n",
      "(9.462887373659635, tensor([8.8000], grad_fn=<UnbindBackward0>))\n",
      "(9.266909723464076, tensor([8.6460], grad_fn=<UnbindBackward0>))\n",
      "(9.671050954310326, tensor([7.9237], grad_fn=<UnbindBackward0>))\n",
      "(8.807621489536043, tensor([6.2486], grad_fn=<UnbindBackward0>))\n",
      "(7.556427969440253, tensor([6.7063], grad_fn=<UnbindBackward0>))\n",
      "(8.865876285425417, tensor([6.7784], grad_fn=<UnbindBackward0>))\n",
      "(8.430327258394575, tensor([10.1222], grad_fn=<UnbindBackward0>))\n",
      "(6.2285110035911835, tensor([7.4044], grad_fn=<UnbindBackward0>))\n",
      "(9.420520256898534, tensor([6.2295], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([8.3582], grad_fn=<UnbindBackward0>))\n",
      "(7.596392304064196, tensor([7.6918], grad_fn=<UnbindBackward0>))\n",
      "(8.451694209183541, tensor([6.1270], grad_fn=<UnbindBackward0>))\n",
      "(7.160069207596127, tensor([9.7152], grad_fn=<UnbindBackward0>))\n",
      "(9.328034514792826, tensor([9.2008], grad_fn=<UnbindBackward0>))\n",
      "(6.699500340161678, tensor([6.4676], grad_fn=<UnbindBackward0>))\n",
      "(8.39072252736229, tensor([9.3962], grad_fn=<UnbindBackward0>))\n",
      "(8.951828676824181, tensor([8.8291], grad_fn=<UnbindBackward0>))\n",
      "(8.39072252736229, tensor([10.0637], grad_fn=<UnbindBackward0>))\n",
      "(7.823645930834952, tensor([6.5956], grad_fn=<UnbindBackward0>))\n",
      "(6.77078942390898, tensor([8.5019], grad_fn=<UnbindBackward0>))\n",
      "(7.986164860332727, tensor([6.3750], grad_fn=<UnbindBackward0>))\n",
      "(9.27199975236291, tensor([8.9803], grad_fn=<UnbindBackward0>))\n",
      "(6.620073206530356, tensor([7.3208], grad_fn=<UnbindBackward0>))\n",
      "(8.001689978099135, tensor([8.4874], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([9.4243], grad_fn=<UnbindBackward0>))\n",
      "(7.280697195384741, tensor([8.6813], grad_fn=<UnbindBackward0>))\n",
      "(7.993619994827744, tensor([9.5009], grad_fn=<UnbindBackward0>))\n",
      "(6.926577033222725, tensor([5.9129], grad_fn=<UnbindBackward0>))\n",
      "(7.344072850573066, tensor([7.5634], grad_fn=<UnbindBackward0>))\n",
      "(9.01225522000275, tensor([6.3319], grad_fn=<UnbindBackward0>))\n",
      "(9.058470422638056, tensor([7.5409], grad_fn=<UnbindBackward0>))\n",
      "(7.356279876550748, tensor([8.2675], grad_fn=<UnbindBackward0>))\n",
      "(8.91085558740525, tensor([6.3136], grad_fn=<UnbindBackward0>))\n",
      "(9.574080300037034, tensor([9.0278], grad_fn=<UnbindBackward0>))\n",
      "(8.25088114470065, tensor([8.6698], grad_fn=<UnbindBackward0>))\n",
      "(9.227197489042606, tensor([7.5304], grad_fn=<UnbindBackward0>))\n",
      "(7.586803535162581, tensor([6.2421], grad_fn=<UnbindBackward0>))\n",
      "(8.42989086301344, tensor([7.2104], grad_fn=<UnbindBackward0>))\n",
      "(7.403670290012373, tensor([8.4402], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([6.2591], grad_fn=<UnbindBackward0>))\n",
      "(8.556029215201436, tensor([7.7840], grad_fn=<UnbindBackward0>))\n",
      "(8.98682175033189, tensor([7.4240], grad_fn=<UnbindBackward0>))\n",
      "(8.239857411018601, tensor([6.3006], grad_fn=<UnbindBackward0>))\n",
      "(9.246479418592056, tensor([10.2107], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([9.9802], grad_fn=<UnbindBackward0>))\n",
      "(6.466144724237619, tensor([8.7912], grad_fn=<UnbindBackward0>))\n",
      "(8.29054350077274, tensor([10.6896], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([7.2104], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([8.8197], grad_fn=<UnbindBackward0>))\n",
      "(6.78332520060396, tensor([8.3810], grad_fn=<UnbindBackward0>))\n",
      "(7.836369760545124, tensor([8.5352], grad_fn=<UnbindBackward0>))\n",
      "(6.953684210870537, tensor([6.2235], grad_fn=<UnbindBackward0>))\n",
      "(8.699348067653093, tensor([8.4139], grad_fn=<UnbindBackward0>))\n",
      "(7.813591552952433, tensor([7.9605], grad_fn=<UnbindBackward0>))\n",
      "(7.3914152346753585, tensor([9.5405], grad_fn=<UnbindBackward0>))\n",
      "(9.418329675518715, tensor([8.6094], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([7.2346], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.3715], grad_fn=<UnbindBackward0>))\n",
      "(6.464588303689961, tensor([6.6549], grad_fn=<UnbindBackward0>))\n",
      "(7.0707241072602764, tensor([8.3182], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([7.8792], grad_fn=<UnbindBackward0>))\n",
      "(8.146129510025405, tensor([6.2024], grad_fn=<UnbindBackward0>))\n",
      "(9.360655132635051, tensor([7.5730], grad_fn=<UnbindBackward0>))\n",
      "(8.066207568006265, tensor([8.9124], grad_fn=<UnbindBackward0>))\n",
      "(7.916807490937603, tensor([6.8234], grad_fn=<UnbindBackward0>))\n",
      "(8.451694209183541, tensor([9.5523], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([6.2700], grad_fn=<UnbindBackward0>))\n",
      "(9.198166571044474, tensor([8.9619], grad_fn=<UnbindBackward0>))\n",
      "(7.757478766584179, tensor([7.9688], grad_fn=<UnbindBackward0>))\n",
      "(8.1786387885907, tensor([9.4392], grad_fn=<UnbindBackward0>))\n",
      "(6.953684210870537, tensor([6.5821], grad_fn=<UnbindBackward0>))\n",
      "(9.214730720277476, tensor([6.3033], grad_fn=<UnbindBackward0>))\n",
      "(6.917705609835305, tensor([8.4710], grad_fn=<UnbindBackward0>))\n",
      "(7.4999765409521215, tensor([9.4573], grad_fn=<UnbindBackward0>))\n",
      "(7.040536390215956, tensor([8.7035], grad_fn=<UnbindBackward0>))\n",
      "(9.134107065976593, tensor([7.8720], grad_fn=<UnbindBackward0>))\n",
      "(8.328692583545568, tensor([8.5605], grad_fn=<UnbindBackward0>))\n",
      "(6.880384082186005, tensor([8.7961], grad_fn=<UnbindBackward0>))\n",
      "(7.882314918980268, tensor([9.1249], grad_fn=<UnbindBackward0>))\n",
      "(6.385194398997726, tensor([8.3772], grad_fn=<UnbindBackward0>))\n",
      "(6.999422467507961, tensor([7.7953], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([8.6666], grad_fn=<UnbindBackward0>))\n",
      "(8.247743887225516, tensor([9.2910], grad_fn=<UnbindBackward0>))\n",
      "(6.3561076606958915, tensor([8.8786], grad_fn=<UnbindBackward0>))\n",
      "(7.500529485395295, tensor([7.4251], grad_fn=<UnbindBackward0>))\n",
      "(8.734881892047483, tensor([8.3162], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([6.8549], grad_fn=<UnbindBackward0>))\n",
      "(8.37770121259764, tensor([6.3517], grad_fn=<UnbindBackward0>))\n",
      "(8.609772372709331, tensor([10.0261], grad_fn=<UnbindBackward0>))\n",
      "(8.938531648680692, tensor([6.1795], grad_fn=<UnbindBackward0>))\n",
      "(9.540219493127752, tensor([7.2177], grad_fn=<UnbindBackward0>))\n",
      "(9.688374172917182, tensor([6.2181], grad_fn=<UnbindBackward0>))\n",
      "(8.713910628493924, tensor([7.2090], grad_fn=<UnbindBackward0>))\n",
      "(8.856376036730422, tensor([6.7668], grad_fn=<UnbindBackward0>))\n",
      "(8.70748291785937, tensor([8.5442], grad_fn=<UnbindBackward0>))\n",
      "(7.085901464365611, tensor([8.6424], grad_fn=<UnbindBackward0>))\n",
      "(8.326516830239528, tensor([8.6403], grad_fn=<UnbindBackward0>))\n",
      "(8.68609172787805, tensor([8.6064], grad_fn=<UnbindBackward0>))\n",
      "(7.00669522683704, tensor([7.0655], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([8.4232], grad_fn=<UnbindBackward0>))\n",
      "(8.278428259199071, tensor([9.4033], grad_fn=<UnbindBackward0>))\n",
      "(8.766550149546351, tensor([7.8879], grad_fn=<UnbindBackward0>))\n",
      "(8.816705015621595, tensor([6.5700], grad_fn=<UnbindBackward0>))\n",
      "(6.447305862541213, tensor([7.2363], grad_fn=<UnbindBackward0>))\n",
      "(8.752107198329362, tensor([8.9274], grad_fn=<UnbindBackward0>))\n",
      "(6.423246963533519, tensor([7.2380], grad_fn=<UnbindBackward0>))\n",
      "(8.436416881388949, tensor([8.9794], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([6.8513], grad_fn=<UnbindBackward0>))\n",
      "(6.98933526597456, tensor([8.3221], grad_fn=<UnbindBackward0>))\n",
      "(7.271703706887368, tensor([8.9248], grad_fn=<UnbindBackward0>))\n",
      "(7.781973234434385, tensor([8.0922], grad_fn=<UnbindBackward0>))\n",
      "(9.080003870248179, tensor([6.8392], grad_fn=<UnbindBackward0>))\n",
      "(7.506591780070841, tensor([8.9147], grad_fn=<UnbindBackward0>))\n",
      "(7.065613363597717, tensor([6.4262], grad_fn=<UnbindBackward0>))\n",
      "(9.741027444837728, tensor([8.1960], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([7.9097], grad_fn=<UnbindBackward0>))\n",
      "(7.183111701743281, tensor([6.8491], grad_fn=<UnbindBackward0>))\n",
      "(7.143617602704121, tensor([8.3248], grad_fn=<UnbindBackward0>))\n",
      "(8.492695559815838, tensor([8.1991], grad_fn=<UnbindBackward0>))\n",
      "(9.771726021353874, tensor([10.2119], grad_fn=<UnbindBackward0>))\n",
      "(8.06117135969092, tensor([6.7043], grad_fn=<UnbindBackward0>))\n",
      "(9.823848929875911, tensor([7.4739], grad_fn=<UnbindBackward0>))\n",
      "(8.764053269347762, tensor([7.9039], grad_fn=<UnbindBackward0>))\n",
      "(6.902742737158593, tensor([8.6134], grad_fn=<UnbindBackward0>))\n",
      "(7.122059881629142, tensor([6.2719], grad_fn=<UnbindBackward0>))\n",
      "(8.679822114864455, tensor([7.0749], grad_fn=<UnbindBackward0>))\n",
      "(8.47407690034261, tensor([8.5386], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([6.4868], grad_fn=<UnbindBackward0>))\n",
      "(9.281171552736774, tensor([8.5353], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([8.5749], grad_fn=<UnbindBackward0>))\n",
      "(7.0825485693553, tensor([7.9040], grad_fn=<UnbindBackward0>))\n",
      "(7.470224135899966, tensor([10.0921], grad_fn=<UnbindBackward0>))\n",
      "(7.388946097618437, tensor([9.5941], grad_fn=<UnbindBackward0>))\n",
      "(7.1459844677143876, tensor([10.0315], grad_fn=<UnbindBackward0>))\n",
      "(7.510430556378006, tensor([8.0408], grad_fn=<UnbindBackward0>))\n",
      "(8.48920515487607, tensor([6.7860], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([7.8592], grad_fn=<UnbindBackward0>))\n",
      "(9.800457652216584, tensor([9.3929], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([6.0805], grad_fn=<UnbindBackward0>))\n",
      "(6.8658910748834385, tensor([9.1844], grad_fn=<UnbindBackward0>))\n",
      "(9.588365764325566, tensor([6.4661], grad_fn=<UnbindBackward0>))\n",
      "(8.758098072309089, tensor([6.7073], grad_fn=<UnbindBackward0>))\n",
      "(8.481773246184977, tensor([6.8242], grad_fn=<UnbindBackward0>))\n",
      "(8.224163512637862, tensor([6.7884], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([6.4572], grad_fn=<UnbindBackward0>))\n",
      "(8.909235279192261, tensor([6.2868], grad_fn=<UnbindBackward0>))\n",
      "(6.608000625296087, tensor([6.3253], grad_fn=<UnbindBackward0>))\n",
      "(9.550021739538344, tensor([9.5879], grad_fn=<UnbindBackward0>))\n",
      "(7.761744984658913, tensor([8.4026], grad_fn=<UnbindBackward0>))\n",
      "(6.648984550024776, tensor([8.4728], grad_fn=<UnbindBackward0>))\n",
      "(6.280395838960195, tensor([7.6587], grad_fn=<UnbindBackward0>))\n",
      "(8.240385115516334, tensor([8.2499], grad_fn=<UnbindBackward0>))\n",
      "(8.53817159780143, tensor([8.1365], grad_fn=<UnbindBackward0>))\n",
      "(9.411647228681218, tensor([9.3460], grad_fn=<UnbindBackward0>))\n",
      "(6.862757913051401, tensor([8.4705], grad_fn=<UnbindBackward0>))\n",
      "(8.658692753689937, tensor([8.9416], grad_fn=<UnbindBackward0>))\n",
      "(7.470793774195062, tensor([6.6653], grad_fn=<UnbindBackward0>))\n",
      "(7.918264686095274, tensor([6.3212], grad_fn=<UnbindBackward0>))\n",
      "(8.418256443556213, tensor([8.5796], grad_fn=<UnbindBackward0>))\n",
      "(7.373374309910049, tensor([6.9037], grad_fn=<UnbindBackward0>))\n",
      "(6.535241271013659, tensor([6.2459], grad_fn=<UnbindBackward0>))\n",
      "(7.53689712956617, tensor([8.4387], grad_fn=<UnbindBackward0>))\n",
      "(9.791885375954825, tensor([7.4015], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([7.8736], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([6.3090], grad_fn=<UnbindBackward0>))\n",
      "(8.464846711044029, tensor([7.2394], grad_fn=<UnbindBackward0>))\n",
      "(9.3729692954244, tensor([9.7665], grad_fn=<UnbindBackward0>))\n",
      "(9.05753878171822, tensor([6.4111], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([9.8524], grad_fn=<UnbindBackward0>))\n",
      "(8.127995055771946, tensor([7.8637], grad_fn=<UnbindBackward0>))\n",
      "(6.20050917404269, tensor([8.3084], grad_fn=<UnbindBackward0>))\n",
      "(8.427706024914702, tensor([8.3970], grad_fn=<UnbindBackward0>))\n",
      "(6.762729506931879, tensor([8.0135], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([8.6638], grad_fn=<UnbindBackward0>))\n",
      "(9.776222201490253, tensor([8.3659], grad_fn=<UnbindBackward0>))\n",
      "(6.928537818164665, tensor([8.3286], grad_fn=<UnbindBackward0>))\n",
      "(6.717804695023691, tensor([7.6063], grad_fn=<UnbindBackward0>))\n",
      "(8.540909718033554, tensor([8.7989], grad_fn=<UnbindBackward0>))\n",
      "(9.077951183930438, tensor([9.9106], grad_fn=<UnbindBackward0>))\n",
      "(6.694562058521095, tensor([5.8843], grad_fn=<UnbindBackward0>))\n",
      "(8.395251520610994, tensor([6.6962], grad_fn=<UnbindBackward0>))\n",
      "(9.102309627762478, tensor([6.8058], grad_fn=<UnbindBackward0>))\n",
      "(7.431299675155903, tensor([10.2454], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([6.0174], grad_fn=<UnbindBackward0>))\n",
      "(9.343296599224809, tensor([7.9186], grad_fn=<UnbindBackward0>))\n",
      "(9.481740611853681, tensor([7.3916], grad_fn=<UnbindBackward0>))\n",
      "(8.404248432400102, tensor([8.6811], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([7.8227], grad_fn=<UnbindBackward0>))\n",
      "(8.309676895987726, tensor([9.2861], grad_fn=<UnbindBackward0>))\n",
      "(8.728749873478527, tensor([7.7576], grad_fn=<UnbindBackward0>))\n",
      "(6.939253946041508, tensor([7.6827], grad_fn=<UnbindBackward0>))\n",
      "(7.7488913372555315, tensor([7.6124], grad_fn=<UnbindBackward0>))\n",
      "(7.396335293800808, tensor([8.2271], grad_fn=<UnbindBackward0>))\n",
      "(6.910750787961936, tensor([6.3215], grad_fn=<UnbindBackward0>))\n",
      "(9.29403793099775, tensor([6.4365], grad_fn=<UnbindBackward0>))\n",
      "(7.950149887652018, tensor([8.7455], grad_fn=<UnbindBackward0>))\n",
      "(7.907651594711089, tensor([6.5389], grad_fn=<UnbindBackward0>))\n",
      "(7.7706452341291765, tensor([7.6995], grad_fn=<UnbindBackward0>))\n",
      "(8.287025025165063, tensor([6.3893], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([8.4118], grad_fn=<UnbindBackward0>))\n",
      "(8.842604480678025, tensor([7.2296], grad_fn=<UnbindBackward0>))\n",
      "(7.550661243105336, tensor([8.1936], grad_fn=<UnbindBackward0>))\n",
      "(7.9865049385539955, tensor([6.0732], grad_fn=<UnbindBackward0>))\n",
      "(7.347943823148687, tensor([6.3373], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([7.3819], grad_fn=<UnbindBackward0>))\n",
      "(8.617219505483362, tensor([6.1724], grad_fn=<UnbindBackward0>))\n",
      "(9.198470199405696, tensor([7.1949], grad_fn=<UnbindBackward0>))\n",
      "(8.54714026778419, tensor([6.7233], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([7.4905], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([6.4491], grad_fn=<UnbindBackward0>))\n",
      "(8.625329850020815, tensor([8.5288], grad_fn=<UnbindBackward0>))\n",
      "(6.910750787961936, tensor([9.0676], grad_fn=<UnbindBackward0>))\n",
      "(7.52131798019924, tensor([7.8191], grad_fn=<UnbindBackward0>))\n",
      "(7.745435610274381, tensor([8.0443], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([6.9857], grad_fn=<UnbindBackward0>))\n",
      "(7.160069207596127, tensor([6.7960], grad_fn=<UnbindBackward0>))\n",
      "(7.0925737159746784, tensor([6.4555], grad_fn=<UnbindBackward0>))\n",
      "(9.40894529884324, tensor([6.9340], grad_fn=<UnbindBackward0>))\n",
      "(8.728749873478527, tensor([7.7855], grad_fn=<UnbindBackward0>))\n",
      "(9.312806703520673, tensor([7.9429], grad_fn=<UnbindBackward0>))\n",
      "(7.3727463664043285, tensor([9.3896], grad_fn=<UnbindBackward0>))\n",
      "(9.76445534384057, tensor([9.1784], grad_fn=<UnbindBackward0>))\n",
      "(7.6953031349635665, tensor([6.2005], grad_fn=<UnbindBackward0>))\n",
      "(7.228388451573604, tensor([7.3195], grad_fn=<UnbindBackward0>))\n",
      "(7.7664168980196555, tensor([7.1183], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([9.6364], grad_fn=<UnbindBackward0>))\n",
      "(8.120886021092838, tensor([7.7211], grad_fn=<UnbindBackward0>))\n",
      "(8.48446336679332, tensor([7.1664], grad_fn=<UnbindBackward0>))\n",
      "(8.19836438996762, tensor([6.4717], grad_fn=<UnbindBackward0>))\n",
      "(6.910750787961936, tensor([8.4720], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([9.3000], grad_fn=<UnbindBackward0>))\n",
      "(7.469654172932128, tensor([7.1863], grad_fn=<UnbindBackward0>))\n",
      "(9.246286498749653, tensor([7.4248], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([8.9627], grad_fn=<UnbindBackward0>))\n",
      "(6.76849321164863, tensor([6.7958], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([6.3023], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([8.2745], grad_fn=<UnbindBackward0>))\n",
      "(8.804325112562537, tensor([9.5096], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([7.4526], grad_fn=<UnbindBackward0>))\n",
      "(9.161990111035136, tensor([7.8448], grad_fn=<UnbindBackward0>))\n",
      "(7.749322464660356, tensor([6.3301], grad_fn=<UnbindBackward0>))\n",
      "(9.232395362757014, tensor([7.2107], grad_fn=<UnbindBackward0>))\n",
      "(8.691314551644853, tensor([8.7961], grad_fn=<UnbindBackward0>))\n",
      "(7.701200180857446, tensor([10.3037], grad_fn=<UnbindBackward0>))\n",
      "(8.411388132519262, tensor([8.1857], grad_fn=<UnbindBackward0>))\n",
      "(6.731018100482083, tensor([8.8793], grad_fn=<UnbindBackward0>))\n",
      "(8.486940148245216, tensor([5.9825], grad_fn=<UnbindBackward0>))\n",
      "(8.890272839380247, tensor([8.5138], grad_fn=<UnbindBackward0>))\n",
      "(8.684231891345675, tensor([7.2383], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([6.6325], grad_fn=<UnbindBackward0>))\n",
      "(9.240384493324559, tensor([6.2944], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([6.5745], grad_fn=<UnbindBackward0>))\n",
      "(8.515191188745565, tensor([7.0259], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.1854], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([7.2371], grad_fn=<UnbindBackward0>))\n",
      "(7.6676260915849905, tensor([8.5413], grad_fn=<UnbindBackward0>))\n",
      "(7.78155595923534, tensor([9.3955], grad_fn=<UnbindBackward0>))\n",
      "(7.7306140660637395, tensor([8.4208], grad_fn=<UnbindBackward0>))\n",
      "(6.499787040655854, tensor([6.7087], grad_fn=<UnbindBackward0>))\n",
      "(9.460709909543985, tensor([7.1384], grad_fn=<UnbindBackward0>))\n",
      "(9.480978009818704, tensor([6.2066], grad_fn=<UnbindBackward0>))\n",
      "(9.534523115464658, tensor([8.9982], grad_fn=<UnbindBackward0>))\n",
      "(6.169610732491456, tensor([9.5345], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([8.2157], grad_fn=<UnbindBackward0>))\n",
      "(9.608377964255551, tensor([8.7519], grad_fn=<UnbindBackward0>))\n",
      "(7.519692404116539, tensor([8.5450], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([6.3309], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([6.7154], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([7.9279], grad_fn=<UnbindBackward0>))\n",
      "(8.848939995030118, tensor([8.8414], grad_fn=<UnbindBackward0>))\n",
      "(7.675081857716334, tensor([8.7378], grad_fn=<UnbindBackward0>))\n",
      "(6.755768921984255, tensor([10.1745], grad_fn=<UnbindBackward0>))\n",
      "(6.854354502255021, tensor([7.9322], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([7.1528], grad_fn=<UnbindBackward0>))\n",
      "(8.423321975806166, tensor([8.4935], grad_fn=<UnbindBackward0>))\n",
      "(8.871224644409553, tensor([8.6670], grad_fn=<UnbindBackward0>))\n",
      "(8.309184527686298, tensor([5.8951], grad_fn=<UnbindBackward0>))\n",
      "(8.204398418149381, tensor([8.9249], grad_fn=<UnbindBackward0>))\n",
      "(6.93828448401696, tensor([6.8311], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([8.3958], grad_fn=<UnbindBackward0>))\n",
      "(6.90975328164481, tensor([5.9631], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.1789], grad_fn=<UnbindBackward0>))\n",
      "(8.637284671674058, tensor([10.0813], grad_fn=<UnbindBackward0>))\n",
      "(7.029972911706386, tensor([9.7228], grad_fn=<UnbindBackward0>))\n",
      "(7.944492163932159, tensor([7.6734], grad_fn=<UnbindBackward0>))\n",
      "(9.204624064980074, tensor([9.4699], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([6.8152], grad_fn=<UnbindBackward0>))\n",
      "(8.302761580704049, tensor([8.8956], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([6.3893], grad_fn=<UnbindBackward0>))\n",
      "(7.8252452914317745, tensor([7.8329], grad_fn=<UnbindBackward0>))\n",
      "(7.795646536334594, tensor([7.0297], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([7.7495], grad_fn=<UnbindBackward0>))\n",
      "(7.273092595999522, tensor([9.0573], grad_fn=<UnbindBackward0>))\n",
      "(8.363575702750637, tensor([6.2659], grad_fn=<UnbindBackward0>))\n",
      "(8.743053050224676, tensor([6.3693], grad_fn=<UnbindBackward0>))\n",
      "(7.004881989712859, tensor([6.2297], grad_fn=<UnbindBackward0>))\n",
      "(7.059617628291383, tensor([6.9119], grad_fn=<UnbindBackward0>))\n",
      "(6.90975328164481, tensor([8.7303], grad_fn=<UnbindBackward0>))\n",
      "(9.744374580879056, tensor([9.4746], grad_fn=<UnbindBackward0>))\n",
      "(7.908387159290043, tensor([8.5578], grad_fn=<UnbindBackward0>))\n",
      "(7.529943370601589, tensor([6.5204], grad_fn=<UnbindBackward0>))\n",
      "(7.2682230211595655, tensor([7.2644], grad_fn=<UnbindBackward0>))\n",
      "(7.275172319452771, tensor([6.3594], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.9055], grad_fn=<UnbindBackward0>))\n",
      "(8.706490361946617, tensor([6.9792], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([6.7918], grad_fn=<UnbindBackward0>))\n",
      "(8.460834457746854, tensor([7.3156], grad_fn=<UnbindBackward0>))\n",
      "(6.148468295917647, tensor([7.9267], grad_fn=<UnbindBackward0>))\n",
      "(7.703459047867175, tensor([7.9076], grad_fn=<UnbindBackward0>))\n",
      "(9.228376734462282, tensor([8.6347], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([6.4275], grad_fn=<UnbindBackward0>))\n",
      "(6.440946540632921, tensor([6.2359], grad_fn=<UnbindBackward0>))\n",
      "(8.288534459413917, tensor([6.3869], grad_fn=<UnbindBackward0>))\n",
      "(7.817222785508166, tensor([10.3659], grad_fn=<UnbindBackward0>))\n",
      "(9.426741242423, tensor([8.4595], grad_fn=<UnbindBackward0>))\n",
      "(7.511524648390866, tensor([8.6433], grad_fn=<UnbindBackward0>))\n",
      "(6.898714534329988, tensor([6.2568], grad_fn=<UnbindBackward0>))\n",
      "(7.742835955430749, tensor([9.8013], grad_fn=<UnbindBackward0>))\n",
      "(8.23323750070527, tensor([7.7894], grad_fn=<UnbindBackward0>))\n",
      "(7.618251097876695, tensor([5.9006], grad_fn=<UnbindBackward0>))\n",
      "(7.757906208351747, tensor([7.8452], grad_fn=<UnbindBackward0>))\n",
      "(8.595264726836392, tensor([6.3477], grad_fn=<UnbindBackward0>))\n",
      "(7.943782692458625, tensor([6.2600], grad_fn=<UnbindBackward0>))\n",
      "(6.07073772800249, tensor([9.7698], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([10.2499], grad_fn=<UnbindBackward0>))\n",
      "(9.210040326967182, tensor([7.7404], grad_fn=<UnbindBackward0>))\n",
      "(7.200424892944957, tensor([7.0655], grad_fn=<UnbindBackward0>))\n",
      "(7.635303886259415, tensor([6.6741], grad_fn=<UnbindBackward0>))\n",
      "(7.257002707092073, tensor([8.8393], grad_fn=<UnbindBackward0>))\n",
      "(6.877296071497429, tensor([5.8829], grad_fn=<UnbindBackward0>))\n",
      "(8.39479954320217, tensor([8.7411], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([6.4951], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([8.6333], grad_fn=<UnbindBackward0>))\n",
      "(8.583916823459145, tensor([8.7031], grad_fn=<UnbindBackward0>))\n",
      "(6.90875477931522, tensor([6.0733], grad_fn=<UnbindBackward0>))\n",
      "(7.0317412587631285, tensor([7.7202], grad_fn=<UnbindBackward0>))\n",
      "(8.914491710191342, tensor([8.7826], grad_fn=<UnbindBackward0>))\n",
      "(8.704999678440762, tensor([6.3516], grad_fn=<UnbindBackward0>))\n",
      "(6.628041376179533, tensor([6.3008], grad_fn=<UnbindBackward0>))\n",
      "(7.01571242048723, tensor([6.8563], grad_fn=<UnbindBackward0>))\n",
      "(8.568076401730806, tensor([6.2871], grad_fn=<UnbindBackward0>))\n",
      "(9.316050826398296, tensor([10.2214], grad_fn=<UnbindBackward0>))\n",
      "(8.491670234185152, tensor([8.0045], grad_fn=<UnbindBackward0>))\n",
      "(7.478734825567875, tensor([8.1862], grad_fn=<UnbindBackward0>))\n",
      "(7.671360923190644, tensor([7.6843], grad_fn=<UnbindBackward0>))\n",
      "(7.448333860897476, tensor([7.9662], grad_fn=<UnbindBackward0>))\n",
      "(7.183111701743281, tensor([9.7590], grad_fn=<UnbindBackward0>))\n",
      "(7.995643604287271, tensor([6.6552], grad_fn=<UnbindBackward0>))\n",
      "(7.938088726896952, tensor([6.8810], grad_fn=<UnbindBackward0>))\n",
      "(7.499423290592229, tensor([8.3686], grad_fn=<UnbindBackward0>))\n",
      "(8.087640287778983, tensor([6.2534], grad_fn=<UnbindBackward0>))\n",
      "(6.222576268071369, tensor([8.6485], grad_fn=<UnbindBackward0>))\n",
      "(9.300455261418444, tensor([7.7808], grad_fn=<UnbindBackward0>))\n",
      "(7.474204806496124, tensor([8.5737], grad_fn=<UnbindBackward0>))\n",
      "(6.230481447578482, tensor([8.3001], grad_fn=<UnbindBackward0>))\n",
      "(9.13140538388804, tensor([6.3376], grad_fn=<UnbindBackward0>))\n",
      "(8.67008593751938, tensor([9.0505], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.8818], grad_fn=<UnbindBackward0>))\n",
      "(7.804659297056102, tensor([7.0440], grad_fn=<UnbindBackward0>))\n",
      "(8.727454116899434, tensor([8.6812], grad_fn=<UnbindBackward0>))\n",
      "(8.045588280803528, tensor([6.5642], grad_fn=<UnbindBackward0>))\n",
      "(8.335191583433202, tensor([6.7622], grad_fn=<UnbindBackward0>))\n",
      "(8.582980931954241, tensor([6.2266], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([6.7827], grad_fn=<UnbindBackward0>))\n",
      "(8.84980082722101, tensor([6.9323], grad_fn=<UnbindBackward0>))\n",
      "(9.444621667700849, tensor([7.7221], grad_fn=<UnbindBackward0>))\n",
      "(8.405814603432848, tensor([10.0803], grad_fn=<UnbindBackward0>))\n",
      "(9.802616942151154, tensor([8.7441], grad_fn=<UnbindBackward0>))\n",
      "(7.138866999945524, tensor([8.1321], grad_fn=<UnbindBackward0>))\n",
      "(8.005367067316664, tensor([7.8310], grad_fn=<UnbindBackward0>))\n",
      "(7.83002808253384, tensor([9.3911], grad_fn=<UnbindBackward0>))\n",
      "(9.108418382250798, tensor([6.2361], grad_fn=<UnbindBackward0>))\n",
      "(7.072421900537371, tensor([6.2591], grad_fn=<UnbindBackward0>))\n",
      "(7.419380582918692, tensor([8.9819], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([9.4115], grad_fn=<UnbindBackward0>))\n",
      "(7.488293515159428, tensor([6.1955], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([6.4374], grad_fn=<UnbindBackward0>))\n",
      "(7.646831391430482, tensor([8.5695], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([8.6447], grad_fn=<UnbindBackward0>))\n",
      "(8.543445562560303, tensor([5.8577], grad_fn=<UnbindBackward0>))\n",
      "(8.292298107063221, tensor([6.3821], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([10.2097], grad_fn=<UnbindBackward0>))\n",
      "(7.914983005848394, tensor([6.2147], grad_fn=<UnbindBackward0>))\n",
      "(8.671458150427666, tensor([8.4436], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([6.5003], grad_fn=<UnbindBackward0>))\n",
      "(7.533158807455563, tensor([9.5292], grad_fn=<UnbindBackward0>))\n",
      "(7.756195343948118, tensor([6.5840], grad_fn=<UnbindBackward0>))\n",
      "(8.776630098427717, tensor([6.5020], grad_fn=<UnbindBackward0>))\n",
      "(9.160204302482386, tensor([9.3307], grad_fn=<UnbindBackward0>))\n",
      "(8.88016824790345, tensor([8.3102], grad_fn=<UnbindBackward0>))\n",
      "(7.0909098220799835, tensor([9.2479], grad_fn=<UnbindBackward0>))\n",
      "(8.774776816043985, tensor([8.5240], grad_fn=<UnbindBackward0>))\n",
      "(7.744569809354496, tensor([6.4752], grad_fn=<UnbindBackward0>))\n",
      "(8.353261499733874, tensor([6.7998], grad_fn=<UnbindBackward0>))\n",
      "(8.521384396034705, tensor([6.4584], grad_fn=<UnbindBackward0>))\n",
      "(7.218176838403408, tensor([7.2523], grad_fn=<UnbindBackward0>))\n",
      "(8.658692753689937, tensor([8.5219], grad_fn=<UnbindBackward0>))\n",
      "(8.7509996908987, tensor([9.6582], grad_fn=<UnbindBackward0>))\n",
      "(6.452048954437226, tensor([9.1865], grad_fn=<UnbindBackward0>))\n",
      "(9.081369990679223, tensor([6.7839], grad_fn=<UnbindBackward0>))\n",
      "(6.113682179832232, tensor([6.2701], grad_fn=<UnbindBackward0>))\n",
      "(8.339023005744759, tensor([8.4236], grad_fn=<UnbindBackward0>))\n",
      "(6.100318952020064, tensor([6.7659], grad_fn=<UnbindBackward0>))\n",
      "(7.22402480828583, tensor([6.6898], grad_fn=<UnbindBackward0>))\n",
      "(8.568076401730806, tensor([9.7494], grad_fn=<UnbindBackward0>))\n",
      "(6.2878585601617845, tensor([6.4156], grad_fn=<UnbindBackward0>))\n",
      "(7.062191632286556, tensor([7.0249], grad_fn=<UnbindBackward0>))\n",
      "(6.626717749249025, tensor([7.5762], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([7.2463], grad_fn=<UnbindBackward0>))\n",
      "(9.215526898663482, tensor([8.9958], grad_fn=<UnbindBackward0>))\n",
      "(8.432288684325794, tensor([8.7586], grad_fn=<UnbindBackward0>))\n",
      "(8.479283618343016, tensor([6.3328], grad_fn=<UnbindBackward0>))\n",
      "(7.749322464660356, tensor([7.8561], grad_fn=<UnbindBackward0>))\n",
      "(6.968850378341948, tensor([7.6696], grad_fn=<UnbindBackward0>))\n",
      "(9.68439827154996, tensor([7.0466], grad_fn=<UnbindBackward0>))\n",
      "(8.411388132519262, tensor([6.3673], grad_fn=<UnbindBackward0>))\n",
      "(8.281723990411392, tensor([8.5422], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([6.4279], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([8.5648], grad_fn=<UnbindBackward0>))\n",
      "(8.923324744067562, tensor([8.5240], grad_fn=<UnbindBackward0>))\n",
      "(7.492203042618741, tensor([9.2456], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([9.8311], grad_fn=<UnbindBackward0>))\n",
      "(6.620073206530356, tensor([7.5695], grad_fn=<UnbindBackward0>))\n",
      "(7.499423290592229, tensor([6.5242], grad_fn=<UnbindBackward0>))\n",
      "(7.904703913873747, tensor([7.6448], grad_fn=<UnbindBackward0>))\n",
      "(7.492203042618741, tensor([6.6824], grad_fn=<UnbindBackward0>))\n",
      "(6.732210706467206, tensor([6.1758], grad_fn=<UnbindBackward0>))\n",
      "(7.99799931797973, tensor([6.3314], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([6.2959], grad_fn=<UnbindBackward0>))\n",
      "(6.926577033222725, tensor([8.2981], grad_fn=<UnbindBackward0>))\n",
      "(8.27410200229233, tensor([6.6792], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.4899], grad_fn=<UnbindBackward0>))\n",
      "(6.794586580876499, tensor([6.7399], grad_fn=<UnbindBackward0>))\n",
      "(7.963807953231451, tensor([6.1244], grad_fn=<UnbindBackward0>))\n",
      "(8.648045999835, tensor([8.4872], grad_fn=<UnbindBackward0>))\n",
      "(8.734881892047483, tensor([9.3396], grad_fn=<UnbindBackward0>))\n",
      "(8.837681215593197, tensor([8.6448], grad_fn=<UnbindBackward0>))\n",
      "(8.256866848974312, tensor([8.0357], grad_fn=<UnbindBackward0>))\n",
      "(6.692083742506628, tensor([8.7048], grad_fn=<UnbindBackward0>))\n",
      "(8.242229891372231, tensor([8.2410], grad_fn=<UnbindBackward0>))\n",
      "(6.150602768446279, tensor([6.3331], grad_fn=<UnbindBackward0>))\n",
      "(8.475746001502063, tensor([9.7207], grad_fn=<UnbindBackward0>))\n",
      "(8.561592778712923, tensor([8.0958], grad_fn=<UnbindBackward0>))\n",
      "(7.582738488914411, tensor([6.3497], grad_fn=<UnbindBackward0>))\n",
      "(8.762646029650282, tensor([6.2026], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([9.2343], grad_fn=<UnbindBackward0>))\n",
      "(7.605392364814935, tensor([6.7043], grad_fn=<UnbindBackward0>))\n",
      "(6.311734809152915, tensor([8.9359], grad_fn=<UnbindBackward0>))\n",
      "(8.531490496117062, tensor([8.2471], grad_fn=<UnbindBackward0>))\n",
      "(6.895682697747868, tensor([7.8189], grad_fn=<UnbindBackward0>))\n",
      "(7.974532844130228, tensor([8.0203], grad_fn=<UnbindBackward0>))\n",
      "(8.694167141883597, tensor([6.2278], grad_fn=<UnbindBackward0>))\n",
      "(8.652423140676342, tensor([6.2475], grad_fn=<UnbindBackward0>))\n",
      "(6.683360945766275, tensor([8.0464], grad_fn=<UnbindBackward0>))\n",
      "(9.044049632254756, tensor([6.5760], grad_fn=<UnbindBackward0>))\n",
      "(7.816416983691801, tensor([7.1831], grad_fn=<UnbindBackward0>))\n",
      "(7.8582541821860294, tensor([7.8164], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([8.3311], grad_fn=<UnbindBackward0>))\n",
      "(9.67990640549087, tensor([6.7455], grad_fn=<UnbindBackward0>))\n",
      "(9.625359692889159, tensor([8.5777], grad_fn=<UnbindBackward0>))\n",
      "(7.1316985104669115, tensor([8.7056], grad_fn=<UnbindBackward0>))\n",
      "(8.527539483470381, tensor([8.7004], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([9.0146], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([7.6460], grad_fn=<UnbindBackward0>))\n",
      "(8.393216011596527, tensor([8.4607], grad_fn=<UnbindBackward0>))\n",
      "(7.622174594817622, tensor([6.5702], grad_fn=<UnbindBackward0>))\n",
      "(8.889997357284841, tensor([6.9889], grad_fn=<UnbindBackward0>))\n",
      "(9.142596719889664, tensor([6.5001], grad_fn=<UnbindBackward0>))\n",
      "(8.698847859222488, tensor([7.9898], grad_fn=<UnbindBackward0>))\n",
      "(8.686260632531775, tensor([6.7081], grad_fn=<UnbindBackward0>))\n",
      "(8.666474894131992, tensor([7.8760], grad_fn=<UnbindBackward0>))\n",
      "(8.022568946988255, tensor([6.4954], grad_fn=<UnbindBackward0>))\n",
      "(8.92319149068606, tensor([7.7623], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([6.9848], grad_fn=<UnbindBackward0>))\n",
      "(9.205830216498297, tensor([6.0337], grad_fn=<UnbindBackward0>))\n",
      "(6.532334292222349, tensor([8.4039], grad_fn=<UnbindBackward0>))\n",
      "(6.837332814685591, tensor([8.7040], grad_fn=<UnbindBackward0>))\n",
      "(8.981430225767635, tensor([8.8889], grad_fn=<UnbindBackward0>))\n",
      "(8.374938143835367, tensor([8.8232], grad_fn=<UnbindBackward0>))\n",
      "(8.802823159741887, tensor([9.0928], grad_fn=<UnbindBackward0>))\n",
      "(7.524021415206125, tensor([7.7581], grad_fn=<UnbindBackward0>))\n",
      "(9.825958350026891, tensor([6.8179], grad_fn=<UnbindBackward0>))\n",
      "(8.699514748210191, tensor([7.0571], grad_fn=<UnbindBackward0>))\n",
      "(8.038512020976814, tensor([7.7099], grad_fn=<UnbindBackward0>))\n",
      "(6.6240652277998935, tensor([6.2812], grad_fn=<UnbindBackward0>))\n",
      "(8.562931083090092, tensor([7.0235], grad_fn=<UnbindBackward0>))\n",
      "(8.604471199523298, tensor([7.2073], grad_fn=<UnbindBackward0>))\n",
      "(8.523175263093785, tensor([8.8276], grad_fn=<UnbindBackward0>))\n",
      "(8.481566013773087, tensor([6.6520], grad_fn=<UnbindBackward0>))\n",
      "(6.2285110035911835, tensor([7.3946], grad_fn=<UnbindBackward0>))\n",
      "(9.549166977751137, tensor([6.3812], grad_fn=<UnbindBackward0>))\n",
      "(7.7142311448490855, tensor([7.0276], grad_fn=<UnbindBackward0>))\n",
      "(6.025865973825314, tensor([5.8881], grad_fn=<UnbindBackward0>))\n",
      "(8.735203590618699, tensor([10.2514], grad_fn=<UnbindBackward0>))\n",
      "(7.781973234434385, tensor([9.1325], grad_fn=<UnbindBackward0>))\n",
      "(8.860782895806315, tensor([8.6685], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([7.8551], grad_fn=<UnbindBackward0>))\n",
      "(8.706324840138741, tensor([8.7647], grad_fn=<UnbindBackward0>))\n",
      "(9.280798835624744, tensor([9.0941], grad_fn=<UnbindBackward0>))\n",
      "(8.03365842788615, tensor([8.7444], grad_fn=<UnbindBackward0>))\n",
      "(8.636574948436317, tensor([6.7501], grad_fn=<UnbindBackward0>))\n",
      "(9.46815584482167, tensor([8.2854], grad_fn=<UnbindBackward0>))\n",
      "(8.38776764397578, tensor([7.1045], grad_fn=<UnbindBackward0>))\n",
      "(7.095893221097532, tensor([9.0869], grad_fn=<UnbindBackward0>))\n",
      "(7.237059026124737, tensor([6.3395], grad_fn=<UnbindBackward0>))\n",
      "(7.455298485683291, tensor([6.3139], grad_fn=<UnbindBackward0>))\n",
      "(7.801800401908973, tensor([7.6291], grad_fn=<UnbindBackward0>))\n",
      "(7.983098940710892, tensor([7.8336], grad_fn=<UnbindBackward0>))\n",
      "(8.694167141883597, tensor([6.5576], grad_fn=<UnbindBackward0>))\n",
      "(9.71571114505921, tensor([9.8452], grad_fn=<UnbindBackward0>))\n",
      "(6.752270376141742, tensor([7.3672], grad_fn=<UnbindBackward0>))\n",
      "(6.980075940561763, tensor([5.9907], grad_fn=<UnbindBackward0>))\n",
      "(8.780172651227646, tensor([6.7896], grad_fn=<UnbindBackward0>))\n",
      "(7.238496840894365, tensor([6.3411], grad_fn=<UnbindBackward0>))\n",
      "(8.411610428841172, tensor([9.2508], grad_fn=<UnbindBackward0>))\n",
      "(8.726481196440007, tensor([8.6151], grad_fn=<UnbindBackward0>))\n",
      "(9.422382530219219, tensor([7.2692], grad_fn=<UnbindBackward0>))\n",
      "(6.810142450115136, tensor([7.8400], grad_fn=<UnbindBackward0>))\n",
      "(8.62245370207373, tensor([8.2961], grad_fn=<UnbindBackward0>))\n",
      "(7.844240718141811, tensor([6.1047], grad_fn=<UnbindBackward0>))\n",
      "(7.637234388789473, tensor([6.8232], grad_fn=<UnbindBackward0>))\n",
      "(9.625161575008372, tensor([9.5058], grad_fn=<UnbindBackward0>))\n",
      "(8.076515327552329, tensor([8.1226], grad_fn=<UnbindBackward0>))\n",
      "(6.51025834052315, tensor([6.8643], grad_fn=<UnbindBackward0>))\n",
      "(6.398594934535208, tensor([6.4448], grad_fn=<UnbindBackward0>))\n",
      "(8.53934599605737, tensor([6.6895], grad_fn=<UnbindBackward0>))\n",
      "(7.7488913372555315, tensor([7.6855], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([7.7598], grad_fn=<UnbindBackward0>))\n",
      "(9.052750451401412, tensor([9.4254], grad_fn=<UnbindBackward0>))\n",
      "(7.738923757439457, tensor([7.4909], grad_fn=<UnbindBackward0>))\n",
      "(8.822616945344176, tensor([7.8000], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([8.4753], grad_fn=<UnbindBackward0>))\n",
      "(9.058819564334941, tensor([7.7594], grad_fn=<UnbindBackward0>))\n",
      "(6.880384082186005, tensor([7.9921], grad_fn=<UnbindBackward0>))\n",
      "(8.343315881404946, tensor([6.0395], grad_fn=<UnbindBackward0>))\n",
      "(8.429672593886743, tensor([8.7064], grad_fn=<UnbindBackward0>))\n",
      "(6.8658910748834385, tensor([6.3088], grad_fn=<UnbindBackward0>))\n",
      "(9.329367078397823, tensor([8.6500], grad_fn=<UnbindBackward0>))\n",
      "(7.557994958530806, tensor([8.9697], grad_fn=<UnbindBackward0>))\n",
      "(8.470311205516108, tensor([7.0901], grad_fn=<UnbindBackward0>))\n",
      "(8.691482576512929, tensor([8.3401], grad_fn=<UnbindBackward0>))\n",
      "(9.668081624649718, tensor([7.7180], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([8.0861], grad_fn=<UnbindBackward0>))\n",
      "(7.195937226475569, tensor([6.6874], grad_fn=<UnbindBackward0>))\n",
      "(8.395703293828527, tensor([8.7093], grad_fn=<UnbindBackward0>))\n",
      "(7.780720886117918, tensor([7.6549], grad_fn=<UnbindBackward0>))\n",
      "(9.439625028047155, tensor([6.6049], grad_fn=<UnbindBackward0>))\n",
      "(7.555905093611346, tensor([6.7493], grad_fn=<UnbindBackward0>))\n",
      "(6.586171654854675, tensor([8.1232], grad_fn=<UnbindBackward0>))\n",
      "(8.787678239039497, tensor([6.2246], grad_fn=<UnbindBackward0>))\n",
      "(8.089482474360754, tensor([8.8247], grad_fn=<UnbindBackward0>))\n",
      "(7.152268856032539, tensor([8.5492], grad_fn=<UnbindBackward0>))\n",
      "(9.009691898489343, tensor([6.7707], grad_fn=<UnbindBackward0>))\n",
      "(6.401917196727186, tensor([9.5438], grad_fn=<UnbindBackward0>))\n",
      "(6.891625897052253, tensor([7.9977], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([6.7260], grad_fn=<UnbindBackward0>))\n",
      "(7.763446388727362, tensor([7.6727], grad_fn=<UnbindBackward0>))\n",
      "(8.415603335654604, tensor([8.5241], grad_fn=<UnbindBackward0>))\n",
      "(7.233455418621439, tensor([7.7162], grad_fn=<UnbindBackward0>))\n",
      "(8.814776088545281, tensor([6.7391], grad_fn=<UnbindBackward0>))\n",
      "(7.931284761525891, tensor([9.3277], grad_fn=<UnbindBackward0>))\n",
      "(8.793915423631676, tensor([8.5788], grad_fn=<UnbindBackward0>))\n",
      "(8.556029215201436, tensor([6.6743], grad_fn=<UnbindBackward0>))\n",
      "(8.0861025356691, tensor([6.6414], grad_fn=<UnbindBackward0>))\n",
      "(7.155396301896734, tensor([6.3120], grad_fn=<UnbindBackward0>))\n",
      "(8.150756470275551, tensor([9.1470], grad_fn=<UnbindBackward0>))\n",
      "(7.959625305098115, tensor([7.4343], grad_fn=<UnbindBackward0>))\n",
      "(7.838737559599282, tensor([7.6674], grad_fn=<UnbindBackward0>))\n",
      "(7.539027055823995, tensor([6.1916], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([6.7467], grad_fn=<UnbindBackward0>))\n",
      "(9.316050826398296, tensor([8.4209], grad_fn=<UnbindBackward0>))\n",
      "(6.877296071497429, tensor([7.9689], grad_fn=<UnbindBackward0>))\n",
      "(7.10002716662926, tensor([6.6866], grad_fn=<UnbindBackward0>))\n",
      "(8.440959885416648, tensor([6.4611], grad_fn=<UnbindBackward0>))\n",
      "(9.280892027927127, tensor([6.2017], grad_fn=<UnbindBackward0>))\n",
      "(8.492900498847193, tensor([6.2949], grad_fn=<UnbindBackward0>))\n",
      "(9.396653950039674, tensor([8.8485], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([6.2309], grad_fn=<UnbindBackward0>))\n",
      "(8.301521654940728, tensor([7.1086], grad_fn=<UnbindBackward0>))\n",
      "(7.56734567601324, tensor([8.7381], grad_fn=<UnbindBackward0>))\n",
      "(8.96482339168508, tensor([6.3471], grad_fn=<UnbindBackward0>))\n",
      "(9.743905482711662, tensor([7.0796], grad_fn=<UnbindBackward0>))\n",
      "(6.040254711277414, tensor([8.9384], grad_fn=<UnbindBackward0>))\n",
      "(8.803123730829212, tensor([7.2974], grad_fn=<UnbindBackward0>))\n",
      "(8.751474487140904, tensor([8.7112], grad_fn=<UnbindBackward0>))\n",
      "(6.740519359606223, tensor([7.1328], grad_fn=<UnbindBackward0>))\n",
      "(6.932447891572509, tensor([7.8291], grad_fn=<UnbindBackward0>))\n",
      "(6.6895992691789665, tensor([9.2682], grad_fn=<UnbindBackward0>))\n",
      "(9.465602553171852, tensor([9.0149], grad_fn=<UnbindBackward0>))\n",
      "(7.933796874815411, tensor([6.7514], grad_fn=<UnbindBackward0>))\n",
      "(7.593877844605118, tensor([6.4503], grad_fn=<UnbindBackward0>))\n",
      "(8.216088098632316, tensor([8.2941], grad_fn=<UnbindBackward0>))\n",
      "(7.970049304976135, tensor([7.7783], grad_fn=<UnbindBackward0>))\n",
      "(8.62998601889136, tensor([7.2833], grad_fn=<UnbindBackward0>))\n",
      "(8.225503097566918, tensor([9.3877], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([9.0907], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([6.2114], grad_fn=<UnbindBackward0>))\n",
      "(6.943122422819428, tensor([9.2932], grad_fn=<UnbindBackward0>))\n",
      "(6.884486652042782, tensor([6.8617], grad_fn=<UnbindBackward0>))\n",
      "(7.941295570906532, tensor([8.1483], grad_fn=<UnbindBackward0>))\n",
      "(8.715388097366482, tensor([8.8128], grad_fn=<UnbindBackward0>))\n",
      "(7.813591552952433, tensor([6.3868], grad_fn=<UnbindBackward0>))\n",
      "(8.676075516476429, tensor([5.8709], grad_fn=<UnbindBackward0>))\n",
      "(8.287025025165063, tensor([7.7286], grad_fn=<UnbindBackward0>))\n",
      "(8.030084094267563, tensor([6.7596], grad_fn=<UnbindBackward0>))\n",
      "(9.812084729062784, tensor([6.1950], grad_fn=<UnbindBackward0>))\n",
      "(7.671360923190644, tensor([8.8479], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([8.6194], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([6.7628], grad_fn=<UnbindBackward0>))\n",
      "(8.68946441235669, tensor([6.4398], grad_fn=<UnbindBackward0>))\n",
      "(7.431299675155903, tensor([6.8159], grad_fn=<UnbindBackward0>))\n",
      "(8.16876982367527, tensor([6.5183], grad_fn=<UnbindBackward0>))\n",
      "(8.146709052203319, tensor([9.2914], grad_fn=<UnbindBackward0>))\n",
      "(9.049936850483158, tensor([6.7395], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([5.9233], grad_fn=<UnbindBackward0>))\n",
      "(9.809616336926057, tensor([6.7761], grad_fn=<UnbindBackward0>))\n",
      "(6.668228248417403, tensor([7.5362], grad_fn=<UnbindBackward0>))\n",
      "(8.263848131368906, tensor([6.2740], grad_fn=<UnbindBackward0>))\n",
      "(8.79920924224139, tensor([6.1655], grad_fn=<UnbindBackward0>))\n",
      "(8.056109659545061, tensor([6.8005], grad_fn=<UnbindBackward0>))\n",
      "(6.883462586413092, tensor([5.9633], grad_fn=<UnbindBackward0>))\n",
      "(9.808297365532379, tensor([6.2434], grad_fn=<UnbindBackward0>))\n",
      "(8.469472455204826, tensor([8.8748], grad_fn=<UnbindBackward0>))\n",
      "(7.00850518208228, tensor([5.9884], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([9.0264], grad_fn=<UnbindBackward0>))\n",
      "(9.745780556506093, tensor([8.4282], grad_fn=<UnbindBackward0>))\n",
      "(8.408493774492896, tensor([6.8197], grad_fn=<UnbindBackward0>))\n",
      "(8.085486772102845, tensor([8.3976], grad_fn=<UnbindBackward0>))\n",
      "(8.582793648500186, tensor([8.3849], grad_fn=<UnbindBackward0>))\n",
      "(8.442469645220301, tensor([6.0762], grad_fn=<UnbindBackward0>))\n",
      "(7.252053951852814, tensor([9.5826], grad_fn=<UnbindBackward0>))\n",
      "(8.879472402074802, tensor([6.2159], grad_fn=<UnbindBackward0>))\n",
      "(6.80903930604298, tensor([7.2839], grad_fn=<UnbindBackward0>))\n",
      "(8.303752415563412, tensor([7.8180], grad_fn=<UnbindBackward0>))\n",
      "(6.142037405587356, tensor([5.9200], grad_fn=<UnbindBackward0>))\n",
      "(9.06843112579349, tensor([7.8320], grad_fn=<UnbindBackward0>))\n",
      "(8.865029186687766, tensor([9.5644], grad_fn=<UnbindBackward0>))\n",
      "(8.452761331256848, tensor([6.3120], grad_fn=<UnbindBackward0>))\n",
      "(9.841824892144022, tensor([8.5450], grad_fn=<UnbindBackward0>))\n",
      "(7.426549072397305, tensor([6.3716], grad_fn=<UnbindBackward0>))\n",
      "(8.449556542700426, tensor([7.2640], grad_fn=<UnbindBackward0>))\n",
      "(8.086410275323782, tensor([9.5624], grad_fn=<UnbindBackward0>))\n",
      "(9.582455500402792, tensor([7.4358], grad_fn=<UnbindBackward0>))\n",
      "(9.28961355287069, tensor([8.3676], grad_fn=<UnbindBackward0>))\n",
      "(9.72877695874351, tensor([7.1127], grad_fn=<UnbindBackward0>))\n",
      "(6.901737206656574, tensor([8.4885], grad_fn=<UnbindBackward0>))\n",
      "(7.9291264873067995, tensor([7.6439], grad_fn=<UnbindBackward0>))\n",
      "(6.628041376179533, tensor([6.3040], grad_fn=<UnbindBackward0>))\n",
      "(6.040254711277414, tensor([8.5009], grad_fn=<UnbindBackward0>))\n",
      "(6.421622267806518, tensor([6.6396], grad_fn=<UnbindBackward0>))\n",
      "(6.543911845564792, tensor([6.2527], grad_fn=<UnbindBackward0>))\n",
      "(7.4377951216719325, tensor([7.9179], grad_fn=<UnbindBackward0>))\n",
      "(7.680637427560936, tensor([6.4272], grad_fn=<UnbindBackward0>))\n",
      "(7.527793987721444, tensor([9.4031], grad_fn=<UnbindBackward0>))\n",
      "(6.298949246855942, tensor([7.2675], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([7.2370], grad_fn=<UnbindBackward0>))\n",
      "(8.61341204915678, tensor([6.2899], grad_fn=<UnbindBackward0>))\n",
      "(8.681011276645632, tensor([6.2335], grad_fn=<UnbindBackward0>))\n",
      "(7.381501894506707, tensor([6.1884], grad_fn=<UnbindBackward0>))\n",
      "(8.157657015196472, tensor([9.6205], grad_fn=<UnbindBackward0>))\n",
      "(6.525029657843462, tensor([8.2077], grad_fn=<UnbindBackward0>))\n",
      "(9.06681636189014, tensor([6.7945], grad_fn=<UnbindBackward0>))\n",
      "(8.629449873761905, tensor([7.1242], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([6.3833], grad_fn=<UnbindBackward0>))\n",
      "(8.165363632473982, tensor([6.3893], grad_fn=<UnbindBackward0>))\n",
      "(9.11305824916963, tensor([7.9374], grad_fn=<UnbindBackward0>))\n",
      "(8.523970175695261, tensor([6.1776], grad_fn=<UnbindBackward0>))\n",
      "(8.79527937019457, tensor([6.4948], grad_fn=<UnbindBackward0>))\n",
      "(8.669914278433902, tensor([6.6468], grad_fn=<UnbindBackward0>))\n",
      "(8.265907334155747, tensor([6.2787], grad_fn=<UnbindBackward0>))\n",
      "(7.741099090035366, tensor([9.9853], grad_fn=<UnbindBackward0>))\n",
      "(7.843064016692054, tensor([6.4705], grad_fn=<UnbindBackward0>))\n",
      "(6.246106765481563, tensor([8.3905], grad_fn=<UnbindBackward0>))\n",
      "(7.92443418488756, tensor([6.2602], grad_fn=<UnbindBackward0>))\n",
      "(7.065613363597717, tensor([6.7626], grad_fn=<UnbindBackward0>))\n",
      "(8.068716192714781, tensor([6.7978], grad_fn=<UnbindBackward0>))\n",
      "(6.866933284461882, tensor([7.5745], grad_fn=<UnbindBackward0>))\n",
      "(8.440528106480752, tensor([8.1756], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([8.1735], grad_fn=<UnbindBackward0>))\n",
      "(7.272398392570047, tensor([7.3340], grad_fn=<UnbindBackward0>))\n",
      "(7.504391559161238, tensor([7.8777], grad_fn=<UnbindBackward0>))\n",
      "(7.424165281042028, tensor([6.7475], grad_fn=<UnbindBackward0>))\n",
      "(7.597897950521784, tensor([8.9331], grad_fn=<UnbindBackward0>))\n",
      "(8.407601514786142, tensor([7.2465], grad_fn=<UnbindBackward0>))\n",
      "(7.621684998724611, tensor([9.5842], grad_fn=<UnbindBackward0>))\n",
      "(6.317164686747284, tensor([6.3857], grad_fn=<UnbindBackward0>))\n",
      "(7.843848638152472, tensor([8.4788], grad_fn=<UnbindBackward0>))\n",
      "(7.694392802629421, tensor([7.7554], grad_fn=<UnbindBackward0>))\n",
      "(7.894318063841624, tensor([6.2777], grad_fn=<UnbindBackward0>))\n",
      "(9.106534203250593, tensor([8.7699], grad_fn=<UnbindBackward0>))\n",
      "(8.777555453213056, tensor([7.7594], grad_fn=<UnbindBackward0>))\n",
      "(7.016609683894219, tensor([6.8092], grad_fn=<UnbindBackward0>))\n",
      "(8.642062173462106, tensor([7.7636], grad_fn=<UnbindBackward0>))\n",
      "(6.077642243349034, tensor([7.7646], grad_fn=<UnbindBackward0>))\n",
      "(7.853993087224244, tensor([8.8347], grad_fn=<UnbindBackward0>))\n",
      "(7.564757012905729, tensor([6.4643], grad_fn=<UnbindBackward0>))\n",
      "(6.828712071641684, tensor([8.5622], grad_fn=<UnbindBackward0>))\n",
      "(8.16876982367527, tensor([7.4514], grad_fn=<UnbindBackward0>))\n",
      "(7.463936604468925, tensor([7.1440], grad_fn=<UnbindBackward0>))\n",
      "(6.295266001439646, tensor([8.3552], grad_fn=<UnbindBackward0>))\n",
      "(6.7650389767805414, tensor([7.1882], grad_fn=<UnbindBackward0>))\n",
      "(8.357259153499912, tensor([9.5510], grad_fn=<UnbindBackward0>))\n",
      "(9.063926353520646, tensor([7.1369], grad_fn=<UnbindBackward0>))\n",
      "(8.912877287669296, tensor([6.3290], grad_fn=<UnbindBackward0>))\n",
      "(9.234252192022495, tensor([6.8624], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([8.2060], grad_fn=<UnbindBackward0>))\n",
      "(9.606293719711752, tensor([10.4434], grad_fn=<UnbindBackward0>))\n",
      "(7.8351837552667485, tensor([7.5952], grad_fn=<UnbindBackward0>))\n",
      "(7.640603826393634, tensor([6.2807], grad_fn=<UnbindBackward0>))\n",
      "(9.774517171239168, tensor([8.4638], grad_fn=<UnbindBackward0>))\n",
      "(7.673223121121708, tensor([7.1824], grad_fn=<UnbindBackward0>))\n",
      "(8.217708406845306, tensor([9.3301], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([8.9191], grad_fn=<UnbindBackward0>))\n",
      "(9.736842475127181, tensor([7.1992], grad_fn=<UnbindBackward0>))\n",
      "(9.30273722124215, tensor([8.3288], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([6.7982], grad_fn=<UnbindBackward0>))\n",
      "(9.119320973589014, tensor([7.1650], grad_fn=<UnbindBackward0>))\n",
      "(6.898714534329988, tensor([8.5946], grad_fn=<UnbindBackward0>))\n",
      "(8.274357006756292, tensor([6.2792], grad_fn=<UnbindBackward0>))\n",
      "(7.860185057472165, tensor([7.3350], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([5.8935], grad_fn=<UnbindBackward0>))\n",
      "(8.495765244002618, tensor([7.6154], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([7.4004], grad_fn=<UnbindBackward0>))\n",
      "(9.18574025510795, tensor([8.2533], grad_fn=<UnbindBackward0>))\n",
      "(7.271703706887368, tensor([6.5591], grad_fn=<UnbindBackward0>))\n",
      "(7.7706452341291765, tensor([8.3458], grad_fn=<UnbindBackward0>))\n",
      "(7.558516743045645, tensor([8.6289], grad_fn=<UnbindBackward0>))\n",
      "(8.55468163582723, tensor([8.9513], grad_fn=<UnbindBackward0>))\n",
      "(8.734399150063695, tensor([10.3235], grad_fn=<UnbindBackward0>))\n",
      "(7.951207156472972, tensor([9.2560], grad_fn=<UnbindBackward0>))\n",
      "(7.80057265467065, tensor([6.3529], grad_fn=<UnbindBackward0>))\n",
      "(8.689632748355741, tensor([6.3410], grad_fn=<UnbindBackward0>))\n",
      "(9.322865162818028, tensor([8.2385], grad_fn=<UnbindBackward0>))\n",
      "(6.635946555686647, tensor([8.3022], grad_fn=<UnbindBackward0>))\n",
      "(8.11969625295725, tensor([6.5953], grad_fn=<UnbindBackward0>))\n",
      "(8.927314111060596, tensor([6.2863], grad_fn=<UnbindBackward0>))\n",
      "(6.898714534329988, tensor([9.3346], grad_fn=<UnbindBackward0>))\n",
      "(7.738488122494646, tensor([6.3451], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([9.4433], grad_fn=<UnbindBackward0>))\n",
      "(8.826294231241318, tensor([8.4698], grad_fn=<UnbindBackward0>))\n",
      "(8.19836438996762, tensor([9.4443], grad_fn=<UnbindBackward0>))\n",
      "(7.951207156472972, tensor([6.2921], grad_fn=<UnbindBackward0>))\n",
      "(8.409385238781931, tensor([6.2716], grad_fn=<UnbindBackward0>))\n",
      "(8.784774592161016, tensor([6.1219], grad_fn=<UnbindBackward0>))\n",
      "(9.560997243589352, tensor([8.7447], grad_fn=<UnbindBackward0>))\n",
      "(9.440896383005846, tensor([8.8498], grad_fn=<UnbindBackward0>))\n",
      "(8.446341450444287, tensor([6.6225], grad_fn=<UnbindBackward0>))\n",
      "(8.315566483564277, tensor([6.4299], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([6.2590], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([7.1889], grad_fn=<UnbindBackward0>))\n",
      "(8.522578663692576, tensor([8.2905], grad_fn=<UnbindBackward0>))\n",
      "(9.683401821321157, tensor([6.5018], grad_fn=<UnbindBackward0>))\n",
      "(7.68937110752969, tensor([10.1379], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([6.5148], grad_fn=<UnbindBackward0>))\n",
      "(7.917536353943631, tensor([7.8438], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([6.7622], grad_fn=<UnbindBackward0>))\n",
      "(7.779466967458324, tensor([8.7173], grad_fn=<UnbindBackward0>))\n",
      "(7.537430036586509, tensor([7.8430], grad_fn=<UnbindBackward0>))\n",
      "(8.801017833540714, tensor([7.0842], grad_fn=<UnbindBackward0>))\n",
      "(7.483806687665835, tensor([7.7716], grad_fn=<UnbindBackward0>))\n",
      "(6.755768921984255, tensor([7.3494], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([6.2883], grad_fn=<UnbindBackward0>))\n",
      "(7.736743682453495, tensor([9.0586], grad_fn=<UnbindBackward0>))\n",
      "(8.429454277108231, tensor([9.2881], grad_fn=<UnbindBackward0>))\n",
      "(7.928406026180535, tensor([8.6419], grad_fn=<UnbindBackward0>))\n",
      "(8.870382066070137, tensor([6.2771], grad_fn=<UnbindBackward0>))\n",
      "(7.804659297056102, tensor([8.8690], grad_fn=<UnbindBackward0>))\n",
      "(6.943122422819428, tensor([6.4749], grad_fn=<UnbindBackward0>))\n",
      "(7.470224135899966, tensor([7.7251], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([6.5552], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.8809], grad_fn=<UnbindBackward0>))\n",
      "(7.540090320145325, tensor([9.3188], grad_fn=<UnbindBackward0>))\n",
      "(6.421622267806518, tensor([6.2743], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([9.5065], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([6.3891], grad_fn=<UnbindBackward0>))\n",
      "(8.127995055771946, tensor([7.2272], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([8.9992], grad_fn=<UnbindBackward0>))\n",
      "(8.053887083618223, tensor([6.7586], grad_fn=<UnbindBackward0>))\n",
      "(8.84779106484485, tensor([7.7157], grad_fn=<UnbindBackward0>))\n",
      "(6.386879319362645, tensor([6.8965], grad_fn=<UnbindBackward0>))\n",
      "(8.286269452783065, tensor([9.4017], grad_fn=<UnbindBackward0>))\n",
      "(8.646289764750648, tensor([6.7500], grad_fn=<UnbindBackward0>))\n",
      "(9.231612507251722, tensor([10.4207], grad_fn=<UnbindBackward0>))\n",
      "(6.504288173536645, tensor([8.3644], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([8.3819], grad_fn=<UnbindBackward0>))\n",
      "(8.227108234348146, tensor([6.6381], grad_fn=<UnbindBackward0>))\n",
      "(7.101675971619444, tensor([8.3177], grad_fn=<UnbindBackward0>))\n",
      "(9.714926570952155, tensor([6.7452], grad_fn=<UnbindBackward0>))\n",
      "(8.498418036089904, tensor([6.9783], grad_fn=<UnbindBackward0>))\n",
      "(8.74766979009724, tensor([8.5795], grad_fn=<UnbindBackward0>))\n",
      "(6.542471960506805, tensor([8.6778], grad_fn=<UnbindBackward0>))\n",
      "(6.855408798609928, tensor([9.0354], grad_fn=<UnbindBackward0>))\n",
      "(8.929567707825337, tensor([5.8464], grad_fn=<UnbindBackward0>))\n",
      "(6.948897222313312, tensor([6.2805], grad_fn=<UnbindBackward0>))\n",
      "(6.419994928147142, tensor([9.7063], grad_fn=<UnbindBackward0>))\n",
      "(7.5569505720129, tensor([7.3653], grad_fn=<UnbindBackward0>))\n",
      "(6.601230118728877, tensor([7.6324], grad_fn=<UnbindBackward0>))\n",
      "(6.882437470997847, tensor([6.3865], grad_fn=<UnbindBackward0>))\n",
      "(9.816076095525158, tensor([6.6822], grad_fn=<UnbindBackward0>))\n",
      "(9.668081624649718, tensor([7.8529], grad_fn=<UnbindBackward0>))\n",
      "(8.340933226000878, tensor([6.9170], grad_fn=<UnbindBackward0>))\n",
      "(8.329658067569396, tensor([6.0829], grad_fn=<UnbindBackward0>))\n",
      "(8.138272638530186, tensor([6.7285], grad_fn=<UnbindBackward0>))\n",
      "(8.766082459148864, tensor([6.3245], grad_fn=<UnbindBackward0>))\n",
      "(7.697575346802343, tensor([7.9779], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([7.8366], grad_fn=<UnbindBackward0>))\n",
      "(8.248790733696413, tensor([6.9844], grad_fn=<UnbindBackward0>))\n",
      "(9.203517146628057, tensor([9.1928], grad_fn=<UnbindBackward0>))\n",
      "(7.366445148327599, tensor([7.1310], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([7.1054], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([9.4308], grad_fn=<UnbindBackward0>))\n",
      "(7.880048200971577, tensor([8.9759], grad_fn=<UnbindBackward0>))\n",
      "(7.993619994827744, tensor([7.1899], grad_fn=<UnbindBackward0>))\n",
      "(9.740321354781914, tensor([6.4854], grad_fn=<UnbindBackward0>))\n",
      "(7.8674885686991285, tensor([6.8126], grad_fn=<UnbindBackward0>))\n",
      "(7.757051142032013, tensor([6.3228], grad_fn=<UnbindBackward0>))\n",
      "(9.252633284166434, tensor([6.3725], grad_fn=<UnbindBackward0>))\n",
      "(9.199784858036667, tensor([9.2266], grad_fn=<UnbindBackward0>))\n",
      "(7.504391559161238, tensor([6.7483], grad_fn=<UnbindBackward0>))\n",
      "(8.707978826622321, tensor([9.3989], grad_fn=<UnbindBackward0>))\n",
      "(7.895436006942965, tensor([8.4280], grad_fn=<UnbindBackward0>))\n",
      "(6.684611727667927, tensor([8.2341], grad_fn=<UnbindBackward0>))\n",
      "(7.646831391430482, tensor([9.2296], grad_fn=<UnbindBackward0>))\n",
      "(7.781138509845015, tensor([9.5582], grad_fn=<UnbindBackward0>))\n",
      "(7.451241684987676, tensor([5.8784], grad_fn=<UnbindBackward0>))\n",
      "(8.063377822367027, tensor([6.0283], grad_fn=<UnbindBackward0>))\n",
      "(8.999001866111735, tensor([8.7005], grad_fn=<UnbindBackward0>))\n",
      "(8.442469645220301, tensor([8.5439], grad_fn=<UnbindBackward0>))\n",
      "(9.169205828617628, tensor([6.5584], grad_fn=<UnbindBackward0>))\n",
      "(7.438383530044307, tensor([6.3430], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([8.7521], grad_fn=<UnbindBackward0>))\n",
      "(8.758883680017025, tensor([7.8125], grad_fn=<UnbindBackward0>))\n",
      "(8.320448113956559, tensor([9.9257], grad_fn=<UnbindBackward0>))\n",
      "(8.022240916806537, tensor([6.1820], grad_fn=<UnbindBackward0>))\n",
      "(8.460622839927844, tensor([9.1297], grad_fn=<UnbindBackward0>))\n",
      "(8.482187582217422, tensor([8.7431], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([10.3692], grad_fn=<UnbindBackward0>))\n",
      "(8.797699580118923, tensor([7.6273], grad_fn=<UnbindBackward0>))\n",
      "(9.11063052782717, tensor([6.2799], grad_fn=<UnbindBackward0>))\n",
      "(9.417842227300435, tensor([8.5965], grad_fn=<UnbindBackward0>))\n",
      "(8.11552088154677, tensor([6.3122], grad_fn=<UnbindBackward0>))\n",
      "(7.407924322559599, tensor([8.4226], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([7.2913], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([8.0543], grad_fn=<UnbindBackward0>))\n",
      "(6.040254711277414, tensor([6.6270], grad_fn=<UnbindBackward0>))\n",
      "(9.368369236201092, tensor([6.0756], grad_fn=<UnbindBackward0>))\n",
      "(9.249753374333018, tensor([8.5473], grad_fn=<UnbindBackward0>))\n",
      "(8.130059039992796, tensor([8.5013], grad_fn=<UnbindBackward0>))\n",
      "(9.517015920920498, tensor([8.4087], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.0376], grad_fn=<UnbindBackward0>))\n",
      "(8.043663352393944, tensor([9.3797], grad_fn=<UnbindBackward0>))\n",
      "(9.059866258621348, tensor([7.8434], grad_fn=<UnbindBackward0>))\n",
      "(8.77971129020447, tensor([6.8511], grad_fn=<UnbindBackward0>))\n",
      "(7.656810091480378, tensor([6.3805], grad_fn=<UnbindBackward0>))\n",
      "(8.648572269472618, tensor([6.2870], grad_fn=<UnbindBackward0>))\n",
      "(8.502282578680484, tensor([8.8682], grad_fn=<UnbindBackward0>))\n",
      "(8.140898460607852, tensor([9.3968], grad_fn=<UnbindBackward0>))\n",
      "(7.8528278122817445, tensor([6.2169], grad_fn=<UnbindBackward0>))\n",
      "(6.447305862541213, tensor([7.7724], grad_fn=<UnbindBackward0>))\n",
      "(7.812782818577581, tensor([8.6236], grad_fn=<UnbindBackward0>))\n",
      "(9.698122522573561, tensor([9.5283], grad_fn=<UnbindBackward0>))\n",
      "(7.715123603632105, tensor([8.9630], grad_fn=<UnbindBackward0>))\n",
      "(7.365812837209472, tensor([8.2302], grad_fn=<UnbindBackward0>))\n",
      "(8.247743887225516, tensor([8.2425], grad_fn=<UnbindBackward0>))\n",
      "(8.75904072752422, tensor([7.9335], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([7.1115], grad_fn=<UnbindBackward0>))\n",
      "(8.346167594364134, tensor([7.4040], grad_fn=<UnbindBackward0>))\n",
      "(8.91972065553706, tensor([7.2454], grad_fn=<UnbindBackward0>))\n",
      "(7.518607216815252, tensor([6.3619], grad_fn=<UnbindBackward0>))\n",
      "(8.103191752285786, tensor([8.2368], grad_fn=<UnbindBackward0>))\n",
      "(9.310366686043308, tensor([6.2797], grad_fn=<UnbindBackward0>))\n",
      "(8.155649270366002, tensor([6.2070], grad_fn=<UnbindBackward0>))\n",
      "(7.568379267836522, tensor([7.8141], grad_fn=<UnbindBackward0>))\n",
      "(8.896177422274805, tensor([6.6089], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([8.5752], grad_fn=<UnbindBackward0>))\n",
      "(6.124683390894205, tensor([7.3220], grad_fn=<UnbindBackward0>))\n",
      "(6.432940092739179, tensor([8.1449], grad_fn=<UnbindBackward0>))\n",
      "(9.335562336729565, tensor([8.2254], grad_fn=<UnbindBackward0>))\n",
      "(8.421122722665503, tensor([6.4839], grad_fn=<UnbindBackward0>))\n",
      "(7.438383530044307, tensor([9.5089], grad_fn=<UnbindBackward0>))\n",
      "(9.000483164987708, tensor([6.3351], grad_fn=<UnbindBackward0>))\n",
      "(6.476972362889683, tensor([6.5597], grad_fn=<UnbindBackward0>))\n",
      "(8.45914025996762, tensor([6.3474], grad_fn=<UnbindBackward0>))\n",
      "(9.232688775639945, tensor([7.2110], grad_fn=<UnbindBackward0>))\n",
      "(7.126890808898808, tensor([7.8842], grad_fn=<UnbindBackward0>))\n",
      "(8.415603335654604, tensor([7.7560], grad_fn=<UnbindBackward0>))\n",
      "(9.571574905170932, tensor([8.4876], grad_fn=<UnbindBackward0>))\n",
      "(7.469654172932128, tensor([9.5745], grad_fn=<UnbindBackward0>))\n",
      "(7.02197642307216, tensor([6.2764], grad_fn=<UnbindBackward0>))\n",
      "(6.536691597591305, tensor([6.3115], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([8.2650], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.4210], grad_fn=<UnbindBackward0>))\n",
      "(6.434546518787453, tensor([6.7611], grad_fn=<UnbindBackward0>))\n",
      "(7.642044402873258, tensor([6.2658], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([10.0150], grad_fn=<UnbindBackward0>))\n",
      "(8.960724500330858, tensor([8.2316], grad_fn=<UnbindBackward0>))\n",
      "(8.602636673233706, tensor([7.7816], grad_fn=<UnbindBackward0>))\n",
      "(7.3987862754199485, tensor([6.6899], grad_fn=<UnbindBackward0>))\n",
      "(7.866338923046544, tensor([5.9437], grad_fn=<UnbindBackward0>))\n",
      "(7.820840879907344, tensor([8.5662], grad_fn=<UnbindBackward0>))\n",
      "(7.504391559161238, tensor([10.0745], grad_fn=<UnbindBackward0>))\n",
      "(7.531552381407289, tensor([9.0978], grad_fn=<UnbindBackward0>))\n",
      "(8.939318740417509, tensor([8.1946], grad_fn=<UnbindBackward0>))\n",
      "(8.579792333172758, tensor([6.8034], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([8.0844], grad_fn=<UnbindBackward0>))\n",
      "(7.715569534520208, tensor([8.1786], grad_fn=<UnbindBackward0>))\n",
      "(8.961879012677683, tensor([7.0930], grad_fn=<UnbindBackward0>))\n",
      "(7.188412736496954, tensor([6.8890], grad_fn=<UnbindBackward0>))\n",
      "(6.169610732491456, tensor([9.9423], grad_fn=<UnbindBackward0>))\n",
      "(9.333884558067638, tensor([9.0995], grad_fn=<UnbindBackward0>))\n",
      "(9.014447135152134, tensor([6.2320], grad_fn=<UnbindBackward0>))\n",
      "(7.344072850573066, tensor([8.3663], grad_fn=<UnbindBackward0>))\n",
      "(8.587651655064798, tensor([10.3817], grad_fn=<UnbindBackward0>))\n",
      "(6.163314804034641, tensor([6.6591], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([6.7092], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([6.9198], grad_fn=<UnbindBackward0>))\n",
      "(7.53689712956617, tensor([6.5953], grad_fn=<UnbindBackward0>))\n",
      "(6.375024819828097, tensor([8.3355], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([7.8698], grad_fn=<UnbindBackward0>))\n",
      "(8.45914025996762, tensor([6.4407], grad_fn=<UnbindBackward0>))\n",
      "(8.098946748943339, tensor([8.2733], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([6.3623], grad_fn=<UnbindBackward0>))\n",
      "(7.868636894184167, tensor([9.4834], grad_fn=<UnbindBackward0>))\n",
      "(8.393216011596527, tensor([10.2312], grad_fn=<UnbindBackward0>))\n",
      "(8.212025804623437, tensor([6.6742], grad_fn=<UnbindBackward0>))\n",
      "(7.904334842085095, tensor([7.2415], grad_fn=<UnbindBackward0>))\n",
      "(7.134890851565884, tensor([6.2061], grad_fn=<UnbindBackward0>))\n",
      "(9.160309438370618, tensor([9.5268], grad_fn=<UnbindBackward0>))\n",
      "(7.696667081526462, tensor([6.3090], grad_fn=<UnbindBackward0>))\n",
      "(6.543911845564792, tensor([7.8468], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([9.2939], grad_fn=<UnbindBackward0>))\n",
      "(8.861066543517762, tensor([8.2842], grad_fn=<UnbindBackward0>))\n",
      "(8.924523226133914, tensor([7.2059], grad_fn=<UnbindBackward0>))\n",
      "(8.443115988019922, tensor([8.8374], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([8.7403], grad_fn=<UnbindBackward0>))\n",
      "(8.68676717538769, tensor([6.3280], grad_fn=<UnbindBackward0>))\n",
      "(8.117610746466228, tensor([10.2815], grad_fn=<UnbindBackward0>))\n",
      "(6.3835066348840055, tensor([7.1067], grad_fn=<UnbindBackward0>))\n",
      "(8.164794804244766, tensor([9.3008], grad_fn=<UnbindBackward0>))\n",
      "(8.307705966549513, tensor([6.3463], grad_fn=<UnbindBackward0>))\n",
      "(7.745435610274381, tensor([8.0408], grad_fn=<UnbindBackward0>))\n",
      "(9.694061978050115, tensor([9.3302], grad_fn=<UnbindBackward0>))\n",
      "(8.301769763117166, tensor([8.3563], grad_fn=<UnbindBackward0>))\n",
      "(9.552581649917311, tensor([7.2321], grad_fn=<UnbindBackward0>))\n",
      "(7.924072324923417, tensor([7.1979], grad_fn=<UnbindBackward0>))\n",
      "(8.527341522468053, tensor([7.2980], grad_fn=<UnbindBackward0>))\n",
      "(8.358900612421644, tensor([7.4559], grad_fn=<UnbindBackward0>))\n",
      "(6.434546518787453, tensor([8.5197], grad_fn=<UnbindBackward0>))\n",
      "(8.602453035367061, tensor([8.5178], grad_fn=<UnbindBackward0>))\n",
      "(6.35088571671474, tensor([8.5368], grad_fn=<UnbindBackward0>))\n",
      "(7.158513997329321, tensor([7.2228], grad_fn=<UnbindBackward0>))\n",
      "(7.491087593534876, tensor([7.3610], grad_fn=<UnbindBackward0>))\n",
      "(8.561018670956267, tensor([7.7661], grad_fn=<UnbindBackward0>))\n",
      "(7.683403681053826, tensor([6.8038], grad_fn=<UnbindBackward0>))\n",
      "(8.342601680684194, tensor([8.3562], grad_fn=<UnbindBackward0>))\n",
      "(8.360773272144936, tensor([10.4583], grad_fn=<UnbindBackward0>))\n",
      "(7.147559271189454, tensor([9.5169], grad_fn=<UnbindBackward0>))\n",
      "(8.803424211600703, tensor([9.5188], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([8.5475], grad_fn=<UnbindBackward0>))\n",
      "(6.668228248417403, tensor([6.6207], grad_fn=<UnbindBackward0>))\n",
      "(8.072779333169498, tensor([6.3899], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([7.4625], grad_fn=<UnbindBackward0>))\n",
      "(8.704004653483045, tensor([7.9516], grad_fn=<UnbindBackward0>))\n",
      "(8.64241515616962, tensor([7.7484], grad_fn=<UnbindBackward0>))\n",
      "(8.414274137408396, tensor([6.1480], grad_fn=<UnbindBackward0>))\n",
      "(7.729735331385051, tensor([6.6628], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.6767], grad_fn=<UnbindBackward0>))\n",
      "(7.00850518208228, tensor([8.5547], grad_fn=<UnbindBackward0>))\n",
      "(8.505929999137527, tensor([6.2061], grad_fn=<UnbindBackward0>))\n",
      "(8.610683534503575, tensor([7.8256], grad_fn=<UnbindBackward0>))\n",
      "(8.910990494656719, tensor([7.8334], grad_fn=<UnbindBackward0>))\n",
      "(9.001222992395064, tensor([8.8927], grad_fn=<UnbindBackward0>))\n",
      "(7.3864708488298945, tensor([6.3689], grad_fn=<UnbindBackward0>))\n",
      "(8.5016733797582, tensor([7.3594], grad_fn=<UnbindBackward0>))\n",
      "(9.241548299100376, tensor([7.6743], grad_fn=<UnbindBackward0>))\n",
      "(9.631153757031147, tensor([7.7138], grad_fn=<UnbindBackward0>))\n",
      "(6.3835066348840055, tensor([8.7845], grad_fn=<UnbindBackward0>))\n",
      "(8.17131687471973, tensor([8.8209], grad_fn=<UnbindBackward0>))\n",
      "(8.721602344674197, tensor([7.8617], grad_fn=<UnbindBackward0>))\n",
      "(7.633369649679584, tensor([7.8379], grad_fn=<UnbindBackward0>))\n",
      "(9.470317173335742, tensor([6.8612], grad_fn=<UnbindBackward0>))\n",
      "(8.441175704992322, tensor([6.2207], grad_fn=<UnbindBackward0>))\n",
      "(8.150756470275551, tensor([8.2266], grad_fn=<UnbindBackward0>))\n",
      "(8.0507033814703, tensor([7.0971], grad_fn=<UnbindBackward0>))\n",
      "(7.112327444710911, tensor([6.2543], grad_fn=<UnbindBackward0>))\n",
      "(7.237059026124737, tensor([6.7083], grad_fn=<UnbindBackward0>))\n",
      "(8.757154527656606, tensor([7.3537], grad_fn=<UnbindBackward0>))\n",
      "(6.333279628139691, tensor([6.4682], grad_fn=<UnbindBackward0>))\n",
      "(6.1092475827643655, tensor([8.8221], grad_fn=<UnbindBackward0>))\n",
      "(6.932447891572509, tensor([6.7093], grad_fn=<UnbindBackward0>))\n",
      "(8.885717651712119, tensor([9.3994], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([7.7409], grad_fn=<UnbindBackward0>))\n",
      "(8.494538500851432, tensor([8.3941], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([8.4388], grad_fn=<UnbindBackward0>))\n",
      "(8.57810012631976, tensor([9.1410], grad_fn=<UnbindBackward0>))\n",
      "(7.2682230211595655, tensor([10.1922], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([8.1639], grad_fn=<UnbindBackward0>))\n",
      "(9.603125426926972, tensor([7.2664], grad_fn=<UnbindBackward0>))\n",
      "(7.520776415062797, tensor([8.7598], grad_fn=<UnbindBackward0>))\n",
      "(9.23815007261545, tensor([6.1383], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([6.8012], grad_fn=<UnbindBackward0>))\n",
      "(6.322565239927284, tensor([7.3383], grad_fn=<UnbindBackward0>))\n",
      "(8.629807335785372, tensor([9.4755], grad_fn=<UnbindBackward0>))\n",
      "(7.350516171833998, tensor([8.3074], grad_fn=<UnbindBackward0>))\n",
      "(6.785587645007929, tensor([9.0995], grad_fn=<UnbindBackward0>))\n",
      "(6.78332520060396, tensor([7.9152], grad_fn=<UnbindBackward0>))\n",
      "(9.259416209622776, tensor([10.3342], grad_fn=<UnbindBackward0>))\n",
      "(7.853216388156072, tensor([8.3679], grad_fn=<UnbindBackward0>))\n",
      "(8.485496104672983, tensor([8.2620], grad_fn=<UnbindBackward0>))\n",
      "(6.773080375655535, tensor([10.4067], grad_fn=<UnbindBackward0>))\n",
      "(9.287116210778231, tensor([6.1916], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([9.6679], grad_fn=<UnbindBackward0>))\n",
      "(7.6362696033793735, tensor([6.4422], grad_fn=<UnbindBackward0>))\n",
      "(8.310169021981912, tensor([9.7108], grad_fn=<UnbindBackward0>))\n",
      "(7.845024417241484, tensor([8.7035], grad_fn=<UnbindBackward0>))\n",
      "(6.917705609835305, tensor([7.4024], grad_fn=<UnbindBackward0>))\n",
      "(8.277920258172143, tensor([9.7678], grad_fn=<UnbindBackward0>))\n",
      "(7.65728279297819, tensor([6.3196], grad_fn=<UnbindBackward0>))\n",
      "(8.958797346140274, tensor([6.9296], grad_fn=<UnbindBackward0>))\n",
      "(9.709295823171686, tensor([6.4480], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([8.2950], grad_fn=<UnbindBackward0>))\n",
      "(7.488293515159428, tensor([7.2122], grad_fn=<UnbindBackward0>))\n",
      "(7.72356247227797, tensor([7.3861], grad_fn=<UnbindBackward0>))\n",
      "(9.053686561930807, tensor([5.9534], grad_fn=<UnbindBackward0>))\n",
      "(8.37930948405285, tensor([6.1384], grad_fn=<UnbindBackward0>))\n",
      "(8.272315147956022, tensor([9.4030], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([8.2000], grad_fn=<UnbindBackward0>))\n",
      "(9.035867930125628, tensor([7.1244], grad_fn=<UnbindBackward0>))\n",
      "(8.192016914536875, tensor([6.4104], grad_fn=<UnbindBackward0>))\n",
      "(9.38907215991958, tensor([7.2208], grad_fn=<UnbindBackward0>))\n",
      "(6.415096959171596, tensor([8.5764], grad_fn=<UnbindBackward0>))\n",
      "(7.365812837209472, tensor([7.1676], grad_fn=<UnbindBackward0>))\n",
      "(8.019283792916793, tensor([6.4933], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.7883], grad_fn=<UnbindBackward0>))\n",
      "(7.358830898342354, tensor([6.2748], grad_fn=<UnbindBackward0>))\n",
      "(9.135940049399933, tensor([9.5664], grad_fn=<UnbindBackward0>))\n",
      "(7.842278779117352, tensor([7.2266], grad_fn=<UnbindBackward0>))\n",
      "(7.736743682453495, tensor([6.2752], grad_fn=<UnbindBackward0>))\n",
      "(8.217438537730187, tensor([7.3559], grad_fn=<UnbindBackward0>))\n",
      "(8.622273667866745, tensor([7.7187], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([8.7710], grad_fn=<UnbindBackward0>))\n",
      "(6.816735880594968, tensor([8.9285], grad_fn=<UnbindBackward0>))\n",
      "(7.838737559599282, tensor([8.4519], grad_fn=<UnbindBackward0>))\n",
      "(8.12296471523406, tensor([8.6539], grad_fn=<UnbindBackward0>))\n",
      "(8.697345730925353, tensor([6.4429], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.7620], grad_fn=<UnbindBackward0>))\n",
      "(6.60934924316738, tensor([7.9747], grad_fn=<UnbindBackward0>))\n",
      "(8.177796683277778, tensor([7.5770], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.6769], grad_fn=<UnbindBackward0>))\n",
      "(6.966024187106113, tensor([8.4309], grad_fn=<UnbindBackward0>))\n",
      "(6.421622267806518, tensor([8.1293], grad_fn=<UnbindBackward0>))\n",
      "(8.696844519654313, tensor([8.5597], grad_fn=<UnbindBackward0>))\n",
      "(6.779921907472252, tensor([8.9363], grad_fn=<UnbindBackward0>))\n",
      "(7.990576881743923, tensor([7.1400], grad_fn=<UnbindBackward0>))\n",
      "(9.256460307589736, tensor([7.1431], grad_fn=<UnbindBackward0>))\n",
      "(6.364750756851911, tensor([6.2473], grad_fn=<UnbindBackward0>))\n",
      "(8.728587995695898, tensor([6.3023], grad_fn=<UnbindBackward0>))\n",
      "(9.124128675157117, tensor([7.6141], grad_fn=<UnbindBackward0>))\n",
      "(8.576593534697684, tensor([7.9441], grad_fn=<UnbindBackward0>))\n",
      "(6.248042874508429, tensor([9.4398], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([6.4704], grad_fn=<UnbindBackward0>))\n",
      "(7.7306140660637395, tensor([9.6646], grad_fn=<UnbindBackward0>))\n",
      "(6.966024187106113, tensor([8.2795], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([6.7410], grad_fn=<UnbindBackward0>))\n",
      "(9.372204086867741, tensor([9.0077], grad_fn=<UnbindBackward0>))\n",
      "(5.958424693029782, tensor([7.0121], grad_fn=<UnbindBackward0>))\n",
      "(9.225327501784431, tensor([9.1554], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.4429], grad_fn=<UnbindBackward0>))\n",
      "(9.807912337613098, tensor([7.2494], grad_fn=<UnbindBackward0>))\n",
      "(8.736489351001554, tensor([6.2149], grad_fn=<UnbindBackward0>))\n",
      "(8.68929604801586, tensor([6.4774], grad_fn=<UnbindBackward0>))\n",
      "(8.422662707570003, tensor([6.4114], grad_fn=<UnbindBackward0>))\n",
      "(7.588829878307813, tensor([7.8471], grad_fn=<UnbindBackward0>))\n",
      "(8.653470809708786, tensor([9.8185], grad_fn=<UnbindBackward0>))\n",
      "(8.262300941787448, tensor([9.2781], grad_fn=<UnbindBackward0>))\n",
      "(7.7752758464868625, tensor([10.0440], grad_fn=<UnbindBackward0>))\n",
      "(7.52131798019924, tensor([8.7469], grad_fn=<UnbindBackward0>))\n",
      "(8.679822114864455, tensor([8.7634], grad_fn=<UnbindBackward0>))\n",
      "(7.271703706887368, tensor([7.2683], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([6.4498], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([9.4508], grad_fn=<UnbindBackward0>))\n",
      "(6.5638555265321274, tensor([7.7182], grad_fn=<UnbindBackward0>))\n",
      "(7.5688956634069955, tensor([8.4675], grad_fn=<UnbindBackward0>))\n",
      "(8.055157731819678, tensor([8.7198], grad_fn=<UnbindBackward0>))\n",
      "(7.528331766707247, tensor([10.0366], grad_fn=<UnbindBackward0>))\n",
      "(9.740792136877351, tensor([7.4747], grad_fn=<UnbindBackward0>))\n",
      "(9.684958338772663, tensor([5.8986], grad_fn=<UnbindBackward0>))\n",
      "(9.164610520270008, tensor([6.7773], grad_fn=<UnbindBackward0>))\n",
      "(8.340456012916183, tensor([6.6091], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([6.2445], grad_fn=<UnbindBackward0>))\n",
      "(7.680637427560936, tensor([7.3528], grad_fn=<UnbindBackward0>))\n",
      "(8.560444233410552, tensor([8.8408], grad_fn=<UnbindBackward0>))\n",
      "(8.953898535260459, tensor([6.5527], grad_fn=<UnbindBackward0>))\n",
      "(9.681530796701484, tensor([9.4747], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([7.3251], grad_fn=<UnbindBackward0>))\n",
      "(9.064620717626777, tensor([8.8121], grad_fn=<UnbindBackward0>))\n",
      "(7.68017564043659, tensor([7.6707], grad_fn=<UnbindBackward0>))\n",
      "(8.3707791729607, tensor([6.3258], grad_fn=<UnbindBackward0>))\n",
      "(7.631431664576906, tensor([6.7671], grad_fn=<UnbindBackward0>))\n",
      "(7.3901814282264295, tensor([6.5392], grad_fn=<UnbindBackward0>))\n",
      "(7.1372784372603855, tensor([8.9900], grad_fn=<UnbindBackward0>))\n",
      "(7.766840537085513, tensor([7.5028], grad_fn=<UnbindBackward0>))\n",
      "(7.016609683894219, tensor([8.0605], grad_fn=<UnbindBackward0>))\n",
      "(7.27931883541462, tensor([9.4331], grad_fn=<UnbindBackward0>))\n",
      "(6.678342114654332, tensor([6.7731], grad_fn=<UnbindBackward0>))\n",
      "(7.669961995473577, tensor([6.8302], grad_fn=<UnbindBackward0>))\n",
      "(8.078688229229872, tensor([9.0295], grad_fn=<UnbindBackward0>))\n",
      "(6.951772164398911, tensor([6.6529], grad_fn=<UnbindBackward0>))\n",
      "(8.389814262086407, tensor([8.5131], grad_fn=<UnbindBackward0>))\n",
      "(9.069237530998182, tensor([6.2100], grad_fn=<UnbindBackward0>))\n",
      "(7.557994958530806, tensor([6.5767], grad_fn=<UnbindBackward0>))\n",
      "(9.700146628518098, tensor([10.0547], grad_fn=<UnbindBackward0>))\n",
      "(9.560504164299259, tensor([7.0721], grad_fn=<UnbindBackward0>))\n",
      "(7.1049654482698426, tensor([10.1882], grad_fn=<UnbindBackward0>))\n",
      "(8.210668031162976, tensor([6.4154], grad_fn=<UnbindBackward0>))\n",
      "(8.721439305625983, tensor([8.3957], grad_fn=<UnbindBackward0>))\n",
      "(8.739536422559677, tensor([8.3542], grad_fn=<UnbindBackward0>))\n",
      "(6.386879319362645, tensor([6.2296], grad_fn=<UnbindBackward0>))\n",
      "(7.887208585813932, tensor([9.5390], grad_fn=<UnbindBackward0>))\n",
      "(8.273846932784508, tensor([8.4893], grad_fn=<UnbindBackward0>))\n",
      "(8.933004591578547, tensor([8.2493], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([9.5546], grad_fn=<UnbindBackward0>))\n",
      "(7.738488122494646, tensor([7.9770], grad_fn=<UnbindBackward0>))\n",
      "(7.502738210754851, tensor([6.2626], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([8.9018], grad_fn=<UnbindBackward0>))\n",
      "(7.239214973779806, tensor([8.3012], grad_fn=<UnbindBackward0>))\n",
      "(9.747768977536708, tensor([9.5725], grad_fn=<UnbindBackward0>))\n",
      "(9.673948594133059, tensor([9.4155], grad_fn=<UnbindBackward0>))\n",
      "(7.473637108496206, tensor([6.4201], grad_fn=<UnbindBackward0>))\n",
      "(6.683360945766275, tensor([10.1526], grad_fn=<UnbindBackward0>))\n",
      "(7.470224135899966, tensor([8.9412], grad_fn=<UnbindBackward0>))\n",
      "(8.637993891561942, tensor([7.6454], grad_fn=<UnbindBackward0>))\n",
      "(7.984121958702927, tensor([6.5862], grad_fn=<UnbindBackward0>))\n",
      "(8.045588280803528, tensor([7.5155], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([6.3921], grad_fn=<UnbindBackward0>))\n",
      "(8.717518372649767, tensor([8.4715], grad_fn=<UnbindBackward0>))\n",
      "(8.919988070968524, tensor([7.9693], grad_fn=<UnbindBackward0>))\n",
      "(6.70073110954781, tensor([9.5538], grad_fn=<UnbindBackward0>))\n",
      "(9.156834104453042, tensor([7.0600], grad_fn=<UnbindBackward0>))\n",
      "(7.4377951216719325, tensor([6.7487], grad_fn=<UnbindBackward0>))\n",
      "(8.047509510981422, tensor([9.3755], grad_fn=<UnbindBackward0>))\n",
      "(8.30671904320269, tensor([9.1064], grad_fn=<UnbindBackward0>))\n",
      "(7.9294865233142895, tensor([7.6913], grad_fn=<UnbindBackward0>))\n",
      "(7.866338923046544, tensor([6.3735], grad_fn=<UnbindBackward0>))\n",
      "(8.307705966549513, tensor([6.2285], grad_fn=<UnbindBackward0>))\n",
      "(6.824373670043086, tensor([7.8644], grad_fn=<UnbindBackward0>))\n",
      "(7.114769448366463, tensor([6.6571], grad_fn=<UnbindBackward0>))\n",
      "(7.362010551259734, tensor([6.7239], grad_fn=<UnbindBackward0>))\n",
      "(6.401917196727186, tensor([6.7730], grad_fn=<UnbindBackward0>))\n",
      "(8.16763571524637, tensor([8.4469], grad_fn=<UnbindBackward0>))\n",
      "(6.876264611890766, tensor([6.2230], grad_fn=<UnbindBackward0>))\n",
      "(7.416378479192928, tensor([6.4916], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([9.9369], grad_fn=<UnbindBackward0>))\n",
      "(7.9391588179567965, tensor([8.2199], grad_fn=<UnbindBackward0>))\n",
      "(7.478734825567875, tensor([6.3561], grad_fn=<UnbindBackward0>))\n",
      "(6.244166900663736, tensor([8.9596], grad_fn=<UnbindBackward0>))\n",
      "(8.133000218583613, tensor([8.5883], grad_fn=<UnbindBackward0>))\n",
      "(7.523481312573497, tensor([6.4068], grad_fn=<UnbindBackward0>))\n",
      "(6.652863029353347, tensor([7.2492], grad_fn=<UnbindBackward0>))\n",
      "(9.066008001086264, tensor([7.2275], grad_fn=<UnbindBackward0>))\n",
      "(6.077642243349034, tensor([8.5185], grad_fn=<UnbindBackward0>))\n",
      "(9.041803370152845, tensor([7.9160], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([7.1447], grad_fn=<UnbindBackward0>))\n",
      "(8.872487182278038, tensor([8.0274], grad_fn=<UnbindBackward0>))\n",
      "(7.850103545175582, tensor([8.3896], grad_fn=<UnbindBackward0>))\n",
      "(8.5814816812986, tensor([6.3728], grad_fn=<UnbindBackward0>))\n",
      "(6.841615476477592, tensor([8.9411], grad_fn=<UnbindBackward0>))\n",
      "(6.499787040655854, tensor([8.4531], grad_fn=<UnbindBackward0>))\n",
      "(8.90557995798965, tensor([8.5594], grad_fn=<UnbindBackward0>))\n",
      "(7.877017895622398, tensor([8.7784], grad_fn=<UnbindBackward0>))\n",
      "(7.554334823725748, tensor([5.8198], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.5899], grad_fn=<UnbindBackward0>))\n",
      "(7.463936604468925, tensor([10.0250], grad_fn=<UnbindBackward0>))\n",
      "(8.408716715080153, tensor([9.3107], grad_fn=<UnbindBackward0>))\n",
      "(8.122371243406553, tensor([8.2144], grad_fn=<UnbindBackward0>))\n",
      "(7.804251383528112, tensor([8.2473], grad_fn=<UnbindBackward0>))\n",
      "(8.452974619089586, tensor([9.4155], grad_fn=<UnbindBackward0>))\n",
      "(9.720225553665117, tensor([8.8758], grad_fn=<UnbindBackward0>))\n",
      "(7.252053951852814, tensor([6.4037], grad_fn=<UnbindBackward0>))\n",
      "(7.213031659834869, tensor([6.1637], grad_fn=<UnbindBackward0>))\n",
      "(6.807934943699926, tensor([8.4395], grad_fn=<UnbindBackward0>))\n",
      "(9.117896081584902, tensor([7.7197], grad_fn=<UnbindBackward0>))\n",
      "(7.1631723908466425, tensor([6.1775], grad_fn=<UnbindBackward0>))\n",
      "(8.954286157204713, tensor([5.9870], grad_fn=<UnbindBackward0>))\n",
      "(7.788211557847076, tensor([7.7073], grad_fn=<UnbindBackward0>))\n",
      "(8.431853144249223, tensor([6.6881], grad_fn=<UnbindBackward0>))\n",
      "(8.07402621612406, tensor([7.6716], grad_fn=<UnbindBackward0>))\n",
      "(9.254739959272866, tensor([8.0183], grad_fn=<UnbindBackward0>))\n",
      "(8.231109840328154, tensor([6.7037], grad_fn=<UnbindBackward0>))\n",
      "(6.489204931325317, tensor([9.2595], grad_fn=<UnbindBackward0>))\n",
      "(6.982862751468942, tensor([9.6306], grad_fn=<UnbindBackward0>))\n",
      "(7.771488760117616, tensor([8.5262], grad_fn=<UnbindBackward0>))\n",
      "(6.336825731146441, tensor([7.8731], grad_fn=<UnbindBackward0>))\n",
      "(9.529084969879781, tensor([7.9975], grad_fn=<UnbindBackward0>))\n",
      "(7.717351272185329, tensor([7.9577], grad_fn=<UnbindBackward0>))\n",
      "(8.138564737261632, tensor([6.2031], grad_fn=<UnbindBackward0>))\n",
      "(8.638702608813434, tensor([8.9053], grad_fn=<UnbindBackward0>))\n",
      "(8.661120360222881, tensor([10.0904], grad_fn=<UnbindBackward0>))\n",
      "(7.773173680482536, tensor([7.9770], grad_fn=<UnbindBackward0>))\n",
      "(7.0967213784947605, tensor([6.3189], grad_fn=<UnbindBackward0>))\n",
      "(6.78105762593618, tensor([6.1940], grad_fn=<UnbindBackward0>))\n",
      "(8.648221453822641, tensor([6.2634], grad_fn=<UnbindBackward0>))\n",
      "(8.531884740159228, tensor([8.4809], grad_fn=<UnbindBackward0>))\n",
      "(7.502738210754851, tensor([10.1614], grad_fn=<UnbindBackward0>))\n",
      "(8.525756422076725, tensor([6.2162], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([8.4238], grad_fn=<UnbindBackward0>))\n",
      "(7.77863014732581, tensor([9.3348], grad_fn=<UnbindBackward0>))\n",
      "(8.599141774063405, tensor([7.3441], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([8.5117], grad_fn=<UnbindBackward0>))\n",
      "(7.193685818395112, tensor([5.9206], grad_fn=<UnbindBackward0>))\n",
      "(8.5956346177228, tensor([6.2373], grad_fn=<UnbindBackward0>))\n",
      "(8.541690663016626, tensor([6.4429], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([6.3991], grad_fn=<UnbindBackward0>))\n",
      "(7.515344571180436, tensor([8.6175], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.9357], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([7.1889], grad_fn=<UnbindBackward0>))\n",
      "(8.415160465851086, tensor([6.3190], grad_fn=<UnbindBackward0>))\n",
      "(8.82922635473185, tensor([6.1375], grad_fn=<UnbindBackward0>))\n",
      "(6.773080375655535, tensor([7.0808], grad_fn=<UnbindBackward0>))\n",
      "(9.447071196798909, tensor([5.8053], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([7.7364], grad_fn=<UnbindBackward0>))\n",
      "(7.069023426578259, tensor([6.3580], grad_fn=<UnbindBackward0>))\n",
      "(9.212238569259263, tensor([7.3788], grad_fn=<UnbindBackward0>))\n",
      "(8.724207360800564, tensor([6.5052], grad_fn=<UnbindBackward0>))\n",
      "(7.162397497355718, tensor([7.1949], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([9.5327], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([9.9613], grad_fn=<UnbindBackward0>))\n",
      "(8.566935283311052, tensor([6.7401], grad_fn=<UnbindBackward0>))\n",
      "(9.03931477492408, tensor([7.7293], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([6.2248], grad_fn=<UnbindBackward0>))\n",
      "(8.27664912542186, tensor([8.8045], grad_fn=<UnbindBackward0>))\n",
      "(7.701652362642226, tensor([8.8959], grad_fn=<UnbindBackward0>))\n",
      "(9.497772413172754, tensor([6.4415], grad_fn=<UnbindBackward0>))\n",
      "(8.822027322685583, tensor([8.1171], grad_fn=<UnbindBackward0>))\n",
      "(6.532334292222349, tensor([8.2085], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([7.8429], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([8.7242], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([8.8187], grad_fn=<UnbindBackward0>))\n",
      "(8.54461378699223, tensor([6.8324], grad_fn=<UnbindBackward0>))\n",
      "(8.28576542051433, tensor([7.7767], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([8.4978], grad_fn=<UnbindBackward0>))\n",
      "(6.322565239927284, tensor([8.3534], grad_fn=<UnbindBackward0>))\n",
      "(6.947937068614969, tensor([7.3301], grad_fn=<UnbindBackward0>))\n",
      "(9.099967314494812, tensor([7.3790], grad_fn=<UnbindBackward0>))\n",
      "(8.841158975945264, tensor([8.7297], grad_fn=<UnbindBackward0>))\n",
      "(6.71295620067707, tensor([9.0093], grad_fn=<UnbindBackward0>))\n",
      "(6.975413927455952, tensor([8.6242], grad_fn=<UnbindBackward0>))\n",
      "(9.092907275084087, tensor([7.7950], grad_fn=<UnbindBackward0>))\n",
      "(8.64259160081057, tensor([9.3839], grad_fn=<UnbindBackward0>))\n",
      "(8.4096079807363, tensor([6.1446], grad_fn=<UnbindBackward0>))\n",
      "(7.738052297689316, tensor([7.3621], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([7.3530], grad_fn=<UnbindBackward0>))\n",
      "(6.8690144506657065, tensor([7.1667], grad_fn=<UnbindBackward0>))\n",
      "(8.401333305321703, tensor([7.8292], grad_fn=<UnbindBackward0>))\n",
      "(7.503289630675082, tensor([8.4147], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([7.8613], grad_fn=<UnbindBackward0>))\n",
      "(7.480992162869525, tensor([8.1797], grad_fn=<UnbindBackward0>))\n",
      "(8.13739583005665, tensor([8.3192], grad_fn=<UnbindBackward0>))\n",
      "(8.319473692442186, tensor([8.8607], grad_fn=<UnbindBackward0>))\n",
      "(8.700347734514084, tensor([9.1025], grad_fn=<UnbindBackward0>))\n",
      "(6.340359303727752, tensor([6.4205], grad_fn=<UnbindBackward0>))\n",
      "(9.28720881623276, tensor([8.6642], grad_fn=<UnbindBackward0>))\n",
      "(7.52131798019924, tensor([8.2458], grad_fn=<UnbindBackward0>))\n",
      "(6.393590753950631, tensor([7.2256], grad_fn=<UnbindBackward0>))\n",
      "(8.718663567048953, tensor([7.7960], grad_fn=<UnbindBackward0>))\n",
      "(8.572060092857077, tensor([6.7691], grad_fn=<UnbindBackward0>))\n",
      "(6.848005274576363, tensor([8.3230], grad_fn=<UnbindBackward0>))\n",
      "(7.141245122350491, tensor([8.5792], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([6.2756], grad_fn=<UnbindBackward0>))\n",
      "(7.781138509845015, tensor([7.9798], grad_fn=<UnbindBackward0>))\n",
      "(9.299358068293836, tensor([6.7238], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([9.7696], grad_fn=<UnbindBackward0>))\n",
      "(6.343880434126331, tensor([6.3437], grad_fn=<UnbindBackward0>))\n",
      "(9.28618968425962, tensor([6.5904], grad_fn=<UnbindBackward0>))\n",
      "(7.653494909661253, tensor([6.4850], grad_fn=<UnbindBackward0>))\n",
      "(7.952263308657046, tensor([6.0374], grad_fn=<UnbindBackward0>))\n",
      "(8.630164670075398, tensor([8.6160], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([7.1155], grad_fn=<UnbindBackward0>))\n",
      "(7.763446388727362, tensor([8.4371], grad_fn=<UnbindBackward0>))\n",
      "(7.365180126021013, tensor([6.1918], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([7.9032], grad_fn=<UnbindBackward0>))\n",
      "(6.692083742506628, tensor([7.1167], grad_fn=<UnbindBackward0>))\n",
      "(7.786136437783072, tensor([8.1656], grad_fn=<UnbindBackward0>))\n",
      "(8.488999457045455, tensor([9.8856], grad_fn=<UnbindBackward0>))\n",
      "(6.9930151229329605, tensor([6.7407], grad_fn=<UnbindBackward0>))\n",
      "(7.742835955430749, tensor([6.4643], grad_fn=<UnbindBackward0>))\n",
      "(7.68662133494462, tensor([7.5628], grad_fn=<UnbindBackward0>))\n",
      "(6.492239835020471, tensor([6.3829], grad_fn=<UnbindBackward0>))\n",
      "(6.910750787961936, tensor([6.1574], grad_fn=<UnbindBackward0>))\n",
      "(8.6628507638386, tensor([7.9840], grad_fn=<UnbindBackward0>))\n",
      "(8.602269363771356, tensor([9.3821], grad_fn=<UnbindBackward0>))\n",
      "(7.559038255443384, tensor([6.1812], grad_fn=<UnbindBackward0>))\n",
      "(6.202535517187923, tensor([8.8353], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([8.6016], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([8.5434], grad_fn=<UnbindBackward0>))\n",
      "(9.060911858484253, tensor([6.5673], grad_fn=<UnbindBackward0>))\n",
      "(6.658011045870748, tensor([8.3973], grad_fn=<UnbindBackward0>))\n",
      "(7.411556287811163, tensor([8.5165], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([6.5665], grad_fn=<UnbindBackward0>))\n",
      "(8.07837810362652, tensor([9.4077], grad_fn=<UnbindBackward0>))\n",
      "(7.887208585813932, tensor([9.8815], grad_fn=<UnbindBackward0>))\n",
      "(6.962243464266207, tensor([8.4779], grad_fn=<UnbindBackward0>))\n",
      "(7.789868559054706, tensor([6.9017], grad_fn=<UnbindBackward0>))\n",
      "(9.157993891603974, tensor([6.4957], grad_fn=<UnbindBackward0>))\n",
      "(8.459987717645458, tensor([7.8314], grad_fn=<UnbindBackward0>))\n",
      "(9.368539986649727, tensor([10.0921], grad_fn=<UnbindBackward0>))\n",
      "(7.6093665379542115, tensor([6.4339], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([6.3155], grad_fn=<UnbindBackward0>))\n",
      "(7.729735331385051, tensor([7.6731], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([6.3594], grad_fn=<UnbindBackward0>))\n",
      "(8.763740720509464, tensor([10.4839], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([8.7699], grad_fn=<UnbindBackward0>))\n",
      "(8.820551743253025, tensor([7.5361], grad_fn=<UnbindBackward0>))\n",
      "(8.35936910622267, tensor([7.8389], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([6.8171], grad_fn=<UnbindBackward0>))\n",
      "(7.419380582918692, tensor([8.2147], grad_fn=<UnbindBackward0>))\n",
      "(8.945984124827898, tensor([7.5468], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([6.2726], grad_fn=<UnbindBackward0>))\n",
      "(7.249215057114389, tensor([8.6615], grad_fn=<UnbindBackward0>))\n",
      "(7.004881989712859, tensor([9.6813], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([8.4969], grad_fn=<UnbindBackward0>))\n",
      "(7.9714309977693505, tensor([7.8824], grad_fn=<UnbindBackward0>))\n",
      "(8.776167099731275, tensor([7.3218], grad_fn=<UnbindBackward0>))\n",
      "(8.509564164251746, tensor([6.2847], grad_fn=<UnbindBackward0>))\n",
      "(8.568076401730806, tensor([6.7326], grad_fn=<UnbindBackward0>))\n",
      "(8.874308038583354, tensor([10.2569], grad_fn=<UnbindBackward0>))\n",
      "(7.803435056952168, tensor([8.9613], grad_fn=<UnbindBackward0>))\n",
      "(7.647786045440933, tensor([9.5628], grad_fn=<UnbindBackward0>))\n",
      "(7.436617265234227, tensor([8.4545], grad_fn=<UnbindBackward0>))\n",
      "(9.563458999712138, tensor([6.1762], grad_fn=<UnbindBackward0>))\n",
      "(6.875232087276577, tensor([6.8557], grad_fn=<UnbindBackward0>))\n",
      "(7.022868086082641, tensor([8.4813], grad_fn=<UnbindBackward0>))\n",
      "(7.1569563646156364, tensor([7.7858], grad_fn=<UnbindBackward0>))\n",
      "(8.452761331256848, tensor([6.2662], grad_fn=<UnbindBackward0>))\n",
      "(7.682021510826875, tensor([6.8414], grad_fn=<UnbindBackward0>))\n",
      "(8.87570644462861, tensor([7.9511], grad_fn=<UnbindBackward0>))\n",
      "(8.207946941048617, tensor([9.1215], grad_fn=<UnbindBackward0>))\n",
      "(8.963544291996744, tensor([8.9849], grad_fn=<UnbindBackward0>))\n",
      "(6.3561076606958915, tensor([7.1798], grad_fn=<UnbindBackward0>))\n",
      "(7.8160138391590275, tensor([7.8178], grad_fn=<UnbindBackward0>))\n",
      "(9.798127036878302, tensor([8.5009], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([7.9012], grad_fn=<UnbindBackward0>))\n",
      "(8.301025253838453, tensor([6.7823], grad_fn=<UnbindBackward0>))\n",
      "(9.156412029950626, tensor([6.3621], grad_fn=<UnbindBackward0>))\n",
      "(6.68586094706836, tensor([8.4388], grad_fn=<UnbindBackward0>))\n",
      "(7.122059881629142, tensor([6.7726], grad_fn=<UnbindBackward0>))\n",
      "(7.520776415062797, tensor([6.2588], grad_fn=<UnbindBackward0>))\n",
      "(6.883462586413092, tensor([9.5192], grad_fn=<UnbindBackward0>))\n",
      "(8.828494129466652, tensor([6.3329], grad_fn=<UnbindBackward0>))\n",
      "(9.230437073675304, tensor([6.5064], grad_fn=<UnbindBackward0>))\n",
      "(6.249975242259483, tensor([8.0743], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([9.0552], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([6.8007], grad_fn=<UnbindBackward0>))\n",
      "(8.37930948405285, tensor([6.9229], grad_fn=<UnbindBackward0>))\n",
      "(9.002701007197938, tensor([8.0291], grad_fn=<UnbindBackward0>))\n",
      "(8.195057690895077, tensor([7.7658], grad_fn=<UnbindBackward0>))\n",
      "(7.856319571406588, tensor([7.9417], grad_fn=<UnbindBackward0>))\n",
      "(6.375024819828097, tensor([7.7882], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([9.3604], grad_fn=<UnbindBackward0>))\n",
      "(7.8383433155571165, tensor([6.2396], grad_fn=<UnbindBackward0>))\n",
      "(7.5352967024440884, tensor([7.8566], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([7.8995], grad_fn=<UnbindBackward0>))\n",
      "(7.519692404116539, tensor([8.6074], grad_fn=<UnbindBackward0>))\n",
      "(7.646353722445999, tensor([8.8797], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([7.8302], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([6.4951], grad_fn=<UnbindBackward0>))\n",
      "(8.434463543817241, tensor([8.8699], grad_fn=<UnbindBackward0>))\n",
      "(7.52131798019924, tensor([6.2956], grad_fn=<UnbindBackward0>))\n",
      "(9.149315670138408, tensor([7.0536], grad_fn=<UnbindBackward0>))\n",
      "(8.569975376855206, tensor([6.3216], grad_fn=<UnbindBackward0>))\n",
      "(8.478452363099807, tensor([8.3620], grad_fn=<UnbindBackward0>))\n",
      "(6.131226489483141, tensor([9.2206], grad_fn=<UnbindBackward0>))\n",
      "(7.7354333524996886, tensor([6.5916], grad_fn=<UnbindBackward0>))\n",
      "(7.255591274253665, tensor([8.2154], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([8.7275], grad_fn=<UnbindBackward0>))\n",
      "(9.197660318818809, tensor([7.8276], grad_fn=<UnbindBackward0>))\n",
      "(8.406708458240965, tensor([5.9211], grad_fn=<UnbindBackward0>))\n",
      "(7.6251071482389, tensor([6.7634], grad_fn=<UnbindBackward0>))\n",
      "(9.462887373659635, tensor([6.6586], grad_fn=<UnbindBackward0>))\n",
      "(7.023758954738443, tensor([9.0343], grad_fn=<UnbindBackward0>))\n",
      "(6.542471960506805, tensor([9.7593], grad_fn=<UnbindBackward0>))\n",
      "(9.263312256742289, tensor([8.9727], grad_fn=<UnbindBackward0>))\n",
      "(9.33467964623179, tensor([8.5462], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([6.4998], grad_fn=<UnbindBackward0>))\n",
      "(8.173575486634153, tensor([7.8548], grad_fn=<UnbindBackward0>))\n",
      "(8.09437844497296, tensor([6.8265], grad_fn=<UnbindBackward0>))\n",
      "(8.0519780789023, tensor([9.0919], grad_fn=<UnbindBackward0>))\n",
      "(9.592536853144992, tensor([7.8203], grad_fn=<UnbindBackward0>))\n",
      "(8.797397374012059, tensor([8.6278], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([8.3883], grad_fn=<UnbindBackward0>))\n",
      "(6.962243464266207, tensor([9.3529], grad_fn=<UnbindBackward0>))\n",
      "(9.31533081916712, tensor([8.2097], grad_fn=<UnbindBackward0>))\n",
      "(9.62092568277892, tensor([6.4851], grad_fn=<UnbindBackward0>))\n",
      "(6.9874902470009905, tensor([5.5825], grad_fn=<UnbindBackward0>))\n",
      "(7.540090320145325, tensor([6.9030], grad_fn=<UnbindBackward0>))\n",
      "(8.32845106681936, tensor([8.0667], grad_fn=<UnbindBackward0>))\n",
      "(6.075346031088684, tensor([8.5958], grad_fn=<UnbindBackward0>))\n",
      "(8.14902386805177, tensor([9.9568], grad_fn=<UnbindBackward0>))\n",
      "(9.648982324778904, tensor([6.6028], grad_fn=<UnbindBackward0>))\n",
      "(7.739794458408701, tensor([9.5873], grad_fn=<UnbindBackward0>))\n",
      "(8.591929537530255, tensor([9.5461], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([8.9598], grad_fn=<UnbindBackward0>))\n",
      "(7.401231264413015, tensor([6.1562], grad_fn=<UnbindBackward0>))\n",
      "(7.870547844507712, tensor([7.7662], grad_fn=<UnbindBackward0>))\n",
      "(6.175867270105761, tensor([8.4782], grad_fn=<UnbindBackward0>))\n",
      "(8.47907586930311, tensor([7.4267], grad_fn=<UnbindBackward0>))\n",
      "(7.988542982737695, tensor([8.6218], grad_fn=<UnbindBackward0>))\n",
      "(6.568077911411976, tensor([6.1980], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([10.3840], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([6.8047], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([9.6925], grad_fn=<UnbindBackward0>))\n",
      "(8.811503250158239, tensor([8.4468], grad_fn=<UnbindBackward0>))\n",
      "(8.019941687677365, tensor([9.0880], grad_fn=<UnbindBackward0>))\n",
      "(8.375860015299594, tensor([7.4372], grad_fn=<UnbindBackward0>))\n",
      "(8.336150816120663, tensor([9.8792], grad_fn=<UnbindBackward0>))\n",
      "(7.937731775260109, tensor([8.6135], grad_fn=<UnbindBackward0>))\n",
      "(6.07993319509559, tensor([10.2200], grad_fn=<UnbindBackward0>))\n",
      "(8.087025470667701, tensor([9.5803], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([9.2244], grad_fn=<UnbindBackward0>))\n",
      "(9.744022777885101, tensor([7.9585], grad_fn=<UnbindBackward0>))\n",
      "(7.042286171939743, tensor([8.9522], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([7.3599], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([6.1817], grad_fn=<UnbindBackward0>))\n",
      "(6.2878585601617845, tensor([8.4019], grad_fn=<UnbindBackward0>))\n",
      "(9.320807825002154, tensor([7.7640], grad_fn=<UnbindBackward0>))\n",
      "(8.152774052744075, tensor([8.2664], grad_fn=<UnbindBackward0>))\n",
      "(8.630878955820053, tensor([9.4570], grad_fn=<UnbindBackward0>))\n",
      "(8.263848131368906, tensor([5.9655], grad_fn=<UnbindBackward0>))\n",
      "(6.731018100482083, tensor([7.8110], grad_fn=<UnbindBackward0>))\n",
      "(8.053887083618223, tensor([8.5729], grad_fn=<UnbindBackward0>))\n",
      "(8.389359819906353, tensor([8.9615], grad_fn=<UnbindBackward0>))\n",
      "(7.216709486709457, tensor([8.5397], grad_fn=<UnbindBackward0>))\n",
      "(8.465899897028686, tensor([7.8705], grad_fn=<UnbindBackward0>))\n",
      "(9.288966692431373, tensor([9.3314], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.5282], grad_fn=<UnbindBackward0>))\n",
      "(8.488382109562117, tensor([9.7425], grad_fn=<UnbindBackward0>))\n",
      "(6.3835066348840055, tensor([5.9616], grad_fn=<UnbindBackward0>))\n",
      "(8.09925056179696, tensor([8.5846], grad_fn=<UnbindBackward0>))\n",
      "(8.371704884667631, tensor([8.8712], grad_fn=<UnbindBackward0>))\n",
      "(9.521787797978225, tensor([6.2169], grad_fn=<UnbindBackward0>))\n",
      "(9.388235479817224, tensor([7.3825], grad_fn=<UnbindBackward0>))\n",
      "(6.910750787961936, tensor([7.3947], grad_fn=<UnbindBackward0>))\n",
      "(8.534640105019959, tensor([8.4654], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([6.6888], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([8.5489], grad_fn=<UnbindBackward0>))\n",
      "(6.8885724595653635, tensor([9.0390], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([8.5490], grad_fn=<UnbindBackward0>))\n",
      "(7.5406215286571525, tensor([7.8325], grad_fn=<UnbindBackward0>))\n",
      "(8.00736706798333, tensor([8.0965], grad_fn=<UnbindBackward0>))\n",
      "(8.227108234348146, tensor([8.8105], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([6.0423], grad_fn=<UnbindBackward0>))\n",
      "(8.181720455128108, tensor([6.1529], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([7.8246], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([9.4196], grad_fn=<UnbindBackward0>))\n",
      "(9.009080614161977, tensor([7.8094], grad_fn=<UnbindBackward0>))\n",
      "(7.7142311448490855, tensor([6.3316], grad_fn=<UnbindBackward0>))\n",
      "(8.926517509850122, tensor([6.1707], grad_fn=<UnbindBackward0>))\n",
      "(7.532623618788788, tensor([6.2686], grad_fn=<UnbindBackward0>))\n",
      "(6.403574197934815, tensor([10.1711], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([10.0386], grad_fn=<UnbindBackward0>))\n",
      "(8.37930948405285, tensor([8.5572], grad_fn=<UnbindBackward0>))\n",
      "(7.3901814282264295, tensor([10.0338], grad_fn=<UnbindBackward0>))\n",
      "(8.32239411311117, tensor([7.9624], grad_fn=<UnbindBackward0>))\n",
      "(9.457200449907708, tensor([5.8956], grad_fn=<UnbindBackward0>))\n",
      "(7.97522083865341, tensor([5.9252], grad_fn=<UnbindBackward0>))\n",
      "(5.8916442118257715, tensor([6.7706], grad_fn=<UnbindBackward0>))\n",
      "(8.109826276018477, tensor([7.1422], grad_fn=<UnbindBackward0>))\n",
      "(7.60339933974067, tensor([8.8386], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([8.6563], grad_fn=<UnbindBackward0>))\n",
      "(8.536407410340042, tensor([9.9895], grad_fn=<UnbindBackward0>))\n",
      "(8.863757191604241, tensor([9.3699], grad_fn=<UnbindBackward0>))\n",
      "(7.652545692693921, tensor([7.1608], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([6.2393], grad_fn=<UnbindBackward0>))\n",
      "(7.755338812846501, tensor([5.8873], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([6.7520], grad_fn=<UnbindBackward0>))\n",
      "(8.209580483475577, tensor([8.8306], grad_fn=<UnbindBackward0>))\n",
      "(7.149916836132109, tensor([8.9629], grad_fn=<UnbindBackward0>))\n",
      "(7.672292455628756, tensor([7.0916], grad_fn=<UnbindBackward0>))\n",
      "(7.362010551259734, tensor([7.4494], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([6.7378], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([7.3006], grad_fn=<UnbindBackward0>))\n",
      "(7.183111701743281, tensor([9.0203], grad_fn=<UnbindBackward0>))\n",
      "(8.504513138258863, tensor([8.4998], grad_fn=<UnbindBackward0>))\n",
      "(7.228388451573604, tensor([9.1757], grad_fn=<UnbindBackward0>))\n",
      "(8.09437844497296, tensor([6.3265], grad_fn=<UnbindBackward0>))\n",
      "(6.543911845564792, tensor([9.4231], grad_fn=<UnbindBackward0>))\n",
      "(8.627123250788433, tensor([8.8528], grad_fn=<UnbindBackward0>))\n",
      "(8.470520783217808, tensor([6.6620], grad_fn=<UnbindBackward0>))\n",
      "(7.254177846456518, tensor([9.9808], grad_fn=<UnbindBackward0>))\n",
      "(7.60339933974067, tensor([6.3570], grad_fn=<UnbindBackward0>))\n",
      "(6.824373670043086, tensor([8.5293], grad_fn=<UnbindBackward0>))\n",
      "(6.77078942390898, tensor([6.1936], grad_fn=<UnbindBackward0>))\n",
      "(7.844632644464681, tensor([7.7160], grad_fn=<UnbindBackward0>))\n",
      "(7.672292455628756, tensor([8.6779], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.3698], grad_fn=<UnbindBackward0>))\n",
      "(8.161089512845797, tensor([8.3339], grad_fn=<UnbindBackward0>))\n",
      "(7.468513271496337, tensor([6.1718], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([6.3958], grad_fn=<UnbindBackward0>))\n",
      "(6.80903930604298, tensor([6.0542], grad_fn=<UnbindBackward0>))\n",
      "(9.456262510179732, tensor([6.2280], grad_fn=<UnbindBackward0>))\n",
      "(6.812345094177479, tensor([6.7179], grad_fn=<UnbindBackward0>))\n",
      "(9.046290859969677, tensor([8.5997], grad_fn=<UnbindBackward0>))\n",
      "(8.338544879988579, tensor([6.7444], grad_fn=<UnbindBackward0>))\n",
      "(8.510168576479273, tensor([8.5309], grad_fn=<UnbindBackward0>))\n",
      "(9.705524299752216, tensor([7.1484], grad_fn=<UnbindBackward0>))\n",
      "(8.799058378546453, tensor([6.3200], grad_fn=<UnbindBackward0>))\n",
      "(8.780018887869153, tensor([9.4710], grad_fn=<UnbindBackward0>))\n",
      "(6.159095388491933, tensor([7.4322], grad_fn=<UnbindBackward0>))\n",
      "(9.171807422259398, tensor([7.7417], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([8.4393], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([6.5478], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([7.0797], grad_fn=<UnbindBackward0>))\n",
      "(7.500529485395295, tensor([8.0034], grad_fn=<UnbindBackward0>))\n",
      "(7.79770203551669, tensor([9.9499], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([9.3141], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([7.0419], grad_fn=<UnbindBackward0>))\n",
      "(8.458504195067558, tensor([8.5386], grad_fn=<UnbindBackward0>))\n",
      "(7.832014180505469, tensor([9.9621], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([6.1374], grad_fn=<UnbindBackward0>))\n",
      "(6.077642243349034, tensor([7.2056], grad_fn=<UnbindBackward0>))\n",
      "(8.589327789175437, tensor([6.9166], grad_fn=<UnbindBackward0>))\n",
      "(7.99260665240021, tensor([7.7730], grad_fn=<UnbindBackward0>))\n",
      "(7.617759576608505, tensor([6.3878], grad_fn=<UnbindBackward0>))\n",
      "(8.064950891749143, tensor([9.8406], grad_fn=<UnbindBackward0>))\n",
      "(7.346655163176539, tensor([9.2412], grad_fn=<UnbindBackward0>))\n",
      "(7.833203948641057, tensor([8.9455], grad_fn=<UnbindBackward0>))\n",
      "(7.034387929915503, tensor([9.2852], grad_fn=<UnbindBackward0>))\n",
      "(6.532334292222349, tensor([7.8222], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([8.5115], grad_fn=<UnbindBackward0>))\n",
      "(7.193685818395112, tensor([8.5644], grad_fn=<UnbindBackward0>))\n",
      "(7.25063551189868, tensor([6.2997], grad_fn=<UnbindBackward0>))\n",
      "(8.767017621311778, tensor([9.3560], grad_fn=<UnbindBackward0>))\n",
      "(6.807934943699926, tensor([8.6856], grad_fn=<UnbindBackward0>))\n",
      "(6.0867747269123065, tensor([7.4958], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([7.3836], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([6.2406], grad_fn=<UnbindBackward0>))\n",
      "(8.520188700396035, tensor([9.8035], grad_fn=<UnbindBackward0>))\n",
      "(9.226705726062447, tensor([7.1900], grad_fn=<UnbindBackward0>))\n",
      "(7.658227526161352, tensor([8.8186], grad_fn=<UnbindBackward0>))\n",
      "(9.476313501265748, tensor([7.7848], grad_fn=<UnbindBackward0>))\n",
      "(7.807916628926408, tensor([8.1457], grad_fn=<UnbindBackward0>))\n",
      "(7.664346632098617, tensor([8.7646], grad_fn=<UnbindBackward0>))\n",
      "(6.907755278982137, tensor([7.1545], grad_fn=<UnbindBackward0>))\n",
      "(8.653994232908383, tensor([7.7127], grad_fn=<UnbindBackward0>))\n",
      "(6.466144724237619, tensor([6.9192], grad_fn=<UnbindBackward0>))\n",
      "(8.35936910622267, tensor([6.6571], grad_fn=<UnbindBackward0>))\n",
      "(6.169610732491456, tensor([8.6906], grad_fn=<UnbindBackward0>))\n",
      "(8.648045999835, tensor([8.8462], grad_fn=<UnbindBackward0>))\n",
      "(7.484368643286131, tensor([7.3774], grad_fn=<UnbindBackward0>))\n",
      "(9.319194776888265, tensor([6.6992], grad_fn=<UnbindBackward0>))\n",
      "(8.91798070997329, tensor([8.5312], grad_fn=<UnbindBackward0>))\n",
      "(8.578476419833136, tensor([6.7497], grad_fn=<UnbindBackward0>))\n",
      "(8.034306936339489, tensor([6.7528], grad_fn=<UnbindBackward0>))\n",
      "(7.60489448081162, tensor([8.8327], grad_fn=<UnbindBackward0>))\n",
      "(9.567525185909115, tensor([7.8172], grad_fn=<UnbindBackward0>))\n",
      "(9.097283649683886, tensor([6.0001], grad_fn=<UnbindBackward0>))\n",
      "(7.438383530044307, tensor([6.3065], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([7.5264], grad_fn=<UnbindBackward0>))\n",
      "(7.002155954403621, tensor([6.4118], grad_fn=<UnbindBackward0>))\n",
      "(6.416732282512326, tensor([7.8250], grad_fn=<UnbindBackward0>))\n",
      "(6.779921907472252, tensor([7.9458], grad_fn=<UnbindBackward0>))\n",
      "(8.267448958304849, tensor([6.7029], grad_fn=<UnbindBackward0>))\n",
      "(8.189244525735901, tensor([8.7148], grad_fn=<UnbindBackward0>))\n",
      "(8.332789468417959, tensor([6.0299], grad_fn=<UnbindBackward0>))\n",
      "(8.573951525234847, tensor([6.4772], grad_fn=<UnbindBackward0>))\n",
      "(9.229848838364227, tensor([6.1702], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([9.5233], grad_fn=<UnbindBackward0>))\n",
      "(8.84404789894249, tensor([6.9784], grad_fn=<UnbindBackward0>))\n",
      "(8.984066927653044, tensor([8.4816], grad_fn=<UnbindBackward0>))\n",
      "(6.88653164253051, tensor([7.4127], grad_fn=<UnbindBackward0>))\n",
      "(7.088408778675395, tensor([8.5675], grad_fn=<UnbindBackward0>))\n",
      "(7.8399193600125825, tensor([6.6674], grad_fn=<UnbindBackward0>))\n",
      "(9.323490469908839, tensor([8.7270], grad_fn=<UnbindBackward0>))\n",
      "(6.182084906716632, tensor([7.2177], grad_fn=<UnbindBackward0>))\n",
      "(5.8805329864007, tensor([8.5563], grad_fn=<UnbindBackward0>))\n",
      "(8.36730010184162, tensor([7.9724], grad_fn=<UnbindBackward0>))\n",
      "(8.875287128108384, tensor([6.6248], grad_fn=<UnbindBackward0>))\n",
      "(6.5722825426940075, tensor([7.9988], grad_fn=<UnbindBackward0>))\n",
      "(8.453187861440325, tensor([6.3134], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([8.6075], grad_fn=<UnbindBackward0>))\n",
      "(6.762729506931879, tensor([8.0211], grad_fn=<UnbindBackward0>))\n",
      "(9.524713059681009, tensor([9.7922], grad_fn=<UnbindBackward0>))\n",
      "(8.09040229659332, tensor([7.7105], grad_fn=<UnbindBackward0>))\n",
      "(7.150701457592526, tensor([10.5888], grad_fn=<UnbindBackward0>))\n",
      "(8.766705997750515, tensor([6.3435], grad_fn=<UnbindBackward0>))\n",
      "(7.639642287858013, tensor([7.3574], grad_fn=<UnbindBackward0>))\n",
      "(7.956476798036782, tensor([8.9671], grad_fn=<UnbindBackward0>))\n",
      "(9.164191715950203, tensor([9.3282], grad_fn=<UnbindBackward0>))\n",
      "(7.903226808730733, tensor([10.2857], grad_fn=<UnbindBackward0>))\n",
      "(8.748463629942055, tensor([8.5622], grad_fn=<UnbindBackward0>))\n",
      "(9.24483841238375, tensor([6.1874], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([6.6557], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([8.5931], grad_fn=<UnbindBackward0>))\n",
      "(7.143617602704121, tensor([8.5117], grad_fn=<UnbindBackward0>))\n",
      "(6.07073772800249, tensor([6.5173], grad_fn=<UnbindBackward0>))\n",
      "(9.480138475329403, tensor([8.7118], grad_fn=<UnbindBackward0>))\n",
      "(7.539027055823995, tensor([8.6540], grad_fn=<UnbindBackward0>))\n",
      "(6.610696044717759, tensor([9.4197], grad_fn=<UnbindBackward0>))\n",
      "(7.508238774678663, tensor([9.2446], grad_fn=<UnbindBackward0>))\n",
      "(8.622813673279921, tensor([8.6137], grad_fn=<UnbindBackward0>))\n",
      "(7.418180822726788, tensor([9.6398], grad_fn=<UnbindBackward0>))\n",
      "(7.877017895622398, tensor([7.0439], grad_fn=<UnbindBackward0>))\n",
      "(8.482601746646619, tensor([9.2534], grad_fn=<UnbindBackward0>))\n",
      "(8.150756470275551, tensor([8.7948], grad_fn=<UnbindBackward0>))\n",
      "(7.8551570058813445, tensor([8.7761], grad_fn=<UnbindBackward0>))\n",
      "(6.953684210870537, tensor([9.1231], grad_fn=<UnbindBackward0>))\n",
      "(9.56836440548421, tensor([6.3098], grad_fn=<UnbindBackward0>))\n",
      "(9.388235479817224, tensor([6.2661], grad_fn=<UnbindBackward0>))\n",
      "(8.66785206770135, tensor([6.1522], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([7.8744], grad_fn=<UnbindBackward0>))\n",
      "(7.280008252884188, tensor([7.2283], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([8.4667], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([6.2297], grad_fn=<UnbindBackward0>))\n",
      "(7.848543482456679, tensor([7.2819], grad_fn=<UnbindBackward0>))\n",
      "(8.94023623179847, tensor([6.6978], grad_fn=<UnbindBackward0>))\n",
      "(7.512071245835466, tensor([7.1698], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([6.7554], grad_fn=<UnbindBackward0>))\n",
      "(6.760414691083428, tensor([6.2335], grad_fn=<UnbindBackward0>))\n",
      "(9.519661619531444, tensor([8.8180], grad_fn=<UnbindBackward0>))\n",
      "(8.514389264083503, tensor([10.2371], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([8.9993], grad_fn=<UnbindBackward0>))\n",
      "(6.70196036600254, tensor([9.3390], grad_fn=<UnbindBackward0>))\n",
      "(7.179307969504034, tensor([7.2038], grad_fn=<UnbindBackward0>))\n",
      "(7.039660349862076, tensor([8.6713], grad_fn=<UnbindBackward0>))\n",
      "(7.001245622069476, tensor([6.5160], grad_fn=<UnbindBackward0>))\n",
      "(8.303504798872783, tensor([8.3820], grad_fn=<UnbindBackward0>))\n",
      "(8.756839814829462, tensor([8.1946], grad_fn=<UnbindBackward0>))\n",
      "(6.882437470997847, tensor([6.2475], grad_fn=<UnbindBackward0>))\n",
      "(8.537387898701757, tensor([6.3653], grad_fn=<UnbindBackward0>))\n",
      "(8.190354403763262, tensor([6.2764], grad_fn=<UnbindBackward0>))\n",
      "(7.091742115095153, tensor([9.5826], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([6.8117], grad_fn=<UnbindBackward0>))\n",
      "(7.029972911706386, tensor([7.0947], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([9.4248], grad_fn=<UnbindBackward0>))\n",
      "(8.434897948689407, tensor([6.3125], grad_fn=<UnbindBackward0>))\n",
      "(7.4815557019095165, tensor([6.6425], grad_fn=<UnbindBackward0>))\n",
      "(7.971776122880628, tensor([7.7997], grad_fn=<UnbindBackward0>))\n",
      "(6.297109319933935, tensor([7.9130], grad_fn=<UnbindBackward0>))\n",
      "(6.651571873589727, tensor([9.5065], grad_fn=<UnbindBackward0>))\n",
      "(8.72029728739272, tensor([10.1965], grad_fn=<UnbindBackward0>))\n",
      "(7.117205503164344, tensor([6.3788], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([5.8834], grad_fn=<UnbindBackward0>))\n",
      "(7.424761761823209, tensor([6.7502], grad_fn=<UnbindBackward0>))\n",
      "(8.42639282708974, tensor([8.9011], grad_fn=<UnbindBackward0>))\n",
      "(7.018401799069201, tensor([7.4990], grad_fn=<UnbindBackward0>))\n",
      "(9.3492323708428, tensor([7.3838], grad_fn=<UnbindBackward0>))\n",
      "(8.9595686535445, tensor([6.3772], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([6.8101], grad_fn=<UnbindBackward0>))\n",
      "(8.847503625923641, tensor([6.7839], grad_fn=<UnbindBackward0>))\n",
      "(8.259716961021523, tensor([7.2934], grad_fn=<UnbindBackward0>))\n",
      "(8.206037762778815, tensor([8.4212], grad_fn=<UnbindBackward0>))\n",
      "(5.908082938168931, tensor([6.7938], grad_fn=<UnbindBackward0>))\n",
      "(7.1276936993473985, tensor([8.1339], grad_fn=<UnbindBackward0>))\n",
      "(7.763446388727362, tensor([6.4746], grad_fn=<UnbindBackward0>))\n",
      "(8.992806059426483, tensor([7.3672], grad_fn=<UnbindBackward0>))\n",
      "(8.09009578318096, tensor([6.1827], grad_fn=<UnbindBackward0>))\n",
      "(7.569927655242652, tensor([6.2880], grad_fn=<UnbindBackward0>))\n",
      "(9.773948181181144, tensor([6.3587], grad_fn=<UnbindBackward0>))\n",
      "(7.532623618788788, tensor([8.3129], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([9.7508], grad_fn=<UnbindBackward0>))\n",
      "(9.215029361462314, tensor([8.1930], grad_fn=<UnbindBackward0>))\n",
      "(7.275172319452771, tensor([7.0912], grad_fn=<UnbindBackward0>))\n",
      "(6.7912214627261855, tensor([8.6778], grad_fn=<UnbindBackward0>))\n",
      "(7.741099090035366, tensor([8.7507], grad_fn=<UnbindBackward0>))\n",
      "(7.979338895262328, tensor([7.9483], grad_fn=<UnbindBackward0>))\n",
      "(8.452974619089586, tensor([10.0396], grad_fn=<UnbindBackward0>))\n",
      "(8.178919332848396, tensor([8.3771], grad_fn=<UnbindBackward0>))\n",
      "(9.242613932526478, tensor([7.1460], grad_fn=<UnbindBackward0>))\n",
      "(8.350666240520924, tensor([7.8759], grad_fn=<UnbindBackward0>))\n",
      "(8.710619527942297, tensor([9.6222], grad_fn=<UnbindBackward0>))\n",
      "(6.625392368007956, tensor([8.6187], grad_fn=<UnbindBackward0>))\n",
      "(7.43955930913332, tensor([7.8182], grad_fn=<UnbindBackward0>))\n",
      "(6.6240652277998935, tensor([5.8410], grad_fn=<UnbindBackward0>))\n",
      "(7.395107546562485, tensor([7.9652], grad_fn=<UnbindBackward0>))\n",
      "(8.27410200229233, tensor([6.2102], grad_fn=<UnbindBackward0>))\n",
      "(6.340359303727752, tensor([8.4728], grad_fn=<UnbindBackward0>))\n",
      "(8.268731832117737, tensor([10.1050], grad_fn=<UnbindBackward0>))\n",
      "(8.884194633072273, tensor([9.2574], grad_fn=<UnbindBackward0>))\n",
      "(8.042056410058754, tensor([6.4784], grad_fn=<UnbindBackward0>))\n",
      "(9.344696424475261, tensor([7.8102], grad_fn=<UnbindBackward0>))\n",
      "(6.814542897259958, tensor([7.7017], grad_fn=<UnbindBackward0>))\n",
      "(8.53089883847235, tensor([8.5047], grad_fn=<UnbindBackward0>))\n",
      "(9.692272572087193, tensor([7.7700], grad_fn=<UnbindBackward0>))\n",
      "(6.777646593635117, tensor([8.2081], grad_fn=<UnbindBackward0>))\n",
      "(7.5595594960077, tensor([8.0837], grad_fn=<UnbindBackward0>))\n",
      "(8.748304912379623, tensor([7.4553], grad_fn=<UnbindBackward0>))\n",
      "(8.469052816088302, tensor([9.3512], grad_fn=<UnbindBackward0>))\n",
      "(7.074116816197362, tensor([8.2663], grad_fn=<UnbindBackward0>))\n",
      "(6.984716320118266, tensor([6.2723], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([6.7101], grad_fn=<UnbindBackward0>))\n",
      "(9.623773649733524, tensor([7.0123], grad_fn=<UnbindBackward0>))\n",
      "(7.923348211930154, tensor([7.2260], grad_fn=<UnbindBackward0>))\n",
      "(8.429672593886743, tensor([8.0888], grad_fn=<UnbindBackward0>))\n",
      "(7.223295679562314, tensor([10.0089], grad_fn=<UnbindBackward0>))\n",
      "(6.767343125265392, tensor([9.2086], grad_fn=<UnbindBackward0>))\n",
      "(8.742733867330168, tensor([6.4985], grad_fn=<UnbindBackward0>))\n",
      "(7.198931240688173, tensor([6.3452], grad_fn=<UnbindBackward0>))\n",
      "(6.651571873589727, tensor([8.1278], grad_fn=<UnbindBackward0>))\n",
      "(9.808462332127018, tensor([6.0338], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([9.5077], grad_fn=<UnbindBackward0>))\n",
      "(7.349230824613334, tensor([7.8946], grad_fn=<UnbindBackward0>))\n",
      "(8.57130251706327, tensor([7.0963], grad_fn=<UnbindBackward0>))\n",
      "(7.508238774678663, tensor([6.7538], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([7.0901], grad_fn=<UnbindBackward0>))\n",
      "(6.7580945044277305, tensor([9.4487], grad_fn=<UnbindBackward0>))\n",
      "(8.266164436612492, tensor([6.3087], grad_fn=<UnbindBackward0>))\n",
      "(8.227909837597483, tensor([6.7453], grad_fn=<UnbindBackward0>))\n",
      "(9.329367078397823, tensor([7.1274], grad_fn=<UnbindBackward0>))\n",
      "(7.503289630675082, tensor([7.2055], grad_fn=<UnbindBackward0>))\n",
      "(9.210340371976184, tensor([7.9829], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.5996], grad_fn=<UnbindBackward0>))\n",
      "(8.546169299652755, tensor([6.2563], grad_fn=<UnbindBackward0>))\n",
      "(8.657998068007258, tensor([9.3257], grad_fn=<UnbindBackward0>))\n",
      "(8.034306936339489, tensor([6.3504], grad_fn=<UnbindBackward0>))\n",
      "(8.0507033814703, tensor([6.4057], grad_fn=<UnbindBackward0>))\n",
      "(8.192570471152173, tensor([8.9123], grad_fn=<UnbindBackward0>))\n",
      "(7.3783837129967145, tensor([6.2712], grad_fn=<UnbindBackward0>))\n",
      "(7.431299675155903, tensor([8.1651], grad_fn=<UnbindBackward0>))\n",
      "(8.725832056527565, tensor([9.9687], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([7.3252], grad_fn=<UnbindBackward0>))\n",
      "(8.740336742730447, tensor([8.4089], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([6.3391], grad_fn=<UnbindBackward0>))\n",
      "(7.69484807238461, tensor([9.3585], grad_fn=<UnbindBackward0>))\n",
      "(9.81569405158546, tensor([6.7729], grad_fn=<UnbindBackward0>))\n",
      "(9.440499258184298, tensor([6.6768], grad_fn=<UnbindBackward0>))\n",
      "(8.361007108226909, tensor([7.6525], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([6.7054], grad_fn=<UnbindBackward0>))\n",
      "(7.943782692458625, tensor([6.3031], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([6.2087], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([10.3627], grad_fn=<UnbindBackward0>))\n",
      "(6.786716950605081, tensor([7.8012], grad_fn=<UnbindBackward0>))\n",
      "(9.018574356354229, tensor([8.2144], grad_fn=<UnbindBackward0>))\n",
      "(8.654168646443315, tensor([8.6788], grad_fn=<UnbindBackward0>))\n",
      "(8.636397438894713, tensor([9.4236], grad_fn=<UnbindBackward0>))\n",
      "(8.638702608813434, tensor([8.4072], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([9.3345], grad_fn=<UnbindBackward0>))\n",
      "(9.363576347875041, tensor([7.8356], grad_fn=<UnbindBackward0>))\n",
      "(9.178230318057949, tensor([10.3204], grad_fn=<UnbindBackward0>))\n",
      "(7.6093665379542115, tensor([7.2171], grad_fn=<UnbindBackward0>))\n",
      "(9.607168286688683, tensor([7.9513], grad_fn=<UnbindBackward0>))\n",
      "(7.213031659834869, tensor([8.8891], grad_fn=<UnbindBackward0>))\n",
      "(7.190676034332207, tensor([9.6429], grad_fn=<UnbindBackward0>))\n",
      "(6.6240652277998935, tensor([6.0375], grad_fn=<UnbindBackward0>))\n",
      "(8.933400396056303, tensor([8.7900], grad_fn=<UnbindBackward0>))\n",
      "(8.23403420769204, tensor([8.9177], grad_fn=<UnbindBackward0>))\n",
      "(8.53070154144103, tensor([8.5958], grad_fn=<UnbindBackward0>))\n",
      "(6.937314081223682, tensor([7.8310], grad_fn=<UnbindBackward0>))\n",
      "(9.610859937691332, tensor([8.6455], grad_fn=<UnbindBackward0>))\n",
      "(9.80659111529043, tensor([8.8401], grad_fn=<UnbindBackward0>))\n",
      "(7.753623546559746, tensor([6.8274], grad_fn=<UnbindBackward0>))\n",
      "(7.087573705557973, tensor([8.2519], grad_fn=<UnbindBackward0>))\n",
      "(9.504575923397805, tensor([8.8460], grad_fn=<UnbindBackward0>))\n",
      "(7.580699752224563, tensor([7.7219], grad_fn=<UnbindBackward0>))\n",
      "(7.255591274253665, tensor([6.3391], grad_fn=<UnbindBackward0>))\n",
      "(6.835184586147301, tensor([8.6990], grad_fn=<UnbindBackward0>))\n",
      "(7.900266036767701, tensor([8.5346], grad_fn=<UnbindBackward0>))\n",
      "(7.039660349862076, tensor([8.4865], grad_fn=<UnbindBackward0>))\n",
      "(9.319912008405474, tensor([6.3542], grad_fn=<UnbindBackward0>))\n",
      "(9.125871215349733, tensor([7.8424], grad_fn=<UnbindBackward0>))\n",
      "(7.732807530422021, tensor([9.1777], grad_fn=<UnbindBackward0>))\n",
      "(6.6240652277998935, tensor([6.3880], grad_fn=<UnbindBackward0>))\n",
      "(7.9919305198524775, tensor([6.4034], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.0047], grad_fn=<UnbindBackward0>))\n",
      "(7.38770923908104, tensor([8.3365], grad_fn=<UnbindBackward0>))\n",
      "(8.880585523102495, tensor([7.0604], grad_fn=<UnbindBackward0>))\n",
      "(6.960347729101308, tensor([6.8737], grad_fn=<UnbindBackward0>))\n",
      "(7.570443252057374, tensor([8.5254], grad_fn=<UnbindBackward0>))\n",
      "(8.353025845202325, tensor([8.0939], grad_fn=<UnbindBackward0>))\n",
      "(8.327726166461412, tensor([7.5517], grad_fn=<UnbindBackward0>))\n",
      "(8.514589805546123, tensor([6.2757], grad_fn=<UnbindBackward0>))\n",
      "(6.663132695990803, tensor([8.1261], grad_fn=<UnbindBackward0>))\n",
      "(6.920671504248683, tensor([6.7836], grad_fn=<UnbindBackward0>))\n",
      "(7.895063498091573, tensor([6.2540], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([7.0744], grad_fn=<UnbindBackward0>))\n",
      "(8.542080906924017, tensor([6.3553], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([7.9448], grad_fn=<UnbindBackward0>))\n",
      "(7.527793987721444, tensor([10.0810], grad_fn=<UnbindBackward0>))\n",
      "(7.783640596221253, tensor([8.4192], grad_fn=<UnbindBackward0>))\n",
      "(6.769641976852503, tensor([6.2553], grad_fn=<UnbindBackward0>))\n",
      "(7.0967213784947605, tensor([6.3706], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([7.8796], grad_fn=<UnbindBackward0>))\n",
      "(6.851184927493743, tensor([6.6862], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([7.3512], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([6.3486], grad_fn=<UnbindBackward0>))\n",
      "(8.669227347271736, tensor([8.4781], grad_fn=<UnbindBackward0>))\n",
      "(6.169610732491456, tensor([6.9574], grad_fn=<UnbindBackward0>))\n",
      "(8.764053269347762, tensor([6.2896], grad_fn=<UnbindBackward0>))\n",
      "(7.97522083865341, tensor([6.3673], grad_fn=<UnbindBackward0>))\n",
      "(7.2196420401307355, tensor([9.4119], grad_fn=<UnbindBackward0>))\n",
      "(7.349230824613334, tensor([8.2842], grad_fn=<UnbindBackward0>))\n",
      "(7.557994958530806, tensor([6.7941], grad_fn=<UnbindBackward0>))\n",
      "(6.536691597591305, tensor([9.0952], grad_fn=<UnbindBackward0>))\n",
      "(7.894318063841624, tensor([6.7588], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([7.2496], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([7.6768], grad_fn=<UnbindBackward0>))\n",
      "(9.046644279305394, tensor([7.7003], grad_fn=<UnbindBackward0>))\n",
      "(6.8966943316227125, tensor([6.5020], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.7036], grad_fn=<UnbindBackward0>))\n",
      "(6.19644412779452, tensor([9.1449], grad_fn=<UnbindBackward0>))\n",
      "(6.652863029353347, tensor([9.5389], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([9.5642], grad_fn=<UnbindBackward0>))\n",
      "(6.175867270105761, tensor([9.1207], grad_fn=<UnbindBackward0>))\n",
      "(8.089175678837561, tensor([6.3681], grad_fn=<UnbindBackward0>))\n",
      "(8.012680929706839, tensor([10.1512], grad_fn=<UnbindBackward0>))\n",
      "(7.875119281040293, tensor([9.3573], grad_fn=<UnbindBackward0>))\n",
      "(6.74993119378857, tensor([10.1707], grad_fn=<UnbindBackward0>))\n",
      "(8.112827478751374, tensor([8.5133], grad_fn=<UnbindBackward0>))\n",
      "(8.64541048921699, tensor([8.5360], grad_fn=<UnbindBackward0>))\n",
      "(6.210600077024653, tensor([9.0564], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([8.4167], grad_fn=<UnbindBackward0>))\n",
      "(7.601901959875166, tensor([7.8336], grad_fn=<UnbindBackward0>))\n",
      "(9.409273194575334, tensor([6.2246], grad_fn=<UnbindBackward0>))\n",
      "(8.680671660408713, tensor([7.7382], grad_fn=<UnbindBackward0>))\n",
      "(7.246368080102461, tensor([6.3178], grad_fn=<UnbindBackward0>))\n",
      "(8.235890725928495, tensor([8.7533], grad_fn=<UnbindBackward0>))\n",
      "(8.36287583103188, tensor([6.2997], grad_fn=<UnbindBackward0>))\n",
      "(7.398174092970465, tensor([8.5220], grad_fn=<UnbindBackward0>))\n",
      "(8.225503097566918, tensor([7.1550], grad_fn=<UnbindBackward0>))\n",
      "(6.8966943316227125, tensor([7.1038], grad_fn=<UnbindBackward0>))\n",
      "(7.4067107301776405, tensor([6.8376], grad_fn=<UnbindBackward0>))\n",
      "(8.625329850020815, tensor([6.1551], grad_fn=<UnbindBackward0>))\n",
      "(6.748759547491679, tensor([9.4959], grad_fn=<UnbindBackward0>))\n",
      "(8.90815361332152, tensor([7.1804], grad_fn=<UnbindBackward0>))\n",
      "(8.785080636539838, tensor([8.3276], grad_fn=<UnbindBackward0>))\n",
      "(8.08794755464267, tensor([6.8467], grad_fn=<UnbindBackward0>))\n",
      "(9.326789184619429, tensor([6.3014], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([7.8200], grad_fn=<UnbindBackward0>))\n",
      "(8.228443883004033, tensor([10.9710], grad_fn=<UnbindBackward0>))\n",
      "(6.960347729101308, tensor([6.4119], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([8.4814], grad_fn=<UnbindBackward0>))\n",
      "(7.618251097876695, tensor([6.7000], grad_fn=<UnbindBackward0>))\n",
      "(7.874358824729881, tensor([6.3478], grad_fn=<UnbindBackward0>))\n",
      "(8.133880887949207, tensor([6.3184], grad_fn=<UnbindBackward0>))\n",
      "(9.387900611668426, tensor([5.8121], grad_fn=<UnbindBackward0>))\n",
      "(9.490242257109001, tensor([9.8731], grad_fn=<UnbindBackward0>))\n",
      "(9.395906658935026, tensor([7.9288], grad_fn=<UnbindBackward0>))\n",
      "(9.196748418456716, tensor([6.6827], grad_fn=<UnbindBackward0>))\n",
      "(8.612139668725192, tensor([8.0436], grad_fn=<UnbindBackward0>))\n",
      "(7.912056888179006, tensor([8.4378], grad_fn=<UnbindBackward0>))\n",
      "(9.261033486291701, tensor([9.5009], grad_fn=<UnbindBackward0>))\n",
      "(7.9355873855892, tensor([8.6603], grad_fn=<UnbindBackward0>))\n",
      "(8.905308661189288, tensor([6.6554], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([6.1583], grad_fn=<UnbindBackward0>))\n",
      "(7.765569081097317, tensor([6.7025], grad_fn=<UnbindBackward0>))\n",
      "(8.56483984488359, tensor([8.6004], grad_fn=<UnbindBackward0>))\n",
      "(8.44741429680832, tensor([9.6241], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([9.6487], grad_fn=<UnbindBackward0>))\n",
      "(8.983816112502495, tensor([9.6536], grad_fn=<UnbindBackward0>))\n",
      "(6.774223886357614, tensor([9.3402], grad_fn=<UnbindBackward0>))\n",
      "(8.041091003708633, tensor([9.1900], grad_fn=<UnbindBackward0>))\n",
      "(7.417580402414544, tensor([8.4746], grad_fn=<UnbindBackward0>))\n",
      "(8.45638105201948, tensor([8.7438], grad_fn=<UnbindBackward0>))\n",
      "(8.185350223178686, tensor([6.2937], grad_fn=<UnbindBackward0>))\n",
      "(7.503840746698951, tensor([8.6342], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([6.3535], grad_fn=<UnbindBackward0>))\n",
      "(8.104703468371108, tensor([6.1977], grad_fn=<UnbindBackward0>))\n",
      "(9.103756886573398, tensor([6.2892], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([7.6770], grad_fn=<UnbindBackward0>))\n",
      "(8.501267040865978, tensor([8.4538], grad_fn=<UnbindBackward0>))\n",
      "(7.845416036592485, tensor([6.4064], grad_fn=<UnbindBackward0>))\n",
      "(9.059052257762401, tensor([6.2985], grad_fn=<UnbindBackward0>))\n",
      "(6.546785410760524, tensor([6.7876], grad_fn=<UnbindBackward0>))\n",
      "(6.97914527506881, tensor([8.2924], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([6.5272], grad_fn=<UnbindBackward0>))\n",
      "(8.458716261657262, tensor([8.4403], grad_fn=<UnbindBackward0>))\n",
      "(8.844624683385302, tensor([6.2716], grad_fn=<UnbindBackward0>))\n",
      "(9.788413344690662, tensor([8.5076], grad_fn=<UnbindBackward0>))\n",
      "(6.834108738813838, tensor([6.2359], grad_fn=<UnbindBackward0>))\n",
      "(7.503840746698951, tensor([8.3533], grad_fn=<UnbindBackward0>))\n",
      "(8.1786387885907, tensor([7.1981], grad_fn=<UnbindBackward0>))\n",
      "(6.3561076606958915, tensor([8.6798], grad_fn=<UnbindBackward0>))\n",
      "(7.653494909661253, tensor([8.5219], grad_fn=<UnbindBackward0>))\n",
      "(7.4413203897176174, tensor([7.8923], grad_fn=<UnbindBackward0>))\n",
      "(7.65491704784832, tensor([7.8531], grad_fn=<UnbindBackward0>))\n",
      "(8.087025470667701, tensor([6.7544], grad_fn=<UnbindBackward0>))\n",
      "(7.646831391430482, tensor([7.4940], grad_fn=<UnbindBackward0>))\n",
      "(8.133293861222633, tensor([7.8168], grad_fn=<UnbindBackward0>))\n",
      "(8.818038250394299, tensor([6.2664], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([6.2484], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.2593], grad_fn=<UnbindBackward0>))\n",
      "(8.339500903005945, tensor([7.6799], grad_fn=<UnbindBackward0>))\n",
      "(8.557951183888406, tensor([6.8531], grad_fn=<UnbindBackward0>))\n",
      "(7.8399193600125825, tensor([7.4994], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([8.5124], grad_fn=<UnbindBackward0>))\n",
      "(7.1785454837637, tensor([8.8284], grad_fn=<UnbindBackward0>))\n",
      "(7.231287004327616, tensor([7.8865], grad_fn=<UnbindBackward0>))\n",
      "(8.85609105525229, tensor([6.5739], grad_fn=<UnbindBackward0>))\n",
      "(8.518592212329946, tensor([7.8938], grad_fn=<UnbindBackward0>))\n",
      "(6.837332814685591, tensor([8.3744], grad_fn=<UnbindBackward0>))\n",
      "(8.073091199693154, tensor([8.7411], grad_fn=<UnbindBackward0>))\n",
      "(6.628041376179533, tensor([7.7587], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([8.7681], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([7.4650], grad_fn=<UnbindBackward0>))\n",
      "(8.17695386822578, tensor([8.8394], grad_fn=<UnbindBackward0>))\n",
      "(8.411388132519262, tensor([9.9375], grad_fn=<UnbindBackward0>))\n",
      "(8.966611387052865, tensor([8.2454], grad_fn=<UnbindBackward0>))\n",
      "(7.566828479208331, tensor([6.8238], grad_fn=<UnbindBackward0>))\n",
      "(6.4692503167957724, tensor([6.3686], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([6.5070], grad_fn=<UnbindBackward0>))\n",
      "(6.587550014824796, tensor([8.2851], grad_fn=<UnbindBackward0>))\n",
      "(7.060476365999801, tensor([8.0402], grad_fn=<UnbindBackward0>))\n",
      "(7.891704659330107, tensor([8.6446], grad_fn=<UnbindBackward0>))\n",
      "(7.575071699507561, tensor([6.8577], grad_fn=<UnbindBackward0>))\n",
      "(9.21890360263667, tensor([8.5297], grad_fn=<UnbindBackward0>))\n",
      "(9.27049429479593, tensor([6.5398], grad_fn=<UnbindBackward0>))\n",
      "(8.37930948405285, tensor([5.9084], grad_fn=<UnbindBackward0>))\n",
      "(8.355379895253634, tensor([8.6480], grad_fn=<UnbindBackward0>))\n",
      "(8.487558386286548, tensor([6.1916], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([8.5008], grad_fn=<UnbindBackward0>))\n",
      "(8.462314529906248, tensor([6.3343], grad_fn=<UnbindBackward0>))\n",
      "(8.889721799278137, tensor([7.8711], grad_fn=<UnbindBackward0>))\n",
      "(6.459904454377535, tensor([8.7982], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([9.5164], grad_fn=<UnbindBackward0>))\n",
      "(8.522976436171964, tensor([7.7938], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.3935], grad_fn=<UnbindBackward0>))\n",
      "(8.263848131368906, tensor([6.3363], grad_fn=<UnbindBackward0>))\n",
      "(8.657302899400882, tensor([9.2071], grad_fn=<UnbindBackward0>))\n",
      "(9.368881400104733, tensor([8.6252], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([7.1486], grad_fn=<UnbindBackward0>))\n",
      "(8.317033476492403, tensor([6.2681], grad_fn=<UnbindBackward0>))\n",
      "(6.690842277418564, tensor([7.2494], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([9.2987], grad_fn=<UnbindBackward0>))\n",
      "(6.413458957167357, tensor([9.8843], grad_fn=<UnbindBackward0>))\n",
      "(8.476162841858246, tensor([7.7911], grad_fn=<UnbindBackward0>))\n",
      "(7.2399325913204695, tensor([8.9667], grad_fn=<UnbindBackward0>))\n",
      "(6.9363427358340495, tensor([7.1045], grad_fn=<UnbindBackward0>))\n",
      "(8.334951631422454, tensor([6.2753], grad_fn=<UnbindBackward0>))\n",
      "(7.106606137727303, tensor([6.7381], grad_fn=<UnbindBackward0>))\n",
      "(6.542471960506805, tensor([6.6366], grad_fn=<UnbindBackward0>))\n",
      "(8.776475789346321, tensor([8.2251], grad_fn=<UnbindBackward0>))\n",
      "(8.064950891749143, tensor([7.8170], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([7.7025], grad_fn=<UnbindBackward0>))\n",
      "(8.448700194970938, tensor([6.8798], grad_fn=<UnbindBackward0>))\n",
      "(8.027802848370312, tensor([8.7567], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([8.6619], grad_fn=<UnbindBackward0>))\n",
      "(8.327484416188264, tensor([8.9961], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([9.4801], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([8.4568], grad_fn=<UnbindBackward0>))\n",
      "(6.78105762593618, tensor([6.7128], grad_fn=<UnbindBackward0>))\n",
      "(7.827639546366422, tensor([7.0470], grad_fn=<UnbindBackward0>))\n",
      "(8.562548893137034, tensor([7.0941], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([8.8677], grad_fn=<UnbindBackward0>))\n",
      "(9.178023770499639, tensor([6.3136], grad_fn=<UnbindBackward0>))\n",
      "(7.56164174558878, tensor([6.9647], grad_fn=<UnbindBackward0>))\n",
      "(9.117896081584902, tensor([7.4216], grad_fn=<UnbindBackward0>))\n",
      "(9.097171673870545, tensor([6.3914], grad_fn=<UnbindBackward0>))\n",
      "(8.343077871169383, tensor([6.7644], grad_fn=<UnbindBackward0>))\n",
      "(6.8679744089702925, tensor([8.6261], grad_fn=<UnbindBackward0>))\n",
      "(8.149601735736155, tensor([7.8385], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([7.0953], grad_fn=<UnbindBackward0>))\n",
      "(8.950014029534257, tensor([8.7568], grad_fn=<UnbindBackward0>))\n",
      "(8.66681936537205, tensor([8.7483], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([8.4371], grad_fn=<UnbindBackward0>))\n",
      "(6.893656354602635, tensor([6.4026], grad_fn=<UnbindBackward0>))\n",
      "(9.117896081584902, tensor([6.2119], grad_fn=<UnbindBackward0>))\n",
      "(7.158513997329321, tensor([6.6708], grad_fn=<UnbindBackward0>))\n",
      "(7.66105638236183, tensor([6.2905], grad_fn=<UnbindBackward0>))\n",
      "(8.499436469826978, tensor([7.9105], grad_fn=<UnbindBackward0>))\n",
      "(7.914983005848394, tensor([8.4568], grad_fn=<UnbindBackward0>))\n",
      "(7.573017256052546, tensor([6.7295], grad_fn=<UnbindBackward0>))\n",
      "(8.696175846944678, tensor([7.5674], grad_fn=<UnbindBackward0>))\n",
      "(8.786303878282583, tensor([6.2716], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([8.8239], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([8.7870], grad_fn=<UnbindBackward0>))\n",
      "(8.017637159908478, tensor([6.4732], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([8.2996], grad_fn=<UnbindBackward0>))\n",
      "(6.826545223556594, tensor([6.5424], grad_fn=<UnbindBackward0>))\n",
      "(7.818430272070656, tensor([7.9113], grad_fn=<UnbindBackward0>))\n",
      "(6.670766320845874, tensor([7.2455], grad_fn=<UnbindBackward0>))\n",
      "(8.414938957377482, tensor([9.0745], grad_fn=<UnbindBackward0>))\n",
      "(7.555905093611346, tensor([7.4760], grad_fn=<UnbindBackward0>))\n",
      "(7.709756864454165, tensor([7.7644], grad_fn=<UnbindBackward0>))\n",
      "(9.097283649683886, tensor([6.7291], grad_fn=<UnbindBackward0>))\n",
      "(7.988203597022576, tensor([6.6943], grad_fn=<UnbindBackward0>))\n",
      "(8.748463629942055, tensor([7.7304], grad_fn=<UnbindBackward0>))\n",
      "(6.440946540632921, tensor([6.3433], grad_fn=<UnbindBackward0>))\n",
      "(8.428143374582726, tensor([6.1452], grad_fn=<UnbindBackward0>))\n",
      "(7.488293515159428, tensor([6.7285], grad_fn=<UnbindBackward0>))\n",
      "(8.29154650988391, tensor([9.3926], grad_fn=<UnbindBackward0>))\n",
      "(8.574140471857987, tensor([8.5034], grad_fn=<UnbindBackward0>))\n",
      "(8.45638105201948, tensor([6.6536], grad_fn=<UnbindBackward0>))\n",
      "(8.468002947225466, tensor([6.6920], grad_fn=<UnbindBackward0>))\n",
      "(7.557994958530806, tensor([6.2096], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([7.2710], grad_fn=<UnbindBackward0>))\n",
      "(9.351232110673664, tensor([5.8994], grad_fn=<UnbindBackward0>))\n",
      "(8.33302993974291, tensor([7.6937], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([8.2431], grad_fn=<UnbindBackward0>))\n",
      "(8.758569510991506, tensor([6.2585], grad_fn=<UnbindBackward0>))\n",
      "(8.341886969516187, tensor([9.5051], grad_fn=<UnbindBackward0>))\n",
      "(7.59337419312129, tensor([8.3022], grad_fn=<UnbindBackward0>))\n",
      "(6.756932389247553, tensor([6.6284], grad_fn=<UnbindBackward0>))\n",
      "(7.768956044538332, tensor([6.6610], grad_fn=<UnbindBackward0>))\n",
      "(7.228388451573604, tensor([8.8116], grad_fn=<UnbindBackward0>))\n",
      "(8.225503097566918, tensor([9.6784], grad_fn=<UnbindBackward0>))\n",
      "(8.794673401383422, tensor([8.7709], grad_fn=<UnbindBackward0>))\n",
      "(6.013715156042802, tensor([9.1821], grad_fn=<UnbindBackward0>))\n",
      "(7.560080465021827, tensor([9.3274], grad_fn=<UnbindBackward0>))\n",
      "(9.221478116386638, tensor([8.4708], grad_fn=<UnbindBackward0>))\n",
      "(6.71174039505618, tensor([7.1314], grad_fn=<UnbindBackward0>))\n",
      "(6.78105762593618, tensor([8.6133], grad_fn=<UnbindBackward0>))\n",
      "(6.424869023905388, tensor([10.1188], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([7.1174], grad_fn=<UnbindBackward0>))\n",
      "(8.574140471857987, tensor([8.1006], grad_fn=<UnbindBackward0>))\n",
      "(7.0707241072602764, tensor([6.7780], grad_fn=<UnbindBackward0>))\n",
      "(6.822197390620491, tensor([8.9976], grad_fn=<UnbindBackward0>))\n",
      "(6.831953565565855, tensor([5.8285], grad_fn=<UnbindBackward0>))\n",
      "(7.729735331385051, tensor([9.4919], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([8.9698], grad_fn=<UnbindBackward0>))\n",
      "(6.895682697747868, tensor([8.3732], grad_fn=<UnbindBackward0>))\n",
      "(7.807916628926408, tensor([7.5661], grad_fn=<UnbindBackward0>))\n",
      "(8.482187582217422, tensor([6.2815], grad_fn=<UnbindBackward0>))\n",
      "(7.809135398120538, tensor([8.8543], grad_fn=<UnbindBackward0>))\n",
      "(8.495560891289124, tensor([6.5136], grad_fn=<UnbindBackward0>))\n",
      "(9.263596738276615, tensor([8.2638], grad_fn=<UnbindBackward0>))\n",
      "(7.860956364876389, tensor([7.1152], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([8.0576], grad_fn=<UnbindBackward0>))\n",
      "(8.655562860681009, tensor([10.2893], grad_fn=<UnbindBackward0>))\n",
      "(7.945201132412759, tensor([8.1115], grad_fn=<UnbindBackward0>))\n",
      "(8.846209127360996, tensor([9.2246], grad_fn=<UnbindBackward0>))\n",
      "(9.616804980417431, tensor([6.2501], grad_fn=<UnbindBackward0>))\n",
      "(7.769378609513984, tensor([6.0065], grad_fn=<UnbindBackward0>))\n",
      "(9.60541838719786, tensor([8.8067], grad_fn=<UnbindBackward0>))\n",
      "(6.839476438228843, tensor([9.2532], grad_fn=<UnbindBackward0>))\n",
      "(8.044305406990638, tensor([8.2742], grad_fn=<UnbindBackward0>))\n",
      "(7.114769448366463, tensor([7.1703], grad_fn=<UnbindBackward0>))\n",
      "(7.1316985104669115, tensor([7.8647], grad_fn=<UnbindBackward0>))\n",
      "(6.642486801367256, tensor([7.2260], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([7.8509], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([8.7327], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([8.7881], grad_fn=<UnbindBackward0>))\n",
      "(8.046869510959576, tensor([6.3613], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([8.4750], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([6.7689], grad_fn=<UnbindBackward0>))\n",
      "(9.24483841238375, tensor([7.8878], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([7.2545], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([8.2090], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([7.3098], grad_fn=<UnbindBackward0>))\n",
      "(8.867709208039386, tensor([7.4843], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([7.9519], grad_fn=<UnbindBackward0>))\n",
      "(9.148464968258095, tensor([6.8292], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([6.0012], grad_fn=<UnbindBackward0>))\n",
      "(8.369388996647842, tensor([6.3141], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([8.1003], grad_fn=<UnbindBackward0>))\n",
      "(9.292012520620208, tensor([8.5596], grad_fn=<UnbindBackward0>))\n",
      "(8.430981494597171, tensor([8.2749], grad_fn=<UnbindBackward0>))\n",
      "(7.742835955430749, tensor([8.8860], grad_fn=<UnbindBackward0>))\n",
      "(7.352441100243583, tensor([6.7222], grad_fn=<UnbindBackward0>))\n",
      "(6.86171134048073, tensor([8.7566], grad_fn=<UnbindBackward0>))\n",
      "(6.918695219020472, tensor([8.5839], grad_fn=<UnbindBackward0>))\n",
      "(8.132412674500905, tensor([6.2937], grad_fn=<UnbindBackward0>))\n",
      "(7.2682230211595655, tensor([9.6140], grad_fn=<UnbindBackward0>))\n",
      "(7.629975707027789, tensor([7.2866], grad_fn=<UnbindBackward0>))\n",
      "(8.992681750892542, tensor([8.7383], grad_fn=<UnbindBackward0>))\n",
      "(8.721928343047091, tensor([7.2851], grad_fn=<UnbindBackward0>))\n",
      "(8.923857580099885, tensor([7.8106], grad_fn=<UnbindBackward0>))\n",
      "(9.081938657171658, tensor([7.3510], grad_fn=<UnbindBackward0>))\n",
      "(6.670766320845874, tensor([6.1270], grad_fn=<UnbindBackward0>))\n",
      "(7.568379267836522, tensor([6.6105], grad_fn=<UnbindBackward0>))\n",
      "(9.281730368062856, tensor([7.8813], grad_fn=<UnbindBackward0>))\n",
      "(7.271008538280992, tensor([6.3843], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([6.5178], grad_fn=<UnbindBackward0>))\n",
      "(7.418780882750794, tensor([8.0633], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([8.5524], grad_fn=<UnbindBackward0>))\n",
      "(8.885025658050846, tensor([8.4679], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([7.7812], grad_fn=<UnbindBackward0>))\n",
      "(6.68586094706836, tensor([6.5655], grad_fn=<UnbindBackward0>))\n",
      "(8.391403185357937, tensor([7.3766], grad_fn=<UnbindBackward0>))\n",
      "(6.920671504248683, tensor([7.0815], grad_fn=<UnbindBackward0>))\n",
      "(6.259581464064923, tensor([6.8565], grad_fn=<UnbindBackward0>))\n",
      "(9.076123029856374, tensor([8.4038], grad_fn=<UnbindBackward0>))\n",
      "(9.646851848489673, tensor([6.3208], grad_fn=<UnbindBackward0>))\n",
      "(6.9363427358340495, tensor([6.6238], grad_fn=<UnbindBackward0>))\n",
      "(8.479906606630221, tensor([8.2406], grad_fn=<UnbindBackward0>))\n",
      "(6.07073772800249, tensor([7.9445], grad_fn=<UnbindBackward0>))\n",
      "(8.592486175451668, tensor([6.2246], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([9.2763], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([7.8048], grad_fn=<UnbindBackward0>))\n",
      "(7.775695749915245, tensor([7.8097], grad_fn=<UnbindBackward0>))\n",
      "(9.686636660844938, tensor([8.8635], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.6947], grad_fn=<UnbindBackward0>))\n",
      "(6.90875477931522, tensor([6.8401], grad_fn=<UnbindBackward0>))\n",
      "(7.669495251007694, tensor([6.2989], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([6.3141], grad_fn=<UnbindBackward0>))\n",
      "(9.523105224795426, tensor([7.6372], grad_fn=<UnbindBackward0>))\n",
      "(9.836813419268136, tensor([8.1728], grad_fn=<UnbindBackward0>))\n",
      "(8.977020214210413, tensor([9.7750], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([8.7128], grad_fn=<UnbindBackward0>))\n",
      "(7.664815785285735, tensor([8.2817], grad_fn=<UnbindBackward0>))\n",
      "(7.4109518755836366, tensor([6.7472], grad_fn=<UnbindBackward0>))\n",
      "(8.095293776844649, tensor([6.7839], grad_fn=<UnbindBackward0>))\n",
      "(8.328692583545568, tensor([9.5521], grad_fn=<UnbindBackward0>))\n",
      "(8.409162447202533, tensor([8.5070], grad_fn=<UnbindBackward0>))\n",
      "(7.5406215286571525, tensor([9.7242], grad_fn=<UnbindBackward0>))\n",
      "(7.4377951216719325, tensor([8.4753], grad_fn=<UnbindBackward0>))\n",
      "(6.18826412308259, tensor([6.3462], grad_fn=<UnbindBackward0>))\n",
      "(7.939515260662406, tensor([8.0189], grad_fn=<UnbindBackward0>))\n",
      "(8.653296274408579, tensor([6.5299], grad_fn=<UnbindBackward0>))\n",
      "(9.54873932271967, tensor([8.6652], grad_fn=<UnbindBackward0>))\n",
      "(9.219399216464529, tensor([9.2775], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([6.3376], grad_fn=<UnbindBackward0>))\n",
      "(8.293549515060345, tensor([6.7092], grad_fn=<UnbindBackward0>))\n",
      "(7.940583827104244, tensor([6.8720], grad_fn=<UnbindBackward0>))\n",
      "(7.114769448366463, tensor([7.1667], grad_fn=<UnbindBackward0>))\n",
      "(7.057897937411856, tensor([7.3990], grad_fn=<UnbindBackward0>))\n",
      "(7.417580402414544, tensor([7.4832], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([8.4838], grad_fn=<UnbindBackward0>))\n",
      "(9.450380370950123, tensor([6.8816], grad_fn=<UnbindBackward0>))\n",
      "(6.037870919922137, tensor([9.7319], grad_fn=<UnbindBackward0>))\n",
      "(8.500047032581268, tensor([8.7143], grad_fn=<UnbindBackward0>))\n",
      "(7.357556200910353, tensor([7.7953], grad_fn=<UnbindBackward0>))\n",
      "(9.364519622823858, tensor([9.1185], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([6.2804], grad_fn=<UnbindBackward0>))\n",
      "(6.6052979209482015, tensor([7.9827], grad_fn=<UnbindBackward0>))\n",
      "(7.416378479192928, tensor([6.0710], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([8.3338], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([6.2082], grad_fn=<UnbindBackward0>))\n",
      "(8.665957964681349, tensor([9.4407], grad_fn=<UnbindBackward0>))\n",
      "(7.890582534656536, tensor([7.1036], grad_fn=<UnbindBackward0>))\n",
      "(5.8944028342648505, tensor([8.2689], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([8.5307], grad_fn=<UnbindBackward0>))\n",
      "(7.579167967396076, tensor([8.8889], grad_fn=<UnbindBackward0>))\n",
      "(8.916506080039204, tensor([6.2341], grad_fn=<UnbindBackward0>))\n",
      "(7.48324441607385, tensor([6.2847], grad_fn=<UnbindBackward0>))\n",
      "(7.383989457978509, tensor([8.9933], grad_fn=<UnbindBackward0>))\n",
      "(8.33302993974291, tensor([6.7486], grad_fn=<UnbindBackward0>))\n",
      "(8.679482094459956, tensor([6.7860], grad_fn=<UnbindBackward0>))\n",
      "(8.027476530860483, tensor([7.2553], grad_fn=<UnbindBackward0>))\n",
      "(7.431299675155903, tensor([8.1147], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([7.8086], grad_fn=<UnbindBackward0>))\n",
      "(9.574080300037034, tensor([7.1218], grad_fn=<UnbindBackward0>))\n",
      "(8.57828829077605, tensor([7.9877], grad_fn=<UnbindBackward0>))\n",
      "(8.116417072794205, tensor([7.2464], grad_fn=<UnbindBackward0>))\n",
      "(6.169610732491456, tensor([8.6295], grad_fn=<UnbindBackward0>))\n",
      "(8.705662478796427, tensor([7.7999], grad_fn=<UnbindBackward0>))\n",
      "(8.62245370207373, tensor([8.4049], grad_fn=<UnbindBackward0>))\n",
      "(8.533263371593732, tensor([6.8723], grad_fn=<UnbindBackward0>))\n",
      "(8.073402968986406, tensor([7.0778], grad_fn=<UnbindBackward0>))\n",
      "(6.416732282512326, tensor([6.4093], grad_fn=<UnbindBackward0>))\n",
      "(7.040536390215956, tensor([6.3445], grad_fn=<UnbindBackward0>))\n",
      "(6.964135612418245, tensor([6.5245], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([6.1624], grad_fn=<UnbindBackward0>))\n",
      "(6.831953565565855, tensor([5.9961], grad_fn=<UnbindBackward0>))\n",
      "(6.489204931325317, tensor([6.6903], grad_fn=<UnbindBackward0>))\n",
      "(7.266128779556451, tensor([6.5325], grad_fn=<UnbindBackward0>))\n",
      "(6.763884908562435, tensor([6.5118], grad_fn=<UnbindBackward0>))\n",
      "(6.028278520230698, tensor([7.8694], grad_fn=<UnbindBackward0>))\n",
      "(9.569412440514636, tensor([6.7260], grad_fn=<UnbindBackward0>))\n",
      "(8.03073492409854, tensor([6.7890], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([9.2841], grad_fn=<UnbindBackward0>))\n",
      "(8.323851131338817, tensor([7.4916], grad_fn=<UnbindBackward0>))\n",
      "(7.885329239273191, tensor([8.1754], grad_fn=<UnbindBackward0>))\n",
      "(7.215239978730097, tensor([7.3926], grad_fn=<UnbindBackward0>))\n",
      "(6.9584483932976555, tensor([9.3744], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([7.1277], grad_fn=<UnbindBackward0>))\n",
      "(9.735128070758227, tensor([5.8237], grad_fn=<UnbindBackward0>))\n",
      "(8.426173793029069, tensor([10.2360], grad_fn=<UnbindBackward0>))\n",
      "(8.620651899784468, tensor([6.7395], grad_fn=<UnbindBackward0>))\n",
      "(9.454540665816003, tensor([8.7870], grad_fn=<UnbindBackward0>))\n",
      "(8.630343289348893, tensor([7.2877], grad_fn=<UnbindBackward0>))\n",
      "(6.824373670043086, tensor([6.4727], grad_fn=<UnbindBackward0>))\n",
      "(6.925595197110468, tensor([8.6692], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([7.9483], grad_fn=<UnbindBackward0>))\n",
      "(5.8944028342648505, tensor([8.5538], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([8.8741], grad_fn=<UnbindBackward0>))\n",
      "(9.783013399068254, tensor([8.3296], grad_fn=<UnbindBackward0>))\n",
      "(7.252053951852814, tensor([7.9659], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([8.3320], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([6.2221], grad_fn=<UnbindBackward0>))\n",
      "(9.45053767785622, tensor([6.2341], grad_fn=<UnbindBackward0>))\n",
      "(7.798933310041217, tensor([7.1031], grad_fn=<UnbindBackward0>))\n",
      "(8.404472321352118, tensor([6.2455], grad_fn=<UnbindBackward0>))\n",
      "(8.70996000607173, tensor([9.5109], grad_fn=<UnbindBackward0>))\n",
      "(8.579980179515003, tensor([6.2851], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([8.8716], grad_fn=<UnbindBackward0>))\n",
      "(8.10832229017324, tensor([10.3484], grad_fn=<UnbindBackward0>))\n",
      "(8.779557455883728, tensor([6.2277], grad_fn=<UnbindBackward0>))\n",
      "(6.922643891475888, tensor([7.1554], grad_fn=<UnbindBackward0>))\n",
      "(8.91139510724569, tensor([8.1863], grad_fn=<UnbindBackward0>))\n",
      "(8.159946655578548, tensor([6.6817], grad_fn=<UnbindBackward0>))\n",
      "(9.541584681053092, tensor([6.8071], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([7.4593], grad_fn=<UnbindBackward0>))\n",
      "(8.994420665751292, tensor([9.4211], grad_fn=<UnbindBackward0>))\n",
      "(6.3473892096560105, tensor([6.2984], grad_fn=<UnbindBackward0>))\n",
      "(7.594381242551817, tensor([8.4516], grad_fn=<UnbindBackward0>))\n",
      "(7.930206206684683, tensor([8.7984], grad_fn=<UnbindBackward0>))\n",
      "(9.276970454273945, tensor([6.2937], grad_fn=<UnbindBackward0>))\n",
      "(7.5569505720129, tensor([8.5215], grad_fn=<UnbindBackward0>))\n",
      "(9.595534743241988, tensor([8.7108], grad_fn=<UnbindBackward0>))\n",
      "(8.01102337918644, tensor([8.4620], grad_fn=<UnbindBackward0>))\n",
      "(9.377886643187983, tensor([7.0718], grad_fn=<UnbindBackward0>))\n",
      "(8.599878558034845, tensor([6.3198], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([7.2543], grad_fn=<UnbindBackward0>))\n",
      "(8.23323750070527, tensor([6.7708], grad_fn=<UnbindBackward0>))\n",
      "(9.237858258012922, tensor([7.9599], grad_fn=<UnbindBackward0>))\n",
      "(8.644530439877432, tensor([6.2703], grad_fn=<UnbindBackward0>))\n",
      "(8.275885669474356, tensor([6.2387], grad_fn=<UnbindBackward0>))\n",
      "(9.158309963881798, tensor([6.7809], grad_fn=<UnbindBackward0>))\n",
      "(8.975503722070927, tensor([8.6749], grad_fn=<UnbindBackward0>))\n",
      "(8.561401446080557, tensor([7.9180], grad_fn=<UnbindBackward0>))\n",
      "(8.332308352219117, tensor([7.1097], grad_fn=<UnbindBackward0>))\n",
      "(7.218176838403408, tensor([8.7713], grad_fn=<UnbindBackward0>))\n",
      "(7.347299700743164, tensor([7.7226], grad_fn=<UnbindBackward0>))\n",
      "(7.271703706887368, tensor([8.1870], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([10.3409], grad_fn=<UnbindBackward0>))\n",
      "(8.499640032168648, tensor([6.5305], grad_fn=<UnbindBackward0>))\n",
      "(7.868636894184167, tensor([6.3759], grad_fn=<UnbindBackward0>))\n",
      "(6.882437470997847, tensor([8.7715], grad_fn=<UnbindBackward0>))\n",
      "(6.3561076606958915, tensor([7.8675], grad_fn=<UnbindBackward0>))\n",
      "(8.638702608813434, tensor([6.6930], grad_fn=<UnbindBackward0>))\n",
      "(8.77137018531062, tensor([8.8468], grad_fn=<UnbindBackward0>))\n",
      "(9.075322160298095, tensor([9.3941], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([6.3621], grad_fn=<UnbindBackward0>))\n",
      "(6.876264611890766, tensor([6.3080], grad_fn=<UnbindBackward0>))\n",
      "(7.7354333524996886, tensor([7.9697], grad_fn=<UnbindBackward0>))\n",
      "(9.333796175903101, tensor([8.8665], grad_fn=<UnbindBackward0>))\n",
      "(9.526609901279876, tensor([8.5592], grad_fn=<UnbindBackward0>))\n",
      "(6.7357800142423265, tensor([7.7517], grad_fn=<UnbindBackward0>))\n",
      "(8.768263145371286, tensor([6.3107], grad_fn=<UnbindBackward0>))\n",
      "(8.322151070212902, tensor([8.5482], grad_fn=<UnbindBackward0>))\n",
      "(7.00033446027523, tensor([8.4154], grad_fn=<UnbindBackward0>))\n",
      "(8.439663988907, tensor([8.5992], grad_fn=<UnbindBackward0>))\n",
      "(6.846943139585379, tensor([8.2690], grad_fn=<UnbindBackward0>))\n",
      "(9.495444123413163, tensor([8.3586], grad_fn=<UnbindBackward0>))\n",
      "(6.504288173536645, tensor([6.8354], grad_fn=<UnbindBackward0>))\n",
      "(7.025538314638521, tensor([8.4414], grad_fn=<UnbindBackward0>))\n",
      "(7.027314514039777, tensor([6.8289], grad_fn=<UnbindBackward0>))\n",
      "(7.579167967396076, tensor([8.9163], grad_fn=<UnbindBackward0>))\n",
      "(9.202005735186125, tensor([6.2290], grad_fn=<UnbindBackward0>))\n",
      "(7.979338895262328, tensor([7.2058], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([9.5906], grad_fn=<UnbindBackward0>))\n",
      "(6.728628613084702, tensor([6.7768], grad_fn=<UnbindBackward0>))\n",
      "(6.6895992691789665, tensor([7.1949], grad_fn=<UnbindBackward0>))\n",
      "(8.189799618728228, tensor([8.2443], grad_fn=<UnbindBackward0>))\n",
      "(7.5411524551363085, tensor([9.4066], grad_fn=<UnbindBackward0>))\n",
      "(8.831127635012084, tensor([6.5102], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.1735], grad_fn=<UnbindBackward0>))\n",
      "(6.590301048196686, tensor([7.7966], grad_fn=<UnbindBackward0>))\n",
      "(8.756839814829462, tensor([8.8404], grad_fn=<UnbindBackward0>))\n",
      "(6.617402977974478, tensor([7.6519], grad_fn=<UnbindBackward0>))\n",
      "(8.43750042250699, tensor([7.5192], grad_fn=<UnbindBackward0>))\n",
      "(7.00397413672268, tensor([6.2004], grad_fn=<UnbindBackward0>))\n",
      "(7.039660349862076, tensor([9.3610], grad_fn=<UnbindBackward0>))\n",
      "(7.097548850614793, tensor([6.2396], grad_fn=<UnbindBackward0>))\n",
      "(8.232971790593437, tensor([6.2389], grad_fn=<UnbindBackward0>))\n",
      "(6.473890696352274, tensor([7.5661], grad_fn=<UnbindBackward0>))\n",
      "(7.01301578963963, tensor([9.4030], grad_fn=<UnbindBackward0>))\n",
      "(8.48673398393153, tensor([6.3673], grad_fn=<UnbindBackward0>))\n",
      "(7.85979918056211, tensor([8.3040], grad_fn=<UnbindBackward0>))\n",
      "(9.423029465386534, tensor([7.7163], grad_fn=<UnbindBackward0>))\n",
      "(7.798933310041217, tensor([7.8059], grad_fn=<UnbindBackward0>))\n",
      "(7.7488913372555315, tensor([7.1301], grad_fn=<UnbindBackward0>))\n",
      "(7.8632667240095735, tensor([5.9001], grad_fn=<UnbindBackward0>))\n",
      "(8.297792626380861, tensor([9.5661], grad_fn=<UnbindBackward0>))\n",
      "(6.0844994130751715, tensor([6.2753], grad_fn=<UnbindBackward0>))\n",
      "(7.098375638590786, tensor([9.4752], grad_fn=<UnbindBackward0>))\n",
      "(7.067319848653476, tensor([8.2654], grad_fn=<UnbindBackward0>))\n",
      "(7.502738210754851, tensor([7.2517], grad_fn=<UnbindBackward0>))\n",
      "(9.288041879640035, tensor([10.2248], grad_fn=<UnbindBackward0>))\n",
      "(8.08886878916199, tensor([9.6072], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([7.3924], grad_fn=<UnbindBackward0>))\n",
      "(7.447751280047908, tensor([6.7775], grad_fn=<UnbindBackward0>))\n",
      "(7.629975707027789, tensor([6.9335], grad_fn=<UnbindBackward0>))\n",
      "(7.738923757439457, tensor([8.0096], grad_fn=<UnbindBackward0>))\n",
      "(9.18307194482199, tensor([8.7692], grad_fn=<UnbindBackward0>))\n",
      "(7.459338895220296, tensor([7.9233], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([6.7995], grad_fn=<UnbindBackward0>))\n",
      "(6.508769136971682, tensor([7.1336], grad_fn=<UnbindBackward0>))\n",
      "(8.946505025998682, tensor([8.5182], grad_fn=<UnbindBackward0>))\n",
      "(7.416378479192928, tensor([8.4005], grad_fn=<UnbindBackward0>))\n",
      "(8.40804774415544, tensor([7.3586], grad_fn=<UnbindBackward0>))\n",
      "(9.070388411548738, tensor([8.1957], grad_fn=<UnbindBackward0>))\n",
      "(8.685246776412487, tensor([7.1307], grad_fn=<UnbindBackward0>))\n",
      "(8.79497643168877, tensor([8.4568], grad_fn=<UnbindBackward0>))\n",
      "(6.646390514847729, tensor([6.3625], grad_fn=<UnbindBackward0>))\n",
      "(8.207946941048617, tensor([6.3701], grad_fn=<UnbindBackward0>))\n",
      "(7.614805364711073, tensor([8.6417], grad_fn=<UnbindBackward0>))\n",
      "(8.072779333169498, tensor([6.3068], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([8.5111], grad_fn=<UnbindBackward0>))\n",
      "(8.856518497019858, tensor([7.8064], grad_fn=<UnbindBackward0>))\n",
      "(7.506042178518122, tensor([7.9683], grad_fn=<UnbindBackward0>))\n",
      "(7.483806687665835, tensor([6.9013], grad_fn=<UnbindBackward0>))\n",
      "(7.401231264413015, tensor([7.3409], grad_fn=<UnbindBackward0>))\n",
      "(9.742144402127366, tensor([6.4023], grad_fn=<UnbindBackward0>))\n",
      "(9.52602664083756, tensor([8.4463], grad_fn=<UnbindBackward0>))\n",
      "(8.500047032581268, tensor([7.2623], grad_fn=<UnbindBackward0>))\n",
      "(8.607764889600624, tensor([6.3962], grad_fn=<UnbindBackward0>))\n",
      "(7.652070746116482, tensor([8.6422], grad_fn=<UnbindBackward0>))\n",
      "(6.670766320845874, tensor([10.5324], grad_fn=<UnbindBackward0>))\n",
      "(7.109062135687172, tensor([8.4116], grad_fn=<UnbindBackward0>))\n",
      "(7.0326242610280065, tensor([8.9289], grad_fn=<UnbindBackward0>))\n",
      "(8.619027497297505, tensor([6.7730], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([8.6760], grad_fn=<UnbindBackward0>))\n",
      "(7.488293515159428, tensor([8.1504], grad_fn=<UnbindBackward0>))\n",
      "(6.630683385642372, tensor([8.1187], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([7.9784], grad_fn=<UnbindBackward0>))\n",
      "(7.349230824613334, tensor([6.2712], grad_fn=<UnbindBackward0>))\n",
      "(6.876264611890766, tensor([9.7846], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([8.5351], grad_fn=<UnbindBackward0>))\n",
      "(7.201170883281678, tensor([8.1459], grad_fn=<UnbindBackward0>))\n",
      "(8.587651655064798, tensor([10.0102], grad_fn=<UnbindBackward0>))\n",
      "(9.76457025334946, tensor([8.4803], grad_fn=<UnbindBackward0>))\n",
      "(7.090076835776092, tensor([7.3763], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([9.3900], grad_fn=<UnbindBackward0>))\n",
      "(8.442254104751743, tensor([8.5466], grad_fn=<UnbindBackward0>))\n",
      "(6.872128101338986, tensor([6.7263], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([7.2856], grad_fn=<UnbindBackward0>))\n",
      "(7.1853870155804165, tensor([8.7054], grad_fn=<UnbindBackward0>))\n",
      "(7.451241684987676, tensor([7.7348], grad_fn=<UnbindBackward0>))\n",
      "(8.75573753930647, tensor([8.4128], grad_fn=<UnbindBackward0>))\n",
      "(7.725330037917135, tensor([8.0270], grad_fn=<UnbindBackward0>))\n",
      "(8.328692583545568, tensor([6.3271], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([7.2643], grad_fn=<UnbindBackward0>))\n",
      "(6.923628628138427, tensor([10.1283], grad_fn=<UnbindBackward0>))\n",
      "(6.428105272684596, tensor([9.3477], grad_fn=<UnbindBackward0>))\n",
      "(6.814542897259958, tensor([7.9739], grad_fn=<UnbindBackward0>))\n",
      "(7.506591780070841, tensor([6.5446], grad_fn=<UnbindBackward0>))\n",
      "(8.290292591224315, tensor([7.2139], grad_fn=<UnbindBackward0>))\n",
      "(7.458186157340487, tensor([6.3133], grad_fn=<UnbindBackward0>))\n",
      "(9.104535313079205, tensor([9.3850], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([8.6450], grad_fn=<UnbindBackward0>))\n",
      "(8.707648248106914, tensor([9.0621], grad_fn=<UnbindBackward0>))\n",
      "(7.6953031349635665, tensor([8.6761], grad_fn=<UnbindBackward0>))\n",
      "(8.593042503699674, tensor([9.3679], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([9.5256], grad_fn=<UnbindBackward0>))\n",
      "(7.1853870155804165, tensor([7.2637], grad_fn=<UnbindBackward0>))\n",
      "(8.142645185942795, tensor([6.2285], grad_fn=<UnbindBackward0>))\n",
      "(7.945555428253489, tensor([6.3460], grad_fn=<UnbindBackward0>))\n",
      "(8.9082888855571, tensor([8.9066], grad_fn=<UnbindBackward0>))\n",
      "(6.320768294250582, tensor([6.4121], grad_fn=<UnbindBackward0>))\n",
      "(8.841158975945264, tensor([9.4933], grad_fn=<UnbindBackward0>))\n",
      "(8.171599480345463, tensor([6.9756], grad_fn=<UnbindBackward0>))\n",
      "(9.06681636189014, tensor([6.3929], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([8.7196], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([9.6803], grad_fn=<UnbindBackward0>))\n",
      "(6.5638555265321274, tensor([8.6767], grad_fn=<UnbindBackward0>))\n",
      "(8.323365694436081, tensor([8.5053], grad_fn=<UnbindBackward0>))\n",
      "(6.810142450115136, tensor([6.7698], grad_fn=<UnbindBackward0>))\n",
      "(9.667638628724365, tensor([7.1610], grad_fn=<UnbindBackward0>))\n",
      "(8.446126742982377, tensor([10.2892], grad_fn=<UnbindBackward0>))\n",
      "(6.257667587882639, tensor([8.3045], grad_fn=<UnbindBackward0>))\n",
      "(6.595780513961311, tensor([7.0169], grad_fn=<UnbindBackward0>))\n",
      "(7.7702232041587855, tensor([8.5713], grad_fn=<UnbindBackward0>))\n",
      "(8.711113884053544, tensor([7.4197], grad_fn=<UnbindBackward0>))\n",
      "(6.9411900550683745, tensor([9.8857], grad_fn=<UnbindBackward0>))\n",
      "(7.5883236773352225, tensor([7.8426], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([7.4423], grad_fn=<UnbindBackward0>))\n",
      "(8.195057690895077, tensor([9.4335], grad_fn=<UnbindBackward0>))\n",
      "(7.413970290190444, tensor([8.6948], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([8.5422], grad_fn=<UnbindBackward0>))\n",
      "(7.917900586327916, tensor([7.1524], grad_fn=<UnbindBackward0>))\n",
      "(9.035272443893536, tensor([8.7626], grad_fn=<UnbindBackward0>))\n",
      "(6.452048954437226, tensor([8.5881], grad_fn=<UnbindBackward0>))\n",
      "(6.890609120147166, tensor([6.8414], grad_fn=<UnbindBackward0>))\n",
      "(9.240384493324559, tensor([6.7019], grad_fn=<UnbindBackward0>))\n",
      "(8.401108712395436, tensor([6.7823], grad_fn=<UnbindBackward0>))\n",
      "(6.6052979209482015, tensor([6.8041], grad_fn=<UnbindBackward0>))\n",
      "(7.414572881350589, tensor([6.3381], grad_fn=<UnbindBackward0>))\n",
      "(7.658227526161352, tensor([6.7557], grad_fn=<UnbindBackward0>))\n",
      "(6.602587892189336, tensor([8.2113], grad_fn=<UnbindBackward0>))\n",
      "(7.527255919373784, tensor([6.4995], grad_fn=<UnbindBackward0>))\n",
      "(7.776954403322442, tensor([7.6051], grad_fn=<UnbindBackward0>))\n",
      "(7.894690850425624, tensor([7.8573], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([9.9094], grad_fn=<UnbindBackward0>))\n",
      "(6.588926477533519, tensor([9.9089], grad_fn=<UnbindBackward0>))\n",
      "(7.776535028185241, tensor([8.1005], grad_fn=<UnbindBackward0>))\n",
      "(8.37309184744198, tensor([7.3732], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([7.1440], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([6.3395], grad_fn=<UnbindBackward0>))\n",
      "(6.364750756851911, tensor([6.4205], grad_fn=<UnbindBackward0>))\n",
      "(8.466320861042481, tensor([6.0247], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([7.1083], grad_fn=<UnbindBackward0>))\n",
      "(8.228710798793687, tensor([5.8276], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([6.4787], grad_fn=<UnbindBackward0>))\n",
      "(8.667163717992533, tensor([6.8379], grad_fn=<UnbindBackward0>))\n",
      "(8.568646473005153, tensor([7.4244], grad_fn=<UnbindBackward0>))\n",
      "(6.990256500493881, tensor([8.0571], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([9.3864], grad_fn=<UnbindBackward0>))\n",
      "(6.628041376179533, tensor([5.8116], grad_fn=<UnbindBackward0>))\n",
      "(6.437751649736401, tensor([6.3364], grad_fn=<UnbindBackward0>))\n",
      "(9.73849497786395, tensor([9.9147], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([7.7456], grad_fn=<UnbindBackward0>))\n",
      "(6.726233402358747, tensor([6.8715], grad_fn=<UnbindBackward0>))\n",
      "(8.827321452697849, tensor([6.2186], grad_fn=<UnbindBackward0>))\n",
      "(7.00850518208228, tensor([6.6768], grad_fn=<UnbindBackward0>))\n",
      "(9.026417533815254, tensor([7.3693], grad_fn=<UnbindBackward0>))\n",
      "(8.127995055771946, tensor([6.9115], grad_fn=<UnbindBackward0>))\n",
      "(6.51471269087253, tensor([7.0854], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([6.6826], grad_fn=<UnbindBackward0>))\n",
      "(7.260522598089852, tensor([6.5435], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([7.2214], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([6.0828], grad_fn=<UnbindBackward0>))\n",
      "(6.366470447731438, tensor([8.4654], grad_fn=<UnbindBackward0>))\n",
      "(9.241645221804594, tensor([6.4195], grad_fn=<UnbindBackward0>))\n",
      "(8.618846845142738, tensor([7.7512], grad_fn=<UnbindBackward0>))\n",
      "(7.188412736496954, tensor([7.3697], grad_fn=<UnbindBackward0>))\n",
      "(8.305978210967302, tensor([8.5941], grad_fn=<UnbindBackward0>))\n",
      "(8.527539483470381, tensor([6.3794], grad_fn=<UnbindBackward0>))\n",
      "(9.612399686949514, tensor([6.8149], grad_fn=<UnbindBackward0>))\n",
      "(7.083387847625295, tensor([6.4063], grad_fn=<UnbindBackward0>))\n",
      "(8.886409166849282, tensor([8.7698], grad_fn=<UnbindBackward0>))\n",
      "(8.12740456269308, tensor([8.0705], grad_fn=<UnbindBackward0>))\n",
      "(9.695355758842355, tensor([6.2908], grad_fn=<UnbindBackward0>))\n",
      "(8.034306936339489, tensor([9.5893], grad_fn=<UnbindBackward0>))\n",
      "(9.64943366106433, tensor([9.3383], grad_fn=<UnbindBackward0>))\n",
      "(8.564458383883352, tensor([6.6686], grad_fn=<UnbindBackward0>))\n",
      "(6.8690144506657065, tensor([6.2492], grad_fn=<UnbindBackward0>))\n",
      "(6.810142450115136, tensor([8.6523], grad_fn=<UnbindBackward0>))\n",
      "(6.891625897052253, tensor([7.8430], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([7.1185], grad_fn=<UnbindBackward0>))\n",
      "(6.880384082186005, tensor([9.5932], grad_fn=<UnbindBackward0>))\n",
      "(6.111467339502679, tensor([8.5713], grad_fn=<UnbindBackward0>))\n",
      "(8.518991573357617, tensor([6.5264], grad_fn=<UnbindBackward0>))\n",
      "(7.820840879907344, tensor([8.8520], grad_fn=<UnbindBackward0>))\n",
      "(7.198931240688173, tensor([8.2034], grad_fn=<UnbindBackward0>))\n",
      "(7.6004023345004, tensor([9.0351], grad_fn=<UnbindBackward0>))\n",
      "(6.452048954437226, tensor([7.7860], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([8.5272], grad_fn=<UnbindBackward0>))\n",
      "(8.652423140676342, tensor([7.2359], grad_fn=<UnbindBackward0>))\n",
      "(7.07326971745971, tensor([6.4398], grad_fn=<UnbindBackward0>))\n",
      "(9.11701821474481, tensor([8.8815], grad_fn=<UnbindBackward0>))\n",
      "(8.618485442898109, tensor([8.6168], grad_fn=<UnbindBackward0>))\n",
      "(6.366470447731438, tensor([7.1979], grad_fn=<UnbindBackward0>))\n",
      "(6.7226297948554485, tensor([6.7708], grad_fn=<UnbindBackward0>))\n",
      "(7.6638772587034705, tensor([6.9550], grad_fn=<UnbindBackward0>))\n",
      "(8.104099056143582, tensor([8.6654], grad_fn=<UnbindBackward0>))\n",
      "(7.60887062919126, tensor([8.1648], grad_fn=<UnbindBackward0>))\n",
      "(6.966024187106113, tensor([8.7016], grad_fn=<UnbindBackward0>))\n",
      "(6.903747257584598, tensor([8.9367], grad_fn=<UnbindBackward0>))\n",
      "(9.725735539456544, tensor([7.3136], grad_fn=<UnbindBackward0>))\n",
      "(7.346010209913293, tensor([7.8119], grad_fn=<UnbindBackward0>))\n",
      "(6.854354502255021, tensor([6.9893], grad_fn=<UnbindBackward0>))\n",
      "(6.697034247666484, tensor([7.0908], grad_fn=<UnbindBackward0>))\n",
      "(8.70682132339263, tensor([8.9258], grad_fn=<UnbindBackward0>))\n",
      "(8.29254851397576, tensor([7.3737], grad_fn=<UnbindBackward0>))\n",
      "(8.717518372649767, tensor([8.5519], grad_fn=<UnbindBackward0>))\n",
      "(7.540090320145325, tensor([10.1374], grad_fn=<UnbindBackward0>))\n",
      "(6.45833828334479, tensor([6.4759], grad_fn=<UnbindBackward0>))\n",
      "(8.515191188745565, tensor([8.8389], grad_fn=<UnbindBackward0>))\n",
      "(7.014814351275545, tensor([6.2911], grad_fn=<UnbindBackward0>))\n",
      "(9.771326648933789, tensor([6.7267], grad_fn=<UnbindBackward0>))\n",
      "(8.917310693197807, tensor([8.4344], grad_fn=<UnbindBackward0>))\n",
      "(8.522976436171964, tensor([8.6140], grad_fn=<UnbindBackward0>))\n",
      "(8.10892415597534, tensor([7.7811], grad_fn=<UnbindBackward0>))\n",
      "(8.65084957622891, tensor([8.1500], grad_fn=<UnbindBackward0>))\n",
      "(9.319643106866632, tensor([6.3739], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([6.7971], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([6.3001], grad_fn=<UnbindBackward0>))\n",
      "(9.495444123413163, tensor([7.7646], grad_fn=<UnbindBackward0>))\n",
      "(9.749753452594087, tensor([7.0942], grad_fn=<UnbindBackward0>))\n",
      "(8.145259566516865, tensor([7.1220], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([6.2851], grad_fn=<UnbindBackward0>))\n",
      "(8.862058677395472, tensor([9.3247], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([6.1838], grad_fn=<UnbindBackward0>))\n",
      "(8.90354343566472, tensor([6.2615], grad_fn=<UnbindBackward0>))\n",
      "(6.947937068614969, tensor([9.0173], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([7.9799], grad_fn=<UnbindBackward0>))\n",
      "(6.320768294250582, tensor([8.3566], grad_fn=<UnbindBackward0>))\n",
      "(9.469545806435415, tensor([7.1190], grad_fn=<UnbindBackward0>))\n",
      "(7.954021087278037, tensor([6.1481], grad_fn=<UnbindBackward0>))\n",
      "(9.48006211905097, tensor([7.2324], grad_fn=<UnbindBackward0>))\n",
      "(9.347490210123416, tensor([10.1274], grad_fn=<UnbindBackward0>))\n",
      "(8.478244441277663, tensor([6.9756], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([6.3634], grad_fn=<UnbindBackward0>))\n",
      "(9.358932776026009, tensor([6.5407], grad_fn=<UnbindBackward0>))\n",
      "(8.875007486048396, tensor([10.3613], grad_fn=<UnbindBackward0>))\n",
      "(9.064852065227576, tensor([6.9976], grad_fn=<UnbindBackward0>))\n",
      "(9.494390858193913, tensor([8.2858], grad_fn=<UnbindBackward0>))\n",
      "(9.033841828485016, tensor([6.5429], grad_fn=<UnbindBackward0>))\n",
      "(6.829793737512425, tensor([7.7866], grad_fn=<UnbindBackward0>))\n",
      "(8.745761999375512, tensor([6.8378], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([6.7279], grad_fn=<UnbindBackward0>))\n",
      "(8.726967774991493, tensor([9.6075], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([8.4148], grad_fn=<UnbindBackward0>))\n",
      "(6.890609120147166, tensor([6.2275], grad_fn=<UnbindBackward0>))\n",
      "(9.322150047028188, tensor([7.8293], grad_fn=<UnbindBackward0>))\n",
      "(6.385194398997726, tensor([9.0564], grad_fn=<UnbindBackward0>))\n",
      "(6.775366090936392, tensor([7.1158], grad_fn=<UnbindBackward0>))\n",
      "(6.694562058521095, tensor([7.7503], grad_fn=<UnbindBackward0>))\n",
      "(9.762327132747114, tensor([7.1332], grad_fn=<UnbindBackward0>))\n",
      "(6.498282149476434, tensor([7.1216], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([7.7948], grad_fn=<UnbindBackward0>))\n",
      "(7.363913501405819, tensor([9.9164], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([7.2567], grad_fn=<UnbindBackward0>))\n",
      "(6.97914527506881, tensor([7.8726], grad_fn=<UnbindBackward0>))\n",
      "(8.713910628493924, tensor([6.2245], grad_fn=<UnbindBackward0>))\n",
      "(8.33110454805304, tensor([6.2875], grad_fn=<UnbindBackward0>))\n",
      "(8.219864741912652, tensor([8.6997], grad_fn=<UnbindBackward0>))\n",
      "(8.340933226000878, tensor([6.5880], grad_fn=<UnbindBackward0>))\n",
      "(9.210340371976184, tensor([8.7375], grad_fn=<UnbindBackward0>))\n",
      "(6.440946540632921, tensor([7.7550], grad_fn=<UnbindBackward0>))\n",
      "(6.403574197934815, tensor([7.3532], grad_fn=<UnbindBackward0>))\n",
      "(8.728749873478527, tensor([9.2827], grad_fn=<UnbindBackward0>))\n",
      "(6.361302477572996, tensor([6.2527], grad_fn=<UnbindBackward0>))\n",
      "(7.640123172695364, tensor([8.6658], grad_fn=<UnbindBackward0>))\n",
      "(8.305236829492593, tensor([6.3506], grad_fn=<UnbindBackward0>))\n",
      "(8.18757739559151, tensor([6.6399], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([8.9102], grad_fn=<UnbindBackward0>))\n",
      "(9.590487672100057, tensor([7.8154], grad_fn=<UnbindBackward0>))\n",
      "(8.129174996911793, tensor([9.3133], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([7.1493], grad_fn=<UnbindBackward0>))\n",
      "(6.879355804460439, tensor([9.5612], grad_fn=<UnbindBackward0>))\n",
      "(8.193676665955241, tensor([6.3058], grad_fn=<UnbindBackward0>))\n",
      "(7.261927092702751, tensor([6.7848], grad_fn=<UnbindBackward0>))\n",
      "(7.7142311448490855, tensor([10.0893], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([8.7840], grad_fn=<UnbindBackward0>))\n",
      "(7.9291264873067995, tensor([8.4434], grad_fn=<UnbindBackward0>))\n",
      "(8.707648248106914, tensor([8.6356], grad_fn=<UnbindBackward0>))\n",
      "(9.08851166361105, tensor([9.9615], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([8.7201], grad_fn=<UnbindBackward0>))\n",
      "(8.487764380725425, tensor([8.4870], grad_fn=<UnbindBackward0>))\n",
      "(7.741967899820685, tensor([6.4201], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([6.7688], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([7.9000], grad_fn=<UnbindBackward0>))\n",
      "(7.966239776559467, tensor([8.8999], grad_fn=<UnbindBackward0>))\n",
      "(7.468513271496337, tensor([7.7467], grad_fn=<UnbindBackward0>))\n",
      "(6.824373670043086, tensor([7.4120], grad_fn=<UnbindBackward0>))\n",
      "(8.920121751872404, tensor([8.4887], grad_fn=<UnbindBackward0>))\n",
      "(6.352629396319567, tensor([6.4756], grad_fn=<UnbindBackward0>))\n",
      "(8.093156697722637, tensor([7.4889], grad_fn=<UnbindBackward0>))\n",
      "(7.524021415206125, tensor([6.5735], grad_fn=<UnbindBackward0>))\n",
      "(8.080856419640986, tensor([7.8902], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([10.0314], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([6.1697], grad_fn=<UnbindBackward0>))\n",
      "(9.225228984469933, tensor([7.9475], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.3346], grad_fn=<UnbindBackward0>))\n",
      "(6.677083461247136, tensor([7.3262], grad_fn=<UnbindBackward0>))\n",
      "(7.126890808898808, tensor([6.8700], grad_fn=<UnbindBackward0>))\n",
      "(8.085178748074537, tensor([6.3449], grad_fn=<UnbindBackward0>))\n",
      "(7.75491027202143, tensor([6.5902], grad_fn=<UnbindBackward0>))\n",
      "(7.069874128458572, tensor([7.5599], grad_fn=<UnbindBackward0>))\n",
      "(8.57715877258367, tensor([7.8398], grad_fn=<UnbindBackward0>))\n",
      "(7.884952945759814, tensor([6.3395], grad_fn=<UnbindBackward0>))\n",
      "(7.046647277848756, tensor([7.5382], grad_fn=<UnbindBackward0>))\n",
      "(8.149601735736155, tensor([8.8186], grad_fn=<UnbindBackward0>))\n",
      "(7.568379267836522, tensor([7.1568], grad_fn=<UnbindBackward0>))\n",
      "(7.786136437783072, tensor([8.6816], grad_fn=<UnbindBackward0>))\n",
      "(7.594381242551817, tensor([7.3564], grad_fn=<UnbindBackward0>))\n",
      "(8.091933455979893, tensor([8.7936], grad_fn=<UnbindBackward0>))\n",
      "(8.679822114864455, tensor([6.7384], grad_fn=<UnbindBackward0>))\n",
      "(7.708859601047175, tensor([7.7447], grad_fn=<UnbindBackward0>))\n",
      "(8.9614943233096, tensor([9.0263], grad_fn=<UnbindBackward0>))\n",
      "(8.39502555744203, tensor([7.3972], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([6.2854], grad_fn=<UnbindBackward0>))\n",
      "(7.507690077819904, tensor([8.4522], grad_fn=<UnbindBackward0>))\n",
      "(6.835184586147301, tensor([8.2545], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([9.3088], grad_fn=<UnbindBackward0>))\n",
      "(9.80211905814784, tensor([9.3480], grad_fn=<UnbindBackward0>))\n",
      "(8.804024902413179, tensor([6.9112], grad_fn=<UnbindBackward0>))\n",
      "(7.117205503164344, tensor([6.1452], grad_fn=<UnbindBackward0>))\n",
      "(8.878776071707552, tensor([7.8543], grad_fn=<UnbindBackward0>))\n",
      "(7.943782692458625, tensor([6.7250], grad_fn=<UnbindBackward0>))\n",
      "(8.327726166461412, tensor([7.8453], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([8.4070], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([6.2152], grad_fn=<UnbindBackward0>))\n",
      "(7.4815557019095165, tensor([9.3459], grad_fn=<UnbindBackward0>))\n",
      "(8.28045768658256, tensor([7.2413], grad_fn=<UnbindBackward0>))\n",
      "(9.4776156346503, tensor([8.9575], grad_fn=<UnbindBackward0>))\n",
      "(6.6052979209482015, tensor([8.4927], grad_fn=<UnbindBackward0>))\n",
      "(6.837332814685591, tensor([6.7506], grad_fn=<UnbindBackward0>))\n",
      "(7.818027938530729, tensor([6.7413], grad_fn=<UnbindBackward0>))\n",
      "(6.536691597591305, tensor([6.4433], grad_fn=<UnbindBackward0>))\n",
      "(8.142645185942795, tensor([6.7669], grad_fn=<UnbindBackward0>))\n",
      "(7.685243607975833, tensor([7.4665], grad_fn=<UnbindBackward0>))\n",
      "(9.701432566051784, tensor([6.3898], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([7.1143], grad_fn=<UnbindBackward0>))\n",
      "(9.040974526342444, tensor([6.3795], grad_fn=<UnbindBackward0>))\n",
      "(8.86403999703599, tensor([6.8956], grad_fn=<UnbindBackward0>))\n",
      "(6.664409020350408, tensor([10.3352], grad_fn=<UnbindBackward0>))\n",
      "(7.900636613018005, tensor([8.6926], grad_fn=<UnbindBackward0>))\n",
      "(8.872066513408342, tensor([9.0103], grad_fn=<UnbindBackward0>))\n",
      "(8.275885669474356, tensor([8.4593], grad_fn=<UnbindBackward0>))\n",
      "(8.203304026795282, tensor([7.8354], grad_fn=<UnbindBackward0>))\n",
      "(7.579167967396076, tensor([8.4300], grad_fn=<UnbindBackward0>))\n",
      "(8.78462153484075, tensor([8.4684], grad_fn=<UnbindBackward0>))\n",
      "(9.135940049399933, tensor([5.8589], grad_fn=<UnbindBackward0>))\n",
      "(8.012680929706839, tensor([6.7794], grad_fn=<UnbindBackward0>))\n",
      "(8.442900586834382, tensor([7.7230], grad_fn=<UnbindBackward0>))\n",
      "(6.210600077024653, tensor([8.8099], grad_fn=<UnbindBackward0>))\n",
      "(6.366470447731438, tensor([8.5850], grad_fn=<UnbindBackward0>))\n",
      "(9.693753688131835, tensor([8.4462], grad_fn=<UnbindBackward0>))\n",
      "(8.785080636539838, tensor([8.8294], grad_fn=<UnbindBackward0>))\n",
      "(6.977281341630747, tensor([6.4403], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([6.9818], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([8.0130], grad_fn=<UnbindBackward0>))\n",
      "(8.014335737299424, tensor([7.3777], grad_fn=<UnbindBackward0>))\n",
      "(6.579251212010101, tensor([7.0765], grad_fn=<UnbindBackward0>))\n",
      "(7.397561535524052, tensor([9.5422], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([9.3672], grad_fn=<UnbindBackward0>))\n",
      "(7.419380582918692, tensor([6.3907], grad_fn=<UnbindBackward0>))\n",
      "(8.411388132519262, tensor([6.3322], grad_fn=<UnbindBackward0>))\n",
      "(9.066585468331045, tensor([8.3169], grad_fn=<UnbindBackward0>))\n",
      "(7.828436359157585, tensor([8.4447], grad_fn=<UnbindBackward0>))\n",
      "(7.3938782901077555, tensor([7.5081], grad_fn=<UnbindBackward0>))\n",
      "(8.215276958936633, tensor([6.7684], grad_fn=<UnbindBackward0>))\n",
      "(8.061802274538348, tensor([7.2371], grad_fn=<UnbindBackward0>))\n",
      "(8.022568946988255, tensor([6.3879], grad_fn=<UnbindBackward0>))\n",
      "(7.014814351275545, tensor([6.3200], grad_fn=<UnbindBackward0>))\n",
      "(8.47407690034261, tensor([6.2680], grad_fn=<UnbindBackward0>))\n",
      "(7.621684998724611, tensor([6.2114], grad_fn=<UnbindBackward0>))\n",
      "(9.112727543109182, tensor([8.2928], grad_fn=<UnbindBackward0>))\n",
      "(6.0330862217988015, tensor([8.5782], grad_fn=<UnbindBackward0>))\n",
      "(8.162801353492073, tensor([8.9676], grad_fn=<UnbindBackward0>))\n",
      "(8.31434234336979, tensor([8.2813], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([5.8912], grad_fn=<UnbindBackward0>))\n",
      "(7.681560362559537, tensor([9.2509], grad_fn=<UnbindBackward0>))\n",
      "(8.609042845046117, tensor([7.0884], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([7.8297], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([8.4332], grad_fn=<UnbindBackward0>))\n",
      "(6.8966943316227125, tensor([9.0374], grad_fn=<UnbindBackward0>))\n",
      "(8.918382504661613, tensor([9.0236], grad_fn=<UnbindBackward0>))\n",
      "(9.316500567804573, tensor([7.8875], grad_fn=<UnbindBackward0>))\n",
      "(7.265429723253953, tensor([7.3771], grad_fn=<UnbindBackward0>))\n",
      "(7.763871287820222, tensor([6.2942], grad_fn=<UnbindBackward0>))\n",
      "(8.738735461363474, tensor([9.4952], grad_fn=<UnbindBackward0>))\n",
      "(8.651724084373843, tensor([9.4890], grad_fn=<UnbindBackward0>))\n",
      "(9.161885151706779, tensor([8.5404], grad_fn=<UnbindBackward0>))\n",
      "(6.077642243349034, tensor([7.7044], grad_fn=<UnbindBackward0>))\n",
      "(7.961370201719511, tensor([6.7553], grad_fn=<UnbindBackward0>))\n",
      "(9.625425723461337, tensor([7.2983], grad_fn=<UnbindBackward0>))\n",
      "(8.445911989411274, tensor([6.4863], grad_fn=<UnbindBackward0>))\n",
      "(9.000729834944558, tensor([6.3289], grad_fn=<UnbindBackward0>))\n",
      "(8.693664334532016, tensor([7.2676], grad_fn=<UnbindBackward0>))\n",
      "(6.246106765481563, tensor([6.3304], grad_fn=<UnbindBackward0>))\n",
      "(7.035268599281097, tensor([9.2292], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([7.7984], grad_fn=<UnbindBackward0>))\n",
      "(8.517393171418904, tensor([7.1777], grad_fn=<UnbindBackward0>))\n",
      "(7.741967899820685, tensor([7.1735], grad_fn=<UnbindBackward0>))\n",
      "(9.551942286417393, tensor([5.8961], grad_fn=<UnbindBackward0>))\n",
      "(8.218247926685745, tensor([8.5857], grad_fn=<UnbindBackward0>))\n",
      "(6.447305862541213, tensor([9.0235], grad_fn=<UnbindBackward0>))\n",
      "(9.6580983922746, tensor([7.2167], grad_fn=<UnbindBackward0>))\n",
      "(7.138866999945524, tensor([6.3282], grad_fn=<UnbindBackward0>))\n",
      "(7.774435510302958, tensor([8.9477], grad_fn=<UnbindBackward0>))\n",
      "(9.730383384791782, tensor([7.1022], grad_fn=<UnbindBackward0>))\n",
      "(6.633318433280377, tensor([9.1765], grad_fn=<UnbindBackward0>))\n",
      "(9.341807134718488, tensor([7.7571], grad_fn=<UnbindBackward0>))\n",
      "(8.533656917446903, tensor([6.7799], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([6.6980], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([6.4896], grad_fn=<UnbindBackward0>))\n",
      "(8.658692753689937, tensor([8.2064], grad_fn=<UnbindBackward0>))\n",
      "(6.421622267806518, tensor([7.6426], grad_fn=<UnbindBackward0>))\n",
      "(7.844632644464681, tensor([6.9417], grad_fn=<UnbindBackward0>))\n",
      "(8.503702601233739, tensor([6.9041], grad_fn=<UnbindBackward0>))\n",
      "(7.801800401908973, tensor([7.7687], grad_fn=<UnbindBackward0>))\n",
      "(8.656259239539235, tensor([7.5579], grad_fn=<UnbindBackward0>))\n",
      "(6.568077911411976, tensor([7.3079], grad_fn=<UnbindBackward0>))\n",
      "(7.9420068084898565, tensor([10.0693], grad_fn=<UnbindBackward0>))\n",
      "(8.889583991794973, tensor([7.1478], grad_fn=<UnbindBackward0>))\n",
      "(8.585785982881848, tensor([6.7834], grad_fn=<UnbindBackward0>))\n",
      "(9.081938657171658, tensor([6.3599], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([9.1714], grad_fn=<UnbindBackward0>))\n",
      "(8.406708458240965, tensor([8.8991], grad_fn=<UnbindBackward0>))\n",
      "(9.442879644930755, tensor([6.3906], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([8.5635], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([6.7386], grad_fn=<UnbindBackward0>))\n",
      "(8.740496729931813, tensor([6.7899], grad_fn=<UnbindBackward0>))\n",
      "(7.848153086199526, tensor([7.3380], grad_fn=<UnbindBackward0>))\n",
      "(7.662937850461535, tensor([7.6419], grad_fn=<UnbindBackward0>))\n",
      "(6.855408798609928, tensor([8.5907], grad_fn=<UnbindBackward0>))\n",
      "(7.1372784372603855, tensor([9.5709], grad_fn=<UnbindBackward0>))\n",
      "(8.392763113038061, tensor([6.3621], grad_fn=<UnbindBackward0>))\n",
      "(8.504715669905124, tensor([7.9762], grad_fn=<UnbindBackward0>))\n",
      "(6.7464121285733745, tensor([9.9460], grad_fn=<UnbindBackward0>))\n",
      "(8.056743774975313, tensor([10.2434], grad_fn=<UnbindBackward0>))\n",
      "(7.356918242356021, tensor([6.7972], grad_fn=<UnbindBackward0>))\n",
      "(9.789758787207784, tensor([7.3724], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([8.0683], grad_fn=<UnbindBackward0>))\n",
      "(7.00669522683704, tensor([9.4403], grad_fn=<UnbindBackward0>))\n",
      "(9.270400128403846, tensor([9.3574], grad_fn=<UnbindBackward0>))\n",
      "(7.502186486602924, tensor([6.7972], grad_fn=<UnbindBackward0>))\n",
      "(8.425516402844334, tensor([6.6637], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([8.7734], grad_fn=<UnbindBackward0>))\n",
      "(7.711101251840158, tensor([8.3989], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([6.5247], grad_fn=<UnbindBackward0>))\n",
      "(9.124564594954778, tensor([7.1419], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([8.3990], grad_fn=<UnbindBackward0>))\n",
      "(9.578657284448841, tensor([8.4412], grad_fn=<UnbindBackward0>))\n",
      "(8.457867725331422, tensor([6.3065], grad_fn=<UnbindBackward0>))\n",
      "(8.564649132572534, tensor([6.3375], grad_fn=<UnbindBackward0>))\n",
      "(9.513403546466266, tensor([9.0307], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([6.3536], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([7.7921], grad_fn=<UnbindBackward0>))\n",
      "(7.99598047476376, tensor([9.6775], grad_fn=<UnbindBackward0>))\n",
      "(8.207946941048617, tensor([7.8133], grad_fn=<UnbindBackward0>))\n",
      "(7.699389406256737, tensor([6.2809], grad_fn=<UnbindBackward0>))\n",
      "(8.323365694436081, tensor([6.2486], grad_fn=<UnbindBackward0>))\n",
      "(8.131824785007195, tensor([7.8372], grad_fn=<UnbindBackward0>))\n",
      "(8.686429508661536, tensor([6.3993], grad_fn=<UnbindBackward0>))\n",
      "(8.827174771362785, tensor([6.4939], grad_fn=<UnbindBackward0>))\n",
      "(8.392310009269547, tensor([6.2135], grad_fn=<UnbindBackward0>))\n",
      "(7.516977224604321, tensor([6.3373], grad_fn=<UnbindBackward0>))\n",
      "(6.20050917404269, tensor([9.5796], grad_fn=<UnbindBackward0>))\n",
      "(6.511745329644728, tensor([6.7717], grad_fn=<UnbindBackward0>))\n",
      "(9.574705669198922, tensor([6.7139], grad_fn=<UnbindBackward0>))\n",
      "(7.531552381407289, tensor([6.7009], grad_fn=<UnbindBackward0>))\n",
      "(7.955775781534187, tensor([8.2390], grad_fn=<UnbindBackward0>))\n",
      "(6.9930151229329605, tensor([9.2521], grad_fn=<UnbindBackward0>))\n",
      "(6.587550014824796, tensor([7.3195], grad_fn=<UnbindBackward0>))\n",
      "(7.8083230503910555, tensor([7.8426], grad_fn=<UnbindBackward0>))\n",
      "(6.70808408385307, tensor([7.9770], grad_fn=<UnbindBackward0>))\n",
      "(8.96890555068972, tensor([8.6836], grad_fn=<UnbindBackward0>))\n",
      "(8.006367567650246, tensor([6.6305], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([9.8912], grad_fn=<UnbindBackward0>))\n",
      "(8.522976436171964, tensor([6.2783], grad_fn=<UnbindBackward0>))\n",
      "(6.395261598115449, tensor([6.2164], grad_fn=<UnbindBackward0>))\n",
      "(7.459338895220296, tensor([6.2872], grad_fn=<UnbindBackward0>))\n",
      "(7.554858521040676, tensor([7.8036], grad_fn=<UnbindBackward0>))\n",
      "(7.1785454837637, tensor([9.0073], grad_fn=<UnbindBackward0>))\n",
      "(8.407601514786142, tensor([7.1974], grad_fn=<UnbindBackward0>))\n",
      "(7.8434564043761155, tensor([7.2067], grad_fn=<UnbindBackward0>))\n",
      "(8.226038429485484, tensor([9.1213], grad_fn=<UnbindBackward0>))\n",
      "(9.608646582885868, tensor([7.7805], grad_fn=<UnbindBackward0>))\n",
      "(8.916506080039204, tensor([8.5252], grad_fn=<UnbindBackward0>))\n",
      "(9.631022445889208, tensor([10.1502], grad_fn=<UnbindBackward0>))\n",
      "(7.3864708488298945, tensor([6.7199], grad_fn=<UnbindBackward0>))\n",
      "(7.237059026124737, tensor([6.6844], grad_fn=<UnbindBackward0>))\n",
      "(7.465655310134056, tensor([9.2212], grad_fn=<UnbindBackward0>))\n",
      "(6.320768294250582, tensor([7.7703], grad_fn=<UnbindBackward0>))\n",
      "(8.68996933536666, tensor([6.3346], grad_fn=<UnbindBackward0>))\n",
      "(8.866017398810255, tensor([7.2164], grad_fn=<UnbindBackward0>))\n",
      "(8.129469764784231, tensor([7.0628], grad_fn=<UnbindBackward0>))\n",
      "(7.469083884921234, tensor([7.7812], grad_fn=<UnbindBackward0>))\n",
      "(8.378850241794492, tensor([8.3332], grad_fn=<UnbindBackward0>))\n",
      "(6.831953565565855, tensor([6.7817], grad_fn=<UnbindBackward0>))\n",
      "(8.293299358711323, tensor([7.2078], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([7.3212], grad_fn=<UnbindBackward0>))\n",
      "(8.119993827725105, tensor([6.1861], grad_fn=<UnbindBackward0>))\n",
      "(6.478509642208569, tensor([8.6936], grad_fn=<UnbindBackward0>))\n",
      "(7.447751280047908, tensor([8.9789], grad_fn=<UnbindBackward0>))\n",
      "(7.472500744737558, tensor([6.4469], grad_fn=<UnbindBackward0>))\n",
      "(6.434546518787453, tensor([6.3107], grad_fn=<UnbindBackward0>))\n",
      "(8.658519127506672, tensor([9.2071], grad_fn=<UnbindBackward0>))\n",
      "(7.489970898834801, tensor([7.2821], grad_fn=<UnbindBackward0>))\n",
      "(8.80492526261806, tensor([10.1082], grad_fn=<UnbindBackward0>))\n",
      "(9.748236267264836, tensor([8.8867], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.3053], grad_fn=<UnbindBackward0>))\n",
      "(9.560504164299259, tensor([7.0225], grad_fn=<UnbindBackward0>))\n",
      "(8.51097389160232, tensor([7.7924], grad_fn=<UnbindBackward0>))\n",
      "(7.259116128097101, tensor([8.6007], grad_fn=<UnbindBackward0>))\n",
      "(6.437751649736401, tensor([8.4740], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([8.4801], grad_fn=<UnbindBackward0>))\n",
      "(8.251142139090751, tensor([8.0964], grad_fn=<UnbindBackward0>))\n",
      "(7.388327859577107, tensor([7.8612], grad_fn=<UnbindBackward0>))\n",
      "(6.70196036600254, tensor([6.2662], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.5880], grad_fn=<UnbindBackward0>))\n",
      "(6.9975959829819265, tensor([6.3433], grad_fn=<UnbindBackward0>))\n",
      "(6.040254711277414, tensor([8.5154], grad_fn=<UnbindBackward0>))\n",
      "(7.60887062919126, tensor([6.4237], grad_fn=<UnbindBackward0>))\n",
      "(9.273033441314606, tensor([6.6039], grad_fn=<UnbindBackward0>))\n",
      "(8.718172930028725, tensor([6.7349], grad_fn=<UnbindBackward0>))\n",
      "(7.682943169878292, tensor([6.1766], grad_fn=<UnbindBackward0>))\n",
      "(6.610696044717759, tensor([8.7587], grad_fn=<UnbindBackward0>))\n",
      "(9.49016666846238, tensor([6.8376], grad_fn=<UnbindBackward0>))\n",
      "(8.439231649946526, tensor([5.8634], grad_fn=<UnbindBackward0>))\n",
      "(7.822845290279774, tensor([6.4201], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([8.5243], grad_fn=<UnbindBackward0>))\n",
      "(6.6240652277998935, tensor([7.9624], grad_fn=<UnbindBackward0>))\n",
      "(7.149916836132109, tensor([6.1554], grad_fn=<UnbindBackward0>))\n",
      "(7.714677473800927, tensor([6.6194], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([8.2137], grad_fn=<UnbindBackward0>))\n",
      "(8.59637398929068, tensor([5.8206], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([6.2944], grad_fn=<UnbindBackward0>))\n",
      "(7.934155233536322, tensor([8.8957], grad_fn=<UnbindBackward0>))\n",
      "(6.773080375655535, tensor([8.7125], grad_fn=<UnbindBackward0>))\n",
      "(7.122059881629142, tensor([8.3093], grad_fn=<UnbindBackward0>))\n",
      "(9.5302475917227, tensor([8.0223], grad_fn=<UnbindBackward0>))\n",
      "(8.163371316459912, tensor([7.1975], grad_fn=<UnbindBackward0>))\n",
      "(7.5953872788539725, tensor([5.8432], grad_fn=<UnbindBackward0>))\n",
      "(8.034306936339489, tensor([9.3308], grad_fn=<UnbindBackward0>))\n",
      "(6.854354502255021, tensor([8.6136], grad_fn=<UnbindBackward0>))\n",
      "(8.756367559802975, tensor([6.3575], grad_fn=<UnbindBackward0>))\n",
      "(8.157657015196472, tensor([8.3014], grad_fn=<UnbindBackward0>))\n",
      "(7.450660796211539, tensor([7.5995], grad_fn=<UnbindBackward0>))\n",
      "(8.559486103606488, tensor([7.1484], grad_fn=<UnbindBackward0>))\n",
      "(6.651571873589727, tensor([8.7888], grad_fn=<UnbindBackward0>))\n",
      "(7.503289630675082, tensor([7.3418], grad_fn=<UnbindBackward0>))\n",
      "(8.521981708148035, tensor([9.0813], grad_fn=<UnbindBackward0>))\n",
      "(8.756210091886738, tensor([6.1721], grad_fn=<UnbindBackward0>))\n",
      "(8.525359754082631, tensor([7.9082], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.3228], grad_fn=<UnbindBackward0>))\n",
      "(7.552237287560802, tensor([6.5591], grad_fn=<UnbindBackward0>))\n",
      "(7.428927194802272, tensor([10.3476], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([8.3717], grad_fn=<UnbindBackward0>))\n",
      "(8.550241045462437, tensor([8.2175], grad_fn=<UnbindBackward0>))\n",
      "(8.41715183723601, tensor([8.3865], grad_fn=<UnbindBackward0>))\n",
      "(8.239593454305968, tensor([6.3892], grad_fn=<UnbindBackward0>))\n",
      "(6.88653164253051, tensor([7.1672], grad_fn=<UnbindBackward0>))\n",
      "(6.670766320845874, tensor([7.8445], grad_fn=<UnbindBackward0>))\n",
      "(9.033006356693267, tensor([6.2352], grad_fn=<UnbindBackward0>))\n",
      "(7.102499355774649, tensor([7.7248], grad_fn=<UnbindBackward0>))\n",
      "(8.124446855715847, tensor([10.2931], grad_fn=<UnbindBackward0>))\n",
      "(7.631916513071252, tensor([6.8929], grad_fn=<UnbindBackward0>))\n",
      "(6.630683385642372, tensor([9.1549], grad_fn=<UnbindBackward0>))\n",
      "(7.749753406274437, tensor([7.6870], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([7.1193], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([6.8107], grad_fn=<UnbindBackward0>))\n",
      "(8.333510708982942, tensor([7.2093], grad_fn=<UnbindBackward0>))\n",
      "(6.673297967767654, tensor([8.0648], grad_fn=<UnbindBackward0>))\n",
      "(8.423102268016642, tensor([7.8105], grad_fn=<UnbindBackward0>))\n",
      "(9.347054195292005, tensor([7.2016], grad_fn=<UnbindBackward0>))\n",
      "(6.366470447731438, tensor([7.8426], grad_fn=<UnbindBackward0>))\n",
      "(8.729720590267258, tensor([7.4593], grad_fn=<UnbindBackward0>))\n",
      "(6.6895992691789665, tensor([10.0983], grad_fn=<UnbindBackward0>))\n",
      "(6.3835066348840055, tensor([7.7815], grad_fn=<UnbindBackward0>))\n",
      "(6.26530121273771, tensor([6.3833], grad_fn=<UnbindBackward0>))\n",
      "(7.246368080102461, tensor([8.8299], grad_fn=<UnbindBackward0>))\n",
      "(7.845416036592485, tensor([7.7691], grad_fn=<UnbindBackward0>))\n",
      "(8.731497794063243, tensor([8.3916], grad_fn=<UnbindBackward0>))\n",
      "(9.28609698437543, tensor([6.2885], grad_fn=<UnbindBackward0>))\n",
      "(8.796490207333578, tensor([8.3008], grad_fn=<UnbindBackward0>))\n",
      "(7.104144092987527, tensor([9.0210], grad_fn=<UnbindBackward0>))\n",
      "(8.471986598578159, tensor([7.2522], grad_fn=<UnbindBackward0>))\n",
      "(8.993551586299981, tensor([7.1822], grad_fn=<UnbindBackward0>))\n",
      "(8.674196940225903, tensor([9.3657], grad_fn=<UnbindBackward0>))\n",
      "(7.639161171659173, tensor([8.7971], grad_fn=<UnbindBackward0>))\n",
      "(9.464672464746409, tensor([6.3922], grad_fn=<UnbindBackward0>))\n",
      "(9.37424334324612, tensor([10.6696], grad_fn=<UnbindBackward0>))\n",
      "(8.345692873253865, tensor([6.7967], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([8.6173], grad_fn=<UnbindBackward0>))\n",
      "(7.1770187659099, tensor([8.8031], grad_fn=<UnbindBackward0>))\n",
      "(9.578934005308442, tensor([6.9170], grad_fn=<UnbindBackward0>))\n",
      "(8.561592778712923, tensor([8.2576], grad_fn=<UnbindBackward0>))\n",
      "(7.428333194190806, tensor([6.9237], grad_fn=<UnbindBackward0>))\n",
      "(9.164505835632383, tensor([7.3742], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.6797], grad_fn=<UnbindBackward0>))\n",
      "(8.217438537730187, tensor([7.8468], grad_fn=<UnbindBackward0>))\n",
      "(8.144679183447758, tensor([7.4877], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([7.5694], grad_fn=<UnbindBackward0>))\n",
      "(6.3835066348840055, tensor([7.7321], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([6.5283], grad_fn=<UnbindBackward0>))\n",
      "(6.3835066348840055, tensor([6.8366], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([7.0094], grad_fn=<UnbindBackward0>))\n",
      "(8.388905171114706, tensor([8.4845], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([7.7827], grad_fn=<UnbindBackward0>))\n",
      "(6.131226489483141, tensor([8.3419], grad_fn=<UnbindBackward0>))\n",
      "(8.10892415597534, tensor([9.8264], grad_fn=<UnbindBackward0>))\n",
      "(6.975413927455952, tensor([6.6260], grad_fn=<UnbindBackward0>))\n",
      "(8.513385953073284, tensor([7.8142], grad_fn=<UnbindBackward0>))\n",
      "(7.739794458408701, tensor([6.2475], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([7.7385], grad_fn=<UnbindBackward0>))\n",
      "(6.840546529288687, tensor([7.9297], grad_fn=<UnbindBackward0>))\n",
      "(8.105609402299896, tensor([9.0370], grad_fn=<UnbindBackward0>))\n",
      "(8.801619971147346, tensor([9.9033], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([8.6732], grad_fn=<UnbindBackward0>))\n",
      "(8.058643712215618, tensor([6.3221], grad_fn=<UnbindBackward0>))\n",
      "(9.729253205924792, tensor([6.4228], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([7.7852], grad_fn=<UnbindBackward0>))\n",
      "(8.461257559085935, tensor([6.4505], grad_fn=<UnbindBackward0>))\n",
      "(9.108639817149843, tensor([7.5007], grad_fn=<UnbindBackward0>))\n",
      "(7.710653323501202, tensor([9.2392], grad_fn=<UnbindBackward0>))\n",
      "(6.878326468291325, tensor([8.7417], grad_fn=<UnbindBackward0>))\n",
      "(7.5480289699350145, tensor([8.4248], grad_fn=<UnbindBackward0>))\n",
      "(7.10085190894405, tensor([7.1617], grad_fn=<UnbindBackward0>))\n",
      "(8.074649075066652, tensor([9.0928], grad_fn=<UnbindBackward0>))\n",
      "(7.556427969440253, tensor([6.3229], grad_fn=<UnbindBackward0>))\n",
      "(8.721113147762688, tensor([8.1381], grad_fn=<UnbindBackward0>))\n",
      "(8.23323750070527, tensor([7.3321], grad_fn=<UnbindBackward0>))\n",
      "(9.648982324778904, tensor([7.2702], grad_fn=<UnbindBackward0>))\n",
      "(6.077642243349034, tensor([7.5718], grad_fn=<UnbindBackward0>))\n",
      "(6.1903154058531475, tensor([7.1554], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([9.3540], grad_fn=<UnbindBackward0>))\n",
      "(6.507277712385012, tensor([7.1882], grad_fn=<UnbindBackward0>))\n",
      "(7.039660349862076, tensor([6.5579], grad_fn=<UnbindBackward0>))\n",
      "(7.726212650507529, tensor([6.4652], grad_fn=<UnbindBackward0>))\n",
      "(7.069023426578259, tensor([7.7248], grad_fn=<UnbindBackward0>))\n",
      "(6.92951677076365, tensor([8.4020], grad_fn=<UnbindBackward0>))\n",
      "(7.387090235656757, tensor([8.1114], grad_fn=<UnbindBackward0>))\n",
      "(6.802394763324311, tensor([9.7893], grad_fn=<UnbindBackward0>))\n",
      "(9.089189170412032, tensor([6.7081], grad_fn=<UnbindBackward0>))\n",
      "(7.499423290592229, tensor([6.1908], grad_fn=<UnbindBackward0>))\n",
      "(7.566311014772463, tensor([6.5045], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([7.8362], grad_fn=<UnbindBackward0>))\n",
      "(8.840580188488795, tensor([8.6991], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([7.9611], grad_fn=<UnbindBackward0>))\n",
      "(8.06054004653864, tensor([9.8449], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([7.6446], grad_fn=<UnbindBackward0>))\n",
      "(9.558600006313569, tensor([8.3471], grad_fn=<UnbindBackward0>))\n",
      "(8.912473274466036, tensor([6.2790], grad_fn=<UnbindBackward0>))\n",
      "(8.168486417126681, tensor([6.1865], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.9551], grad_fn=<UnbindBackward0>))\n",
      "(7.203405521083095, tensor([7.2491], grad_fn=<UnbindBackward0>))\n",
      "(8.217978203150732, tensor([7.1975], grad_fn=<UnbindBackward0>))\n",
      "(7.014814351275545, tensor([7.9482], grad_fn=<UnbindBackward0>))\n",
      "(7.924072324923417, tensor([8.2043], grad_fn=<UnbindBackward0>))\n",
      "(7.7437032581737535, tensor([7.1120], grad_fn=<UnbindBackward0>))\n",
      "(8.225503097566918, tensor([8.6356], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([5.9419], grad_fn=<UnbindBackward0>))\n",
      "(8.098034756176071, tensor([6.3445], grad_fn=<UnbindBackward0>))\n",
      "(9.700759185964511, tensor([6.8036], grad_fn=<UnbindBackward0>))\n",
      "(7.672292455628756, tensor([7.2698], grad_fn=<UnbindBackward0>))\n",
      "(8.62515033292133, tensor([7.6330], grad_fn=<UnbindBackward0>))\n",
      "(9.355565624128962, tensor([6.1080], grad_fn=<UnbindBackward0>))\n",
      "(7.400620577371135, tensor([9.3593], grad_fn=<UnbindBackward0>))\n",
      "(8.048788283534199, tensor([7.3517], grad_fn=<UnbindBackward0>))\n",
      "(7.622174594817622, tensor([7.8039], grad_fn=<UnbindBackward0>))\n",
      "(7.437206366871292, tensor([8.2033], grad_fn=<UnbindBackward0>))\n",
      "(6.926577033222725, tensor([8.4179], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([7.2872], grad_fn=<UnbindBackward0>))\n",
      "(7.773173680482536, tensor([7.3161], grad_fn=<UnbindBackward0>))\n",
      "(6.824373670043086, tensor([7.8487], grad_fn=<UnbindBackward0>))\n",
      "(6.966024187106113, tensor([9.0445], grad_fn=<UnbindBackward0>))\n",
      "(8.127995055771946, tensor([9.3810], grad_fn=<UnbindBackward0>))\n",
      "(7.239214973779806, tensor([6.2063], grad_fn=<UnbindBackward0>))\n",
      "(9.448648359810164, tensor([8.4938], grad_fn=<UnbindBackward0>))\n",
      "(6.891625897052253, tensor([8.0713], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([8.8076], grad_fn=<UnbindBackward0>))\n",
      "(9.234056898593499, tensor([8.7449], grad_fn=<UnbindBackward0>))\n",
      "(9.098179005087852, tensor([6.3791], grad_fn=<UnbindBackward0>))\n",
      "(8.515591910049263, tensor([8.6308], grad_fn=<UnbindBackward0>))\n",
      "(6.19644412779452, tensor([7.7692], grad_fn=<UnbindBackward0>))\n",
      "(9.393411647660683, tensor([9.6359], grad_fn=<UnbindBackward0>))\n",
      "(7.2456550675945355, tensor([9.4943], grad_fn=<UnbindBackward0>))\n",
      "(7.870547844507712, tensor([7.5078], grad_fn=<UnbindBackward0>))\n",
      "(8.258163361537619, tensor([8.6856], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([6.3129], grad_fn=<UnbindBackward0>))\n",
      "(7.4133673356952405, tensor([6.3914], grad_fn=<UnbindBackward0>))\n",
      "(8.119993827725105, tensor([6.7809], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([6.3301], grad_fn=<UnbindBackward0>))\n",
      "(7.974532844130228, tensor([7.7220], grad_fn=<UnbindBackward0>))\n",
      "(6.9650803456014065, tensor([6.7112], grad_fn=<UnbindBackward0>))\n",
      "(9.200492035921368, tensor([7.8335], grad_fn=<UnbindBackward0>))\n",
      "(7.748460023899697, tensor([7.4108], grad_fn=<UnbindBackward0>))\n",
      "(8.293799608846818, tensor([6.2374], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([9.9237], grad_fn=<UnbindBackward0>))\n",
      "(7.801800401908973, tensor([6.2265], grad_fn=<UnbindBackward0>))\n",
      "(9.036106025364846, tensor([9.0875], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([7.9557], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([8.2206], grad_fn=<UnbindBackward0>))\n",
      "(9.027738775974909, tensor([8.7214], grad_fn=<UnbindBackward0>))\n",
      "(8.697846691109495, tensor([6.4326], grad_fn=<UnbindBackward0>))\n",
      "(7.934871565945177, tensor([6.7088], grad_fn=<UnbindBackward0>))\n",
      "(7.90100705199242, tensor([8.6913], grad_fn=<UnbindBackward0>))\n",
      "(9.248117736310213, tensor([9.6599], grad_fn=<UnbindBackward0>))\n",
      "(6.675823221634848, tensor([8.9564], grad_fn=<UnbindBackward0>))\n",
      "(9.205830216498297, tensor([7.3697], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([7.4775], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([6.7925], grad_fn=<UnbindBackward0>))\n",
      "(8.39615486303918, tensor([7.8317], grad_fn=<UnbindBackward0>))\n",
      "(9.434523382865056, tensor([7.9289], grad_fn=<UnbindBackward0>))\n",
      "(7.018401799069201, tensor([7.2311], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([6.8121], grad_fn=<UnbindBackward0>))\n",
      "(6.855408798609928, tensor([6.2643], grad_fn=<UnbindBackward0>))\n",
      "(8.497398564088058, tensor([8.9413], grad_fn=<UnbindBackward0>))\n",
      "(7.382746449738912, tensor([9.5195], grad_fn=<UnbindBackward0>))\n",
      "(6.9930151229329605, tensor([8.1900], grad_fn=<UnbindBackward0>))\n",
      "(7.529943370601589, tensor([7.3234], grad_fn=<UnbindBackward0>))\n",
      "(6.996681488176539, tensor([8.1957], grad_fn=<UnbindBackward0>))\n",
      "(8.821289805136113, tensor([7.9810], grad_fn=<UnbindBackward0>))\n",
      "(9.082166033252674, tensor([8.6208], grad_fn=<UnbindBackward0>))\n",
      "(8.367532416861831, tensor([9.3258], grad_fn=<UnbindBackward0>))\n",
      "(6.665683717782408, tensor([7.2110], grad_fn=<UnbindBackward0>))\n",
      "(9.024733424190698, tensor([7.6950], grad_fn=<UnbindBackward0>))\n",
      "(8.563122123304638, tensor([6.7068], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([8.8910], grad_fn=<UnbindBackward0>))\n",
      "(9.644781588320864, tensor([8.4696], grad_fn=<UnbindBackward0>))\n",
      "(8.17103418920548, tensor([7.2190], grad_fn=<UnbindBackward0>))\n",
      "(8.489410810403786, tensor([7.1975], grad_fn=<UnbindBackward0>))\n",
      "(6.810142450115136, tensor([6.2354], grad_fn=<UnbindBackward0>))\n",
      "(7.059617628291383, tensor([8.2115], grad_fn=<UnbindBackward0>))\n",
      "(9.696278864234223, tensor([8.4693], grad_fn=<UnbindBackward0>))\n",
      "(8.307705966549513, tensor([6.4568], grad_fn=<UnbindBackward0>))\n",
      "(8.488176242345745, tensor([8.6067], grad_fn=<UnbindBackward0>))\n",
      "(6.947937068614969, tensor([7.5791], grad_fn=<UnbindBackward0>))\n",
      "(9.093132171070641, tensor([6.1865], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([6.8151], grad_fn=<UnbindBackward0>))\n",
      "(6.375024819828097, tensor([8.6557], grad_fn=<UnbindBackward0>))\n",
      "(8.749415406663651, tensor([6.4900], grad_fn=<UnbindBackward0>))\n",
      "(7.187657164114956, tensor([7.9278], grad_fn=<UnbindBackward0>))\n",
      "(9.332115428902103, tensor([6.5329], grad_fn=<UnbindBackward0>))\n",
      "(9.16711966952162, tensor([9.1991], grad_fn=<UnbindBackward0>))\n",
      "(6.77078942390898, tensor([6.2537], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([8.6157], grad_fn=<UnbindBackward0>))\n",
      "(7.344072850573066, tensor([9.6485], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([8.5179], grad_fn=<UnbindBackward0>))\n",
      "(7.64826303090192, tensor([7.4244], grad_fn=<UnbindBackward0>))\n",
      "(6.734591659972948, tensor([8.9161], grad_fn=<UnbindBackward0>))\n",
      "(8.867709208039386, tensor([6.2374], grad_fn=<UnbindBackward0>))\n",
      "(7.638198244285779, tensor([6.3532], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([8.6591], grad_fn=<UnbindBackward0>))\n",
      "(8.381373468273702, tensor([5.9245], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([6.0707], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([8.0815], grad_fn=<UnbindBackward0>))\n",
      "(7.016609683894219, tensor([8.6660], grad_fn=<UnbindBackward0>))\n",
      "(9.023890304598448, tensor([8.6800], grad_fn=<UnbindBackward0>))\n",
      "(7.457609289715606, tensor([9.3315], grad_fn=<UnbindBackward0>))\n",
      "(7.9102237070973445, tensor([7.7333], grad_fn=<UnbindBackward0>))\n",
      "(9.18122047340233, tensor([6.8095], grad_fn=<UnbindBackward0>))\n",
      "(7.940939762327791, tensor([6.0369], grad_fn=<UnbindBackward0>))\n",
      "(7.080867896690782, tensor([6.3222], grad_fn=<UnbindBackward0>))\n",
      "(7.382746449738912, tensor([9.0734], grad_fn=<UnbindBackward0>))\n",
      "(7.553286605600419, tensor([7.1886], grad_fn=<UnbindBackward0>))\n",
      "(7.0909098220799835, tensor([8.8410], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([7.9673], grad_fn=<UnbindBackward0>))\n",
      "(9.68439827154996, tensor([6.4636], grad_fn=<UnbindBackward0>))\n",
      "(8.537191877922927, tensor([8.5077], grad_fn=<UnbindBackward0>))\n",
      "(8.955706153570603, tensor([8.5045], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([6.3282], grad_fn=<UnbindBackward0>))\n",
      "(6.895682697747868, tensor([8.9558], grad_fn=<UnbindBackward0>))\n",
      "(8.914626127827137, tensor([6.2582], grad_fn=<UnbindBackward0>))\n",
      "(7.7702232041587855, tensor([7.2698], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([7.1494], grad_fn=<UnbindBackward0>))\n",
      "(9.463819123086465, tensor([6.3497], grad_fn=<UnbindBackward0>))\n",
      "(7.997326822998097, tensor([6.8136], grad_fn=<UnbindBackward0>))\n",
      "(6.579251212010101, tensor([7.3366], grad_fn=<UnbindBackward0>))\n",
      "(6.230481447578482, tensor([7.8272], grad_fn=<UnbindBackward0>))\n",
      "(8.053887083618223, tensor([5.9026], grad_fn=<UnbindBackward0>))\n",
      "(8.943114308091785, tensor([7.4315], grad_fn=<UnbindBackward0>))\n",
      "(6.810142450115136, tensor([9.0230], grad_fn=<UnbindBackward0>))\n",
      "(9.102643796520809, tensor([9.2585], grad_fn=<UnbindBackward0>))\n",
      "(6.2285110035911835, tensor([6.6226], grad_fn=<UnbindBackward0>))\n",
      "(7.110696122978827, tensor([8.1974], grad_fn=<UnbindBackward0>))\n",
      "(9.470240063415512, tensor([6.6579], grad_fn=<UnbindBackward0>))\n",
      "(6.133398042996649, tensor([6.6507], grad_fn=<UnbindBackward0>))\n",
      "(8.350902451694811, tensor([7.1078], grad_fn=<UnbindBackward0>))\n",
      "(6.635946555686647, tensor([6.7168], grad_fn=<UnbindBackward0>))\n",
      "(6.57507584059962, tensor([8.4089], grad_fn=<UnbindBackward0>))\n",
      "(8.713746330456916, tensor([9.4718], grad_fn=<UnbindBackward0>))\n",
      "(6.324358962381311, tensor([8.2457], grad_fn=<UnbindBackward0>))\n",
      "(6.1092475827643655, tensor([8.9292], grad_fn=<UnbindBackward0>))\n",
      "(9.645040605352682, tensor([6.3004], grad_fn=<UnbindBackward0>))\n",
      "(6.882437470997847, tensor([8.6077], grad_fn=<UnbindBackward0>))\n",
      "(7.6236419465115715, tensor([6.2159], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([8.8203], grad_fn=<UnbindBackward0>))\n",
      "(7.4205789054108005, tensor([7.7459], grad_fn=<UnbindBackward0>))\n",
      "(7.520234556474628, tensor([10.1288], grad_fn=<UnbindBackward0>))\n",
      "(7.80016307039296, tensor([6.3366], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.7301], grad_fn=<UnbindBackward0>))\n",
      "(8.517393171418904, tensor([8.9132], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([8.8114], grad_fn=<UnbindBackward0>))\n",
      "(7.382746449738912, tensor([8.4513], grad_fn=<UnbindBackward0>))\n",
      "(7.3914152346753585, tensor([10.2565], grad_fn=<UnbindBackward0>))\n",
      "(9.49461665129313, tensor([6.7669], grad_fn=<UnbindBackward0>))\n",
      "(6.0014148779611505, tensor([7.6164], grad_fn=<UnbindBackward0>))\n",
      "(8.371242135931933, tensor([6.0625], grad_fn=<UnbindBackward0>))\n",
      "(8.70317470904168, tensor([6.5030], grad_fn=<UnbindBackward0>))\n",
      "(8.559869465696673, tensor([6.3452], grad_fn=<UnbindBackward0>))\n",
      "(8.719644119505357, tensor([8.7196], grad_fn=<UnbindBackward0>))\n",
      "(9.358415490041546, tensor([6.1906], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([6.4913], grad_fn=<UnbindBackward0>))\n",
      "(9.517604459155686, tensor([6.6359], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([6.9196], grad_fn=<UnbindBackward0>))\n",
      "(8.733916174927524, tensor([8.6943], grad_fn=<UnbindBackward0>))\n",
      "(7.613818684808629, tensor([6.2940], grad_fn=<UnbindBackward0>))\n",
      "(8.535229553902337, tensor([6.6189], grad_fn=<UnbindBackward0>))\n",
      "(9.442800389927742, tensor([7.7987], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([8.0342], grad_fn=<UnbindBackward0>))\n",
      "(7.713784616598755, tensor([7.7729], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([8.5358], grad_fn=<UnbindBackward0>))\n",
      "(8.890548245606167, tensor([6.2083], grad_fn=<UnbindBackward0>))\n",
      "(7.568379267836522, tensor([8.1801], grad_fn=<UnbindBackward0>))\n",
      "(8.173011311724972, tensor([8.6522], grad_fn=<UnbindBackward0>))\n",
      "(8.994296557777195, tensor([8.5035], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([6.3235], grad_fn=<UnbindBackward0>))\n",
      "(8.936298185228436, tensor([7.6291], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([6.3698], grad_fn=<UnbindBackward0>))\n",
      "(8.4071550862073, tensor([7.8247], grad_fn=<UnbindBackward0>))\n",
      "(6.687108607866515, tensor([6.6572], grad_fn=<UnbindBackward0>))\n",
      "(8.951699168308815, tensor([8.2894], grad_fn=<UnbindBackward0>))\n",
      "(9.355392643675321, tensor([7.1435], grad_fn=<UnbindBackward0>))\n",
      "(8.70930004894499, tensor([6.4311], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([9.3343], grad_fn=<UnbindBackward0>))\n",
      "(8.207946941048617, tensor([6.7169], grad_fn=<UnbindBackward0>))\n",
      "(9.371353167823885, tensor([9.8796], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([9.6581], grad_fn=<UnbindBackward0>))\n",
      "(7.533693709848633, tensor([8.5130], grad_fn=<UnbindBackward0>))\n",
      "(8.537583881063972, tensor([9.9740], grad_fn=<UnbindBackward0>))\n",
      "(8.096512917501594, tensor([8.9441], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([8.6744], grad_fn=<UnbindBackward0>))\n",
      "(7.1316985104669115, tensor([8.1890], grad_fn=<UnbindBackward0>))\n",
      "(7.029972911706386, tensor([8.7287], grad_fn=<UnbindBackward0>))\n",
      "(8.886685639065584, tensor([6.2523], grad_fn=<UnbindBackward0>))\n",
      "(8.55159461813357, tensor([5.7644], grad_fn=<UnbindBackward0>))\n",
      "(8.332789468417959, tensor([9.0506], grad_fn=<UnbindBackward0>))\n",
      "(6.280395838960195, tensor([8.2145], grad_fn=<UnbindBackward0>))\n",
      "(8.886270902072013, tensor([7.2115], grad_fn=<UnbindBackward0>))\n",
      "(8.51197962436335, tensor([9.3334], grad_fn=<UnbindBackward0>))\n",
      "(8.38548870041881, tensor([6.4899], grad_fn=<UnbindBackward0>))\n",
      "(6.679599185844383, tensor([8.6161], grad_fn=<UnbindBackward0>))\n",
      "(9.264260213965613, tensor([9.0859], grad_fn=<UnbindBackward0>))\n",
      "(6.461468176353717, tensor([10.0449], grad_fn=<UnbindBackward0>))\n",
      "(8.01102337918644, tensor([6.7336], grad_fn=<UnbindBackward0>))\n",
      "(8.66957087183712, tensor([6.0256], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([6.3317], grad_fn=<UnbindBackward0>))\n",
      "(8.00202481821611, tensor([6.4327], grad_fn=<UnbindBackward0>))\n",
      "(7.962415680121064, tensor([8.7365], grad_fn=<UnbindBackward0>))\n",
      "(6.996681488176539, tensor([8.3365], grad_fn=<UnbindBackward0>))\n",
      "(7.191429330036379, tensor([8.5627], grad_fn=<UnbindBackward0>))\n",
      "(6.220590170099739, tensor([9.0974], grad_fn=<UnbindBackward0>))\n",
      "(9.210840247017833, tensor([6.4797], grad_fn=<UnbindBackward0>))\n",
      "(6.610696044717759, tensor([6.2117], grad_fn=<UnbindBackward0>))\n",
      "(6.802394763324311, tensor([8.6805], grad_fn=<UnbindBackward0>))\n",
      "(8.324093761450404, tensor([8.4216], grad_fn=<UnbindBackward0>))\n",
      "(8.870803443982124, tensor([8.6801], grad_fn=<UnbindBackward0>))\n",
      "(9.281637253858216, tensor([6.2909], grad_fn=<UnbindBackward0>))\n",
      "(9.426982836948033, tensor([8.2078], grad_fn=<UnbindBackward0>))\n",
      "(6.20050917404269, tensor([9.3765], grad_fn=<UnbindBackward0>))\n",
      "(6.928537818164665, tensor([9.0052], grad_fn=<UnbindBackward0>))\n",
      "(8.340456012916183, tensor([7.2077], grad_fn=<UnbindBackward0>))\n",
      "(8.391176350832751, tensor([8.3646], grad_fn=<UnbindBackward0>))\n",
      "(8.068402958569699, tensor([5.9179], grad_fn=<UnbindBackward0>))\n",
      "(8.495356496807062, tensor([8.4155], grad_fn=<UnbindBackward0>))\n",
      "(7.804251383528112, tensor([7.6893], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([7.2014], grad_fn=<UnbindBackward0>))\n",
      "(6.880384082186005, tensor([8.9367], grad_fn=<UnbindBackward0>))\n",
      "(8.345217926676428, tensor([8.8649], grad_fn=<UnbindBackward0>))\n",
      "(9.605014129006127, tensor([8.9504], grad_fn=<UnbindBackward0>))\n",
      "(8.782476268924539, tensor([6.2537], grad_fn=<UnbindBackward0>))\n",
      "(8.231376045573969, tensor([7.3502], grad_fn=<UnbindBackward0>))\n",
      "(8.536211197252, tensor([9.1096], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.1557], grad_fn=<UnbindBackward0>))\n",
      "(7.722234744709607, tensor([6.7524], grad_fn=<UnbindBackward0>))\n",
      "(5.940171252720432, tensor([7.2697], grad_fn=<UnbindBackward0>))\n",
      "(8.511175119090675, tensor([8.0517], grad_fn=<UnbindBackward0>))\n",
      "(8.773229786032473, tensor([7.8236], grad_fn=<UnbindBackward0>))\n",
      "(9.496120637138368, tensor([7.8359], grad_fn=<UnbindBackward0>))\n",
      "(7.1631723908466425, tensor([7.1756], grad_fn=<UnbindBackward0>))\n",
      "(6.52649485957079, tensor([6.0232], grad_fn=<UnbindBackward0>))\n",
      "(7.488852955733459, tensor([8.4583], grad_fn=<UnbindBackward0>))\n",
      "(8.229244416735913, tensor([8.6953], grad_fn=<UnbindBackward0>))\n",
      "(8.544224530467268, tensor([9.6416], grad_fn=<UnbindBackward0>))\n",
      "(6.642486801367256, tensor([8.8934], grad_fn=<UnbindBackward0>))\n",
      "(9.31739944426959, tensor([8.4046], grad_fn=<UnbindBackward0>))\n",
      "(9.60689927061335, tensor([6.2942], grad_fn=<UnbindBackward0>))\n",
      "(6.827629234502852, tensor([6.6238], grad_fn=<UnbindBackward0>))\n",
      "(6.257667587882639, tensor([7.2792], grad_fn=<UnbindBackward0>))\n",
      "(8.165363632473982, tensor([8.9640], grad_fn=<UnbindBackward0>))\n",
      "(9.684647229041753, tensor([9.9988], grad_fn=<UnbindBackward0>))\n",
      "(7.61972421378267, tensor([8.7702], grad_fn=<UnbindBackward0>))\n",
      "(9.620129451564402, tensor([6.5389], grad_fn=<UnbindBackward0>))\n",
      "(6.660575149839686, tensor([7.6713], grad_fn=<UnbindBackward0>))\n",
      "(8.069655306886165, tensor([7.2330], grad_fn=<UnbindBackward0>))\n",
      "(8.760609847570002, tensor([8.6888], grad_fn=<UnbindBackward0>))\n",
      "(7.162397497355718, tensor([6.7601], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([6.9362], grad_fn=<UnbindBackward0>))\n",
      "(7.080026499922591, tensor([6.6110], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([7.3637], grad_fn=<UnbindBackward0>))\n",
      "(7.917536353943631, tensor([6.4880], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([6.6576], grad_fn=<UnbindBackward0>))\n",
      "(7.113142108707088, tensor([6.5572], grad_fn=<UnbindBackward0>))\n",
      "(6.434546518787453, tensor([7.6749], grad_fn=<UnbindBackward0>))\n",
      "(7.213031659834869, tensor([8.4905], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([7.2526], grad_fn=<UnbindBackward0>))\n",
      "(7.2078598714324755, tensor([8.4016], grad_fn=<UnbindBackward0>))\n",
      "(8.279443487712665, tensor([8.7597], grad_fn=<UnbindBackward0>))\n",
      "(9.566545205430295, tensor([7.1760], grad_fn=<UnbindBackward0>))\n",
      "(8.524962928680598, tensor([6.9369], grad_fn=<UnbindBackward0>))\n",
      "(8.517793011488205, tensor([6.3703], grad_fn=<UnbindBackward0>))\n",
      "(8.902591637374087, tensor([6.2576], grad_fn=<UnbindBackward0>))\n",
      "(8.341886969516187, tensor([9.6721], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([9.0936], grad_fn=<UnbindBackward0>))\n",
      "(7.535830462798367, tensor([10.2510], grad_fn=<UnbindBackward0>))\n",
      "(9.327145151518268, tensor([6.3433], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([8.6946], grad_fn=<UnbindBackward0>))\n",
      "(7.3632795869630385, tensor([7.8068], grad_fn=<UnbindBackward0>))\n",
      "(7.61085279039525, tensor([8.7635], grad_fn=<UnbindBackward0>))\n",
      "(8.564649132572534, tensor([6.0444], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([6.3317], grad_fn=<UnbindBackward0>))\n",
      "(7.573017256052546, tensor([8.1403], grad_fn=<UnbindBackward0>))\n",
      "(9.295416544331301, tensor([8.6267], grad_fn=<UnbindBackward0>))\n",
      "(8.334711621820917, tensor([6.8255], grad_fn=<UnbindBackward0>))\n",
      "(8.861491864286915, tensor([7.8059], grad_fn=<UnbindBackward0>))\n",
      "(7.370860166536716, tensor([6.2034], grad_fn=<UnbindBackward0>))\n",
      "(6.877296071497429, tensor([7.5054], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([7.9182], grad_fn=<UnbindBackward0>))\n",
      "(6.887552571664617, tensor([6.3392], grad_fn=<UnbindBackward0>))\n",
      "(8.593969030218288, tensor([7.9957], grad_fn=<UnbindBackward0>))\n",
      "(7.769800996003896, tensor([6.4346], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([8.0600], grad_fn=<UnbindBackward0>))\n",
      "(7.401231264413015, tensor([9.1608], grad_fn=<UnbindBackward0>))\n",
      "(9.100078977718645, tensor([8.7179], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([7.8305], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([9.1161], grad_fn=<UnbindBackward0>))\n",
      "(6.715383386334681, tensor([9.5506], grad_fn=<UnbindBackward0>))\n",
      "(8.145549631783584, tensor([8.1559], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([8.4572], grad_fn=<UnbindBackward0>))\n",
      "(7.784057002639929, tensor([8.4045], grad_fn=<UnbindBackward0>))\n",
      "(8.651024539049757, tensor([6.4212], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([9.1501], grad_fn=<UnbindBackward0>))\n",
      "(7.442492722794441, tensor([7.8095], grad_fn=<UnbindBackward0>))\n",
      "(9.06820060481506, tensor([6.2378], grad_fn=<UnbindBackward0>))\n",
      "(6.902742737158593, tensor([6.2824], grad_fn=<UnbindBackward0>))\n",
      "(6.74993119378857, tensor([8.3236], grad_fn=<UnbindBackward0>))\n",
      "(6.576469569048224, tensor([9.9728], grad_fn=<UnbindBackward0>))\n",
      "(7.511524648390866, tensor([8.4772], grad_fn=<UnbindBackward0>))\n",
      "(7.971085753505607, tensor([7.1435], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([7.2705], grad_fn=<UnbindBackward0>))\n",
      "(9.18901458526143, tensor([6.4240], grad_fn=<UnbindBackward0>))\n",
      "(7.840312983320164, tensor([10.2214], grad_fn=<UnbindBackward0>))\n",
      "(7.582738488914411, tensor([6.9694], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([6.5436], grad_fn=<UnbindBackward0>))\n",
      "(8.578476419833136, tensor([6.8172], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([7.8434], grad_fn=<UnbindBackward0>))\n",
      "(7.244941546337007, tensor([8.7758], grad_fn=<UnbindBackward0>))\n",
      "(9.184509611941662, tensor([8.4577], grad_fn=<UnbindBackward0>))\n",
      "(8.491259809389733, tensor([6.8932], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([9.9541], grad_fn=<UnbindBackward0>))\n",
      "(7.890582534656536, tensor([5.8577], grad_fn=<UnbindBackward0>))\n",
      "(9.118553976345474, tensor([8.2782], grad_fn=<UnbindBackward0>))\n",
      "(9.296242800976886, tensor([8.6689], grad_fn=<UnbindBackward0>))\n",
      "(7.444833273892193, tensor([6.5404], grad_fn=<UnbindBackward0>))\n",
      "(8.060855752934316, tensor([7.9235], grad_fn=<UnbindBackward0>))\n",
      "(7.701200180857446, tensor([7.2623], grad_fn=<UnbindBackward0>))\n",
      "(8.345217926676428, tensor([7.2735], grad_fn=<UnbindBackward0>))\n",
      "(8.21878715560148, tensor([9.1789], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([7.4082], grad_fn=<UnbindBackward0>))\n",
      "(7.414572881350589, tensor([9.4894], grad_fn=<UnbindBackward0>))\n",
      "(6.169610732491456, tensor([6.2655], grad_fn=<UnbindBackward0>))\n",
      "(6.493753839851686, tensor([7.5038], grad_fn=<UnbindBackward0>))\n",
      "(7.236339342754344, tensor([9.2559], grad_fn=<UnbindBackward0>))\n",
      "(8.384804003370492, tensor([10.1171], grad_fn=<UnbindBackward0>))\n",
      "(8.007700012884026, tensor([7.7051], grad_fn=<UnbindBackward0>))\n",
      "(6.656726524178391, tensor([6.4609], grad_fn=<UnbindBackward0>))\n",
      "(8.713582005421628, tensor([8.4905], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([6.9781], grad_fn=<UnbindBackward0>))\n",
      "(8.23217423638394, tensor([8.4880], grad_fn=<UnbindBackward0>))\n",
      "(6.942156705699469, tensor([7.7285], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([8.7337], grad_fn=<UnbindBackward0>))\n",
      "(6.536691597591305, tensor([7.2304], grad_fn=<UnbindBackward0>))\n",
      "(7.0317412587631285, tensor([8.1779], grad_fn=<UnbindBackward0>))\n",
      "(8.669742589876522, tensor([7.8887], grad_fn=<UnbindBackward0>))\n",
      "(8.010359588919783, tensor([8.7644], grad_fn=<UnbindBackward0>))\n",
      "(9.244741798693514, tensor([6.5311], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([9.0806], grad_fn=<UnbindBackward0>))\n",
      "(8.101071503119543, tensor([9.0632], grad_fn=<UnbindBackward0>))\n",
      "(8.017966703493599, tensor([6.2287], grad_fn=<UnbindBackward0>))\n",
      "(8.371473537066832, tensor([6.7851], grad_fn=<UnbindBackward0>))\n",
      "(7.585788821732034, tensor([9.0179], grad_fn=<UnbindBackward0>))\n",
      "(9.523617089089445, tensor([9.6051], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([7.8607], grad_fn=<UnbindBackward0>))\n",
      "(7.752335163302292, tensor([10.6438], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([8.7122], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([8.6770], grad_fn=<UnbindBackward0>))\n",
      "(7.484930283289661, tensor([8.9029], grad_fn=<UnbindBackward0>))\n",
      "(8.770904744296866, tensor([6.3350], grad_fn=<UnbindBackward0>))\n",
      "(7.0825485693553, tensor([8.9686], grad_fn=<UnbindBackward0>))\n",
      "(6.823286122355687, tensor([8.6113], grad_fn=<UnbindBackward0>))\n",
      "(7.384610383176974, tensor([8.7860], grad_fn=<UnbindBackward0>))\n",
      "(8.355379895253634, tensor([5.9783], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([9.1561], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([9.1721], grad_fn=<UnbindBackward0>))\n",
      "(8.493924564476883, tensor([7.7677], grad_fn=<UnbindBackward0>))\n",
      "(7.99699040583765, tensor([7.9922], grad_fn=<UnbindBackward0>))\n",
      "(7.538494999413465, tensor([8.5854], grad_fn=<UnbindBackward0>))\n",
      "(8.29054350077274, tensor([8.5537], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([6.7015], grad_fn=<UnbindBackward0>))\n",
      "(8.648747631156539, tensor([8.3293], grad_fn=<UnbindBackward0>))\n",
      "(7.454719949364001, tensor([7.2394], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([8.4727], grad_fn=<UnbindBackward0>))\n",
      "(8.559294367434873, tensor([8.1199], grad_fn=<UnbindBackward0>))\n",
      "(9.204020443231363, tensor([5.9342], grad_fn=<UnbindBackward0>))\n",
      "(7.434847875211999, tensor([10.1834], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([8.0428], grad_fn=<UnbindBackward0>))\n",
      "(9.346268889200427, tensor([7.6641], grad_fn=<UnbindBackward0>))\n",
      "(7.7437032581737535, tensor([8.8416], grad_fn=<UnbindBackward0>))\n",
      "(6.675823221634848, tensor([7.2400], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([6.3555], grad_fn=<UnbindBackward0>))\n",
      "(8.695506726812653, tensor([8.0626], grad_fn=<UnbindBackward0>))\n",
      "(6.606650186198215, tensor([8.6491], grad_fn=<UnbindBackward0>))\n",
      "(6.866933284461882, tensor([7.9474], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([7.4898], grad_fn=<UnbindBackward0>))\n",
      "(7.640123172695364, tensor([8.5457], grad_fn=<UnbindBackward0>))\n",
      "(8.341648618901306, tensor([6.7039], grad_fn=<UnbindBackward0>))\n",
      "(9.16711966952162, tensor([10.1506], grad_fn=<UnbindBackward0>))\n",
      "(6.326149473155099, tensor([7.6163], grad_fn=<UnbindBackward0>))\n",
      "(8.043020885298283, tensor([6.1654], grad_fn=<UnbindBackward0>))\n",
      "(7.910957382845589, tensor([6.5192], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([7.9108], grad_fn=<UnbindBackward0>))\n",
      "(8.729720590267258, tensor([7.8539], grad_fn=<UnbindBackward0>))\n",
      "(8.58876938990546, tensor([8.4594], grad_fn=<UnbindBackward0>))\n",
      "(8.838986793496787, tensor([9.7764], grad_fn=<UnbindBackward0>))\n",
      "(8.941807118363164, tensor([6.5230], grad_fn=<UnbindBackward0>))\n",
      "(7.910957382845589, tensor([6.2629], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([9.1129], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([6.6616], grad_fn=<UnbindBackward0>))\n",
      "(8.0519780789023, tensor([7.2237], grad_fn=<UnbindBackward0>))\n",
      "(7.785305182539862, tensor([8.1228], grad_fn=<UnbindBackward0>))\n",
      "(9.654577374584886, tensor([9.7451], grad_fn=<UnbindBackward0>))\n",
      "(8.818482267274236, tensor([6.1986], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([7.1137], grad_fn=<UnbindBackward0>))\n",
      "(6.747586526829315, tensor([6.3165], grad_fn=<UnbindBackward0>))\n",
      "(7.938802248154481, tensor([7.2730], grad_fn=<UnbindBackward0>))\n",
      "(6.473890696352274, tensor([6.4185], grad_fn=<UnbindBackward0>))\n",
      "(8.53915035876828, tensor([6.4375], grad_fn=<UnbindBackward0>))\n",
      "(7.630461261783627, tensor([8.5971], grad_fn=<UnbindBackward0>))\n",
      "(9.614204198717374, tensor([8.5231], grad_fn=<UnbindBackward0>))\n",
      "(8.797850648931053, tensor([8.6112], grad_fn=<UnbindBackward0>))\n",
      "(7.9599745280805365, tensor([8.7994], grad_fn=<UnbindBackward0>))\n",
      "(8.395703293828527, tensor([6.3482], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([6.3229], grad_fn=<UnbindBackward0>))\n",
      "(7.791935956938058, tensor([7.9223], grad_fn=<UnbindBackward0>))\n",
      "(7.358830898342354, tensor([8.6401], grad_fn=<UnbindBackward0>))\n",
      "(6.74993119378857, tensor([8.5478], grad_fn=<UnbindBackward0>))\n",
      "(8.416930769477844, tensor([7.8381], grad_fn=<UnbindBackward0>))\n",
      "(9.11305824916963, tensor([8.9329], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([7.6937], grad_fn=<UnbindBackward0>))\n",
      "(7.5766097669730375, tensor([6.8403], grad_fn=<UnbindBackward0>))\n",
      "(6.8885724595653635, tensor([9.4038], grad_fn=<UnbindBackward0>))\n",
      "(7.527255919373784, tensor([6.2743], grad_fn=<UnbindBackward0>))\n",
      "(8.650499558724462, tensor([6.3712], grad_fn=<UnbindBackward0>))\n",
      "(6.505784060128229, tensor([6.2605], grad_fn=<UnbindBackward0>))\n",
      "(8.20685642839965, tensor([9.3371], grad_fn=<UnbindBackward0>))\n",
      "(7.715569534520208, tensor([7.8760], grad_fn=<UnbindBackward0>))\n",
      "(8.319229938632326, tensor([6.5102], grad_fn=<UnbindBackward0>))\n",
      "(7.1308988302963465, tensor([8.6187], grad_fn=<UnbindBackward0>))\n",
      "(8.510168576479273, tensor([7.1220], grad_fn=<UnbindBackward0>))\n",
      "(7.218176838403408, tensor([7.9389], grad_fn=<UnbindBackward0>))\n",
      "(7.640123172695364, tensor([6.3317], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([6.7379], grad_fn=<UnbindBackward0>))\n",
      "(8.435766192720509, tensor([7.4135], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([8.6335], grad_fn=<UnbindBackward0>))\n",
      "(8.523175263093785, tensor([10.1006], grad_fn=<UnbindBackward0>))\n",
      "(7.995306620290822, tensor([8.4183], grad_fn=<UnbindBackward0>))\n",
      "(9.407796816354407, tensor([7.2257], grad_fn=<UnbindBackward0>))\n",
      "(8.466320861042481, tensor([8.6213], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([8.6853], grad_fn=<UnbindBackward0>))\n",
      "(7.5406215286571525, tensor([7.2157], grad_fn=<UnbindBackward0>))\n",
      "(7.396948602621014, tensor([6.7197], grad_fn=<UnbindBackward0>))\n",
      "(8.332308352219117, tensor([6.1689], grad_fn=<UnbindBackward0>))\n",
      "(7.942717540573791, tensor([8.1782], grad_fn=<UnbindBackward0>))\n",
      "(8.525359754082631, tensor([8.5378], grad_fn=<UnbindBackward0>))\n",
      "(8.48446336679332, tensor([8.5664], grad_fn=<UnbindBackward0>))\n",
      "(8.014666370464942, tensor([7.7668], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([7.2279], grad_fn=<UnbindBackward0>))\n",
      "(9.043458999670106, tensor([9.7924], grad_fn=<UnbindBackward0>))\n",
      "(7.706612913964197, tensor([7.7511], grad_fn=<UnbindBackward0>))\n",
      "(7.766840537085513, tensor([7.7304], grad_fn=<UnbindBackward0>))\n",
      "(8.78462153484075, tensor([6.4055], grad_fn=<UnbindBackward0>))\n",
      "(7.353081920515432, tensor([8.6467], grad_fn=<UnbindBackward0>))\n",
      "(9.319643106866632, tensor([6.8255], grad_fn=<UnbindBackward0>))\n",
      "(6.437751649736401, tensor([8.8408], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([9.3177], grad_fn=<UnbindBackward0>))\n",
      "(7.552237287560802, tensor([7.4987], grad_fn=<UnbindBackward0>))\n",
      "(8.29154650988391, tensor([7.6920], grad_fn=<UnbindBackward0>))\n",
      "(7.998335395952982, tensor([8.7916], grad_fn=<UnbindBackward0>))\n",
      "(7.591861714889934, tensor([6.1608], grad_fn=<UnbindBackward0>))\n",
      "(6.974478911025045, tensor([9.7992], grad_fn=<UnbindBackward0>))\n",
      "(9.590419293693932, tensor([8.8143], grad_fn=<UnbindBackward0>))\n",
      "(7.198931240688173, tensor([8.6796], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([8.8659], grad_fn=<UnbindBackward0>))\n",
      "(9.683837890476173, tensor([7.6925], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([8.1793], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([8.3910], grad_fn=<UnbindBackward0>))\n",
      "(7.267525427828172, tensor([6.2591], grad_fn=<UnbindBackward0>))\n",
      "(8.772455372611907, tensor([6.3464], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([7.8900], grad_fn=<UnbindBackward0>))\n",
      "(6.608000625296087, tensor([6.3560], grad_fn=<UnbindBackward0>))\n",
      "(6.2422232654551655, tensor([6.7087], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([7.7803], grad_fn=<UnbindBackward0>))\n",
      "(6.568077911411976, tensor([6.6311], grad_fn=<UnbindBackward0>))\n",
      "(8.357259153499912, tensor([8.7208], grad_fn=<UnbindBackward0>))\n",
      "(8.432288684325794, tensor([7.7080], grad_fn=<UnbindBackward0>))\n",
      "(8.57338447106598, tensor([7.9426], grad_fn=<UnbindBackward0>))\n",
      "(9.128696382935672, tensor([9.2021], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([8.6079], grad_fn=<UnbindBackward0>))\n",
      "(8.231376045573969, tensor([9.0873], grad_fn=<UnbindBackward0>))\n",
      "(7.711548979629146, tensor([6.8897], grad_fn=<UnbindBackward0>))\n",
      "(7.0967213784947605, tensor([8.2383], grad_fn=<UnbindBackward0>))\n",
      "(7.512071245835466, tensor([7.7884], grad_fn=<UnbindBackward0>))\n",
      "(7.45298232946546, tensor([8.5722], grad_fn=<UnbindBackward0>))\n",
      "(8.117312461601975, tensor([6.9831], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([7.4611], grad_fn=<UnbindBackward0>))\n",
      "(9.225031920719173, tensor([6.7716], grad_fn=<UnbindBackward0>))\n",
      "(8.981052985901483, tensor([8.7282], grad_fn=<UnbindBackward0>))\n",
      "(8.3133619511344, tensor([9.0168], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([5.8720], grad_fn=<UnbindBackward0>))\n",
      "(8.713746330456916, tensor([7.1610], grad_fn=<UnbindBackward0>))\n",
      "(9.051696278536586, tensor([9.9432], grad_fn=<UnbindBackward0>))\n",
      "(8.288534459413917, tensor([6.7809], grad_fn=<UnbindBackward0>))\n",
      "(7.576097340623111, tensor([6.3885], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.3019], grad_fn=<UnbindBackward0>))\n",
      "(7.237778191923443, tensor([10.0358], grad_fn=<UnbindBackward0>))\n",
      "(6.570882962339584, tensor([6.1999], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([5.9364], grad_fn=<UnbindBackward0>))\n",
      "(6.939253946041508, tensor([7.3100], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([8.3721], grad_fn=<UnbindBackward0>))\n",
      "(8.93247660846174, tensor([8.6356], grad_fn=<UnbindBackward0>))\n",
      "(9.660460120624778, tensor([8.1194], grad_fn=<UnbindBackward0>))\n",
      "(9.18574025510795, tensor([6.4016], grad_fn=<UnbindBackward0>))\n",
      "(8.667163717992533, tensor([8.8094], grad_fn=<UnbindBackward0>))\n",
      "(8.556606193773073, tensor([8.5041], grad_fn=<UnbindBackward0>))\n",
      "(7.2682230211595655, tensor([9.1278], grad_fn=<UnbindBackward0>))\n",
      "(7.949091499830517, tensor([7.4128], grad_fn=<UnbindBackward0>))\n",
      "(7.492203042618741, tensor([8.9381], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([8.6796], grad_fn=<UnbindBackward0>))\n",
      "(7.419979923661835, tensor([7.1526], grad_fn=<UnbindBackward0>))\n",
      "(7.921898411023797, tensor([7.1587], grad_fn=<UnbindBackward0>))\n",
      "(9.368369236201092, tensor([7.4236], grad_fn=<UnbindBackward0>))\n",
      "(6.759255270663693, tensor([10.1445], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([8.4898], grad_fn=<UnbindBackward0>))\n",
      "(6.890609120147166, tensor([7.6927], grad_fn=<UnbindBackward0>))\n",
      "(8.96098117384356, tensor([9.0137], grad_fn=<UnbindBackward0>))\n",
      "(6.630683385642372, tensor([6.3250], grad_fn=<UnbindBackward0>))\n",
      "(8.307705966549513, tensor([8.7671], grad_fn=<UnbindBackward0>))\n",
      "(7.6953031349635665, tensor([8.3047], grad_fn=<UnbindBackward0>))\n",
      "(7.68937110752969, tensor([7.2615], grad_fn=<UnbindBackward0>))\n",
      "(6.646390514847729, tensor([6.3776], grad_fn=<UnbindBackward0>))\n",
      "(6.326149473155099, tensor([7.2859], grad_fn=<UnbindBackward0>))\n",
      "(9.745956164549392, tensor([8.4390], grad_fn=<UnbindBackward0>))\n",
      "(8.372629740224884, tensor([7.6275], grad_fn=<UnbindBackward0>))\n",
      "(6.725033642166843, tensor([7.4077], grad_fn=<UnbindBackward0>))\n",
      "(9.179365567676752, tensor([6.9643], grad_fn=<UnbindBackward0>))\n",
      "(7.4073177104694174, tensor([7.2964], grad_fn=<UnbindBackward0>))\n",
      "(8.815073088844464, tensor([6.7598], grad_fn=<UnbindBackward0>))\n",
      "(7.603897968521881, tensor([7.1799], grad_fn=<UnbindBackward0>))\n",
      "(7.763446388727362, tensor([6.2012], grad_fn=<UnbindBackward0>))\n",
      "(6.968850378341948, tensor([6.7906], grad_fn=<UnbindBackward0>))\n",
      "(8.47886807709457, tensor([6.3290], grad_fn=<UnbindBackward0>))\n",
      "(7.614312146452, tensor([6.3702], grad_fn=<UnbindBackward0>))\n",
      "(7.719573989259581, tensor([6.6839], grad_fn=<UnbindBackward0>))\n",
      "(9.660715108832955, tensor([9.4394], grad_fn=<UnbindBackward0>))\n",
      "(7.00850518208228, tensor([8.1605], grad_fn=<UnbindBackward0>))\n",
      "(6.884486652042782, tensor([7.6558], grad_fn=<UnbindBackward0>))\n",
      "(7.470224135899966, tensor([9.7746], grad_fn=<UnbindBackward0>))\n",
      "(8.521185212685776, tensor([8.8950], grad_fn=<UnbindBackward0>))\n",
      "(6.419994928147142, tensor([6.7287], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.3267], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([6.7287], grad_fn=<UnbindBackward0>))\n",
      "(8.383433201236713, tensor([6.3996], grad_fn=<UnbindBackward0>))\n",
      "(6.421622267806518, tensor([6.3846], grad_fn=<UnbindBackward0>))\n",
      "(8.264878262801748, tensor([8.9279], grad_fn=<UnbindBackward0>))\n",
      "(8.34972083747249, tensor([7.3729], grad_fn=<UnbindBackward0>))\n",
      "(8.480321716640333, tensor([9.0944], grad_fn=<UnbindBackward0>))\n",
      "(7.896924656268864, tensor([8.2944], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([7.5010], grad_fn=<UnbindBackward0>))\n",
      "(8.464003362902119, tensor([7.1920], grad_fn=<UnbindBackward0>))\n",
      "(7.105786129481271, tensor([7.7627], grad_fn=<UnbindBackward0>))\n",
      "(6.343880434126331, tensor([6.2335], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([7.3611], grad_fn=<UnbindBackward0>))\n",
      "(9.337501517592862, tensor([7.7660], grad_fn=<UnbindBackward0>))\n",
      "(7.126087273299125, tensor([7.2688], grad_fn=<UnbindBackward0>))\n",
      "(9.581283046045924, tensor([7.2241], grad_fn=<UnbindBackward0>))\n",
      "(7.767687277186908, tensor([8.6949], grad_fn=<UnbindBackward0>))\n",
      "(8.595819511871428, tensor([6.2965], grad_fn=<UnbindBackward0>))\n",
      "(7.160069207596127, tensor([5.8846], grad_fn=<UnbindBackward0>))\n",
      "(8.373553741214627, tensor([7.1121], grad_fn=<UnbindBackward0>))\n",
      "(7.6544432264701125, tensor([9.4033], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([8.7835], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([8.2877], grad_fn=<UnbindBackward0>))\n",
      "(8.24800570160062, tensor([9.6195], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([8.3896], grad_fn=<UnbindBackward0>))\n",
      "(8.258940462988459, tensor([6.0366], grad_fn=<UnbindBackward0>))\n",
      "(8.889997357284841, tensor([7.3215], grad_fn=<UnbindBackward0>))\n",
      "(8.89918494333876, tensor([6.4460], grad_fn=<UnbindBackward0>))\n",
      "(8.55333223803211, tensor([8.4922], grad_fn=<UnbindBackward0>))\n",
      "(7.9919305198524775, tensor([8.6161], grad_fn=<UnbindBackward0>))\n",
      "(6.74993119378857, tensor([7.8291], grad_fn=<UnbindBackward0>))\n",
      "(7.491645473605133, tensor([6.5280], grad_fn=<UnbindBackward0>))\n",
      "(7.579678823090456, tensor([9.4384], grad_fn=<UnbindBackward0>))\n",
      "(7.586803535162581, tensor([6.8407], grad_fn=<UnbindBackward0>))\n",
      "(8.796490207333578, tensor([6.6055], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.8800], grad_fn=<UnbindBackward0>))\n",
      "(7.821643126239982, tensor([8.8180], grad_fn=<UnbindBackward0>))\n",
      "(6.8658910748834385, tensor([9.1099], grad_fn=<UnbindBackward0>))\n",
      "(9.747944236774778, tensor([7.4354], grad_fn=<UnbindBackward0>))\n",
      "(7.7702232041587855, tensor([8.1488], grad_fn=<UnbindBackward0>))\n",
      "(6.6052979209482015, tensor([7.1294], grad_fn=<UnbindBackward0>))\n",
      "(8.826881344130634, tensor([7.9390], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([6.5532], grad_fn=<UnbindBackward0>))\n",
      "(7.96346006663897, tensor([8.3126], grad_fn=<UnbindBackward0>))\n",
      "(9.530828396153142, tensor([8.0548], grad_fn=<UnbindBackward0>))\n",
      "(7.898411092811599, tensor([8.5483], grad_fn=<UnbindBackward0>))\n",
      "(7.221105098182496, tensor([6.4825], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([6.8201], grad_fn=<UnbindBackward0>))\n",
      "(6.975413927455952, tensor([8.7952], grad_fn=<UnbindBackward0>))\n",
      "(7.699389406256737, tensor([8.7522], grad_fn=<UnbindBackward0>))\n",
      "(9.578657284448841, tensor([7.6326], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([7.7815], grad_fn=<UnbindBackward0>))\n",
      "(6.342121418721152, tensor([6.6295], grad_fn=<UnbindBackward0>))\n",
      "(7.94873845481361, tensor([7.7035], grad_fn=<UnbindBackward0>))\n",
      "(7.933796874815411, tensor([7.8101], grad_fn=<UnbindBackward0>))\n",
      "(7.119635638017636, tensor([7.6906], grad_fn=<UnbindBackward0>))\n",
      "(7.495541943884256, tensor([9.4356], grad_fn=<UnbindBackward0>))\n",
      "(8.393216011596527, tensor([9.1545], grad_fn=<UnbindBackward0>))\n",
      "(6.263398262591624, tensor([7.4066], grad_fn=<UnbindBackward0>))\n",
      "(8.557759153162898, tensor([7.8327], grad_fn=<UnbindBackward0>))\n",
      "(6.302618975744905, tensor([8.7817], grad_fn=<UnbindBackward0>))\n",
      "(8.10892415597534, tensor([8.8147], grad_fn=<UnbindBackward0>))\n",
      "(8.33110454805304, tensor([8.5300], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([7.7813], grad_fn=<UnbindBackward0>))\n",
      "(8.059276223305648, tensor([5.8622], grad_fn=<UnbindBackward0>))\n",
      "(8.142354276849835, tensor([8.4369], grad_fn=<UnbindBackward0>))\n",
      "(6.883462586413092, tensor([9.4870], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([8.5838], grad_fn=<UnbindBackward0>))\n",
      "(9.249272581977971, tensor([8.5924], grad_fn=<UnbindBackward0>))\n",
      "(8.451053388911692, tensor([7.6384], grad_fn=<UnbindBackward0>))\n",
      "(7.858640655620791, tensor([9.6514], grad_fn=<UnbindBackward0>))\n",
      "(8.351138607086154, tensor([7.5501], grad_fn=<UnbindBackward0>))\n",
      "(8.038189179973203, tensor([7.6859], grad_fn=<UnbindBackward0>))\n",
      "(9.105313134108476, tensor([6.8141], grad_fn=<UnbindBackward0>))\n",
      "(9.18307194482199, tensor([8.3645], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([8.1736], grad_fn=<UnbindBackward0>))\n",
      "(8.33302993974291, tensor([7.1511], grad_fn=<UnbindBackward0>))\n",
      "(9.433163872079467, tensor([6.6805], grad_fn=<UnbindBackward0>))\n",
      "(9.491828300264636, tensor([7.8861], grad_fn=<UnbindBackward0>))\n",
      "(6.212606095751519, tensor([10.0485], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([8.5770], grad_fn=<UnbindBackward0>))\n",
      "(7.48324441607385, tensor([5.8834], grad_fn=<UnbindBackward0>))\n",
      "(6.584791392385716, tensor([6.4271], grad_fn=<UnbindBackward0>))\n",
      "(8.245384468120747, tensor([8.1684], grad_fn=<UnbindBackward0>))\n",
      "(8.621373010325902, tensor([6.8516], grad_fn=<UnbindBackward0>))\n",
      "(9.218705288307811, tensor([8.7932], grad_fn=<UnbindBackward0>))\n",
      "(7.455298485683291, tensor([9.5274], grad_fn=<UnbindBackward0>))\n",
      "(7.067319848653476, tensor([8.3633], grad_fn=<UnbindBackward0>))\n",
      "(8.610865667278873, tensor([8.4536], grad_fn=<UnbindBackward0>))\n",
      "(7.779048644925556, tensor([7.5894], grad_fn=<UnbindBackward0>))\n",
      "(7.928406026180535, tensor([6.7742], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([8.6494], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([9.4851], grad_fn=<UnbindBackward0>))\n",
      "(7.07326971745971, tensor([7.1967], grad_fn=<UnbindBackward0>))\n",
      "(8.85366542803745, tensor([7.1773], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([7.8913], grad_fn=<UnbindBackward0>))\n",
      "(8.420021279663963, tensor([8.9897], grad_fn=<UnbindBackward0>))\n",
      "(9.084890521258767, tensor([8.4113], grad_fn=<UnbindBackward0>))\n",
      "(7.547501682814967, tensor([8.8979], grad_fn=<UnbindBackward0>))\n",
      "(7.3864708488298945, tensor([8.1573], grad_fn=<UnbindBackward0>))\n",
      "(8.01565761455734, tensor([7.2043], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([6.2438], grad_fn=<UnbindBackward0>))\n",
      "(8.711772645605693, tensor([10.1696], grad_fn=<UnbindBackward0>))\n",
      "(8.913953858894255, tensor([6.8277], grad_fn=<UnbindBackward0>))\n",
      "(7.042286171939743, tensor([10.0292], grad_fn=<UnbindBackward0>))\n",
      "(5.8971538676367405, tensor([7.7675], grad_fn=<UnbindBackward0>))\n",
      "(8.471986598578159, tensor([7.4206], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([8.8826], grad_fn=<UnbindBackward0>))\n",
      "(8.621733370690162, tensor([7.7467], grad_fn=<UnbindBackward0>))\n",
      "(9.49046898877305, tensor([9.2165], grad_fn=<UnbindBackward0>))\n",
      "(8.440528106480752, tensor([9.7052], grad_fn=<UnbindBackward0>))\n",
      "(8.425516402844334, tensor([8.5055], grad_fn=<UnbindBackward0>))\n",
      "(8.035926369891792, tensor([8.9242], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([8.0131], grad_fn=<UnbindBackward0>))\n",
      "(7.9391588179567965, tensor([6.4742], grad_fn=<UnbindBackward0>))\n",
      "(6.812345094177479, tensor([9.4019], grad_fn=<UnbindBackward0>))\n",
      "(8.455743229100015, tensor([8.6267], grad_fn=<UnbindBackward0>))\n",
      "(9.266815218904162, tensor([8.3019], grad_fn=<UnbindBackward0>))\n",
      "(8.193400231952097, tensor([6.5082], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([6.7336], grad_fn=<UnbindBackward0>))\n",
      "(8.156223323194624, tensor([6.2760], grad_fn=<UnbindBackward0>))\n",
      "(6.595780513961311, tensor([6.8207], grad_fn=<UnbindBackward0>))\n",
      "(7.783640596221253, tensor([7.6050], grad_fn=<UnbindBackward0>))\n",
      "(7.353081920515432, tensor([7.9214], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([8.9123], grad_fn=<UnbindBackward0>))\n",
      "(9.044521887281242, tensor([7.2261], grad_fn=<UnbindBackward0>))\n",
      "(7.16703787691222, tensor([6.3053], grad_fn=<UnbindBackward0>))\n",
      "(9.129672468908728, tensor([8.3082], grad_fn=<UnbindBackward0>))\n",
      "(9.299266581170585, tensor([7.7218], grad_fn=<UnbindBackward0>))\n",
      "(9.498897061838624, tensor([6.2661], grad_fn=<UnbindBackward0>))\n",
      "(6.57507584059962, tensor([8.1484], grad_fn=<UnbindBackward0>))\n",
      "(8.90815361332152, tensor([6.5408], grad_fn=<UnbindBackward0>))\n",
      "(9.130647603066935, tensor([8.5801], grad_fn=<UnbindBackward0>))\n",
      "(8.754002933494261, tensor([7.7753], grad_fn=<UnbindBackward0>))\n",
      "(7.272398392570047, tensor([6.6012], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([6.7144], grad_fn=<UnbindBackward0>))\n",
      "(6.860663671448287, tensor([8.6737], grad_fn=<UnbindBackward0>))\n",
      "(8.709630081951286, tensor([7.3687], grad_fn=<UnbindBackward0>))\n",
      "(9.716736198662788, tensor([6.6709], grad_fn=<UnbindBackward0>))\n",
      "(9.038721338315364, tensor([8.1782], grad_fn=<UnbindBackward0>))\n",
      "(7.837948916025283, tensor([8.8036], grad_fn=<UnbindBackward0>))\n",
      "(7.886457270977689, tensor([9.2032], grad_fn=<UnbindBackward0>))\n",
      "(8.35936910622267, tensor([6.8448], grad_fn=<UnbindBackward0>))\n",
      "(7.469083884921234, tensor([7.7786], grad_fn=<UnbindBackward0>))\n",
      "(8.592486175451668, tensor([7.2390], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([7.2494], grad_fn=<UnbindBackward0>))\n",
      "(6.887552571664617, tensor([6.1431], grad_fn=<UnbindBackward0>))\n",
      "(7.868636894184167, tensor([9.1040], grad_fn=<UnbindBackward0>))\n",
      "(7.533158807455563, tensor([7.4323], grad_fn=<UnbindBackward0>))\n",
      "(8.644530439877432, tensor([9.2582], grad_fn=<UnbindBackward0>))\n",
      "(8.162801353492073, tensor([7.9525], grad_fn=<UnbindBackward0>))\n",
      "(6.881411303642535, tensor([8.4361], grad_fn=<UnbindBackward0>))\n",
      "(9.25378292981902, tensor([6.2410], grad_fn=<UnbindBackward0>))\n",
      "(7.138866999945524, tensor([8.1203], grad_fn=<UnbindBackward0>))\n",
      "(9.368796057668156, tensor([6.8513], grad_fn=<UnbindBackward0>))\n",
      "(7.956476798036782, tensor([6.8875], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([8.4828], grad_fn=<UnbindBackward0>))\n",
      "(6.525029657843462, tensor([9.0129], grad_fn=<UnbindBackward0>))\n",
      "(7.822845290279774, tensor([7.3041], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([6.3656], grad_fn=<UnbindBackward0>))\n",
      "(8.25088114470065, tensor([7.1306], grad_fn=<UnbindBackward0>))\n",
      "(6.763884908562435, tensor([8.5266], grad_fn=<UnbindBackward0>))\n",
      "(9.750860711077213, tensor([10.2888], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([7.3835], grad_fn=<UnbindBackward0>))\n",
      "(5.8289456176102075, tensor([8.1491], grad_fn=<UnbindBackward0>))\n",
      "(7.013915474810528, tensor([6.3188], grad_fn=<UnbindBackward0>))\n",
      "(9.744081420312915, tensor([9.5343], grad_fn=<UnbindBackward0>))\n",
      "(7.568379267836522, tensor([8.9116], grad_fn=<UnbindBackward0>))\n",
      "(8.591372589590488, tensor([6.5908], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([7.3736], grad_fn=<UnbindBackward0>))\n",
      "(7.142036574706803, tensor([7.8004], grad_fn=<UnbindBackward0>))\n",
      "(7.509883061154913, tensor([7.5082], grad_fn=<UnbindBackward0>))\n",
      "(8.661466680572662, tensor([7.8305], grad_fn=<UnbindBackward0>))\n",
      "(6.539585955617669, tensor([7.3962], grad_fn=<UnbindBackward0>))\n",
      "(8.283241441385425, tensor([6.6145], grad_fn=<UnbindBackward0>))\n",
      "(9.071423072789514, tensor([6.5611], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([9.4466], grad_fn=<UnbindBackward0>))\n",
      "(8.700680734850161, tensor([8.5043], grad_fn=<UnbindBackward0>))\n",
      "(8.441175704992322, tensor([8.1690], grad_fn=<UnbindBackward0>))\n",
      "(8.16194579946869, tensor([9.9136], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([7.7341], grad_fn=<UnbindBackward0>))\n",
      "(8.822322177471738, tensor([8.6899], grad_fn=<UnbindBackward0>))\n",
      "(9.614270969992395, tensor([6.3635], grad_fn=<UnbindBackward0>))\n",
      "(7.090076835776092, tensor([6.3799], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([6.2548], grad_fn=<UnbindBackward0>))\n",
      "(7.521859252201629, tensor([6.8442], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.4707], grad_fn=<UnbindBackward0>))\n",
      "(8.045908742270779, tensor([6.5857], grad_fn=<UnbindBackward0>))\n",
      "(6.86171134048073, tensor([8.6148], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([7.3025], grad_fn=<UnbindBackward0>))\n",
      "(6.675823221634848, tensor([8.8059], grad_fn=<UnbindBackward0>))\n",
      "(8.342363500380579, tensor([7.1648], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([6.7169], grad_fn=<UnbindBackward0>))\n",
      "(8.406485069431817, tensor([7.2430], grad_fn=<UnbindBackward0>))\n",
      "(6.97914527506881, tensor([6.8907], grad_fn=<UnbindBackward0>))\n",
      "(8.314587291319576, tensor([5.7842], grad_fn=<UnbindBackward0>))\n",
      "(9.246479418592056, tensor([8.4907], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([8.9643], grad_fn=<UnbindBackward0>))\n",
      "(8.926517509850122, tensor([9.3708], grad_fn=<UnbindBackward0>))\n",
      "(6.045005314036012, tensor([8.6226], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([8.6031], grad_fn=<UnbindBackward0>))\n",
      "(8.902047345620277, tensor([7.8853], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([8.6580], grad_fn=<UnbindBackward0>))\n",
      "(7.113142108707088, tensor([7.4018], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([8.8274], grad_fn=<UnbindBackward0>))\n",
      "(9.4244028145542, tensor([8.5149], grad_fn=<UnbindBackward0>))\n",
      "(6.480044561926653, tensor([8.5450], grad_fn=<UnbindBackward0>))\n",
      "(7.813995675002791, tensor([7.8778], grad_fn=<UnbindBackward0>))\n",
      "(6.037870919922137, tensor([7.3029], grad_fn=<UnbindBackward0>))\n",
      "(7.726212650507529, tensor([7.1588], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([6.4023], grad_fn=<UnbindBackward0>))\n",
      "(8.591372589590488, tensor([0.0707], grad_fn=<UnbindBackward0>))\n",
      "(7.738923757439457, tensor([6.3144], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([8.3771], grad_fn=<UnbindBackward0>))\n",
      "(6.834108738813838, tensor([8.5158], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([7.4606], grad_fn=<UnbindBackward0>))\n",
      "(8.361474616416817, tensor([6.3260], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([7.5537], grad_fn=<UnbindBackward0>))\n",
      "(7.527255919373784, tensor([7.7634], grad_fn=<UnbindBackward0>))\n",
      "(6.683360945766275, tensor([6.3100], grad_fn=<UnbindBackward0>))\n",
      "(7.924072324923417, tensor([6.3516], grad_fn=<UnbindBackward0>))\n",
      "(8.86403999703599, tensor([8.2838], grad_fn=<UnbindBackward0>))\n",
      "(6.928537818164665, tensor([7.7914], grad_fn=<UnbindBackward0>))\n",
      "(9.60177416667137, tensor([6.2493], grad_fn=<UnbindBackward0>))\n",
      "(8.62155320674048, tensor([6.4784], grad_fn=<UnbindBackward0>))\n",
      "(6.775366090936392, tensor([9.4431], grad_fn=<UnbindBackward0>))\n",
      "(9.235715678307411, tensor([7.8436], grad_fn=<UnbindBackward0>))\n",
      "(7.450660796211539, tensor([6.2477], grad_fn=<UnbindBackward0>))\n",
      "(8.184513753033722, tensor([7.4239], grad_fn=<UnbindBackward0>))\n",
      "(8.392536586816682, tensor([6.3196], grad_fn=<UnbindBackward0>))\n",
      "(8.256866848974312, tensor([6.2816], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([7.5097], grad_fn=<UnbindBackward0>))\n",
      "(8.070906088787819, tensor([6.2128], grad_fn=<UnbindBackward0>))\n",
      "(7.3632795869630385, tensor([8.1871], grad_fn=<UnbindBackward0>))\n",
      "(7.0317412587631285, tensor([8.7417], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([8.5144], grad_fn=<UnbindBackward0>))\n",
      "(9.667322082870054, tensor([6.3366], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([8.1704], grad_fn=<UnbindBackward0>))\n",
      "(9.321702839829644, tensor([7.0379], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([6.3831], grad_fn=<UnbindBackward0>))\n",
      "(7.6760099320288875, tensor([8.4699], grad_fn=<UnbindBackward0>))\n",
      "(6.507277712385012, tensor([8.2539], grad_fn=<UnbindBackward0>))\n",
      "(8.39615486303918, tensor([8.6250], grad_fn=<UnbindBackward0>))\n",
      "(6.928537818164665, tensor([6.1667], grad_fn=<UnbindBackward0>))\n",
      "(8.263590432617319, tensor([7.7783], grad_fn=<UnbindBackward0>))\n",
      "(6.71174039505618, tensor([6.7860], grad_fn=<UnbindBackward0>))\n",
      "(7.160069207596127, tensor([6.7942], grad_fn=<UnbindBackward0>))\n",
      "(8.454253391642363, tensor([6.2824], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([7.2567], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([7.8386], grad_fn=<UnbindBackward0>))\n",
      "(8.630700432209832, tensor([6.3356], grad_fn=<UnbindBackward0>))\n",
      "(6.855408798609928, tensor([6.8787], grad_fn=<UnbindBackward0>))\n",
      "(9.219993629004321, tensor([8.3343], grad_fn=<UnbindBackward0>))\n",
      "(8.36287583103188, tensor([7.5124], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([7.1915], grad_fn=<UnbindBackward0>))\n",
      "(8.398860004454372, tensor([7.3668], grad_fn=<UnbindBackward0>))\n",
      "(8.717518372649767, tensor([8.7260], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([6.2777], grad_fn=<UnbindBackward0>))\n",
      "(6.988413181999592, tensor([9.7546], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([9.2111], grad_fn=<UnbindBackward0>))\n",
      "(8.890272839380247, tensor([6.2946], grad_fn=<UnbindBackward0>))\n",
      "(7.980023592310645, tensor([8.1802], grad_fn=<UnbindBackward0>))\n",
      "(6.984716320118266, tensor([7.1415], grad_fn=<UnbindBackward0>))\n",
      "(8.55159461813357, tensor([7.3301], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([8.7965], grad_fn=<UnbindBackward0>))\n",
      "(6.777646593635117, tensor([7.7742], grad_fn=<UnbindBackward0>))\n",
      "(8.520985989654934, tensor([6.9532], grad_fn=<UnbindBackward0>))\n",
      "(6.51025834052315, tensor([7.3086], grad_fn=<UnbindBackward0>))\n",
      "(8.722580021141189, tensor([7.6859], grad_fn=<UnbindBackward0>))\n",
      "(9.021960500598263, tensor([8.8402], grad_fn=<UnbindBackward0>))\n",
      "(8.629807335785372, tensor([9.5647], grad_fn=<UnbindBackward0>))\n",
      "(8.612139668725192, tensor([7.7162], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([8.8164], grad_fn=<UnbindBackward0>))\n",
      "(7.503840746698951, tensor([6.2771], grad_fn=<UnbindBackward0>))\n",
      "(6.84587987526405, tensor([7.7115], grad_fn=<UnbindBackward0>))\n",
      "(8.969541885423252, tensor([8.2577], grad_fn=<UnbindBackward0>))\n",
      "(7.704811922932594, tensor([10.0486], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([6.0825], grad_fn=<UnbindBackward0>))\n",
      "(8.892336539638013, tensor([8.8383], grad_fn=<UnbindBackward0>))\n",
      "(8.925454386826402, tensor([9.1265], grad_fn=<UnbindBackward0>))\n",
      "(9.63161321028986, tensor([7.3160], grad_fn=<UnbindBackward0>))\n",
      "(8.041734711487537, tensor([6.2632], grad_fn=<UnbindBackward0>))\n",
      "(5.908082938168931, tensor([6.1423], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([8.8523], grad_fn=<UnbindBackward0>))\n",
      "(7.53689712956617, tensor([8.5303], grad_fn=<UnbindBackward0>))\n",
      "(7.581719640125308, tensor([7.9397], grad_fn=<UnbindBackward0>))\n",
      "(8.101980731853192, tensor([9.8946], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([8.4489], grad_fn=<UnbindBackward0>))\n",
      "(6.493753839851686, tensor([7.1994], grad_fn=<UnbindBackward0>))\n",
      "(8.109826276018477, tensor([8.5595], grad_fn=<UnbindBackward0>))\n",
      "(7.763021309018518, tensor([6.3858], grad_fn=<UnbindBackward0>))\n",
      "(7.450660796211539, tensor([8.7996], grad_fn=<UnbindBackward0>))\n",
      "(7.643961949002529, tensor([6.5458], grad_fn=<UnbindBackward0>))\n",
      "(9.719744986586726, tensor([6.8540], grad_fn=<UnbindBackward0>))\n",
      "(9.691036630760662, tensor([9.5563], grad_fn=<UnbindBackward0>))\n",
      "(7.117205503164344, tensor([7.1959], grad_fn=<UnbindBackward0>))\n",
      "(6.626717749249025, tensor([8.6405], grad_fn=<UnbindBackward0>))\n",
      "(9.681842877345654, tensor([7.1509], grad_fn=<UnbindBackward0>))\n",
      "(8.35608503102148, tensor([7.7099], grad_fn=<UnbindBackward0>))\n",
      "(7.842671474979457, tensor([9.4182], grad_fn=<UnbindBackward0>))\n",
      "(7.7706452341291765, tensor([6.8527], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([8.8335], grad_fn=<UnbindBackward0>))\n",
      "(7.96346006663897, tensor([6.4912], grad_fn=<UnbindBackward0>))\n",
      "(7.895063498091573, tensor([6.2956], grad_fn=<UnbindBackward0>))\n",
      "(8.729882284826589, tensor([8.4694], grad_fn=<UnbindBackward0>))\n",
      "(6.642486801367256, tensor([6.3300], grad_fn=<UnbindBackward0>))\n",
      "(6.704414354964107, tensor([7.3063], grad_fn=<UnbindBackward0>))\n",
      "(8.281723990411392, tensor([6.8218], grad_fn=<UnbindBackward0>))\n",
      "(9.094705028377259, tensor([9.4296], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([8.4008], grad_fn=<UnbindBackward0>))\n",
      "(7.258412150595307, tensor([8.5928], grad_fn=<UnbindBackward0>))\n",
      "(6.928537818164665, tensor([9.5575], grad_fn=<UnbindBackward0>))\n",
      "(9.434763104788773, tensor([8.3079], grad_fn=<UnbindBackward0>))\n",
      "(8.510168576479273, tensor([6.4326], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([8.5878], grad_fn=<UnbindBackward0>))\n",
      "(8.464635940677562, tensor([6.5426], grad_fn=<UnbindBackward0>))\n",
      "(7.253470382684528, tensor([6.5624], grad_fn=<UnbindBackward0>))\n",
      "(8.385260520155413, tensor([7.6862], grad_fn=<UnbindBackward0>))\n",
      "(7.401231264413015, tensor([9.2338], grad_fn=<UnbindBackward0>))\n",
      "(9.136693831807884, tensor([8.4853], grad_fn=<UnbindBackward0>))\n",
      "(6.542471960506805, tensor([7.9659], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([9.8834], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([7.2355], grad_fn=<UnbindBackward0>))\n",
      "(6.20050917404269, tensor([7.7328], grad_fn=<UnbindBackward0>))\n",
      "(9.497097016351463, tensor([10.0334], grad_fn=<UnbindBackward0>))\n",
      "(8.549660381553739, tensor([7.6631], grad_fn=<UnbindBackward0>))\n",
      "(8.61558951327243, tensor([7.2870], grad_fn=<UnbindBackward0>))\n",
      "(8.470939806898775, tensor([8.7076], grad_fn=<UnbindBackward0>))\n",
      "(6.413458957167357, tensor([6.2733], grad_fn=<UnbindBackward0>))\n",
      "(8.440528106480752, tensor([9.0616], grad_fn=<UnbindBackward0>))\n",
      "(9.455401958591324, tensor([8.5361], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([6.2888], grad_fn=<UnbindBackward0>))\n",
      "(7.742402021815782, tensor([6.3087], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.3692], grad_fn=<UnbindBackward0>))\n",
      "(7.382746449738912, tensor([8.4149], grad_fn=<UnbindBackward0>))\n",
      "(8.528528701079983, tensor([7.9682], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.1778], grad_fn=<UnbindBackward0>))\n",
      "(8.502688505213357, tensor([6.4529], grad_fn=<UnbindBackward0>))\n",
      "(8.743850562030243, tensor([6.9173], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([8.9144], grad_fn=<UnbindBackward0>))\n",
      "(9.573106725551233, tensor([7.4308], grad_fn=<UnbindBackward0>))\n",
      "(8.612139668725192, tensor([6.2753], grad_fn=<UnbindBackward0>))\n",
      "(7.684783943522785, tensor([8.8541], grad_fn=<UnbindBackward0>))\n",
      "(8.197814032221203, tensor([6.4919], grad_fn=<UnbindBackward0>))\n",
      "(7.999342952713282, tensor([6.5992], grad_fn=<UnbindBackward0>))\n",
      "(8.730690365678642, tensor([6.2305], grad_fn=<UnbindBackward0>))\n",
      "(9.084890521258767, tensor([8.3034], grad_fn=<UnbindBackward0>))\n",
      "(9.802451008358355, tensor([9.0747], grad_fn=<UnbindBackward0>))\n",
      "(8.352082671352637, tensor([8.6102], grad_fn=<UnbindBackward0>))\n",
      "(8.561592778712923, tensor([8.9146], grad_fn=<UnbindBackward0>))\n",
      "(7.257002707092073, tensor([6.3103], grad_fn=<UnbindBackward0>))\n",
      "(7.419979923661835, tensor([9.1425], grad_fn=<UnbindBackward0>))\n",
      "(8.89959435992585, tensor([7.1892], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([7.3928], grad_fn=<UnbindBackward0>))\n",
      "(7.447751280047908, tensor([8.5103], grad_fn=<UnbindBackward0>))\n",
      "(7.729735331385051, tensor([8.7633], grad_fn=<UnbindBackward0>))\n",
      "(6.923628628138427, tensor([8.5502], grad_fn=<UnbindBackward0>))\n",
      "(7.369600720526409, tensor([8.2191], grad_fn=<UnbindBackward0>))\n",
      "(8.80492526261806, tensor([7.5316], grad_fn=<UnbindBackward0>))\n",
      "(8.448271745949816, tensor([7.2676], grad_fn=<UnbindBackward0>))\n",
      "(6.626717749249025, tensor([6.7941], grad_fn=<UnbindBackward0>))\n",
      "(7.917171988845776, tensor([9.8606], grad_fn=<UnbindBackward0>))\n",
      "(8.381144346952961, tensor([8.8025], grad_fn=<UnbindBackward0>))\n",
      "(6.877296071497429, tensor([8.1008], grad_fn=<UnbindBackward0>))\n",
      "(7.575071699507561, tensor([8.3887], grad_fn=<UnbindBackward0>))\n",
      "(6.3473892096560105, tensor([6.2294], grad_fn=<UnbindBackward0>))\n",
      "(9.153875834995056, tensor([6.2538], grad_fn=<UnbindBackward0>))\n",
      "(9.68439827154996, tensor([9.2559], grad_fn=<UnbindBackward0>))\n",
      "(9.61326893243235, tensor([7.2624], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([7.8075], grad_fn=<UnbindBackward0>))\n",
      "(7.173958319756794, tensor([6.3538], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([6.2522], grad_fn=<UnbindBackward0>))\n",
      "(7.1569563646156364, tensor([6.2704], grad_fn=<UnbindBackward0>))\n",
      "(7.787382026484701, tensor([7.2217], grad_fn=<UnbindBackward0>))\n",
      "(6.274762021241939, tensor([7.2065], grad_fn=<UnbindBackward0>))\n",
      "(6.818924065275521, tensor([6.6698], grad_fn=<UnbindBackward0>))\n",
      "(6.419994928147142, tensor([8.3899], grad_fn=<UnbindBackward0>))\n",
      "(7.368970402194793, tensor([8.9839], grad_fn=<UnbindBackward0>))\n",
      "(8.256607344626158, tensor([9.0185], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([7.0846], grad_fn=<UnbindBackward0>))\n",
      "(6.98933526597456, tensor([7.7808], grad_fn=<UnbindBackward0>))\n",
      "(7.40184157874383, tensor([8.7370], grad_fn=<UnbindBackward0>))\n",
      "(9.406811351874557, tensor([9.6235], grad_fn=<UnbindBackward0>))\n",
      "(7.271703706887368, tensor([7.1546], grad_fn=<UnbindBackward0>))\n",
      "(9.559587800086119, tensor([10.4626], grad_fn=<UnbindBackward0>))\n",
      "(7.027314514039777, tensor([6.0197], grad_fn=<UnbindBackward0>))\n",
      "(9.13227071655426, tensor([8.1184], grad_fn=<UnbindBackward0>))\n",
      "(6.77078942390898, tensor([6.4471], grad_fn=<UnbindBackward0>))\n",
      "(9.308917935337176, tensor([9.0769], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([7.2074], grad_fn=<UnbindBackward0>))\n",
      "(8.387312270561717, tensor([8.8893], grad_fn=<UnbindBackward0>))\n",
      "(9.430519533826377, tensor([8.2417], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([8.6808], grad_fn=<UnbindBackward0>))\n",
      "(8.296795865770052, tensor([6.5049], grad_fn=<UnbindBackward0>))\n",
      "(7.516977224604321, tensor([6.3489], grad_fn=<UnbindBackward0>))\n",
      "(8.17413934342947, tensor([10.1079], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([6.2620], grad_fn=<UnbindBackward0>))\n",
      "(6.975413927455952, tensor([6.6157], grad_fn=<UnbindBackward0>))\n",
      "(6.747586526829315, tensor([6.6519], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([8.3780], grad_fn=<UnbindBackward0>))\n",
      "(7.643961949002529, tensor([9.2156], grad_fn=<UnbindBackward0>))\n",
      "(8.95686647085414, tensor([6.2572], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([8.8410], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([7.9152], grad_fn=<UnbindBackward0>))\n",
      "(7.495541943884256, tensor([7.9698], grad_fn=<UnbindBackward0>))\n",
      "(7.479299637782834, tensor([10.0262], grad_fn=<UnbindBackward0>))\n",
      "(8.352318548226004, tensor([8.5488], grad_fn=<UnbindBackward0>))\n",
      "(7.99260665240021, tensor([7.4548], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([7.0912], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([9.0036], grad_fn=<UnbindBackward0>))\n",
      "(9.5767180913763, tensor([8.4757], grad_fn=<UnbindBackward0>))\n",
      "(7.001245622069476, tensor([6.6340], grad_fn=<UnbindBackward0>))\n",
      "(8.756682421266532, tensor([7.3771], grad_fn=<UnbindBackward0>))\n",
      "(6.431331081933479, tensor([6.2442], grad_fn=<UnbindBackward0>))\n",
      "(7.007600613951853, tensor([6.4978], grad_fn=<UnbindBackward0>))\n",
      "(8.16961956172385, tensor([8.7602], grad_fn=<UnbindBackward0>))\n",
      "(6.559615237493242, tensor([7.0718], grad_fn=<UnbindBackward0>))\n",
      "(6.584791392385716, tensor([10.3479], grad_fn=<UnbindBackward0>))\n",
      "(7.935229539816907, tensor([9.2258], grad_fn=<UnbindBackward0>))\n",
      "(8.93075873555827, tensor([6.2797], grad_fn=<UnbindBackward0>))\n",
      "(6.880384082186005, tensor([8.3384], grad_fn=<UnbindBackward0>))\n",
      "(7.170119543449628, tensor([6.2243], grad_fn=<UnbindBackward0>))\n",
      "(7.8961806086154915, tensor([9.6584], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([7.8829], grad_fn=<UnbindBackward0>))\n",
      "(6.744059186311348, tensor([6.7339], grad_fn=<UnbindBackward0>))\n",
      "(6.536691597591305, tensor([10.1487], grad_fn=<UnbindBackward0>))\n",
      "(9.459541457609681, tensor([7.7677], grad_fn=<UnbindBackward0>))\n",
      "(7.404279118037268, tensor([6.4970], grad_fn=<UnbindBackward0>))\n",
      "(7.374001859350161, tensor([8.5024], grad_fn=<UnbindBackward0>))\n",
      "(7.016609683894219, tensor([8.6430], grad_fn=<UnbindBackward0>))\n",
      "(8.82555985506085, tensor([7.3107], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([8.4642], grad_fn=<UnbindBackward0>))\n",
      "(7.79934339821592, tensor([8.7275], grad_fn=<UnbindBackward0>))\n",
      "(8.063692634269517, tensor([6.3698], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([9.4963], grad_fn=<UnbindBackward0>))\n",
      "(8.395251520610994, tensor([9.1097], grad_fn=<UnbindBackward0>))\n",
      "(7.7336835707759, tensor([8.0768], grad_fn=<UnbindBackward0>))\n",
      "(8.667335849845957, tensor([8.7206], grad_fn=<UnbindBackward0>))\n",
      "(8.39502555744203, tensor([8.6778], grad_fn=<UnbindBackward0>))\n",
      "(7.646353722445999, tensor([6.3708], grad_fn=<UnbindBackward0>))\n",
      "(9.258940041812245, tensor([7.6046], grad_fn=<UnbindBackward0>))\n",
      "(9.022322622637347, tensor([8.1149], grad_fn=<UnbindBackward0>))\n",
      "(6.78332520060396, tensor([6.6507], grad_fn=<UnbindBackward0>))\n",
      "(6.885509670034818, tensor([6.3395], grad_fn=<UnbindBackward0>))\n",
      "(7.467942332285852, tensor([8.7746], grad_fn=<UnbindBackward0>))\n",
      "(8.865170419651774, tensor([6.5687], grad_fn=<UnbindBackward0>))\n",
      "(8.03689677268507, tensor([7.3487], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([8.2780], grad_fn=<UnbindBackward0>))\n",
      "(8.23031079913502, tensor([6.3740], grad_fn=<UnbindBackward0>))\n",
      "(6.821107472256465, tensor([7.3720], grad_fn=<UnbindBackward0>))\n",
      "(7.936660155225426, tensor([8.5350], grad_fn=<UnbindBackward0>))\n",
      "(8.23323750070527, tensor([7.5302], grad_fn=<UnbindBackward0>))\n",
      "(6.5638555265321274, tensor([5.8590], grad_fn=<UnbindBackward0>))\n",
      "(6.9411900550683745, tensor([6.7817], grad_fn=<UnbindBackward0>))\n",
      "(8.03073492409854, tensor([8.3074], grad_fn=<UnbindBackward0>))\n",
      "(9.28637505825218, tensor([8.6404], grad_fn=<UnbindBackward0>))\n",
      "(8.783549477153265, tensor([8.9398], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([8.2240], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([7.9200], grad_fn=<UnbindBackward0>))\n",
      "(8.325548307161398, tensor([9.4942], grad_fn=<UnbindBackward0>))\n",
      "(7.0909098220799835, tensor([6.5091], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([10.3893], grad_fn=<UnbindBackward0>))\n",
      "(8.865029186687766, tensor([6.1986], grad_fn=<UnbindBackward0>))\n",
      "(7.734558844354756, tensor([6.2109], grad_fn=<UnbindBackward0>))\n",
      "(9.212338374638856, tensor([6.3048], grad_fn=<UnbindBackward0>))\n",
      "(9.07783702216309, tensor([7.8174], grad_fn=<UnbindBackward0>))\n",
      "(6.4692503167957724, tensor([8.8634], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([7.0637], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([7.0672], grad_fn=<UnbindBackward0>))\n",
      "(7.755767170102998, tensor([9.3437], grad_fn=<UnbindBackward0>))\n",
      "(8.012018239159062, tensor([7.3124], grad_fn=<UnbindBackward0>))\n",
      "(7.813187267521416, tensor([9.3012], grad_fn=<UnbindBackward0>))\n",
      "(8.521782643750045, tensor([10.0089], grad_fn=<UnbindBackward0>))\n",
      "(7.526717561352706, tensor([6.7530], grad_fn=<UnbindBackward0>))\n",
      "(8.117908942383155, tensor([6.6455], grad_fn=<UnbindBackward0>))\n",
      "(7.462214939768189, tensor([7.6767], grad_fn=<UnbindBackward0>))\n",
      "(8.608677881538416, tensor([6.7861], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([6.3696], grad_fn=<UnbindBackward0>))\n",
      "(9.407878894574997, tensor([7.7176], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([7.4346], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([8.8047], grad_fn=<UnbindBackward0>))\n",
      "(8.735042754269337, tensor([6.0670], grad_fn=<UnbindBackward0>))\n",
      "(8.867709208039386, tensor([6.6947], grad_fn=<UnbindBackward0>))\n",
      "(7.831220214604293, tensor([8.5876], grad_fn=<UnbindBackward0>))\n",
      "(8.74209519574531, tensor([6.8354], grad_fn=<UnbindBackward0>))\n",
      "(9.044403844431551, tensor([7.1489], grad_fn=<UnbindBackward0>))\n",
      "(7.890208213109961, tensor([6.7844], grad_fn=<UnbindBackward0>))\n",
      "(9.280425979542871, tensor([6.4092], grad_fn=<UnbindBackward0>))\n",
      "(7.80057265467065, tensor([8.4145], grad_fn=<UnbindBackward0>))\n",
      "(8.894807371368625, tensor([8.6724], grad_fn=<UnbindBackward0>))\n",
      "(9.513625087655372, tensor([8.7245], grad_fn=<UnbindBackward0>))\n",
      "(8.35490952835879, tensor([6.1608], grad_fn=<UnbindBackward0>))\n",
      "(6.519147287940395, tensor([6.9089], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([6.4609], grad_fn=<UnbindBackward0>))\n",
      "(8.317766166719343, tensor([6.4426], grad_fn=<UnbindBackward0>))\n",
      "(8.961622569542543, tensor([6.2649], grad_fn=<UnbindBackward0>))\n",
      "(8.717518372649767, tensor([7.2648], grad_fn=<UnbindBackward0>))\n",
      "(8.053887083618223, tensor([8.5773], grad_fn=<UnbindBackward0>))\n",
      "(7.088408778675395, tensor([8.7363], grad_fn=<UnbindBackward0>))\n",
      "(7.704811922932594, tensor([6.2810], grad_fn=<UnbindBackward0>))\n",
      "(8.301521654940728, tensor([6.3218], grad_fn=<UnbindBackward0>))\n",
      "(6.900730664045173, tensor([7.7624], grad_fn=<UnbindBackward0>))\n",
      "(6.431331081933479, tensor([6.5934], grad_fn=<UnbindBackward0>))\n",
      "(7.943427767876373, tensor([8.9165], grad_fn=<UnbindBackward0>))\n",
      "(7.034387929915503, tensor([7.0099], grad_fn=<UnbindBackward0>))\n",
      "(6.993932975223189, tensor([7.5826], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.8378], grad_fn=<UnbindBackward0>))\n",
      "(9.816021526754852, tensor([7.7633], grad_fn=<UnbindBackward0>))\n",
      "(7.380879035564116, tensor([8.5485], grad_fn=<UnbindBackward0>))\n",
      "(7.876638460975463, tensor([9.5772], grad_fn=<UnbindBackward0>))\n",
      "(6.26530121273771, tensor([7.5168], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([6.7045], grad_fn=<UnbindBackward0>))\n",
      "(8.356319965828153, tensor([6.7784], grad_fn=<UnbindBackward0>))\n",
      "(7.739359202689098, tensor([8.6753], grad_fn=<UnbindBackward0>))\n",
      "(7.358830898342354, tensor([5.8163], grad_fn=<UnbindBackward0>))\n",
      "(7.195937226475569, tensor([8.5567], grad_fn=<UnbindBackward0>))\n",
      "(6.71174039505618, tensor([6.6003], grad_fn=<UnbindBackward0>))\n",
      "(9.761866382868549, tensor([8.8962], grad_fn=<UnbindBackward0>))\n",
      "(7.997326822998097, tensor([6.2527], grad_fn=<UnbindBackward0>))\n",
      "(9.07280095795412, tensor([10.1862], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([9.3883], grad_fn=<UnbindBackward0>))\n",
      "(8.500047032581268, tensor([8.8062], grad_fn=<UnbindBackward0>))\n",
      "(6.92461239604856, tensor([8.8040], grad_fn=<UnbindBackward0>))\n",
      "(8.746716349694486, tensor([6.4527], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([9.5668], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([6.3409], grad_fn=<UnbindBackward0>))\n",
      "(6.658011045870748, tensor([9.3467], grad_fn=<UnbindBackward0>))\n",
      "(6.154858094016418, tensor([9.1277], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([6.3115], grad_fn=<UnbindBackward0>))\n",
      "(7.655390644826152, tensor([7.5234], grad_fn=<UnbindBackward0>))\n",
      "(8.944028325260595, tensor([5.9007], grad_fn=<UnbindBackward0>))\n",
      "(8.688285266258644, tensor([9.7670], grad_fn=<UnbindBackward0>))\n",
      "(7.121252453244542, tensor([6.2783], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([6.3385], grad_fn=<UnbindBackward0>))\n",
      "(9.176680170483548, tensor([6.1518], grad_fn=<UnbindBackward0>))\n",
      "(8.854522203757432, tensor([8.2127], grad_fn=<UnbindBackward0>))\n",
      "(8.655562860681009, tensor([6.2546], grad_fn=<UnbindBackward0>))\n",
      "(7.988542982737695, tensor([9.8049], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([7.6127], grad_fn=<UnbindBackward0>))\n",
      "(8.659733878198347, tensor([9.9570], grad_fn=<UnbindBackward0>))\n",
      "(7.462214939768189, tensor([8.2985], grad_fn=<UnbindBackward0>))\n",
      "(6.824373670043086, tensor([8.0639], grad_fn=<UnbindBackward0>))\n",
      "(7.697121317282625, tensor([7.8596], grad_fn=<UnbindBackward0>))\n",
      "(8.279443487712665, tensor([8.2561], grad_fn=<UnbindBackward0>))\n",
      "(6.366470447731438, tensor([6.2410], grad_fn=<UnbindBackward0>))\n",
      "(7.017506142941256, tensor([8.2892], grad_fn=<UnbindBackward0>))\n",
      "(7.795234929002173, tensor([8.4661], grad_fn=<UnbindBackward0>))\n",
      "(6.505784060128229, tensor([9.0657], grad_fn=<UnbindBackward0>))\n",
      "(7.773594467360194, tensor([8.2655], grad_fn=<UnbindBackward0>))\n",
      "(9.538204234060796, tensor([7.7936], grad_fn=<UnbindBackward0>))\n",
      "(5.855071922202427, tensor([8.8090], grad_fn=<UnbindBackward0>))\n",
      "(8.379539026117442, tensor([8.7426], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([7.3178], grad_fn=<UnbindBackward0>))\n",
      "(7.037905963447182, tensor([6.7157], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([7.7734], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([8.4572], grad_fn=<UnbindBackward0>))\n",
      "(9.448096635658239, tensor([8.8104], grad_fn=<UnbindBackward0>))\n",
      "(8.577535420422398, tensor([8.9552], grad_fn=<UnbindBackward0>))\n",
      "(7.503840746698951, tensor([8.6869], grad_fn=<UnbindBackward0>))\n",
      "(8.775240458738327, tensor([6.3850], grad_fn=<UnbindBackward0>))\n",
      "(9.357466436866476, tensor([6.2671], grad_fn=<UnbindBackward0>))\n",
      "(7.755338812846501, tensor([7.7911], grad_fn=<UnbindBackward0>))\n",
      "(8.483222671845084, tensor([7.7343], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([6.8068], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([6.3410], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([8.4988], grad_fn=<UnbindBackward0>))\n",
      "(8.445697189711167, tensor([6.3608], grad_fn=<UnbindBackward0>))\n",
      "(8.27410200229233, tensor([8.4571], grad_fn=<UnbindBackward0>))\n",
      "(7.717351272185329, tensor([7.1457], grad_fn=<UnbindBackward0>))\n",
      "(7.3901814282264295, tensor([6.2952], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([8.7004], grad_fn=<UnbindBackward0>))\n",
      "(9.07096335550754, tensor([6.2750], grad_fn=<UnbindBackward0>))\n",
      "(8.173293438966228, tensor([8.1651], grad_fn=<UnbindBackward0>))\n",
      "(7.773594467360194, tensor([8.8237], grad_fn=<UnbindBackward0>))\n",
      "(9.095153962167066, tensor([6.2892], grad_fn=<UnbindBackward0>))\n",
      "(9.105202053852878, tensor([7.0945], grad_fn=<UnbindBackward0>))\n",
      "(6.608000625296087, tensor([6.8245], grad_fn=<UnbindBackward0>))\n",
      "(8.550047528287184, tensor([8.4565], grad_fn=<UnbindBackward0>))\n",
      "(8.07246736935477, tensor([7.8417], grad_fn=<UnbindBackward0>))\n",
      "(9.782674932319381, tensor([8.7952], grad_fn=<UnbindBackward0>))\n",
      "(8.635153989049803, tensor([8.6800], grad_fn=<UnbindBackward0>))\n",
      "(8.37539918579835, tensor([7.7894], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([6.4116], grad_fn=<UnbindBackward0>))\n",
      "(9.357121103184378, tensor([6.4631], grad_fn=<UnbindBackward0>))\n",
      "(7.836369760545124, tensor([8.6351], grad_fn=<UnbindBackward0>))\n",
      "(6.744059186311348, tensor([7.5330], grad_fn=<UnbindBackward0>))\n",
      "(8.277411998949004, tensor([9.1494], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([7.4551], grad_fn=<UnbindBackward0>))\n",
      "(8.23323750070527, tensor([8.2405], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([5.9416], grad_fn=<UnbindBackward0>))\n",
      "(7.436617265234227, tensor([8.3455], grad_fn=<UnbindBackward0>))\n",
      "(8.075893630298857, tensor([8.4511], grad_fn=<UnbindBackward0>))\n",
      "(7.713784616598755, tensor([9.3375], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([6.4884], grad_fn=<UnbindBackward0>))\n",
      "(7.356279876550748, tensor([7.5198], grad_fn=<UnbindBackward0>))\n",
      "(8.918382504661613, tensor([8.7050], grad_fn=<UnbindBackward0>))\n",
      "(8.19450550976564, tensor([8.4993], grad_fn=<UnbindBackward0>))\n",
      "(7.516977224604321, tensor([6.9008], grad_fn=<UnbindBackward0>))\n",
      "(6.822197390620491, tensor([8.5646], grad_fn=<UnbindBackward0>))\n",
      "(8.86177531100083, tensor([9.9427], grad_fn=<UnbindBackward0>))\n",
      "(8.450412157725886, tensor([7.8640], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([6.2038], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.2418], grad_fn=<UnbindBackward0>))\n",
      "(7.635786861395585, tensor([8.4240], grad_fn=<UnbindBackward0>))\n",
      "(9.516942329280475, tensor([8.6684], grad_fn=<UnbindBackward0>))\n",
      "(7.1316985104669115, tensor([5.9764], grad_fn=<UnbindBackward0>))\n",
      "(7.018401799069201, tensor([7.0125], grad_fn=<UnbindBackward0>))\n",
      "(8.031385330625534, tensor([8.2773], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([7.0554], grad_fn=<UnbindBackward0>))\n",
      "(7.065613363597717, tensor([7.6692], grad_fn=<UnbindBackward0>))\n",
      "(8.927977461002001, tensor([6.1505], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([7.2262], grad_fn=<UnbindBackward0>))\n",
      "(6.92461239604856, tensor([8.6949], grad_fn=<UnbindBackward0>))\n",
      "(8.720786883485731, tensor([7.2235], grad_fn=<UnbindBackward0>))\n",
      "(8.412054873292933, tensor([8.6346], grad_fn=<UnbindBackward0>))\n",
      "(7.778211474512493, tensor([9.0147], grad_fn=<UnbindBackward0>))\n",
      "(8.43663368355782, tensor([9.7193], grad_fn=<UnbindBackward0>))\n",
      "(8.81744592104187, tensor([6.3919], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([6.2384], grad_fn=<UnbindBackward0>))\n",
      "(7.708410667257367, tensor([8.6963], grad_fn=<UnbindBackward0>))\n",
      "(8.576781982827894, tensor([6.2339], grad_fn=<UnbindBackward0>))\n",
      "(7.9665866976384025, tensor([6.2899], grad_fn=<UnbindBackward0>))\n",
      "(6.476972362889683, tensor([9.6050], grad_fn=<UnbindBackward0>))\n",
      "(8.303504798872783, tensor([9.0500], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([7.8535], grad_fn=<UnbindBackward0>))\n",
      "(8.034306936339489, tensor([6.4270], grad_fn=<UnbindBackward0>))\n",
      "(7.841885928984623, tensor([6.2592], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([6.2928], grad_fn=<UnbindBackward0>))\n",
      "(8.024207485778577, tensor([9.4191], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([6.6859], grad_fn=<UnbindBackward0>))\n",
      "(9.362803915102651, tensor([9.9455], grad_fn=<UnbindBackward0>))\n",
      "(9.252345666121213, tensor([8.6103], grad_fn=<UnbindBackward0>))\n",
      "(8.715552125907816, tensor([6.4934], grad_fn=<UnbindBackward0>))\n",
      "(6.553933404025811, tensor([9.2881], grad_fn=<UnbindBackward0>))\n",
      "(6.230481447578482, tensor([6.6509], grad_fn=<UnbindBackward0>))\n",
      "(6.508769136971682, tensor([7.7423], grad_fn=<UnbindBackward0>))\n",
      "(7.1308988302963465, tensor([8.6433], grad_fn=<UnbindBackward0>))\n",
      "(8.763584409450138, tensor([6.7054], grad_fn=<UnbindBackward0>))\n",
      "(8.263590432617319, tensor([8.5505], grad_fn=<UnbindBackward0>))\n",
      "(8.976893927666076, tensor([9.9800], grad_fn=<UnbindBackward0>))\n",
      "(7.268920128193722, tensor([8.2470], grad_fn=<UnbindBackward0>))\n",
      "(8.755107121633896, tensor([6.1898], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([9.0152], grad_fn=<UnbindBackward0>))\n",
      "(8.95402775927046, tensor([7.1014], grad_fn=<UnbindBackward0>))\n",
      "(8.587465244401569, tensor([8.6177], grad_fn=<UnbindBackward0>))\n",
      "(8.117908942383155, tensor([6.4066], grad_fn=<UnbindBackward0>))\n",
      "(7.921898411023797, tensor([6.6167], grad_fn=<UnbindBackward0>))\n",
      "(8.50045386741194, tensor([9.3246], grad_fn=<UnbindBackward0>))\n",
      "(8.228710798793687, tensor([6.8094], grad_fn=<UnbindBackward0>))\n",
      "(8.82379514872053, tensor([9.2952], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([7.2875], grad_fn=<UnbindBackward0>))\n",
      "(7.381501894506707, tensor([8.5140], grad_fn=<UnbindBackward0>))\n",
      "(6.437751649736401, tensor([8.5422], grad_fn=<UnbindBackward0>))\n",
      "(7.488852955733459, tensor([9.6188], grad_fn=<UnbindBackward0>))\n",
      "(6.62273632394984, tensor([7.9801], grad_fn=<UnbindBackward0>))\n",
      "(8.470311205516108, tensor([7.1581], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([7.3101], grad_fn=<UnbindBackward0>))\n",
      "(9.011035410141815, tensor([5.8836], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([9.8628], grad_fn=<UnbindBackward0>))\n",
      "(6.115892125483034, tensor([8.7972], grad_fn=<UnbindBackward0>))\n",
      "(8.621733370690162, tensor([7.8329], grad_fn=<UnbindBackward0>))\n",
      "(9.808682245262217, tensor([8.8432], grad_fn=<UnbindBackward0>))\n",
      "(9.328301169609748, tensor([9.7164], grad_fn=<UnbindBackward0>))\n",
      "(9.4136076932856, tensor([6.4521], grad_fn=<UnbindBackward0>))\n",
      "(9.051344640485725, tensor([7.8696], grad_fn=<UnbindBackward0>))\n",
      "(6.716594773520978, tensor([7.9000], grad_fn=<UnbindBackward0>))\n",
      "(9.295783853793651, tensor([6.2957], grad_fn=<UnbindBackward0>))\n",
      "(8.046549357283078, tensor([8.7971], grad_fn=<UnbindBackward0>))\n",
      "(9.565704457114814, tensor([9.4923], grad_fn=<UnbindBackward0>))\n",
      "(7.231287004327616, tensor([7.9442], grad_fn=<UnbindBackward0>))\n",
      "(8.435766192720509, tensor([7.1127], grad_fn=<UnbindBackward0>))\n",
      "(7.1316985104669115, tensor([8.5420], grad_fn=<UnbindBackward0>))\n",
      "(7.945555428253489, tensor([7.1021], grad_fn=<UnbindBackward0>))\n",
      "(7.382746449738912, tensor([9.2137], grad_fn=<UnbindBackward0>))\n",
      "(8.93642970360832, tensor([7.7646], grad_fn=<UnbindBackward0>))\n",
      "(6.2422232654551655, tensor([5.7990], grad_fn=<UnbindBackward0>))\n",
      "(7.794823152179389, tensor([8.5752], grad_fn=<UnbindBackward0>))\n",
      "(7.6760099320288875, tensor([9.2897], grad_fn=<UnbindBackward0>))\n",
      "(8.695171998776056, tensor([8.3627], grad_fn=<UnbindBackward0>))\n",
      "(6.364750756851911, tensor([9.5002], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([6.3941], grad_fn=<UnbindBackward0>))\n",
      "(8.23403420769204, tensor([8.2229], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([8.8024], grad_fn=<UnbindBackward0>))\n",
      "(9.729848196125007, tensor([8.6675], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([7.8920], grad_fn=<UnbindBackward0>))\n",
      "(8.499029220788566, tensor([6.1965], grad_fn=<UnbindBackward0>))\n",
      "(7.0707241072602764, tensor([8.2240], grad_fn=<UnbindBackward0>))\n",
      "(7.67786350067821, tensor([7.2587], grad_fn=<UnbindBackward0>))\n",
      "(7.145196134997171, tensor([8.1927], grad_fn=<UnbindBackward0>))\n",
      "(6.802394763324311, tensor([7.3108], grad_fn=<UnbindBackward0>))\n",
      "(8.662158961666423, tensor([8.3598], grad_fn=<UnbindBackward0>))\n",
      "(8.690642169706594, tensor([6.9013], grad_fn=<UnbindBackward0>))\n",
      "(8.033009498596668, tensor([7.3653], grad_fn=<UnbindBackward0>))\n",
      "(7.409136443920128, tensor([8.8258], grad_fn=<UnbindBackward0>))\n",
      "(8.171599480345463, tensor([8.7702], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([6.2162], grad_fn=<UnbindBackward0>))\n",
      "(6.950814768442584, tensor([9.3825], grad_fn=<UnbindBackward0>))\n",
      "(8.518791912779934, tensor([9.3419], grad_fn=<UnbindBackward0>))\n",
      "(9.284798282894192, tensor([6.6748], grad_fn=<UnbindBackward0>))\n",
      "(8.513385953073284, tensor([8.6243], grad_fn=<UnbindBackward0>))\n",
      "(7.707512194600341, tensor([6.9074], grad_fn=<UnbindBackward0>))\n",
      "(7.501082124259871, tensor([7.0573], grad_fn=<UnbindBackward0>))\n",
      "(7.829630389150193, tensor([6.7429], grad_fn=<UnbindBackward0>))\n",
      "(5.953243334287785, tensor([9.0325], grad_fn=<UnbindBackward0>))\n",
      "(8.54714026778419, tensor([8.5016], grad_fn=<UnbindBackward0>))\n",
      "(8.257644958208228, tensor([9.6912], grad_fn=<UnbindBackward0>))\n",
      "(7.659642954564682, tensor([7.8103], grad_fn=<UnbindBackward0>))\n",
      "(7.749322464660356, tensor([7.7658], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([7.4098], grad_fn=<UnbindBackward0>))\n",
      "(9.412219427985669, tensor([6.5840], grad_fn=<UnbindBackward0>))\n",
      "(8.39049553837028, tensor([7.7684], grad_fn=<UnbindBackward0>))\n",
      "(7.442492722794441, tensor([7.8879], grad_fn=<UnbindBackward0>))\n",
      "(8.34687925374656, tensor([8.2252], grad_fn=<UnbindBackward0>))\n",
      "(9.241063544619024, tensor([8.7687], grad_fn=<UnbindBackward0>))\n",
      "(8.327242607457793, tensor([6.2448], grad_fn=<UnbindBackward0>))\n",
      "(7.488293515159428, tensor([7.6852], grad_fn=<UnbindBackward0>))\n",
      "(9.616338671246005, tensor([6.2883], grad_fn=<UnbindBackward0>))\n",
      "(8.785998208098329, tensor([7.4251], grad_fn=<UnbindBackward0>))\n",
      "(8.471986598578159, tensor([8.6799], grad_fn=<UnbindBackward0>))\n",
      "(9.742673052026605, tensor([7.0898], grad_fn=<UnbindBackward0>))\n",
      "(6.902742737158593, tensor([7.7160], grad_fn=<UnbindBackward0>))\n",
      "(6.0112671744041615, tensor([7.2255], grad_fn=<UnbindBackward0>))\n",
      "(8.541690663016626, tensor([7.3509], grad_fn=<UnbindBackward0>))\n",
      "(9.212338374638856, tensor([7.5128], grad_fn=<UnbindBackward0>))\n",
      "(8.84779106484485, tensor([10.0621], grad_fn=<UnbindBackward0>))\n",
      "(8.094683648698815, tensor([9.5589], grad_fn=<UnbindBackward0>))\n",
      "(7.246368080102461, tensor([9.6423], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([7.0616], grad_fn=<UnbindBackward0>))\n",
      "(7.700295203420117, tensor([8.0776], grad_fn=<UnbindBackward0>))\n",
      "(9.08851166361105, tensor([7.7169], grad_fn=<UnbindBackward0>))\n",
      "(9.339612707680322, tensor([6.3942], grad_fn=<UnbindBackward0>))\n",
      "(7.749322464660356, tensor([9.2067], grad_fn=<UnbindBackward0>))\n",
      "(9.82978718960617, tensor([8.5723], grad_fn=<UnbindBackward0>))\n",
      "(9.756494528795468, tensor([8.2913], grad_fn=<UnbindBackward0>))\n",
      "(6.939253946041508, tensor([8.2550], grad_fn=<UnbindBackward0>))\n",
      "(8.056426767522984, tensor([9.9631], grad_fn=<UnbindBackward0>))\n",
      "(7.870165946469845, tensor([6.7010], grad_fn=<UnbindBackward0>))\n",
      "(9.257605564443297, tensor([6.3606], grad_fn=<UnbindBackward0>))\n",
      "(8.27308133366583, tensor([6.1615], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.1726], grad_fn=<UnbindBackward0>))\n",
      "(8.506940814951886, tensor([6.2460], grad_fn=<UnbindBackward0>))\n",
      "(8.704170559746386, tensor([7.9309], grad_fn=<UnbindBackward0>))\n",
      "(6.902742737158593, tensor([10.3294], grad_fn=<UnbindBackward0>))\n",
      "(7.543802867501509, tensor([9.5767], grad_fn=<UnbindBackward0>))\n",
      "(8.130059039992796, tensor([7.4550], grad_fn=<UnbindBackward0>))\n",
      "(7.798112628829788, tensor([7.2269], grad_fn=<UnbindBackward0>))\n",
      "(7.958576903813898, tensor([7.2036], grad_fn=<UnbindBackward0>))\n",
      "(9.566475170062406, tensor([7.0189], grad_fn=<UnbindBackward0>))\n",
      "(6.732210706467206, tensor([9.5573], grad_fn=<UnbindBackward0>))\n",
      "(6.320768294250582, tensor([8.4539], grad_fn=<UnbindBackward0>))\n",
      "(8.070906088787819, tensor([6.8028], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([8.1177], grad_fn=<UnbindBackward0>))\n",
      "(6.115892125483034, tensor([8.1987], grad_fn=<UnbindBackward0>))\n",
      "(7.606387389772652, tensor([9.3269], grad_fn=<UnbindBackward0>))\n",
      "(8.341410211461865, tensor([8.4817], grad_fn=<UnbindBackward0>))\n",
      "(8.293549515060345, tensor([6.2457], grad_fn=<UnbindBackward0>))\n",
      "(7.3783837129967145, tensor([6.6768], grad_fn=<UnbindBackward0>))\n",
      "(6.919683849847411, tensor([8.0062], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([6.2665], grad_fn=<UnbindBackward0>))\n",
      "(7.763446388727362, tensor([7.3463], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.7607], grad_fn=<UnbindBackward0>))\n",
      "(8.458716261657262, tensor([7.8760], grad_fn=<UnbindBackward0>))\n",
      "(7.787382026484701, tensor([9.1307], grad_fn=<UnbindBackward0>))\n",
      "(7.593877844605118, tensor([6.6570], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([7.1133], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([7.7515], grad_fn=<UnbindBackward0>))\n",
      "(8.09559870137819, tensor([8.4395], grad_fn=<UnbindBackward0>))\n",
      "(8.614501373883236, tensor([7.1759], grad_fn=<UnbindBackward0>))\n",
      "(8.842026529498812, tensor([8.5659], grad_fn=<UnbindBackward0>))\n",
      "(6.946013991099227, tensor([7.1123], grad_fn=<UnbindBackward0>))\n",
      "(9.41865450904439, tensor([9.3160], grad_fn=<UnbindBackward0>))\n",
      "(9.678968055041988, tensor([7.9068], grad_fn=<UnbindBackward0>))\n",
      "(8.97903863296051, tensor([6.3444], grad_fn=<UnbindBackward0>))\n",
      "(8.443977129084978, tensor([7.6657], grad_fn=<UnbindBackward0>))\n",
      "(6.853299093186078, tensor([8.0668], grad_fn=<UnbindBackward0>))\n",
      "(6.403574197934815, tensor([6.2576], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([6.4208], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([6.3689], grad_fn=<UnbindBackward0>))\n",
      "(7.972121128921655, tensor([10.1865], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([9.3393], grad_fn=<UnbindBackward0>))\n",
      "(7.496097345175956, tensor([6.8149], grad_fn=<UnbindBackward0>))\n",
      "(8.509362612301048, tensor([9.0628], grad_fn=<UnbindBackward0>))\n",
      "(7.616283561580385, tensor([5.9393], grad_fn=<UnbindBackward0>))\n",
      "(7.5883236773352225, tensor([6.8099], grad_fn=<UnbindBackward0>))\n",
      "(8.442038517815478, tensor([7.8467], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([8.5433], grad_fn=<UnbindBackward0>))\n",
      "(7.764720544771477, tensor([6.3622], grad_fn=<UnbindBackward0>))\n",
      "(6.126869184114185, tensor([6.7879], grad_fn=<UnbindBackward0>))\n",
      "(7.8009820712577405, tensor([8.3325], grad_fn=<UnbindBackward0>))\n",
      "(9.320628725870305, tensor([9.8852], grad_fn=<UnbindBackward0>))\n",
      "(9.778491062361443, tensor([6.2639], grad_fn=<UnbindBackward0>))\n",
      "(8.23323750070527, tensor([8.9934], grad_fn=<UnbindBackward0>))\n",
      "(8.646641258603124, tensor([7.2233], grad_fn=<UnbindBackward0>))\n",
      "(7.142827401161621, tensor([6.7169], grad_fn=<UnbindBackward0>))\n",
      "(7.508238774678663, tensor([8.6196], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([6.2076], grad_fn=<UnbindBackward0>))\n",
      "(7.681560362559537, tensor([8.0992], grad_fn=<UnbindBackward0>))\n",
      "(6.960347729101308, tensor([10.1556], grad_fn=<UnbindBackward0>))\n",
      "(7.267525427828172, tensor([7.8075], grad_fn=<UnbindBackward0>))\n",
      "(6.920671504248683, tensor([8.9330], grad_fn=<UnbindBackward0>))\n",
      "(8.278174290943738, tensor([8.5778], grad_fn=<UnbindBackward0>))\n",
      "(7.952263308657046, tensor([7.5560], grad_fn=<UnbindBackward0>))\n",
      "(7.552237287560802, tensor([8.0563], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([8.8835], grad_fn=<UnbindBackward0>))\n",
      "(8.118207049405783, tensor([7.1792], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([8.0457], grad_fn=<UnbindBackward0>))\n",
      "(7.853216388156072, tensor([7.9269], grad_fn=<UnbindBackward0>))\n",
      "(8.397508348470257, tensor([7.0801], grad_fn=<UnbindBackward0>))\n",
      "(8.374015421739909, tensor([7.8342], grad_fn=<UnbindBackward0>))\n",
      "(9.458917724547202, tensor([8.3129], grad_fn=<UnbindBackward0>))\n",
      "(8.314587291319576, tensor([9.3256], grad_fn=<UnbindBackward0>))\n",
      "(8.421562960400987, tensor([8.1583], grad_fn=<UnbindBackward0>))\n",
      "(7.829232537543592, tensor([8.8876], grad_fn=<UnbindBackward0>))\n",
      "(7.277247726631484, tensor([8.5939], grad_fn=<UnbindBackward0>))\n",
      "(6.240275845170769, tensor([6.6959], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.2909], grad_fn=<UnbindBackward0>))\n",
      "(8.114623886420098, tensor([6.2000], grad_fn=<UnbindBackward0>))\n",
      "(8.38662882139512, tensor([7.4069], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([6.0529], grad_fn=<UnbindBackward0>))\n",
      "(9.266909723464076, tensor([9.4884], grad_fn=<UnbindBackward0>))\n",
      "(7.96346006663897, tensor([7.0996], grad_fn=<UnbindBackward0>))\n",
      "(8.765302488748196, tensor([7.1901], grad_fn=<UnbindBackward0>))\n",
      "(8.55468163582723, tensor([6.7662], grad_fn=<UnbindBackward0>))\n",
      "(6.673297967767654, tensor([9.0625], grad_fn=<UnbindBackward0>))\n",
      "(6.728628613084702, tensor([6.8641], grad_fn=<UnbindBackward0>))\n",
      "(8.647343875881283, tensor([8.6311], grad_fn=<UnbindBackward0>))\n",
      "(8.477620416296414, tensor([8.7362], grad_fn=<UnbindBackward0>))\n",
      "(8.351138607086154, tensor([7.8630], grad_fn=<UnbindBackward0>))\n",
      "(8.81922185757494, tensor([9.0578], grad_fn=<UnbindBackward0>))\n",
      "(9.071308163280625, tensor([7.8496], grad_fn=<UnbindBackward0>))\n",
      "(5.908082938168931, tensor([9.0936], grad_fn=<UnbindBackward0>))\n",
      "(8.304742269640771, tensor([8.6155], grad_fn=<UnbindBackward0>))\n",
      "(9.133675287040697, tensor([8.6693], grad_fn=<UnbindBackward0>))\n",
      "(6.056784013228625, tensor([6.4244], grad_fn=<UnbindBackward0>))\n",
      "(7.575584651557793, tensor([9.8888], grad_fn=<UnbindBackward0>))\n",
      "(7.106606137727303, tensor([7.3118], grad_fn=<UnbindBackward0>))\n",
      "(7.582738488914411, tensor([7.8634], grad_fn=<UnbindBackward0>))\n",
      "(7.255591274253665, tensor([8.2319], grad_fn=<UnbindBackward0>))\n",
      "(6.520621127558696, tensor([8.5977], grad_fn=<UnbindBackward0>))\n",
      "(7.681099001536359, tensor([10.1342], grad_fn=<UnbindBackward0>))\n",
      "(7.755338812846501, tensor([9.3045], grad_fn=<UnbindBackward0>))\n",
      "(6.6895992691789665, tensor([8.4925], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([6.4730], grad_fn=<UnbindBackward0>))\n",
      "(8.550627967502475, tensor([8.6069], grad_fn=<UnbindBackward0>))\n",
      "(8.813884558025606, tensor([7.9995], grad_fn=<UnbindBackward0>))\n",
      "(7.949797216161852, tensor([8.9516], grad_fn=<UnbindBackward0>))\n",
      "(8.271292652979412, tensor([6.0087], grad_fn=<UnbindBackward0>))\n",
      "(8.46695197497949, tensor([8.7941], grad_fn=<UnbindBackward0>))\n",
      "(9.714685039642484, tensor([8.3265], grad_fn=<UnbindBackward0>))\n",
      "(8.70946507906336, tensor([8.4333], grad_fn=<UnbindBackward0>))\n",
      "(8.746080217357513, tensor([6.4111], grad_fn=<UnbindBackward0>))\n",
      "(6.472346294500901, tensor([7.3733], grad_fn=<UnbindBackward0>))\n",
      "(7.888709524182015, tensor([7.9243], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([9.1203], grad_fn=<UnbindBackward0>))\n",
      "(7.519149957669823, tensor([9.1734], grad_fn=<UnbindBackward0>))\n",
      "(9.792220740107542, tensor([6.7337], grad_fn=<UnbindBackward0>))\n",
      "(6.859614903654202, tensor([8.8812], grad_fn=<UnbindBackward0>))\n",
      "(9.785660765922685, tensor([8.5038], grad_fn=<UnbindBackward0>))\n",
      "(6.97166860472579, tensor([6.6231], grad_fn=<UnbindBackward0>))\n",
      "(5.966146739123692, tensor([9.0608], grad_fn=<UnbindBackward0>))\n",
      "(8.174984532943087, tensor([7.7424], grad_fn=<UnbindBackward0>))\n",
      "(7.611347717403621, tensor([6.6061], grad_fn=<UnbindBackward0>))\n",
      "(8.91972065553706, tensor([8.8887], grad_fn=<UnbindBackward0>))\n",
      "(7.2115567333138015, tensor([6.2743], grad_fn=<UnbindBackward0>))\n",
      "(6.802394763324311, tensor([8.5696], grad_fn=<UnbindBackward0>))\n",
      "(8.407601514786142, tensor([8.7516], grad_fn=<UnbindBackward0>))\n",
      "(6.890609120147166, tensor([7.8037], grad_fn=<UnbindBackward0>))\n",
      "(6.432940092739179, tensor([8.2672], grad_fn=<UnbindBackward0>))\n",
      "(9.310004695089129, tensor([8.0344], grad_fn=<UnbindBackward0>))\n",
      "(8.721602344674197, tensor([6.2162], grad_fn=<UnbindBackward0>))\n",
      "(8.329175442077402, tensor([6.2044], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([6.9877], grad_fn=<UnbindBackward0>))\n",
      "(8.797548488481558, tensor([6.7915], grad_fn=<UnbindBackward0>))\n",
      "(8.252706676567644, tensor([7.7402], grad_fn=<UnbindBackward0>))\n",
      "(6.8501261661455, tensor([9.2753], grad_fn=<UnbindBackward0>))\n",
      "(7.0909098220799835, tensor([6.3449], grad_fn=<UnbindBackward0>))\n",
      "(7.68937110752969, tensor([5.8088], grad_fn=<UnbindBackward0>))\n",
      "(8.666647144584575, tensor([6.7270], grad_fn=<UnbindBackward0>))\n",
      "(7.972810784121404, tensor([9.2723], grad_fn=<UnbindBackward0>))\n",
      "(8.418256443556213, tensor([7.5244], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([9.4778], grad_fn=<UnbindBackward0>))\n",
      "(9.564020836934587, tensor([8.3233], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([6.7701], grad_fn=<UnbindBackward0>))\n",
      "(6.93828448401696, tensor([5.9693], grad_fn=<UnbindBackward0>))\n",
      "(8.138564737261632, tensor([8.2898], grad_fn=<UnbindBackward0>))\n",
      "(6.556778356158042, tensor([8.9674], grad_fn=<UnbindBackward0>))\n",
      "(8.14002395246292, tensor([6.4277], grad_fn=<UnbindBackward0>))\n",
      "(9.391744841707654, tensor([8.7128], grad_fn=<UnbindBackward0>))\n",
      "(9.105646300861517, tensor([8.8791], grad_fn=<UnbindBackward0>))\n",
      "(8.943767262734637, tensor([8.7522], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([7.7914], grad_fn=<UnbindBackward0>))\n",
      "(9.355392643675321, tensor([6.6895], grad_fn=<UnbindBackward0>))\n",
      "(6.326149473155099, tensor([7.3095], grad_fn=<UnbindBackward0>))\n",
      "(8.687948111838727, tensor([6.4026], grad_fn=<UnbindBackward0>))\n",
      "(8.676075516476429, tensor([9.3999], grad_fn=<UnbindBackward0>))\n",
      "(7.396335293800808, tensor([6.2671], grad_fn=<UnbindBackward0>))\n",
      "(8.39049553837028, tensor([7.1909], grad_fn=<UnbindBackward0>))\n",
      "(8.886547412512042, tensor([7.2473], grad_fn=<UnbindBackward0>))\n",
      "(8.48879371689454, tensor([8.2172], grad_fn=<UnbindBackward0>))\n",
      "(8.236420527265391, tensor([7.0407], grad_fn=<UnbindBackward0>))\n",
      "(9.637044983076766, tensor([8.9628], grad_fn=<UnbindBackward0>))\n",
      "(7.969011781106478, tensor([8.6557], grad_fn=<UnbindBackward0>))\n",
      "(6.230481447578482, tensor([8.4518], grad_fn=<UnbindBackward0>))\n",
      "(7.631431664576906, tensor([7.8037], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([6.7866], grad_fn=<UnbindBackward0>))\n",
      "(6.4967749901858625, tensor([9.2929], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([7.4517], grad_fn=<UnbindBackward0>))\n",
      "(6.907755278982137, tensor([6.7952], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([7.2702], grad_fn=<UnbindBackward0>))\n",
      "(7.142036574706803, tensor([7.3306], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([8.3194], grad_fn=<UnbindBackward0>))\n",
      "(9.500768670095988, tensor([9.4196], grad_fn=<UnbindBackward0>))\n",
      "(8.065579427282092, tensor([9.0682], grad_fn=<UnbindBackward0>))\n",
      "(6.525029657843462, tensor([6.5851], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([6.3681], grad_fn=<UnbindBackward0>))\n",
      "(6.983789965258135, tensor([7.5791], grad_fn=<UnbindBackward0>))\n",
      "(8.189244525735901, tensor([7.3754], grad_fn=<UnbindBackward0>))\n",
      "(6.364750756851911, tensor([6.6219], grad_fn=<UnbindBackward0>))\n",
      "(6.990256500493881, tensor([7.1445], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([6.4964], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([8.2495], grad_fn=<UnbindBackward0>))\n",
      "(6.075346031088684, tensor([8.0773], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([8.6983], grad_fn=<UnbindBackward0>))\n",
      "(8.442038517815478, tensor([6.6848], grad_fn=<UnbindBackward0>))\n",
      "(8.882530508433623, tensor([6.9183], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.1584], grad_fn=<UnbindBackward0>))\n",
      "(9.679843876180236, tensor([7.3033], grad_fn=<UnbindBackward0>))\n",
      "(8.48528964240323, tensor([7.3980], grad_fn=<UnbindBackward0>))\n",
      "(8.333991247194975, tensor([5.9860], grad_fn=<UnbindBackward0>))\n",
      "(8.502891406705377, tensor([9.0838], grad_fn=<UnbindBackward0>))\n",
      "(8.545197387825835, tensor([9.4629], grad_fn=<UnbindBackward0>))\n",
      "(7.9294865233142895, tensor([6.3402], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([6.2492], grad_fn=<UnbindBackward0>))\n",
      "(6.885509670034818, tensor([8.5412], grad_fn=<UnbindBackward0>))\n",
      "(9.48029117039765, tensor([7.3312], grad_fn=<UnbindBackward0>))\n",
      "(6.866933284461882, tensor([7.8113], grad_fn=<UnbindBackward0>))\n",
      "(8.510369966068112, tensor([7.0234], grad_fn=<UnbindBackward0>))\n",
      "(9.053803514155955, tensor([8.0478], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([8.5125], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([6.5633], grad_fn=<UnbindBackward0>))\n",
      "(8.352790135124629, tensor([8.5517], grad_fn=<UnbindBackward0>))\n",
      "(8.916237731721479, tensor([9.3359], grad_fn=<UnbindBackward0>))\n",
      "(6.507277712385012, tensor([6.2965], grad_fn=<UnbindBackward0>))\n",
      "(6.835184586147301, tensor([9.5079], grad_fn=<UnbindBackward0>))\n",
      "(6.824373670043086, tensor([6.7026], grad_fn=<UnbindBackward0>))\n",
      "(7.994632311431825, tensor([7.3789], grad_fn=<UnbindBackward0>))\n",
      "(6.851184927493743, tensor([8.3547], grad_fn=<UnbindBackward0>))\n",
      "(7.00850518208228, tensor([7.9542], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([7.7219], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([9.6132], grad_fn=<UnbindBackward0>))\n",
      "(8.275376374836407, tensor([6.8320], grad_fn=<UnbindBackward0>))\n",
      "(7.002155954403621, tensor([7.2481], grad_fn=<UnbindBackward0>))\n",
      "(7.665753431861699, tensor([8.7727], grad_fn=<UnbindBackward0>))\n",
      "(6.501289670540389, tensor([7.8492], grad_fn=<UnbindBackward0>))\n",
      "(8.66544076787644, tensor([7.7512], grad_fn=<UnbindBackward0>))\n",
      "(9.218010878280403, tensor([8.0119], grad_fn=<UnbindBackward0>))\n",
      "(7.400009517162692, tensor([8.6102], grad_fn=<UnbindBackward0>))\n",
      "(6.452048954437226, tensor([7.6531], grad_fn=<UnbindBackward0>))\n",
      "(6.590301048196686, tensor([9.9830], grad_fn=<UnbindBackward0>))\n",
      "(7.760893195851024, tensor([6.2809], grad_fn=<UnbindBackward0>))\n",
      "(7.769378609513984, tensor([9.5379], grad_fn=<UnbindBackward0>))\n",
      "(8.428361977709622, tensor([7.3685], grad_fn=<UnbindBackward0>))\n",
      "(8.517593111437565, tensor([7.1664], grad_fn=<UnbindBackward0>))\n",
      "(9.118334726180159, tensor([8.9208], grad_fn=<UnbindBackward0>))\n",
      "(8.591001118560957, tensor([7.1849], grad_fn=<UnbindBackward0>))\n",
      "(8.703838719690246, tensor([7.6231], grad_fn=<UnbindBackward0>))\n",
      "(9.635085094816304, tensor([9.1507], grad_fn=<UnbindBackward0>))\n",
      "(7.249215057114389, tensor([10.0234], grad_fn=<UnbindBackward0>))\n",
      "(9.24483841238375, tensor([6.7862], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([7.4991], grad_fn=<UnbindBackward0>))\n",
      "(7.962415680121064, tensor([6.7243], grad_fn=<UnbindBackward0>))\n",
      "(7.99799931797973, tensor([8.2638], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([8.6190], grad_fn=<UnbindBackward0>))\n",
      "(9.07783702216309, tensor([7.9181], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([7.8967], grad_fn=<UnbindBackward0>))\n",
      "(6.9411900550683745, tensor([6.3655], grad_fn=<UnbindBackward0>))\n",
      "(7.374001859350161, tensor([7.2475], grad_fn=<UnbindBackward0>))\n",
      "(8.47595444339964, tensor([7.8250], grad_fn=<UnbindBackward0>))\n",
      "(7.487733761436444, tensor([6.2233], grad_fn=<UnbindBackward0>))\n",
      "(7.864419904994565, tensor([6.3170], grad_fn=<UnbindBackward0>))\n",
      "(6.9167150203536085, tensor([7.2710], grad_fn=<UnbindBackward0>))\n",
      "(9.431000842996585, tensor([6.1684], grad_fn=<UnbindBackward0>))\n",
      "(8.072779333169498, tensor([6.2908], grad_fn=<UnbindBackward0>))\n",
      "(9.291736010180177, tensor([8.7064], grad_fn=<UnbindBackward0>))\n",
      "(7.503840746698951, tensor([8.5607], grad_fn=<UnbindBackward0>))\n",
      "(6.326149473155099, tensor([8.3466], grad_fn=<UnbindBackward0>))\n",
      "(8.797548488481558, tensor([8.7337], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([7.8886], grad_fn=<UnbindBackward0>))\n",
      "(7.141245122350491, tensor([7.8886], grad_fn=<UnbindBackward0>))\n",
      "(7.080026499922591, tensor([6.3784], grad_fn=<UnbindBackward0>))\n",
      "(7.9208096792886, tensor([8.8539], grad_fn=<UnbindBackward0>))\n",
      "(9.726213236963687, tensor([9.4330], grad_fn=<UnbindBackward0>))\n",
      "(8.342601680684194, tensor([8.8459], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([9.6997], grad_fn=<UnbindBackward0>))\n",
      "(6.620073206530356, tensor([7.8151], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([7.8028], grad_fn=<UnbindBackward0>))\n",
      "(9.200391041122515, tensor([10.1111], grad_fn=<UnbindBackward0>))\n",
      "(7.615298339825815, tensor([8.9121], grad_fn=<UnbindBackward0>))\n",
      "(8.227108234348146, tensor([6.3760], grad_fn=<UnbindBackward0>))\n",
      "(7.148345743900068, tensor([7.3800], grad_fn=<UnbindBackward0>))\n",
      "(7.017506142941256, tensor([10.3793], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([8.9515], grad_fn=<UnbindBackward0>))\n",
      "(9.276689752517717, tensor([7.7771], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([7.1780], grad_fn=<UnbindBackward0>))\n",
      "(9.48311183169221, tensor([9.2305], grad_fn=<UnbindBackward0>))\n",
      "(9.187174093944343, tensor([6.6874], grad_fn=<UnbindBackward0>))\n",
      "(7.91644286012226, tensor([8.7485], grad_fn=<UnbindBackward0>))\n",
      "(8.418918622147897, tensor([8.9581], grad_fn=<UnbindBackward0>))\n",
      "(6.774223886357614, tensor([7.2973], grad_fn=<UnbindBackward0>))\n",
      "(8.097426298597213, tensor([9.4314], grad_fn=<UnbindBackward0>))\n",
      "(8.867990898182093, tensor([7.8742], grad_fn=<UnbindBackward0>))\n",
      "(9.319643106866632, tensor([6.9140], grad_fn=<UnbindBackward0>))\n",
      "(7.144407180321139, tensor([8.2733], grad_fn=<UnbindBackward0>))\n",
      "(7.5310163320779155, tensor([8.6165], grad_fn=<UnbindBackward0>))\n",
      "(9.749986661550366, tensor([8.0453], grad_fn=<UnbindBackward0>))\n",
      "(7.814399633804487, tensor([6.2812], grad_fn=<UnbindBackward0>))\n",
      "(8.315077007294104, tensor([8.9931], grad_fn=<UnbindBackward0>))\n",
      "(5.958424693029782, tensor([6.6781], grad_fn=<UnbindBackward0>))\n",
      "(8.430981494597171, tensor([8.7585], grad_fn=<UnbindBackward0>))\n",
      "(6.588926477533519, tensor([7.9288], grad_fn=<UnbindBackward0>))\n",
      "(7.69484807238461, tensor([6.4528], grad_fn=<UnbindBackward0>))\n",
      "(6.505784060128229, tensor([7.8201], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([8.5220], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([8.9859], grad_fn=<UnbindBackward0>))\n",
      "(7.755338812846501, tensor([8.5553], grad_fn=<UnbindBackward0>))\n",
      "(7.568379267836522, tensor([7.2292], grad_fn=<UnbindBackward0>))\n",
      "(7.708410667257367, tensor([7.1889], grad_fn=<UnbindBackward0>))\n",
      "(8.100161446936607, tensor([6.8943], grad_fn=<UnbindBackward0>))\n",
      "(8.609772372709331, tensor([8.8242], grad_fn=<UnbindBackward0>))\n",
      "(7.781138509845015, tensor([8.8410], grad_fn=<UnbindBackward0>))\n",
      "(6.889591308354466, tensor([8.1752], grad_fn=<UnbindBackward0>))\n",
      "(6.810142450115136, tensor([9.3163], grad_fn=<UnbindBackward0>))\n",
      "(7.68937110752969, tensor([7.9406], grad_fn=<UnbindBackward0>))\n",
      "(7.944492163932159, tensor([8.8722], grad_fn=<UnbindBackward0>))\n",
      "(8.323365694436081, tensor([10.1656], grad_fn=<UnbindBackward0>))\n",
      "(8.165647925297504, tensor([9.4156], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([7.7414], grad_fn=<UnbindBackward0>))\n",
      "(8.796943893541737, tensor([7.7130], grad_fn=<UnbindBackward0>))\n",
      "(9.061840363657739, tensor([7.3315], grad_fn=<UnbindBackward0>))\n",
      "(8.170468578330674, tensor([8.9097], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([8.4923], grad_fn=<UnbindBackward0>))\n",
      "(8.40200678160712, tensor([6.2002], grad_fn=<UnbindBackward0>))\n",
      "(6.118097198041348, tensor([8.7338], grad_fn=<UnbindBackward0>))\n",
      "(8.215276958936633, tensor([6.8095], grad_fn=<UnbindBackward0>))\n",
      "(8.385260520155413, tensor([9.2748], grad_fn=<UnbindBackward0>))\n",
      "(8.916774356365426, tensor([7.3058], grad_fn=<UnbindBackward0>))\n",
      "(7.477604243197589, tensor([7.9069], grad_fn=<UnbindBackward0>))\n",
      "(8.589699882202986, tensor([8.2774], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([6.7950], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([6.3085], grad_fn=<UnbindBackward0>))\n",
      "(6.115892125483034, tensor([6.1438], grad_fn=<UnbindBackward0>))\n",
      "(8.23217423638394, tensor([8.5618], grad_fn=<UnbindBackward0>))\n",
      "(9.413934064045677, tensor([6.6845], grad_fn=<UnbindBackward0>))\n",
      "(7.632885505395133, tensor([7.8955], grad_fn=<UnbindBackward0>))\n",
      "(6.386879319362645, tensor([6.2063], grad_fn=<UnbindBackward0>))\n",
      "(7.721791776817535, tensor([8.5962], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([7.9484], grad_fn=<UnbindBackward0>))\n",
      "(8.079308192051961, tensor([7.2720], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([8.4395], grad_fn=<UnbindBackward0>))\n",
      "(9.598184471326945, tensor([6.5884], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([7.4065], grad_fn=<UnbindBackward0>))\n",
      "(7.897296472595885, tensor([7.2252], grad_fn=<UnbindBackward0>))\n",
      "(9.839322295060342, tensor([9.1327], grad_fn=<UnbindBackward0>))\n",
      "(8.348064228408266, tensor([7.2806], grad_fn=<UnbindBackward0>))\n",
      "(7.936660155225426, tensor([6.5738], grad_fn=<UnbindBackward0>))\n",
      "(9.423029465386534, tensor([10.1792], grad_fn=<UnbindBackward0>))\n",
      "(9.457122321855792, tensor([6.7447], grad_fn=<UnbindBackward0>))\n",
      "(8.931419805192975, tensor([8.3619], grad_fn=<UnbindBackward0>))\n",
      "(9.614671504043145, tensor([7.2446], grad_fn=<UnbindBackward0>))\n",
      "(9.014447135152134, tensor([6.7433], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([6.2837], grad_fn=<UnbindBackward0>))\n",
      "(9.324561516066206, tensor([9.9182], grad_fn=<UnbindBackward0>))\n",
      "(8.451694209183541, tensor([7.8198], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([6.0037], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([9.3865], grad_fn=<UnbindBackward0>))\n",
      "(7.499423290592229, tensor([10.1572], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([7.6674], grad_fn=<UnbindBackward0>))\n",
      "(9.263786347681808, tensor([8.8540], grad_fn=<UnbindBackward0>))\n",
      "(9.58211080356025, tensor([6.8265], grad_fn=<UnbindBackward0>))\n",
      "(8.431417414394833, tensor([8.6298], grad_fn=<UnbindBackward0>))\n",
      "(8.933927891782632, tensor([6.2224], grad_fn=<UnbindBackward0>))\n",
      "(6.680854678790215, tensor([6.3065], grad_fn=<UnbindBackward0>))\n",
      "(7.149916836132109, tensor([7.3290], grad_fn=<UnbindBackward0>))\n",
      "(6.023447592961033, tensor([7.1939], grad_fn=<UnbindBackward0>))\n",
      "(8.550047528287184, tensor([9.4062], grad_fn=<UnbindBackward0>))\n",
      "(8.352318548226004, tensor([7.0915], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([9.3099], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([8.6589], grad_fn=<UnbindBackward0>))\n",
      "(7.149916836132109, tensor([5.8454], grad_fn=<UnbindBackward0>))\n",
      "(7.101675971619444, tensor([6.7881], grad_fn=<UnbindBackward0>))\n",
      "(8.507950610049305, tensor([6.3464], grad_fn=<UnbindBackward0>))\n",
      "(6.588926477533519, tensor([8.6983], grad_fn=<UnbindBackward0>))\n",
      "(6.891625897052253, tensor([6.3693], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([5.9868], grad_fn=<UnbindBackward0>))\n",
      "(7.7823903355874595, tensor([7.1354], grad_fn=<UnbindBackward0>))\n",
      "(8.165363632473982, tensor([7.4242], grad_fn=<UnbindBackward0>))\n",
      "(9.769155839740836, tensor([8.4263], grad_fn=<UnbindBackward0>))\n",
      "(9.63717550582054, tensor([6.7234], grad_fn=<UnbindBackward0>))\n",
      "(6.932447891572509, tensor([7.8578], grad_fn=<UnbindBackward0>))\n",
      "(8.247743887225516, tensor([6.6520], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([9.6553], grad_fn=<UnbindBackward0>))\n",
      "(7.168579897264035, tensor([8.9191], grad_fn=<UnbindBackward0>))\n",
      "(8.492285555710053, tensor([6.3930], grad_fn=<UnbindBackward0>))\n",
      "(8.908694592507015, tensor([6.2388], grad_fn=<UnbindBackward0>))\n",
      "(8.209308411646937, tensor([8.7009], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([6.5809], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([7.2435], grad_fn=<UnbindBackward0>))\n",
      "(6.18826412308259, tensor([7.8061], grad_fn=<UnbindBackward0>))\n",
      "(8.596189197642735, tensor([6.6247], grad_fn=<UnbindBackward0>))\n",
      "(9.081938657171658, tensor([9.6828], grad_fn=<UnbindBackward0>))\n",
      "(8.438149984075784, tensor([6.8313], grad_fn=<UnbindBackward0>))\n",
      "(8.245909264774093, tensor([7.9036], grad_fn=<UnbindBackward0>))\n",
      "(8.968396191198256, tensor([7.1998], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([8.8899], grad_fn=<UnbindBackward0>))\n",
      "(6.928537818164665, tensor([7.8397], grad_fn=<UnbindBackward0>))\n",
      "(8.952605375892354, tensor([8.5414], grad_fn=<UnbindBackward0>))\n",
      "(8.561592778712923, tensor([6.3178], grad_fn=<UnbindBackward0>))\n",
      "(8.77043908654689, tensor([8.7464], grad_fn=<UnbindBackward0>))\n",
      "(9.037652264150466, tensor([8.6002], grad_fn=<UnbindBackward0>))\n",
      "(6.690842277418564, tensor([7.7684], grad_fn=<UnbindBackward0>))\n",
      "(8.29154650988391, tensor([9.2443], grad_fn=<UnbindBackward0>))\n",
      "(7.273786317844895, tensor([7.0514], grad_fn=<UnbindBackward0>))\n",
      "(8.0925452638913, tensor([6.4346], grad_fn=<UnbindBackward0>))\n",
      "(6.964135612418245, tensor([8.7981], grad_fn=<UnbindBackward0>))\n",
      "(6.646390514847729, tensor([6.3718], grad_fn=<UnbindBackward0>))\n",
      "(7.016609683894219, tensor([7.1311], grad_fn=<UnbindBackward0>))\n",
      "(6.774223886357614, tensor([8.3890], grad_fn=<UnbindBackward0>))\n",
      "(7.97728198675515, tensor([8.6000], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([7.6320], grad_fn=<UnbindBackward0>))\n",
      "(6.726233402358747, tensor([8.5830], grad_fn=<UnbindBackward0>))\n",
      "(9.243581705755984, tensor([6.5825], grad_fn=<UnbindBackward0>))\n",
      "(5.8971538676367405, tensor([9.4373], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([7.7926], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([9.7581], grad_fn=<UnbindBackward0>))\n",
      "(7.465655310134056, tensor([9.5611], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([7.5685], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([7.1127], grad_fn=<UnbindBackward0>))\n",
      "(7.831220214604293, tensor([6.7921], grad_fn=<UnbindBackward0>))\n",
      "(7.974188669286011, tensor([5.9175], grad_fn=<UnbindBackward0>))\n",
      "(6.799055862058796, tensor([7.4125], grad_fn=<UnbindBackward0>))\n",
      "(6.326149473155099, tensor([7.9799], grad_fn=<UnbindBackward0>))\n",
      "(6.29156913955832, tensor([8.3792], grad_fn=<UnbindBackward0>))\n",
      "(9.561419689557756, tensor([9.4084], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([7.7484], grad_fn=<UnbindBackward0>))\n",
      "(9.037771106624906, tensor([8.5358], grad_fn=<UnbindBackward0>))\n",
      "(7.758760544157663, tensor([8.2584], grad_fn=<UnbindBackward0>))\n",
      "(6.214608098422191, tensor([8.4463], grad_fn=<UnbindBackward0>))\n",
      "(7.739794458408701, tensor([6.2853], grad_fn=<UnbindBackward0>))\n",
      "(6.366470447731438, tensor([6.3125], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([8.7602], grad_fn=<UnbindBackward0>))\n",
      "(6.831953565565855, tensor([8.8616], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([6.2251], grad_fn=<UnbindBackward0>))\n",
      "(8.751949058058614, tensor([6.9033], grad_fn=<UnbindBackward0>))\n",
      "(9.180705566846491, tensor([6.5768], grad_fn=<UnbindBackward0>))\n",
      "(9.436838291102966, tensor([7.7127], grad_fn=<UnbindBackward0>))\n",
      "(6.601230118728877, tensor([6.4665], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([6.4550], grad_fn=<UnbindBackward0>))\n",
      "(7.138866999945524, tensor([10.1295], grad_fn=<UnbindBackward0>))\n",
      "(7.132497551660044, tensor([6.7243], grad_fn=<UnbindBackward0>))\n",
      "(8.451053388911692, tensor([7.4651], grad_fn=<UnbindBackward0>))\n",
      "(8.74766979009724, tensor([6.6005], grad_fn=<UnbindBackward0>))\n",
      "(6.897704943128636, tensor([9.4727], grad_fn=<UnbindBackward0>))\n",
      "(8.871505346165781, tensor([7.8775], grad_fn=<UnbindBackward0>))\n",
      "(7.945555428253489, tensor([7.5180], grad_fn=<UnbindBackward0>))\n",
      "(9.019664010799474, tensor([6.8272], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([6.3693], grad_fn=<UnbindBackward0>))\n",
      "(8.571491964823617, tensor([6.7649], grad_fn=<UnbindBackward0>))\n",
      "(8.531096096585228, tensor([6.5566], grad_fn=<UnbindBackward0>))\n",
      "(7.655390644826152, tensor([6.2735], grad_fn=<UnbindBackward0>))\n",
      "(7.9157131993821155, tensor([6.3209], grad_fn=<UnbindBackward0>))\n",
      "(9.315690887584008, tensor([8.4430], grad_fn=<UnbindBackward0>))\n",
      "(9.512886426076506, tensor([8.9488], grad_fn=<UnbindBackward0>))\n",
      "(8.197263371414335, tensor([6.3747], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([8.5530], grad_fn=<UnbindBackward0>))\n",
      "(8.628913441026645, tensor([6.6132], grad_fn=<UnbindBackward0>))\n",
      "(9.10764297373784, tensor([7.9692], grad_fn=<UnbindBackward0>))\n",
      "(8.571870752706934, tensor([6.8946], grad_fn=<UnbindBackward0>))\n",
      "(8.353025845202325, tensor([6.5012], grad_fn=<UnbindBackward0>))\n",
      "(9.00380808646717, tensor([6.5167], grad_fn=<UnbindBackward0>))\n",
      "(7.176254532017144, tensor([7.5460], grad_fn=<UnbindBackward0>))\n",
      "(8.443546651247939, tensor([8.5449], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([7.8335], grad_fn=<UnbindBackward0>))\n",
      "(6.656726524178391, tensor([7.1290], grad_fn=<UnbindBackward0>))\n",
      "(7.049254841255837, tensor([7.7140], grad_fn=<UnbindBackward0>))\n",
      "(9.051344640485725, tensor([7.8074], grad_fn=<UnbindBackward0>))\n",
      "(9.057655284310535, tensor([8.3315], grad_fn=<UnbindBackward0>))\n",
      "(7.753623546559746, tensor([10.1559], grad_fn=<UnbindBackward0>))\n",
      "(6.903747257584598, tensor([8.1208], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([9.1540], grad_fn=<UnbindBackward0>))\n",
      "(8.544808358449211, tensor([6.7774], grad_fn=<UnbindBackward0>))\n",
      "(7.2196420401307355, tensor([8.5460], grad_fn=<UnbindBackward0>))\n",
      "(8.88016824790345, tensor([6.6729], grad_fn=<UnbindBackward0>))\n",
      "(7.37650812632622, tensor([7.3400], grad_fn=<UnbindBackward0>))\n",
      "(6.885509670034818, tensor([9.5351], grad_fn=<UnbindBackward0>))\n",
      "(9.223355309053677, tensor([6.7407], grad_fn=<UnbindBackward0>))\n",
      "(6.970730078143525, tensor([6.6499], grad_fn=<UnbindBackward0>))\n",
      "(8.512783482927537, tensor([9.9739], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.8809], grad_fn=<UnbindBackward0>))\n",
      "(7.242797922793756, tensor([6.4333], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([9.0297], grad_fn=<UnbindBackward0>))\n",
      "(9.211639527707803, tensor([8.9691], grad_fn=<UnbindBackward0>))\n",
      "(7.9854843567338225, tensor([8.5848], grad_fn=<UnbindBackward0>))\n",
      "(7.590852123688581, tensor([9.9102], grad_fn=<UnbindBackward0>))\n",
      "(6.802394763324311, tensor([8.5160], grad_fn=<UnbindBackward0>))\n",
      "(6.212606095751519, tensor([6.2398], grad_fn=<UnbindBackward0>))\n",
      "(8.603554357064281, tensor([6.4868], grad_fn=<UnbindBackward0>))\n",
      "(6.26530121273771, tensor([7.4782], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([6.4661], grad_fn=<UnbindBackward0>))\n",
      "(9.598048758495088, tensor([7.3332], grad_fn=<UnbindBackward0>))\n",
      "(7.01571242048723, tensor([6.8599], grad_fn=<UnbindBackward0>))\n",
      "(9.515027043243787, tensor([9.5708], grad_fn=<UnbindBackward0>))\n",
      "(8.64979915596426, tensor([8.4752], grad_fn=<UnbindBackward0>))\n",
      "(8.225503097566918, tensor([6.3530], grad_fn=<UnbindBackward0>))\n",
      "(9.565143565282119, tensor([6.3839], grad_fn=<UnbindBackward0>))\n",
      "(7.05617528410041, tensor([8.5622], grad_fn=<UnbindBackward0>))\n",
      "(7.939515260662406, tensor([7.9329], grad_fn=<UnbindBackward0>))\n",
      "(9.28933637820268, tensor([6.3577], grad_fn=<UnbindBackward0>))\n",
      "(8.423541635334782, tensor([9.3815], grad_fn=<UnbindBackward0>))\n",
      "(8.188133414510478, tensor([7.8625], grad_fn=<UnbindBackward0>))\n",
      "(9.625887815425859, tensor([8.1623], grad_fn=<UnbindBackward0>))\n",
      "(7.355641102974253, tensor([6.6812], grad_fn=<UnbindBackward0>))\n",
      "(7.438383530044307, tensor([8.7792], grad_fn=<UnbindBackward0>))\n",
      "(7.917171988845776, tensor([6.3625], grad_fn=<UnbindBackward0>))\n",
      "(8.249313746260636, tensor([8.4244], grad_fn=<UnbindBackward0>))\n",
      "(8.991313336173812, tensor([7.6894], grad_fn=<UnbindBackward0>))\n",
      "(8.45553053102413, tensor([8.8449], grad_fn=<UnbindBackward0>))\n",
      "(9.29090601981575, tensor([7.8299], grad_fn=<UnbindBackward0>))\n",
      "(8.699348067653093, tensor([7.1649], grad_fn=<UnbindBackward0>))\n",
      "(8.327000740241713, tensor([8.5876], grad_fn=<UnbindBackward0>))\n",
      "(8.931287626222456, tensor([6.0475], grad_fn=<UnbindBackward0>))\n",
      "(8.4252971767117, tensor([7.3050], grad_fn=<UnbindBackward0>))\n",
      "(8.46884293047519, tensor([7.0295], grad_fn=<UnbindBackward0>))\n",
      "(8.759197750371365, tensor([9.4050], grad_fn=<UnbindBackward0>))\n",
      "(8.776475789346321, tensor([9.3249], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([6.2733], grad_fn=<UnbindBackward0>))\n",
      "(7.1316985104669115, tensor([9.3665], grad_fn=<UnbindBackward0>))\n",
      "(5.986452005284438, tensor([6.3945], grad_fn=<UnbindBackward0>))\n",
      "(6.541029999189903, tensor([6.3145], grad_fn=<UnbindBackward0>))\n",
      "(7.252053951852814, tensor([8.5494], grad_fn=<UnbindBackward0>))\n",
      "(7.846980982138788, tensor([6.6638], grad_fn=<UnbindBackward0>))\n",
      "(8.680162019694377, tensor([6.1893], grad_fn=<UnbindBackward0>))\n",
      "(9.320270431348378, tensor([7.1220], grad_fn=<UnbindBackward0>))\n",
      "(7.528869256642251, tensor([7.6327], grad_fn=<UnbindBackward0>))\n",
      "(7.879291485082271, tensor([8.6023], grad_fn=<UnbindBackward0>))\n",
      "(9.429556219952412, tensor([7.1690], grad_fn=<UnbindBackward0>))\n",
      "(6.939253946041508, tensor([6.2621], grad_fn=<UnbindBackward0>))\n",
      "(9.289705927356923, tensor([8.7745], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([6.7113], grad_fn=<UnbindBackward0>))\n",
      "(7.526717561352706, tensor([6.7361], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([7.8007], grad_fn=<UnbindBackward0>))\n",
      "(6.826545223556594, tensor([8.6448], grad_fn=<UnbindBackward0>))\n",
      "(7.397561535524052, tensor([7.2475], grad_fn=<UnbindBackward0>))\n",
      "(8.140898460607852, tensor([8.7258], grad_fn=<UnbindBackward0>))\n",
      "(6.139884552226255, tensor([7.8715], grad_fn=<UnbindBackward0>))\n",
      "(6.903747257584598, tensor([8.4762], grad_fn=<UnbindBackward0>))\n",
      "(6.517671272912275, tensor([10.0366], grad_fn=<UnbindBackward0>))\n",
      "(9.301551252029567, tensor([7.8524], grad_fn=<UnbindBackward0>))\n",
      "(8.605936401250625, tensor([9.9093], grad_fn=<UnbindBackward0>))\n",
      "(8.236685322712457, tensor([6.2876], grad_fn=<UnbindBackward0>))\n",
      "(7.597897950521784, tensor([10.3941], grad_fn=<UnbindBackward0>))\n",
      "(7.789868559054706, tensor([6.4751], grad_fn=<UnbindBackward0>))\n",
      "(8.878497403738631, tensor([6.2128], grad_fn=<UnbindBackward0>))\n",
      "(9.814492390964565, tensor([6.2602], grad_fn=<UnbindBackward0>))\n",
      "(7.672292455628756, tensor([8.9426], grad_fn=<UnbindBackward0>))\n",
      "(9.740144754339955, tensor([9.0018], grad_fn=<UnbindBackward0>))\n",
      "(9.149421957006766, tensor([8.5739], grad_fn=<UnbindBackward0>))\n",
      "(7.347943823148687, tensor([9.5344], grad_fn=<UnbindBackward0>))\n",
      "(7.8551570058813445, tensor([6.8280], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.6090], grad_fn=<UnbindBackward0>))\n",
      "(8.392763113038061, tensor([9.1151], grad_fn=<UnbindBackward0>))\n",
      "(7.824445930877619, tensor([6.6534], grad_fn=<UnbindBackward0>))\n",
      "(7.368339686311381, tensor([8.6207], grad_fn=<UnbindBackward0>))\n",
      "(8.7157161275482, tensor([9.7935], grad_fn=<UnbindBackward0>))\n",
      "(7.064759027791802, tensor([8.0789], grad_fn=<UnbindBackward0>))\n",
      "(6.432940092739179, tensor([8.9665], grad_fn=<UnbindBackward0>))\n",
      "(8.517793011488205, tensor([6.7627], grad_fn=<UnbindBackward0>))\n",
      "(7.63433723562832, tensor([6.7922], grad_fn=<UnbindBackward0>))\n",
      "(7.491087593534876, tensor([7.7627], grad_fn=<UnbindBackward0>))\n",
      "(6.543911845564792, tensor([6.5784], grad_fn=<UnbindBackward0>))\n",
      "(7.795646536334594, tensor([8.0009], grad_fn=<UnbindBackward0>))\n",
      "(8.58223158759546, tensor([6.2777], grad_fn=<UnbindBackward0>))\n",
      "(7.940939762327791, tensor([9.7139], grad_fn=<UnbindBackward0>))\n",
      "(8.135639903354386, tensor([8.5713], grad_fn=<UnbindBackward0>))\n",
      "(7.1372784372603855, tensor([10.0088], grad_fn=<UnbindBackward0>))\n",
      "(6.263398262591624, tensor([9.6467], grad_fn=<UnbindBackward0>))\n",
      "(8.13622555490846, tensor([7.1789], grad_fn=<UnbindBackward0>))\n",
      "(9.0217190130337, tensor([9.2235], grad_fn=<UnbindBackward0>))\n",
      "(7.860956364876389, tensor([9.7200], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([9.6151], grad_fn=<UnbindBackward0>))\n",
      "(8.921857979353632, tensor([9.4631], grad_fn=<UnbindBackward0>))\n",
      "(9.684958338772663, tensor([9.3355], grad_fn=<UnbindBackward0>))\n",
      "(6.807934943699926, tensor([6.5712], grad_fn=<UnbindBackward0>))\n",
      "(6.1903154058531475, tensor([6.1123], grad_fn=<UnbindBackward0>))\n",
      "(7.204892510204673, tensor([6.6689], grad_fn=<UnbindBackward0>))\n",
      "(9.344521553442203, tensor([6.3414], grad_fn=<UnbindBackward0>))\n",
      "(9.50151633368222, tensor([8.3446], grad_fn=<UnbindBackward0>))\n",
      "(8.728749873478527, tensor([8.3867], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([6.2398], grad_fn=<UnbindBackward0>))\n",
      "(9.519588222342524, tensor([6.7167], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([8.6377], grad_fn=<UnbindBackward0>))\n",
      "(8.48673398393153, tensor([9.8814], grad_fn=<UnbindBackward0>))\n",
      "(8.529911963824013, tensor([6.3981], grad_fn=<UnbindBackward0>))\n",
      "(6.504288173536645, tensor([9.5208], grad_fn=<UnbindBackward0>))\n",
      "(8.138564737261632, tensor([8.3301], grad_fn=<UnbindBackward0>))\n",
      "(8.271804031154709, tensor([9.9726], grad_fn=<UnbindBackward0>))\n",
      "(7.488852955733459, tensor([6.3802], grad_fn=<UnbindBackward0>))\n",
      "(6.802394763324311, tensor([7.7955], grad_fn=<UnbindBackward0>))\n",
      "(7.470224135899966, tensor([8.6272], grad_fn=<UnbindBackward0>))\n",
      "(6.1092475827643655, tensor([6.8664], grad_fn=<UnbindBackward0>))\n",
      "(6.472346294500901, tensor([7.9055], grad_fn=<UnbindBackward0>))\n",
      "(8.332308352219117, tensor([8.4780], grad_fn=<UnbindBackward0>))\n",
      "(7.991253929840199, tensor([6.1983], grad_fn=<UnbindBackward0>))\n",
      "(8.626406276389554, tensor([6.2744], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([8.6795], grad_fn=<UnbindBackward0>))\n",
      "(9.835850903820317, tensor([8.6237], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([9.6319], grad_fn=<UnbindBackward0>))\n",
      "(9.337501517592862, tensor([10.5786], grad_fn=<UnbindBackward0>))\n",
      "(9.512738628263644, tensor([8.9347], grad_fn=<UnbindBackward0>))\n",
      "(7.860956364876389, tensor([7.8893], grad_fn=<UnbindBackward0>))\n",
      "(8.135639903354386, tensor([7.2420], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([7.5068], grad_fn=<UnbindBackward0>))\n",
      "(7.857093864902493, tensor([6.7533], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([9.6105], grad_fn=<UnbindBackward0>))\n",
      "(6.2878585601617845, tensor([8.7291], grad_fn=<UnbindBackward0>))\n",
      "(6.505784060128229, tensor([6.3036], grad_fn=<UnbindBackward0>))\n",
      "(9.09661160664784, tensor([6.3467], grad_fn=<UnbindBackward0>))\n",
      "(8.190077049719049, tensor([7.0455], grad_fn=<UnbindBackward0>))\n",
      "(7.080026499922591, tensor([7.4726], grad_fn=<UnbindBackward0>))\n",
      "(8.16194579946869, tensor([6.8225], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([7.9440], grad_fn=<UnbindBackward0>))\n",
      "(8.932740634865914, tensor([6.1364], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([7.5627], grad_fn=<UnbindBackward0>))\n",
      "(6.212606095751519, tensor([8.9998], grad_fn=<UnbindBackward0>))\n",
      "(8.435332164935916, tensor([9.5355], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([8.8590], grad_fn=<UnbindBackward0>))\n",
      "(6.118097198041348, tensor([6.2957], grad_fn=<UnbindBackward0>))\n",
      "(8.985445287623167, tensor([6.5989], grad_fn=<UnbindBackward0>))\n",
      "(7.798523053625206, tensor([6.5386], grad_fn=<UnbindBackward0>))\n",
      "(9.272375762895635, tensor([7.8263], grad_fn=<UnbindBackward0>))\n",
      "(9.4244028145542, tensor([6.6911], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([6.4883], grad_fn=<UnbindBackward0>))\n",
      "(6.505784060128229, tensor([7.2937], grad_fn=<UnbindBackward0>))\n",
      "(7.8826922062890254, tensor([7.4543], grad_fn=<UnbindBackward0>))\n",
      "(9.254452946594503, tensor([6.2067], grad_fn=<UnbindBackward0>))\n",
      "(8.289288323000317, tensor([7.8573], grad_fn=<UnbindBackward0>))\n",
      "(8.328692583545568, tensor([7.6052], grad_fn=<UnbindBackward0>))\n",
      "(8.580167990577626, tensor([8.9156], grad_fn=<UnbindBackward0>))\n",
      "(8.491465042843506, tensor([6.1829], grad_fn=<UnbindBackward0>))\n",
      "(8.109224953089955, tensor([7.8385], grad_fn=<UnbindBackward0>))\n",
      "(7.119635638017636, tensor([8.1506], grad_fn=<UnbindBackward0>))\n",
      "(8.731497794063243, tensor([8.3612], grad_fn=<UnbindBackward0>))\n",
      "(7.903596289614301, tensor([8.2786], grad_fn=<UnbindBackward0>))\n",
      "(7.0967213784947605, tensor([9.5931], grad_fn=<UnbindBackward0>))\n",
      "(8.371936178759098, tensor([8.7492], grad_fn=<UnbindBackward0>))\n",
      "(8.244071270295786, tensor([9.5987], grad_fn=<UnbindBackward0>))\n",
      "(7.68937110752969, tensor([7.5225], grad_fn=<UnbindBackward0>))\n",
      "(9.200492035921368, tensor([9.5571], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([7.3931], grad_fn=<UnbindBackward0>))\n",
      "(8.104703468371108, tensor([7.1451], grad_fn=<UnbindBackward0>))\n",
      "(8.880863609867356, tensor([6.3466], grad_fn=<UnbindBackward0>))\n",
      "(6.169610732491456, tensor([8.9409], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([7.3883], grad_fn=<UnbindBackward0>))\n",
      "(8.730851903519232, tensor([7.2616], grad_fn=<UnbindBackward0>))\n",
      "(9.101640955052842, tensor([7.1130], grad_fn=<UnbindBackward0>))\n",
      "(9.153664194782626, tensor([6.8956], grad_fn=<UnbindBackward0>))\n",
      "(8.137103389639302, tensor([6.4068], grad_fn=<UnbindBackward0>))\n",
      "(9.059168584174444, tensor([6.2952], grad_fn=<UnbindBackward0>))\n",
      "(7.765569081097317, tensor([7.2491], grad_fn=<UnbindBackward0>))\n",
      "(5.908082938168931, tensor([6.3106], grad_fn=<UnbindBackward0>))\n",
      "(7.909489492673759, tensor([6.2373], grad_fn=<UnbindBackward0>))\n",
      "(8.368693183097793, tensor([6.7020], grad_fn=<UnbindBackward0>))\n",
      "(6.960347729101308, tensor([6.7774], grad_fn=<UnbindBackward0>))\n",
      "(8.981430225767635, tensor([7.8791], grad_fn=<UnbindBackward0>))\n",
      "(8.542470998600505, tensor([9.3104], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([7.9665], grad_fn=<UnbindBackward0>))\n",
      "(9.811207776416833, tensor([7.9102], grad_fn=<UnbindBackward0>))\n",
      "(6.416732282512326, tensor([10.0653], grad_fn=<UnbindBackward0>))\n",
      "(8.223090551161533, tensor([7.4483], grad_fn=<UnbindBackward0>))\n",
      "(8.744009988096744, tensor([7.0179], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([7.4044], grad_fn=<UnbindBackward0>))\n",
      "(9.27453508401818, tensor([9.5552], grad_fn=<UnbindBackward0>))\n",
      "(6.135564891081739, tensor([6.3415], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([9.3946], grad_fn=<UnbindBackward0>))\n",
      "(9.311361486034912, tensor([9.6178], grad_fn=<UnbindBackward0>))\n",
      "(8.616314282284044, tensor([8.1753], grad_fn=<UnbindBackward0>))\n",
      "(8.368925174747135, tensor([6.3174], grad_fn=<UnbindBackward0>))\n",
      "(7.616283561580385, tensor([8.9075], grad_fn=<UnbindBackward0>))\n",
      "(9.116688815894728, tensor([7.0289], grad_fn=<UnbindBackward0>))\n",
      "(6.486160788944089, tensor([7.2356], grad_fn=<UnbindBackward0>))\n",
      "(8.017637159908478, tensor([7.9194], grad_fn=<UnbindBackward0>))\n",
      "(6.150602768446279, tensor([9.3613], grad_fn=<UnbindBackward0>))\n",
      "(9.122164681072457, tensor([6.7413], grad_fn=<UnbindBackward0>))\n",
      "(6.923628628138427, tensor([8.1401], grad_fn=<UnbindBackward0>))\n",
      "(8.358431899031295, tensor([5.9264], grad_fn=<UnbindBackward0>))\n",
      "(6.883462586413092, tensor([7.8256], grad_fn=<UnbindBackward0>))\n",
      "(8.295049140435111, tensor([9.1501], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([8.6921], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([8.5407], grad_fn=<UnbindBackward0>))\n",
      "(8.296297112642508, tensor([7.1854], grad_fn=<UnbindBackward0>))\n",
      "(7.389563953677635, tensor([6.7074], grad_fn=<UnbindBackward0>))\n",
      "(7.07326971745971, tensor([6.9397], grad_fn=<UnbindBackward0>))\n",
      "(8.727454116899434, tensor([6.8250], grad_fn=<UnbindBackward0>))\n",
      "(7.387090235656757, tensor([6.3805], grad_fn=<UnbindBackward0>))\n",
      "(6.558197802812269, tensor([9.3631], grad_fn=<UnbindBackward0>))\n",
      "(6.811244378601294, tensor([7.4061], grad_fn=<UnbindBackward0>))\n",
      "(8.025189321890835, tensor([7.3798], grad_fn=<UnbindBackward0>))\n",
      "(7.6324011266014535, tensor([9.0145], grad_fn=<UnbindBackward0>))\n",
      "(6.903747257584598, tensor([9.3028], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.5125], grad_fn=<UnbindBackward0>))\n",
      "(6.841615476477592, tensor([8.5320], grad_fn=<UnbindBackward0>))\n",
      "(8.57583938684897, tensor([6.4991], grad_fn=<UnbindBackward0>))\n",
      "(8.294799358992574, tensor([6.6925], grad_fn=<UnbindBackward0>))\n",
      "(6.248042874508429, tensor([7.2387], grad_fn=<UnbindBackward0>))\n",
      "(8.411610428841172, tensor([6.3792], grad_fn=<UnbindBackward0>))\n",
      "(8.772145439245099, tensor([7.3133], grad_fn=<UnbindBackward0>))\n",
      "(7.401231264413015, tensor([8.5380], grad_fn=<UnbindBackward0>))\n",
      "(8.515391569469609, tensor([6.8595], grad_fn=<UnbindBackward0>))\n",
      "(8.217708406845306, tensor([6.8134], grad_fn=<UnbindBackward0>))\n",
      "(6.802394763324311, tensor([7.7315], grad_fn=<UnbindBackward0>))\n",
      "(7.9391588179567965, tensor([9.6181], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([6.6000], grad_fn=<UnbindBackward0>))\n",
      "(6.90875477931522, tensor([8.6766], grad_fn=<UnbindBackward0>))\n",
      "(6.042632833682381, tensor([8.3608], grad_fn=<UnbindBackward0>))\n",
      "(6.126869184114185, tensor([8.3114], grad_fn=<UnbindBackward0>))\n",
      "(8.350666240520924, tensor([7.2923], grad_fn=<UnbindBackward0>))\n",
      "(8.322880021769905, tensor([9.0368], grad_fn=<UnbindBackward0>))\n",
      "(7.437206366871292, tensor([7.6207], grad_fn=<UnbindBackward0>))\n",
      "(9.322686431807721, tensor([6.9569], grad_fn=<UnbindBackward0>))\n",
      "(9.282940064390527, tensor([8.0994], grad_fn=<UnbindBackward0>))\n",
      "(8.760923376338836, tensor([6.3573], grad_fn=<UnbindBackward0>))\n",
      "(9.251866118804681, tensor([6.8451], grad_fn=<UnbindBackward0>))\n",
      "(7.994969522697877, tensor([6.2662], grad_fn=<UnbindBackward0>))\n",
      "(7.6787889981991535, tensor([8.7934], grad_fn=<UnbindBackward0>))\n",
      "(7.443663683115591, tensor([7.4869], grad_fn=<UnbindBackward0>))\n",
      "(7.244941546337007, tensor([8.7851], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.3295], grad_fn=<UnbindBackward0>))\n",
      "(6.144185634125646, tensor([6.4044], grad_fn=<UnbindBackward0>))\n",
      "(8.541690663016626, tensor([6.6905], grad_fn=<UnbindBackward0>))\n",
      "(8.720134035412928, tensor([8.9916], grad_fn=<UnbindBackward0>))\n",
      "(8.334951631422454, tensor([7.4472], grad_fn=<UnbindBackward0>))\n",
      "(8.68710472813351, tensor([7.4123], grad_fn=<UnbindBackward0>))\n",
      "(8.411388132519262, tensor([8.4651], grad_fn=<UnbindBackward0>))\n",
      "(9.614070642790631, tensor([7.5123], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([6.8632], grad_fn=<UnbindBackward0>))\n",
      "(7.396335293800808, tensor([6.6401], grad_fn=<UnbindBackward0>))\n",
      "(8.510772623613315, tensor([7.6646], grad_fn=<UnbindBackward0>))\n",
      "(7.8713112033234065, tensor([6.7192], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([7.9395], grad_fn=<UnbindBackward0>))\n",
      "(9.137984709784043, tensor([7.9860], grad_fn=<UnbindBackward0>))\n",
      "(8.349484346990128, tensor([7.1992], grad_fn=<UnbindBackward0>))\n",
      "(7.424761761823209, tensor([6.4332], grad_fn=<UnbindBackward0>))\n",
      "(7.928045600874777, tensor([6.3670], grad_fn=<UnbindBackward0>))\n",
      "(7.75491027202143, tensor([9.5883], grad_fn=<UnbindBackward0>))\n",
      "(6.1070228877422545, tensor([9.4878], grad_fn=<UnbindBackward0>))\n",
      "(8.251663923605589, tensor([6.2505], grad_fn=<UnbindBackward0>))\n",
      "(8.868694776580972, tensor([8.3668], grad_fn=<UnbindBackward0>))\n",
      "(6.755768921984255, tensor([8.5442], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.1600], grad_fn=<UnbindBackward0>))\n",
      "(7.573017256052546, tensor([7.3453], grad_fn=<UnbindBackward0>))\n",
      "(9.203013596589722, tensor([8.8904], grad_fn=<UnbindBackward0>))\n",
      "(7.723120092266331, tensor([9.2784], grad_fn=<UnbindBackward0>))\n",
      "(7.411556287811163, tensor([6.1475], grad_fn=<UnbindBackward0>))\n",
      "(8.36474106822456, tensor([9.2828], grad_fn=<UnbindBackward0>))\n",
      "(7.508238774678663, tensor([6.2275], grad_fn=<UnbindBackward0>))\n",
      "(6.826545223556594, tensor([6.8038], grad_fn=<UnbindBackward0>))\n",
      "(8.453827315794417, tensor([5.8191], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([7.9231], grad_fn=<UnbindBackward0>))\n",
      "(9.576856729945773, tensor([8.4030], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([7.2193], grad_fn=<UnbindBackward0>))\n",
      "(6.52649485957079, tensor([6.4196], grad_fn=<UnbindBackward0>))\n",
      "(6.880384082186005, tensor([5.8209], grad_fn=<UnbindBackward0>))\n",
      "(9.365462008843483, tensor([8.0149], grad_fn=<UnbindBackward0>))\n",
      "(8.041091003708633, tensor([7.9778], grad_fn=<UnbindBackward0>))\n",
      "(8.617038526385954, tensor([7.8753], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([6.1097], grad_fn=<UnbindBackward0>))\n",
      "(8.700847193443972, tensor([9.9051], grad_fn=<UnbindBackward0>))\n",
      "(7.114769448366463, tensor([8.5532], grad_fn=<UnbindBackward0>))\n",
      "(7.818027938530729, tensor([7.8547], grad_fn=<UnbindBackward0>))\n",
      "(9.065083359319043, tensor([8.9454], grad_fn=<UnbindBackward0>))\n",
      "(9.328656598794225, tensor([7.3165], grad_fn=<UnbindBackward0>))\n",
      "(6.78332520060396, tensor([6.8257], grad_fn=<UnbindBackward0>))\n",
      "(8.139148678884066, tensor([7.7936], grad_fn=<UnbindBackward0>))\n",
      "(6.1092475827643655, tensor([6.7296], grad_fn=<UnbindBackward0>))\n",
      "(7.92443418488756, tensor([6.7720], grad_fn=<UnbindBackward0>))\n",
      "(6.966967138613983, tensor([8.8192], grad_fn=<UnbindBackward0>))\n",
      "(8.292047637431354, tensor([6.3496], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([6.6164], grad_fn=<UnbindBackward0>))\n",
      "(9.760194382709651, tensor([6.6815], grad_fn=<UnbindBackward0>))\n",
      "(8.89439598980643, tensor([7.1844], grad_fn=<UnbindBackward0>))\n",
      "(6.327936783729195, tensor([9.2091], grad_fn=<UnbindBackward0>))\n",
      "(9.2299469016151, tensor([6.2553], grad_fn=<UnbindBackward0>))\n",
      "(9.24319470884713, tensor([8.7339], grad_fn=<UnbindBackward0>))\n",
      "(7.516433302915632, tensor([6.7058], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([6.2787], grad_fn=<UnbindBackward0>))\n",
      "(8.30325712085294, tensor([7.4019], grad_fn=<UnbindBackward0>))\n",
      "(7.738923757439457, tensor([6.1831], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([6.3641], grad_fn=<UnbindBackward0>))\n",
      "(6.741700694652055, tensor([7.7824], grad_fn=<UnbindBackward0>))\n",
      "(7.964503363551548, tensor([8.3523], grad_fn=<UnbindBackward0>))\n",
      "(8.080237416216702, tensor([9.9808], grad_fn=<UnbindBackward0>))\n",
      "(6.118097198041348, tensor([7.7765], grad_fn=<UnbindBackward0>))\n",
      "(8.551014739891748, tensor([8.8059], grad_fn=<UnbindBackward0>))\n",
      "(8.909235279192261, tensor([8.7439], grad_fn=<UnbindBackward0>))\n",
      "(9.101640955052842, tensor([8.4483], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([8.4773], grad_fn=<UnbindBackward0>))\n",
      "(9.516279760698795, tensor([9.4433], grad_fn=<UnbindBackward0>))\n",
      "(6.679599185844383, tensor([8.2681], grad_fn=<UnbindBackward0>))\n",
      "(6.891625897052253, tensor([6.5333], grad_fn=<UnbindBackward0>))\n",
      "(6.124683390894205, tensor([7.7815], grad_fn=<UnbindBackward0>))\n",
      "(7.861341795599989, tensor([8.1411], grad_fn=<UnbindBackward0>))\n",
      "(7.583756300707112, tensor([8.6523], grad_fn=<UnbindBackward0>))\n",
      "(6.075346031088684, tensor([7.8076], grad_fn=<UnbindBackward0>))\n",
      "(8.42222295382501, tensor([8.6168], grad_fn=<UnbindBackward0>))\n",
      "(9.290629203204986, tensor([8.5249], grad_fn=<UnbindBackward0>))\n",
      "(8.255308811785596, tensor([8.0721], grad_fn=<UnbindBackward0>))\n",
      "(8.506536611227709, tensor([6.5438], grad_fn=<UnbindBackward0>))\n",
      "(9.808462332127018, tensor([8.4857], grad_fn=<UnbindBackward0>))\n",
      "(9.184612223403452, tensor([8.7140], grad_fn=<UnbindBackward0>))\n",
      "(6.306275286948016, tensor([8.1096], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([7.3264], grad_fn=<UnbindBackward0>))\n",
      "(8.252967195000798, tensor([8.3549], grad_fn=<UnbindBackward0>))\n",
      "(8.25088114470065, tensor([7.8496], grad_fn=<UnbindBackward0>))\n",
      "(6.532334292222349, tensor([6.1556], grad_fn=<UnbindBackward0>))\n",
      "(8.343077871169383, tensor([6.6350], grad_fn=<UnbindBackward0>))\n",
      "(8.995040974685022, tensor([6.2681], grad_fn=<UnbindBackward0>))\n",
      "(7.9561263512135, tensor([8.1115], grad_fn=<UnbindBackward0>))\n",
      "(8.374015421739909, tensor([6.4269], grad_fn=<UnbindBackward0>))\n",
      "(7.821242083523558, tensor([8.9971], grad_fn=<UnbindBackward0>))\n",
      "(8.402679804627477, tensor([7.3889], grad_fn=<UnbindBackward0>))\n",
      "(8.690810307580046, tensor([5.5314], grad_fn=<UnbindBackward0>))\n",
      "(7.845416036592485, tensor([8.7189], grad_fn=<UnbindBackward0>))\n",
      "(9.287764268940986, tensor([6.8358], grad_fn=<UnbindBackward0>))\n",
      "(8.383890344101816, tensor([6.2975], grad_fn=<UnbindBackward0>))\n",
      "(7.9617188159813645, tensor([8.2565], grad_fn=<UnbindBackward0>))\n",
      "(9.081938657171658, tensor([6.5984], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([6.2775], grad_fn=<UnbindBackward0>))\n",
      "(5.910796644040527, tensor([8.1524], grad_fn=<UnbindBackward0>))\n",
      "(8.91139510724569, tensor([6.2876], grad_fn=<UnbindBackward0>))\n",
      "(8.012680929706839, tensor([8.6866], grad_fn=<UnbindBackward0>))\n",
      "(7.596894438144544, tensor([9.7337], grad_fn=<UnbindBackward0>))\n",
      "(9.448333126140598, tensor([5.8272], grad_fn=<UnbindBackward0>))\n",
      "(8.412721169819527, tensor([7.7879], grad_fn=<UnbindBackward0>))\n",
      "(6.950814768442584, tensor([6.3154], grad_fn=<UnbindBackward0>))\n",
      "(9.079889942592118, tensor([8.6706], grad_fn=<UnbindBackward0>))\n",
      "(8.475537516147405, tensor([9.5094], grad_fn=<UnbindBackward0>))\n",
      "(8.392536586816682, tensor([6.2410], grad_fn=<UnbindBackward0>))\n",
      "(7.77863014732581, tensor([8.7380], grad_fn=<UnbindBackward0>))\n",
      "(8.675222055641148, tensor([8.6571], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([7.4209], grad_fn=<UnbindBackward0>))\n",
      "(5.971261839790462, tensor([7.7585], grad_fn=<UnbindBackward0>))\n",
      "(7.633853559681768, tensor([7.4357], grad_fn=<UnbindBackward0>))\n",
      "(8.925454386826402, tensor([6.2640], grad_fn=<UnbindBackward0>))\n",
      "(7.8582541821860294, tensor([6.2415], grad_fn=<UnbindBackward0>))\n",
      "(8.129469764784231, tensor([7.2785], grad_fn=<UnbindBackward0>))\n",
      "(7.776954403322442, tensor([8.8781], grad_fn=<UnbindBackward0>))\n",
      "(8.297294370266917, tensor([9.2955], grad_fn=<UnbindBackward0>))\n",
      "(8.562740006372207, tensor([7.3468], grad_fn=<UnbindBackward0>))\n",
      "(9.038365107236372, tensor([8.6528], grad_fn=<UnbindBackward0>))\n",
      "(7.7226775164680035, tensor([10.0132], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([6.3423], grad_fn=<UnbindBackward0>))\n",
      "(7.43543801981455, tensor([8.1181], grad_fn=<UnbindBackward0>))\n",
      "(7.044032897274685, tensor([6.7858], grad_fn=<UnbindBackward0>))\n",
      "(8.631592731724734, tensor([6.4155], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([7.7557], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([7.5095], grad_fn=<UnbindBackward0>))\n",
      "(9.081824949740755, tensor([8.9097], grad_fn=<UnbindBackward0>))\n",
      "(9.414912537704957, tensor([8.4320], grad_fn=<UnbindBackward0>))\n",
      "(7.748028524432376, tensor([9.2325], grad_fn=<UnbindBackward0>))\n",
      "(8.268475388982598, tensor([7.8669], grad_fn=<UnbindBackward0>))\n",
      "(8.383890344101816, tensor([9.3761], grad_fn=<UnbindBackward0>))\n",
      "(7.174724309836376, tensor([6.7422], grad_fn=<UnbindBackward0>))\n",
      "(7.430707082545968, tensor([8.6123], grad_fn=<UnbindBackward0>))\n",
      "(7.952263308657046, tensor([6.4179], grad_fn=<UnbindBackward0>))\n",
      "(8.453827315794417, tensor([8.7752], grad_fn=<UnbindBackward0>))\n",
      "(8.45638105201948, tensor([7.4033], grad_fn=<UnbindBackward0>))\n",
      "(8.120291313968561, tensor([7.7245], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([6.6414], grad_fn=<UnbindBackward0>))\n",
      "(8.854664928370534, tensor([6.3532], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([6.6260], grad_fn=<UnbindBackward0>))\n",
      "(8.407601514786142, tensor([6.5126], grad_fn=<UnbindBackward0>))\n",
      "(8.793156870913819, tensor([6.3050], grad_fn=<UnbindBackward0>))\n",
      "(5.966146739123692, tensor([7.0859], grad_fn=<UnbindBackward0>))\n",
      "(8.062432791583195, tensor([7.7630], grad_fn=<UnbindBackward0>))\n",
      "(7.659642954564682, tensor([6.7944], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([9.8324], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([7.7943], grad_fn=<UnbindBackward0>))\n",
      "(8.700014623251842, tensor([6.7536], grad_fn=<UnbindBackward0>))\n",
      "(8.338544879988579, tensor([6.2243], grad_fn=<UnbindBackward0>))\n",
      "(7.203405521083095, tensor([8.5586], grad_fn=<UnbindBackward0>))\n",
      "(6.715383386334681, tensor([6.4904], grad_fn=<UnbindBackward0>))\n",
      "(8.548110294050959, tensor([6.3279], grad_fn=<UnbindBackward0>))\n",
      "(8.916506080039204, tensor([6.3284], grad_fn=<UnbindBackward0>))\n",
      "(8.771835409789817, tensor([7.4123], grad_fn=<UnbindBackward0>))\n",
      "(7.16703787691222, tensor([9.2265], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([8.2020], grad_fn=<UnbindBackward0>))\n",
      "(7.757051142032013, tensor([8.9360], grad_fn=<UnbindBackward0>))\n",
      "(6.8690144506657065, tensor([7.1318], grad_fn=<UnbindBackward0>))\n",
      "(7.730174795246222, tensor([9.0054], grad_fn=<UnbindBackward0>))\n",
      "(8.593042503699674, tensor([8.4802], grad_fn=<UnbindBackward0>))\n",
      "(7.729735331385051, tensor([7.7936], grad_fn=<UnbindBackward0>))\n",
      "(9.688436171119255, tensor([7.7805], grad_fn=<UnbindBackward0>))\n",
      "(8.481980435660493, tensor([8.7793], grad_fn=<UnbindBackward0>))\n",
      "(6.669498089857879, tensor([8.4250], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([7.7450], grad_fn=<UnbindBackward0>))\n",
      "(8.343077871169383, tensor([8.8124], grad_fn=<UnbindBackward0>))\n",
      "(7.785305182539862, tensor([9.1143], grad_fn=<UnbindBackward0>))\n",
      "(7.428333194190806, tensor([7.4266], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([6.2858], grad_fn=<UnbindBackward0>))\n",
      "(8.182559264068665, tensor([7.7192], grad_fn=<UnbindBackward0>))\n",
      "(6.878326468291325, tensor([8.9698], grad_fn=<UnbindBackward0>))\n",
      "(6.834108738813838, tensor([6.6836], grad_fn=<UnbindBackward0>))\n",
      "(7.9441374911141125, tensor([7.1202], grad_fn=<UnbindBackward0>))\n",
      "(9.806425839692997, tensor([6.6632], grad_fn=<UnbindBackward0>))\n",
      "(6.9584483932976555, tensor([8.1030], grad_fn=<UnbindBackward0>))\n",
      "(7.956827122090111, tensor([6.5975], grad_fn=<UnbindBackward0>))\n",
      "(8.062432791583195, tensor([9.0049], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([7.3436], grad_fn=<UnbindBackward0>))\n",
      "(8.629807335785372, tensor([6.8255], grad_fn=<UnbindBackward0>))\n",
      "(8.901911226379612, tensor([7.3726], grad_fn=<UnbindBackward0>))\n",
      "(7.783640596221253, tensor([6.8814], grad_fn=<UnbindBackward0>))\n",
      "(8.263848131368906, tensor([6.1616], grad_fn=<UnbindBackward0>))\n",
      "(7.416378479192928, tensor([7.5608], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([6.1941], grad_fn=<UnbindBackward0>))\n",
      "(9.459541457609681, tensor([8.6650], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([8.8721], grad_fn=<UnbindBackward0>))\n",
      "(6.881411303642535, tensor([8.8569], grad_fn=<UnbindBackward0>))\n",
      "(8.236155661683124, tensor([6.1811], grad_fn=<UnbindBackward0>))\n",
      "(7.842278779117352, tensor([6.1832], grad_fn=<UnbindBackward0>))\n",
      "(9.537699784080726, tensor([7.9163], grad_fn=<UnbindBackward0>))\n",
      "(7.272398392570047, tensor([6.2430], grad_fn=<UnbindBackward0>))\n",
      "(8.371010681238156, tensor([6.8389], grad_fn=<UnbindBackward0>))\n",
      "(8.39479954320217, tensor([6.2890], grad_fn=<UnbindBackward0>))\n",
      "(8.40200678160712, tensor([7.2262], grad_fn=<UnbindBackward0>))\n",
      "(8.067462667010057, tensor([6.4061], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([8.4457], grad_fn=<UnbindBackward0>))\n",
      "(8.926251835034533, tensor([6.5323], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([7.2900], grad_fn=<UnbindBackward0>))\n",
      "(7.383989457978509, tensor([8.9254], grad_fn=<UnbindBackward0>))\n",
      "(9.160519676992227, tensor([6.5325], grad_fn=<UnbindBackward0>))\n",
      "(7.9291264873067995, tensor([8.6481], grad_fn=<UnbindBackward0>))\n",
      "(9.776165413960777, tensor([7.6694], grad_fn=<UnbindBackward0>))\n",
      "(6.70808408385307, tensor([6.3741], grad_fn=<UnbindBackward0>))\n",
      "(5.953243334287785, tensor([7.3168], grad_fn=<UnbindBackward0>))\n",
      "(8.411610428841172, tensor([6.2038], grad_fn=<UnbindBackward0>))\n",
      "(7.518064181233078, tensor([6.2871], grad_fn=<UnbindBackward0>))\n",
      "(7.873217054862741, tensor([9.5257], grad_fn=<UnbindBackward0>))\n",
      "(7.584264818389059, tensor([8.6905], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([7.9582], grad_fn=<UnbindBackward0>))\n",
      "(6.829793737512425, tensor([8.0491], grad_fn=<UnbindBackward0>))\n",
      "(8.466320861042481, tensor([6.2271], grad_fn=<UnbindBackward0>))\n",
      "(7.447751280047908, tensor([6.7908], grad_fn=<UnbindBackward0>))\n",
      "(7.007600613951853, tensor([6.4473], grad_fn=<UnbindBackward0>))\n",
      "(7.935945103353701, tensor([8.5243], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([9.4777], grad_fn=<UnbindBackward0>))\n",
      "(8.640295388550221, tensor([8.9826], grad_fn=<UnbindBackward0>))\n",
      "(9.125762395500885, tensor([6.6563], grad_fn=<UnbindBackward0>))\n",
      "(6.6039438246004725, tensor([10.2215], grad_fn=<UnbindBackward0>))\n",
      "(8.317277766221235, tensor([9.3701], grad_fn=<UnbindBackward0>))\n",
      "(7.028201432058005, tensor([6.1155], grad_fn=<UnbindBackward0>))\n",
      "(8.526945482858915, tensor([6.5840], grad_fn=<UnbindBackward0>))\n",
      "(6.747586526829315, tensor([9.9600], grad_fn=<UnbindBackward0>))\n",
      "(6.86171134048073, tensor([6.6940], grad_fn=<UnbindBackward0>))\n",
      "(8.61341204915678, tensor([7.7470], grad_fn=<UnbindBackward0>))\n",
      "(8.80492526261806, tensor([9.5408], grad_fn=<UnbindBackward0>))\n",
      "(7.195937226475569, tensor([6.7986], grad_fn=<UnbindBackward0>))\n",
      "(7.074963197966044, tensor([7.4154], grad_fn=<UnbindBackward0>))\n",
      "(8.200013648175434, tensor([8.5498], grad_fn=<UnbindBackward0>))\n",
      "(8.799812469525556, tensor([8.6482], grad_fn=<UnbindBackward0>))\n",
      "(6.20050917404269, tensor([7.8238], grad_fn=<UnbindBackward0>))\n",
      "(9.20693457884135, tensor([7.7935], grad_fn=<UnbindBackward0>))\n",
      "(8.382747094863314, tensor([9.3097], grad_fn=<UnbindBackward0>))\n",
      "(8.476162841858246, tensor([8.5739], grad_fn=<UnbindBackward0>))\n",
      "(9.09963224999176, tensor([9.1141], grad_fn=<UnbindBackward0>))\n",
      "(9.717157974344635, tensor([7.9790], grad_fn=<UnbindBackward0>))\n",
      "(7.9247959139564355, tensor([8.5274], grad_fn=<UnbindBackward0>))\n",
      "(8.15248607578024, tensor([8.6037], grad_fn=<UnbindBackward0>))\n",
      "(7.563200592358071, tensor([7.8174], grad_fn=<UnbindBackward0>))\n",
      "(6.833031732786201, tensor([8.5987], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([8.9778], grad_fn=<UnbindBackward0>))\n",
      "(7.557472901614746, tensor([6.2105], grad_fn=<UnbindBackward0>))\n",
      "(6.045005314036012, tensor([9.1100], grad_fn=<UnbindBackward0>))\n",
      "(7.431299675155903, tensor([7.4007], grad_fn=<UnbindBackward0>))\n",
      "(7.545918151209323, tensor([8.3366], grad_fn=<UnbindBackward0>))\n",
      "(6.398594934535208, tensor([8.9500], grad_fn=<UnbindBackward0>))\n",
      "(7.254177846456518, tensor([7.7488], grad_fn=<UnbindBackward0>))\n",
      "(8.57866451350434, tensor([7.9745], grad_fn=<UnbindBackward0>))\n",
      "(7.976938756959434, tensor([7.9648], grad_fn=<UnbindBackward0>))\n",
      "(9.072227069846548, tensor([7.5240], grad_fn=<UnbindBackward0>))\n",
      "(9.695294188174604, tensor([6.5453], grad_fn=<UnbindBackward0>))\n",
      "(6.826545223556594, tensor([6.0766], grad_fn=<UnbindBackward0>))\n",
      "(6.359573868672378, tensor([6.5015], grad_fn=<UnbindBackward0>))\n",
      "(7.117205503164344, tensor([7.7752], grad_fn=<UnbindBackward0>))\n",
      "(9.71878315904038, tensor([8.3548], grad_fn=<UnbindBackward0>))\n",
      "(7.369600720526409, tensor([8.5841], grad_fn=<UnbindBackward0>))\n",
      "(9.520175249069098, tensor([9.0852], grad_fn=<UnbindBackward0>))\n",
      "(8.434897948689407, tensor([6.2950], grad_fn=<UnbindBackward0>))\n",
      "(8.89439598980643, tensor([7.2744], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([6.7147], grad_fn=<UnbindBackward0>))\n",
      "(8.0323601479245, tensor([8.5003], grad_fn=<UnbindBackward0>))\n",
      "(8.243282523048375, tensor([6.3561], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([5.9828], grad_fn=<UnbindBackward0>))\n",
      "(9.061492275239766, tensor([8.8513], grad_fn=<UnbindBackward0>))\n",
      "(8.402679804627477, tensor([9.3988], grad_fn=<UnbindBackward0>))\n",
      "(8.550627967502475, tensor([9.0621], grad_fn=<UnbindBackward0>))\n",
      "(7.828436359157585, tensor([6.2873], grad_fn=<UnbindBackward0>))\n",
      "(8.196987927258897, tensor([7.3077], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([8.1915], grad_fn=<UnbindBackward0>))\n",
      "(8.364973978438726, tensor([9.2145], grad_fn=<UnbindBackward0>))\n",
      "(9.571853592555165, tensor([6.2539], grad_fn=<UnbindBackward0>))\n",
      "(8.000684784514748, tensor([8.7801], grad_fn=<UnbindBackward0>))\n",
      "(8.29529885950246, tensor([6.4438], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([8.3648], grad_fn=<UnbindBackward0>))\n",
      "(9.108418382250798, tensor([8.9471], grad_fn=<UnbindBackward0>))\n",
      "(7.990576881743923, tensor([6.1672], grad_fn=<UnbindBackward0>))\n",
      "(8.523572798380277, tensor([7.8034], grad_fn=<UnbindBackward0>))\n",
      "(7.838737559599282, tensor([5.8375], grad_fn=<UnbindBackward0>))\n",
      "(9.1834829178063, tensor([6.5206], grad_fn=<UnbindBackward0>))\n",
      "(8.505727713306959, tensor([9.9595], grad_fn=<UnbindBackward0>))\n",
      "(7.274479558773871, tensor([8.2497], grad_fn=<UnbindBackward0>))\n",
      "(9.134970064951407, tensor([8.3849], grad_fn=<UnbindBackward0>))\n",
      "(7.619233416226805, tensor([8.9721], grad_fn=<UnbindBackward0>))\n",
      "(6.955592608396297, tensor([6.1773], grad_fn=<UnbindBackward0>))\n",
      "(9.020510699691906, tensor([7.7792], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([7.3674], grad_fn=<UnbindBackward0>))\n",
      "(7.651595573857601, tensor([8.5124], grad_fn=<UnbindBackward0>))\n",
      "(7.668093709082406, tensor([10.1689], grad_fn=<UnbindBackward0>))\n",
      "(8.795582216956426, tensor([9.4633], grad_fn=<UnbindBackward0>))\n",
      "(6.922643891475888, tensor([8.1449], grad_fn=<UnbindBackward0>))\n",
      "(8.87626525995469, tensor([6.2135], grad_fn=<UnbindBackward0>))\n",
      "(7.685243607975833, tensor([8.2582], grad_fn=<UnbindBackward0>))\n",
      "(7.767687277186908, tensor([9.5170], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([6.7955], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([6.9923], grad_fn=<UnbindBackward0>))\n",
      "(6.169610732491456, tensor([6.1903], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([6.1906], grad_fn=<UnbindBackward0>))\n",
      "(8.101980731853192, tensor([7.1795], grad_fn=<UnbindBackward0>))\n",
      "(7.972121128921655, tensor([8.5621], grad_fn=<UnbindBackward0>))\n",
      "(9.055322649709574, tensor([6.2236], grad_fn=<UnbindBackward0>))\n",
      "(7.113142108707088, tensor([8.8313], grad_fn=<UnbindBackward0>))\n",
      "(6.748759547491679, tensor([6.3402], grad_fn=<UnbindBackward0>))\n",
      "(8.525359754082631, tensor([7.9074], grad_fn=<UnbindBackward0>))\n",
      "(9.116469156366112, tensor([8.4088], grad_fn=<UnbindBackward0>))\n",
      "(9.665674427237482, tensor([7.7471], grad_fn=<UnbindBackward0>))\n",
      "(8.10258642539079, tensor([8.3317], grad_fn=<UnbindBackward0>))\n",
      "(8.23827262463303, tensor([8.8351], grad_fn=<UnbindBackward0>))\n",
      "(8.747828608488742, tensor([5.8919], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([10.0439], grad_fn=<UnbindBackward0>))\n",
      "(7.859026979751538, tensor([9.8381], grad_fn=<UnbindBackward0>))\n",
      "(8.52793528794814, tensor([6.7122], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([6.8700], grad_fn=<UnbindBackward0>))\n",
      "(6.536691597591305, tensor([9.0761], grad_fn=<UnbindBackward0>))\n",
      "(8.243282523048375, tensor([7.1102], grad_fn=<UnbindBackward0>))\n",
      "(9.163458386076051, tensor([6.5869], grad_fn=<UnbindBackward0>))\n",
      "(6.926577033222725, tensor([8.7697], grad_fn=<UnbindBackward0>))\n",
      "(9.534161491043838, tensor([10.1108], grad_fn=<UnbindBackward0>))\n",
      "(7.727094484779841, tensor([6.2669], grad_fn=<UnbindBackward0>))\n",
      "(9.257128533474942, tensor([8.2617], grad_fn=<UnbindBackward0>))\n",
      "(7.005789019253503, tensor([8.4324], grad_fn=<UnbindBackward0>))\n",
      "(6.52649485957079, tensor([7.4222], grad_fn=<UnbindBackward0>))\n",
      "(8.818038250394299, tensor([7.7650], grad_fn=<UnbindBackward0>))\n",
      "(8.1797604936999, tensor([8.3417], grad_fn=<UnbindBackward0>))\n",
      "(6.975413927455952, tensor([8.5327], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([7.2562], grad_fn=<UnbindBackward0>))\n",
      "(6.340359303727752, tensor([8.6937], grad_fn=<UnbindBackward0>))\n",
      "(6.670766320845874, tensor([7.3619], grad_fn=<UnbindBackward0>))\n",
      "(8.684739462628038, tensor([6.3613], grad_fn=<UnbindBackward0>))\n",
      "(8.335911094196945, tensor([6.5527], grad_fn=<UnbindBackward0>))\n",
      "(5.998936561946683, tensor([8.4466], grad_fn=<UnbindBackward0>))\n",
      "(7.544861068658458, tensor([7.2346], grad_fn=<UnbindBackward0>))\n",
      "(8.12533508671429, tensor([9.0919], grad_fn=<UnbindBackward0>))\n",
      "(6.903747257584598, tensor([8.6655], grad_fn=<UnbindBackward0>))\n",
      "(7.6544432264701125, tensor([6.4095], grad_fn=<UnbindBackward0>))\n",
      "(7.1284959456800365, tensor([8.3920], grad_fn=<UnbindBackward0>))\n",
      "(8.237743803890933, tensor([7.3411], grad_fn=<UnbindBackward0>))\n",
      "(7.122866658599083, tensor([6.9751], grad_fn=<UnbindBackward0>))\n",
      "(9.23863624113103, tensor([8.5371], grad_fn=<UnbindBackward0>))\n",
      "(8.835501457409778, tensor([6.7519], grad_fn=<UnbindBackward0>))\n",
      "(8.630878955820053, tensor([7.9130], grad_fn=<UnbindBackward0>))\n",
      "(6.80903930604298, tensor([6.7128], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.7853], grad_fn=<UnbindBackward0>))\n",
      "(7.773594467360194, tensor([7.0850], grad_fn=<UnbindBackward0>))\n",
      "(8.239065331769268, tensor([9.2980], grad_fn=<UnbindBackward0>))\n",
      "(7.622174594817622, tensor([6.7602], grad_fn=<UnbindBackward0>))\n",
      "(8.28045768658256, tensor([6.2882], grad_fn=<UnbindBackward0>))\n",
      "(7.001245622069476, tensor([8.9880], grad_fn=<UnbindBackward0>))\n",
      "(9.528430400599346, tensor([10.6588], grad_fn=<UnbindBackward0>))\n",
      "(7.719573989259581, tensor([8.5650], grad_fn=<UnbindBackward0>))\n",
      "(8.715388097366482, tensor([8.6516], grad_fn=<UnbindBackward0>))\n",
      "(8.16876982367527, tensor([6.7602], grad_fn=<UnbindBackward0>))\n",
      "(8.559869465696673, tensor([7.1435], grad_fn=<UnbindBackward0>))\n",
      "(7.226209010100671, tensor([6.2389], grad_fn=<UnbindBackward0>))\n",
      "(9.06820060481506, tensor([6.6384], grad_fn=<UnbindBackward0>))\n",
      "(7.9976631270201, tensor([9.4118], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([7.1218], grad_fn=<UnbindBackward0>))\n",
      "(8.764678074116606, tensor([6.3651], grad_fn=<UnbindBackward0>))\n",
      "(7.861341795599989, tensor([8.6317], grad_fn=<UnbindBackward0>))\n",
      "(8.591186871324563, tensor([6.2837], grad_fn=<UnbindBackward0>))\n",
      "(6.111467339502679, tensor([9.2004], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([8.5679], grad_fn=<UnbindBackward0>))\n",
      "(8.29054350077274, tensor([8.7250], grad_fn=<UnbindBackward0>))\n",
      "(7.605392364814935, tensor([8.5688], grad_fn=<UnbindBackward0>))\n",
      "(7.0326242610280065, tensor([9.1029], grad_fn=<UnbindBackward0>))\n",
      "(9.252345666121213, tensor([6.4321], grad_fn=<UnbindBackward0>))\n",
      "(7.557994958530806, tensor([9.4775], grad_fn=<UnbindBackward0>))\n",
      "(6.570882962339584, tensor([6.5660], grad_fn=<UnbindBackward0>))\n",
      "(9.042276686928927, tensor([9.2706], grad_fn=<UnbindBackward0>))\n",
      "(8.480944058741116, tensor([8.6792], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.4821], grad_fn=<UnbindBackward0>))\n",
      "(8.774776816043985, tensor([7.0989], grad_fn=<UnbindBackward0>))\n",
      "(7.874358824729881, tensor([6.2050], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([6.2915], grad_fn=<UnbindBackward0>))\n",
      "(7.499423290592229, tensor([7.2104], grad_fn=<UnbindBackward0>))\n",
      "(7.381501894506707, tensor([8.0689], grad_fn=<UnbindBackward0>))\n",
      "(8.755107121633896, tensor([8.5476], grad_fn=<UnbindBackward0>))\n",
      "(7.097548850614793, tensor([8.1906], grad_fn=<UnbindBackward0>))\n",
      "(9.365547636278565, tensor([6.4816], grad_fn=<UnbindBackward0>))\n",
      "(8.963288275610298, tensor([9.7406], grad_fn=<UnbindBackward0>))\n",
      "(7.978996370854115, tensor([8.6401], grad_fn=<UnbindBackward0>))\n",
      "(7.632885505395133, tensor([9.0559], grad_fn=<UnbindBackward0>))\n",
      "(7.365812837209472, tensor([6.4791], grad_fn=<UnbindBackward0>))\n",
      "(8.54714026778419, tensor([9.1985], grad_fn=<UnbindBackward0>))\n",
      "(7.607381425639791, tensor([7.6649], grad_fn=<UnbindBackward0>))\n",
      "(8.420241665339788, tensor([7.7620], grad_fn=<UnbindBackward0>))\n",
      "(8.237743803890933, tensor([7.2947], grad_fn=<UnbindBackward0>))\n",
      "(9.820051594294094, tensor([9.2551], grad_fn=<UnbindBackward0>))\n",
      "(8.206583614320753, tensor([7.1864], grad_fn=<UnbindBackward0>))\n",
      "(8.85879510585745, tensor([7.9307], grad_fn=<UnbindBackward0>))\n",
      "(8.098034756176071, tensor([7.2460], grad_fn=<UnbindBackward0>))\n",
      "(9.673948594133059, tensor([7.3724], grad_fn=<UnbindBackward0>))\n",
      "(7.838737559599282, tensor([7.9995], grad_fn=<UnbindBackward0>))\n",
      "(6.86484777797086, tensor([7.5846], grad_fn=<UnbindBackward0>))\n",
      "(6.440946540632921, tensor([8.4942], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([8.3754], grad_fn=<UnbindBackward0>))\n",
      "(8.356319965828153, tensor([7.6783], grad_fn=<UnbindBackward0>))\n",
      "(9.144093726157257, tensor([7.1403], grad_fn=<UnbindBackward0>))\n",
      "(8.911799556189527, tensor([6.9731], grad_fn=<UnbindBackward0>))\n",
      "(7.784057002639929, tensor([8.5519], grad_fn=<UnbindBackward0>))\n",
      "(8.513586695822125, tensor([6.4182], grad_fn=<UnbindBackward0>))\n",
      "(6.525029657843462, tensor([8.0756], grad_fn=<UnbindBackward0>))\n",
      "(7.765569081097317, tensor([9.1624], grad_fn=<UnbindBackward0>))\n",
      "(6.7226297948554485, tensor([8.1951], grad_fn=<UnbindBackward0>))\n",
      "(6.741700694652055, tensor([6.3333], grad_fn=<UnbindBackward0>))\n",
      "(7.029972911706386, tensor([6.5629], grad_fn=<UnbindBackward0>))\n",
      "(7.482681828154651, tensor([9.9295], grad_fn=<UnbindBackward0>))\n",
      "(9.075436609511382, tensor([7.3228], grad_fn=<UnbindBackward0>))\n",
      "(6.111467339502679, tensor([10.0038], grad_fn=<UnbindBackward0>))\n",
      "(6.815639990074331, tensor([7.9665], grad_fn=<UnbindBackward0>))\n",
      "(8.414938957377482, tensor([6.7030], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([7.1164], grad_fn=<UnbindBackward0>))\n",
      "(9.354007721788596, tensor([6.1965], grad_fn=<UnbindBackward0>))\n",
      "(6.694562058521095, tensor([7.1021], grad_fn=<UnbindBackward0>))\n",
      "(8.410498452745275, tensor([6.4206], grad_fn=<UnbindBackward0>))\n",
      "(8.994296557777195, tensor([6.6859], grad_fn=<UnbindBackward0>))\n",
      "(6.934397209928558, tensor([7.2810], grad_fn=<UnbindBackward0>))\n",
      "(7.7039102096163115, tensor([8.1203], grad_fn=<UnbindBackward0>))\n",
      "(7.5126175446745105, tensor([8.3909], grad_fn=<UnbindBackward0>))\n",
      "(8.987321812850125, tensor([5.9734], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([7.3447], grad_fn=<UnbindBackward0>))\n",
      "(9.47738596957752, tensor([6.1823], grad_fn=<UnbindBackward0>))\n",
      "(6.144185634125646, tensor([6.3486], grad_fn=<UnbindBackward0>))\n",
      "(9.54029139160713, tensor([8.4791], grad_fn=<UnbindBackward0>))\n",
      "(7.97349996402463, tensor([6.5072], grad_fn=<UnbindBackward0>))\n",
      "(7.621684998724611, tensor([9.2729], grad_fn=<UnbindBackward0>))\n",
      "(7.444833273892193, tensor([6.8348], grad_fn=<UnbindBackward0>))\n",
      "(5.976350909297934, tensor([9.1191], grad_fn=<UnbindBackward0>))\n",
      "(6.70073110954781, tensor([6.7925], grad_fn=<UnbindBackward0>))\n",
      "(9.378140332251117, tensor([8.1911], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([9.3836], grad_fn=<UnbindBackward0>))\n",
      "(9.290444616217844, tensor([6.6889], grad_fn=<UnbindBackward0>))\n",
      "(6.115892125483034, tensor([7.6099], grad_fn=<UnbindBackward0>))\n",
      "(8.340456012916183, tensor([8.9324], grad_fn=<UnbindBackward0>))\n",
      "(9.31847703058076, tensor([6.8370], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([7.2525], grad_fn=<UnbindBackward0>))\n",
      "(8.229511118964457, tensor([8.1594], grad_fn=<UnbindBackward0>))\n",
      "(6.419994928147142, tensor([9.4267], grad_fn=<UnbindBackward0>))\n",
      "(6.175867270105761, tensor([6.4124], grad_fn=<UnbindBackward0>))\n",
      "(7.201170883281678, tensor([6.3390], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.8129], grad_fn=<UnbindBackward0>))\n",
      "(6.648984550024776, tensor([6.5751], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.5956], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([8.4512], grad_fn=<UnbindBackward0>))\n",
      "(8.153637486385282, tensor([8.7092], grad_fn=<UnbindBackward0>))\n",
      "(7.059617628291383, tensor([8.4769], grad_fn=<UnbindBackward0>))\n",
      "(9.58211080356025, tensor([8.5144], grad_fn=<UnbindBackward0>))\n",
      "(7.741099090035366, tensor([6.2677], grad_fn=<UnbindBackward0>))\n",
      "(8.404472321352118, tensor([6.1421], grad_fn=<UnbindBackward0>))\n",
      "(9.644004134443026, tensor([6.7252], grad_fn=<UnbindBackward0>))\n",
      "(8.22817689595132, tensor([6.8100], grad_fn=<UnbindBackward0>))\n",
      "(6.429719478039138, tensor([8.6283], grad_fn=<UnbindBackward0>))\n",
      "(7.1316985104669115, tensor([8.4370], grad_fn=<UnbindBackward0>))\n",
      "(9.817003309314352, tensor([8.3924], grad_fn=<UnbindBackward0>))\n",
      "(6.829793737512425, tensor([7.6738], grad_fn=<UnbindBackward0>))\n",
      "(8.285513309079741, tensor([9.0257], grad_fn=<UnbindBackward0>))\n",
      "(9.19786285046507, tensor([8.0754], grad_fn=<UnbindBackward0>))\n",
      "(9.3492323708428, tensor([8.8600], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([9.3054], grad_fn=<UnbindBackward0>))\n",
      "(6.792344427470809, tensor([9.3992], grad_fn=<UnbindBackward0>))\n",
      "(8.525359754082631, tensor([8.2205], grad_fn=<UnbindBackward0>))\n",
      "(9.272093768251665, tensor([8.8584], grad_fn=<UnbindBackward0>))\n",
      "(7.524021415206125, tensor([6.7403], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([6.2541], grad_fn=<UnbindBackward0>))\n",
      "(7.829232537543592, tensor([8.0222], grad_fn=<UnbindBackward0>))\n",
      "(9.321076413570848, tensor([6.1775], grad_fn=<UnbindBackward0>))\n",
      "(6.939253946041508, tensor([7.5135], grad_fn=<UnbindBackward0>))\n",
      "(7.201170883281678, tensor([6.2724], grad_fn=<UnbindBackward0>))\n",
      "(8.115819701211327, tensor([6.7259], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([8.8044], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([9.7540], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([9.4232], grad_fn=<UnbindBackward0>))\n",
      "(8.477412321404392, tensor([6.0613], grad_fn=<UnbindBackward0>))\n",
      "(7.9599745280805365, tensor([8.8227], grad_fn=<UnbindBackward0>))\n",
      "(8.574140471857987, tensor([8.0709], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([8.9336], grad_fn=<UnbindBackward0>))\n",
      "(8.096512917501594, tensor([7.2741], grad_fn=<UnbindBackward0>))\n",
      "(6.113682179832232, tensor([6.3385], grad_fn=<UnbindBackward0>))\n",
      "(8.214194414852564, tensor([9.1028], grad_fn=<UnbindBackward0>))\n",
      "(7.488852955733459, tensor([9.0329], grad_fn=<UnbindBackward0>))\n",
      "(8.25088114470065, tensor([7.1465], grad_fn=<UnbindBackward0>))\n",
      "(9.329810871952349, tensor([9.0025], grad_fn=<UnbindBackward0>))\n",
      "(7.794411205726601, tensor([8.8280], grad_fn=<UnbindBackward0>))\n",
      "(8.19478163844336, tensor([6.3590], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([6.2623], grad_fn=<UnbindBackward0>))\n",
      "(8.372629740224884, tensor([7.8302], grad_fn=<UnbindBackward0>))\n",
      "(8.434028950155469, tensor([9.7523], grad_fn=<UnbindBackward0>))\n",
      "(7.693025748417888, tensor([6.5357], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([6.6416], grad_fn=<UnbindBackward0>))\n",
      "(6.839476438228843, tensor([7.2622], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([7.3051], grad_fn=<UnbindBackward0>))\n",
      "(9.219993629004321, tensor([8.9166], grad_fn=<UnbindBackward0>))\n",
      "(7.6324011266014535, tensor([8.9330], grad_fn=<UnbindBackward0>))\n",
      "(8.775085935057266, tensor([8.7381], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([7.1669], grad_fn=<UnbindBackward0>))\n",
      "(8.318010277546872, tensor([7.9015], grad_fn=<UnbindBackward0>))\n",
      "(7.824845691026856, tensor([6.8764], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([7.7332], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([8.7999], grad_fn=<UnbindBackward0>))\n",
      "(8.385260520155413, tensor([8.7711], grad_fn=<UnbindBackward0>))\n",
      "(7.685703061234547, tensor([6.2361], grad_fn=<UnbindBackward0>))\n",
      "(8.247220052745229, tensor([8.5569], grad_fn=<UnbindBackward0>))\n",
      "(6.610696044717759, tensor([6.3722], grad_fn=<UnbindBackward0>))\n",
      "(7.906178840394815, tensor([8.6887], grad_fn=<UnbindBackward0>))\n",
      "(8.500250470685925, tensor([8.9174], grad_fn=<UnbindBackward0>))\n",
      "(6.376726947898627, tensor([6.6418], grad_fn=<UnbindBackward0>))\n",
      "(9.142489705068009, tensor([6.4890], grad_fn=<UnbindBackward0>))\n",
      "(8.183955717304954, tensor([7.3669], grad_fn=<UnbindBackward0>))\n",
      "(7.352441100243583, tensor([6.3791], grad_fn=<UnbindBackward0>))\n",
      "(6.775366090936392, tensor([7.9535], grad_fn=<UnbindBackward0>))\n",
      "(9.253112463820052, tensor([8.6287], grad_fn=<UnbindBackward0>))\n",
      "(6.118097198041348, tensor([7.4565], grad_fn=<UnbindBackward0>))\n",
      "(9.090317329376452, tensor([6.1720], grad_fn=<UnbindBackward0>))\n",
      "(8.507344855361422, tensor([10.1281], grad_fn=<UnbindBackward0>))\n",
      "(6.78332520060396, tensor([7.1994], grad_fn=<UnbindBackward0>))\n",
      "(6.434546518787453, tensor([6.1854], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.8191], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([6.7436], grad_fn=<UnbindBackward0>))\n",
      "(6.61472560020376, tensor([9.8574], grad_fn=<UnbindBackward0>))\n",
      "(6.692083742506628, tensor([7.3371], grad_fn=<UnbindBackward0>))\n",
      "(8.443115988019922, tensor([9.7877], grad_fn=<UnbindBackward0>))\n",
      "(8.208764045819667, tensor([6.6667], grad_fn=<UnbindBackward0>))\n",
      "(6.660575149839686, tensor([10.3056], grad_fn=<UnbindBackward0>))\n",
      "(6.2285110035911835, tensor([6.3049], grad_fn=<UnbindBackward0>))\n",
      "(9.489486113405677, tensor([7.2961], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.3968], grad_fn=<UnbindBackward0>))\n",
      "(7.433666540166168, tensor([5.8915], grad_fn=<UnbindBackward0>))\n",
      "(9.618003063627834, tensor([8.7628], grad_fn=<UnbindBackward0>))\n",
      "(8.59840444684106, tensor([7.6375], grad_fn=<UnbindBackward0>))\n",
      "(7.606387389772652, tensor([9.6230], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.3535], grad_fn=<UnbindBackward0>))\n",
      "(7.271703706887368, tensor([7.3847], grad_fn=<UnbindBackward0>))\n",
      "(5.996452088619021, tensor([6.3482], grad_fn=<UnbindBackward0>))\n",
      "(6.368187186350492, tensor([8.3013], grad_fn=<UnbindBackward0>))\n",
      "(7.261927092702751, tensor([6.7244], grad_fn=<UnbindBackward0>))\n",
      "(7.618251097876695, tensor([7.3371], grad_fn=<UnbindBackward0>))\n",
      "(6.115892125483034, tensor([7.4303], grad_fn=<UnbindBackward0>))\n",
      "(8.386172928977834, tensor([8.1987], grad_fn=<UnbindBackward0>))\n",
      "(7.774435510302958, tensor([6.7363], grad_fn=<UnbindBackward0>))\n",
      "(7.777373602657861, tensor([7.8836], grad_fn=<UnbindBackward0>))\n",
      "(8.237743803890933, tensor([7.8797], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([8.4662], grad_fn=<UnbindBackward0>))\n",
      "(8.005367067316664, tensor([6.9911], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.4504], grad_fn=<UnbindBackward0>))\n",
      "(7.6246189861593985, tensor([8.4541], grad_fn=<UnbindBackward0>))\n",
      "(8.663369301573839, tensor([6.2166], grad_fn=<UnbindBackward0>))\n",
      "(8.154500175151941, tensor([7.8673], grad_fn=<UnbindBackward0>))\n",
      "(6.583409222158765, tensor([6.1645], grad_fn=<UnbindBackward0>))\n",
      "(7.829232537543592, tensor([7.0992], grad_fn=<UnbindBackward0>))\n",
      "(7.1891677384203225, tensor([6.3798], grad_fn=<UnbindBackward0>))\n",
      "(8.526351129201004, tensor([9.6937], grad_fn=<UnbindBackward0>))\n",
      "(6.175867270105761, tensor([6.2100], grad_fn=<UnbindBackward0>))\n",
      "(8.525756422076725, tensor([6.3456], grad_fn=<UnbindBackward0>))\n",
      "(6.440946540632921, tensor([9.3850], grad_fn=<UnbindBackward0>))\n",
      "(7.071573364211532, tensor([8.9720], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([6.2421], grad_fn=<UnbindBackward0>))\n",
      "(7.793586803371584, tensor([10.0262], grad_fn=<UnbindBackward0>))\n",
      "(8.696008608880904, tensor([7.1045], grad_fn=<UnbindBackward0>))\n",
      "(7.8632667240095735, tensor([6.3349], grad_fn=<UnbindBackward0>))\n",
      "(8.762176509334903, tensor([8.5457], grad_fn=<UnbindBackward0>))\n",
      "(6.974478911025045, tensor([8.6810], grad_fn=<UnbindBackward0>))\n",
      "(8.454040376410969, tensor([7.1271], grad_fn=<UnbindBackward0>))\n",
      "(7.7437032581737535, tensor([8.5071], grad_fn=<UnbindBackward0>))\n",
      "(6.907755278982137, tensor([6.2970], grad_fn=<UnbindBackward0>))\n",
      "(9.724301076237646, tensor([6.4201], grad_fn=<UnbindBackward0>))\n",
      "(9.497547331579726, tensor([6.6689], grad_fn=<UnbindBackward0>))\n",
      "(8.479698986988657, tensor([6.4257], grad_fn=<UnbindBackward0>))\n",
      "(7.049254841255837, tensor([8.0262], grad_fn=<UnbindBackward0>))\n",
      "(7.917171988845776, tensor([5.8454], grad_fn=<UnbindBackward0>))\n",
      "(8.661812881026181, tensor([6.5251], grad_fn=<UnbindBackward0>))\n",
      "(8.55333223803211, tensor([7.8007], grad_fn=<UnbindBackward0>))\n",
      "(9.455636727931525, tensor([6.8569], grad_fn=<UnbindBackward0>))\n",
      "(6.871091294610546, tensor([9.6481], grad_fn=<UnbindBackward0>))\n",
      "(9.591990815318308, tensor([8.5488], grad_fn=<UnbindBackward0>))\n",
      "(8.864322722511439, tensor([6.3343], grad_fn=<UnbindBackward0>))\n",
      "(8.810161268297255, tensor([7.6003], grad_fn=<UnbindBackward0>))\n",
      "(6.88653164253051, tensor([7.8611], grad_fn=<UnbindBackward0>))\n",
      "(8.588024372176829, tensor([8.0261], grad_fn=<UnbindBackward0>))\n",
      "(7.943782692458625, tensor([6.7716], grad_fn=<UnbindBackward0>))\n",
      "(8.591743922680534, tensor([8.5828], grad_fn=<UnbindBackward0>))\n",
      "(9.522300336887486, tensor([8.5018], grad_fn=<UnbindBackward0>))\n",
      "(6.2166061010848646, tensor([8.5130], grad_fn=<UnbindBackward0>))\n",
      "(6.950814768442584, tensor([7.2923], grad_fn=<UnbindBackward0>))\n",
      "(8.150756470275551, tensor([10.4816], grad_fn=<UnbindBackward0>))\n",
      "(8.101374671228582, tensor([9.4073], grad_fn=<UnbindBackward0>))\n",
      "(8.21554741194707, tensor([8.4822], grad_fn=<UnbindBackward0>))\n",
      "(8.156510226079966, tensor([6.7279], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([6.7301], grad_fn=<UnbindBackward0>))\n",
      "(8.342363500380579, tensor([6.6294], grad_fn=<UnbindBackward0>))\n",
      "(8.424858580213442, tensor([8.5989], grad_fn=<UnbindBackward0>))\n",
      "(8.556221578383715, tensor([7.4110], grad_fn=<UnbindBackward0>))\n",
      "(7.86940171257709, tensor([7.7374], grad_fn=<UnbindBackward0>))\n",
      "(8.807771066980044, tensor([5.9074], grad_fn=<UnbindBackward0>))\n",
      "(9.090542808520587, tensor([7.7632], grad_fn=<UnbindBackward0>))\n",
      "(8.788745881938135, tensor([8.5209], grad_fn=<UnbindBackward0>))\n",
      "(7.491645473605133, tensor([7.7819], grad_fn=<UnbindBackward0>))\n",
      "(6.464588303689961, tensor([6.2989], grad_fn=<UnbindBackward0>))\n",
      "(8.976135873302548, tensor([7.4238], grad_fn=<UnbindBackward0>))\n",
      "(8.310169021981912, tensor([7.4308], grad_fn=<UnbindBackward0>))\n",
      "(6.823286122355687, tensor([9.4323], grad_fn=<UnbindBackward0>))\n",
      "(9.297893268984044, tensor([7.2520], grad_fn=<UnbindBackward0>))\n",
      "(6.923628628138427, tensor([8.8627], grad_fn=<UnbindBackward0>))\n",
      "(7.520234556474628, tensor([6.4950], grad_fn=<UnbindBackward0>))\n",
      "(9.036225051729327, tensor([6.7662], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([6.3447], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([9.6676], grad_fn=<UnbindBackward0>))\n",
      "(7.046647277848756, tensor([6.5892], grad_fn=<UnbindBackward0>))\n",
      "(6.932447891572509, tensor([7.3802], grad_fn=<UnbindBackward0>))\n",
      "(9.108418382250798, tensor([8.5388], grad_fn=<UnbindBackward0>))\n",
      "(6.68586094706836, tensor([7.1204], grad_fn=<UnbindBackward0>))\n",
      "(8.45807992692373, tensor([7.3216], grad_fn=<UnbindBackward0>))\n",
      "(8.816260208734581, tensor([10.4348], grad_fn=<UnbindBackward0>))\n",
      "(6.760414691083428, tensor([8.4807], grad_fn=<UnbindBackward0>))\n",
      "(8.88751459738882, tensor([7.0761], grad_fn=<UnbindBackward0>))\n",
      "(8.404472321352118, tensor([7.9001], grad_fn=<UnbindBackward0>))\n",
      "(8.292298107063221, tensor([8.4396], grad_fn=<UnbindBackward0>))\n",
      "(9.256555795773146, tensor([8.2195], grad_fn=<UnbindBackward0>))\n",
      "(8.750841375375533, tensor([8.2233], grad_fn=<UnbindBackward0>))\n",
      "(6.75343791859778, tensor([7.6845], grad_fn=<UnbindBackward0>))\n",
      "(8.159374736775426, tensor([10.5887], grad_fn=<UnbindBackward0>))\n",
      "(7.650168700845001, tensor([5.8996], grad_fn=<UnbindBackward0>))\n",
      "(8.666474894131992, tensor([6.3391], grad_fn=<UnbindBackward0>))\n",
      "(8.133880887949207, tensor([6.7046], grad_fn=<UnbindBackward0>))\n",
      "(9.015905747298426, tensor([9.1059], grad_fn=<UnbindBackward0>))\n",
      "(8.128290171607052, tensor([8.3052], grad_fn=<UnbindBackward0>))\n",
      "(8.288031567776464, tensor([7.3464], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([7.6294], grad_fn=<UnbindBackward0>))\n",
      "(8.55159461813357, tensor([7.2604], grad_fn=<UnbindBackward0>))\n",
      "(8.845489236753274, tensor([8.4139], grad_fn=<UnbindBackward0>))\n",
      "(8.868835492826895, tensor([6.0745], grad_fn=<UnbindBackward0>))\n",
      "(7.126087273299125, tensor([9.7864], grad_fn=<UnbindBackward0>))\n",
      "(6.131226489483141, tensor([8.2522], grad_fn=<UnbindBackward0>))\n",
      "(7.1372784372603855, tensor([6.5264], grad_fn=<UnbindBackward0>))\n",
      "(7.0909098220799835, tensor([8.5280], grad_fn=<UnbindBackward0>))\n",
      "(7.062191632286556, tensor([8.4955], grad_fn=<UnbindBackward0>))\n",
      "(7.827240901752812, tensor([6.5783], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([7.0969], grad_fn=<UnbindBackward0>))\n",
      "(6.90875477931522, tensor([6.5901], grad_fn=<UnbindBackward0>))\n",
      "(8.564649132572534, tensor([6.3916], grad_fn=<UnbindBackward0>))\n",
      "(9.142489705068009, tensor([6.4564], grad_fn=<UnbindBackward0>))\n",
      "(7.237059026124737, tensor([6.7634], grad_fn=<UnbindBackward0>))\n",
      "(9.388402871849754, tensor([8.5516], grad_fn=<UnbindBackward0>))\n",
      "(8.041091003708633, tensor([8.4297], grad_fn=<UnbindBackward0>))\n",
      "(6.726233402358747, tensor([7.4432], grad_fn=<UnbindBackward0>))\n",
      "(8.289037098278483, tensor([6.4269], grad_fn=<UnbindBackward0>))\n",
      "(6.126869184114185, tensor([6.2363], grad_fn=<UnbindBackward0>))\n",
      "(8.470101583882387, tensor([6.4495], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([8.4277], grad_fn=<UnbindBackward0>))\n",
      "(7.744569809354496, tensor([6.7256], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([7.4583], grad_fn=<UnbindBackward0>))\n",
      "(6.111467339502679, tensor([8.7105], grad_fn=<UnbindBackward0>))\n",
      "(8.043663352393944, tensor([6.2095], grad_fn=<UnbindBackward0>))\n",
      "(9.600759521879075, tensor([9.5934], grad_fn=<UnbindBackward0>))\n",
      "(8.334951631422454, tensor([8.8916], grad_fn=<UnbindBackward0>))\n",
      "(7.7501841622578365, tensor([6.6576], grad_fn=<UnbindBackward0>))\n",
      "(8.415160465851086, tensor([8.0221], grad_fn=<UnbindBackward0>))\n",
      "(6.697034247666484, tensor([7.8568], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([9.9793], grad_fn=<UnbindBackward0>))\n",
      "(8.877939834725023, tensor([8.6325], grad_fn=<UnbindBackward0>))\n",
      "(7.503289630675082, tensor([7.2762], grad_fn=<UnbindBackward0>))\n",
      "(8.393894975071744, tensor([8.8507], grad_fn=<UnbindBackward0>))\n",
      "(8.992806059426483, tensor([7.4190], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([7.8317], grad_fn=<UnbindBackward0>))\n",
      "(6.664409020350408, tensor([7.3009], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([8.1373], grad_fn=<UnbindBackward0>))\n",
      "(8.48446336679332, tensor([7.7194], grad_fn=<UnbindBackward0>))\n",
      "(6.660575149839686, tensor([9.0630], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([6.5001], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([7.1946], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([8.6820], grad_fn=<UnbindBackward0>))\n",
      "(9.090542808520587, tensor([7.4210], grad_fn=<UnbindBackward0>))\n",
      "(8.737292112544221, tensor([6.3078], grad_fn=<UnbindBackward0>))\n",
      "(8.571870752706934, tensor([8.8299], grad_fn=<UnbindBackward0>))\n",
      "(8.605387202152153, tensor([8.8642], grad_fn=<UnbindBackward0>))\n",
      "(8.342363500380579, tensor([8.5618], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([7.2778], grad_fn=<UnbindBackward0>))\n",
      "(9.343121483210002, tensor([6.5235], grad_fn=<UnbindBackward0>))\n",
      "(8.423980809694058, tensor([8.9440], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([9.3781], grad_fn=<UnbindBackward0>))\n",
      "(9.577064651767916, tensor([6.7765], grad_fn=<UnbindBackward0>))\n",
      "(7.917536353943631, tensor([6.1965], grad_fn=<UnbindBackward0>))\n",
      "(7.460490305825338, tensor([8.3303], grad_fn=<UnbindBackward0>))\n",
      "(8.789812386190972, tensor([8.3960], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([6.2162], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([8.5959], grad_fn=<UnbindBackward0>))\n",
      "(8.672314828283538, tensor([8.7616], grad_fn=<UnbindBackward0>))\n",
      "(9.08851166361105, tensor([5.7491], grad_fn=<UnbindBackward0>))\n",
      "(6.478509642208569, tensor([7.9091], grad_fn=<UnbindBackward0>))\n",
      "(8.676246121270838, tensor([8.1895], grad_fn=<UnbindBackward0>))\n",
      "(7.781973234434385, tensor([8.0916], grad_fn=<UnbindBackward0>))\n",
      "(8.133880887949207, tensor([8.4348], grad_fn=<UnbindBackward0>))\n",
      "(6.9167150203536085, tensor([7.9157], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.2948], grad_fn=<UnbindBackward0>))\n",
      "(8.82893352896109, tensor([7.6603], grad_fn=<UnbindBackward0>))\n",
      "(7.69484807238461, tensor([6.7838], grad_fn=<UnbindBackward0>))\n",
      "(8.725669705687043, tensor([9.5440], grad_fn=<UnbindBackward0>))\n",
      "(5.976350909297934, tensor([10.3584], grad_fn=<UnbindBackward0>))\n",
      "(8.371473537066832, tensor([8.2124], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.9168], grad_fn=<UnbindBackward0>))\n",
      "(6.900730664045173, tensor([6.5985], grad_fn=<UnbindBackward0>))\n",
      "(7.483806687665835, tensor([6.2892], grad_fn=<UnbindBackward0>))\n",
      "(8.012349639327795, tensor([8.6182], grad_fn=<UnbindBackward0>))\n",
      "(9.026417533815254, tensor([8.7658], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([6.5174], grad_fn=<UnbindBackward0>))\n",
      "(8.471149252914831, tensor([9.6364], grad_fn=<UnbindBackward0>))\n",
      "(6.871091294610546, tensor([6.2763], grad_fn=<UnbindBackward0>))\n",
      "(7.719129840906732, tensor([6.4124], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([7.0022], grad_fn=<UnbindBackward0>))\n",
      "(6.818924065275521, tensor([8.4853], grad_fn=<UnbindBackward0>))\n",
      "(8.065893546964274, tensor([8.1347], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.7782], grad_fn=<UnbindBackward0>))\n",
      "(6.440946540632921, tensor([8.5520], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([8.6540], grad_fn=<UnbindBackward0>))\n",
      "(8.011355109161286, tensor([8.1986], grad_fn=<UnbindBackward0>))\n",
      "(8.791941988456118, tensor([8.5776], grad_fn=<UnbindBackward0>))\n",
      "(8.340933226000878, tensor([9.1489], grad_fn=<UnbindBackward0>))\n",
      "(8.500047032581268, tensor([7.7850], grad_fn=<UnbindBackward0>))\n",
      "(8.522379718103538, tensor([8.3835], grad_fn=<UnbindBackward0>))\n",
      "(6.668228248417403, tensor([8.6161], grad_fn=<UnbindBackward0>))\n",
      "(9.093357016490364, tensor([6.2183], grad_fn=<UnbindBackward0>))\n",
      "(7.77485576666552, tensor([9.2207], grad_fn=<UnbindBackward0>))\n",
      "(8.32845106681936, tensor([6.3250], grad_fn=<UnbindBackward0>))\n",
      "(9.260938424739274, tensor([6.2975], grad_fn=<UnbindBackward0>))\n",
      "(7.0707241072602764, tensor([8.3511], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([6.5001], grad_fn=<UnbindBackward0>))\n",
      "(7.365812837209472, tensor([7.1057], grad_fn=<UnbindBackward0>))\n",
      "(8.40200678160712, tensor([6.1119], grad_fn=<UnbindBackward0>))\n",
      "(8.276903481267057, tensor([8.0687], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([8.0142], grad_fn=<UnbindBackward0>))\n",
      "(8.566364235422688, tensor([10.3538], grad_fn=<UnbindBackward0>))\n",
      "(8.23031079913502, tensor([7.3055], grad_fn=<UnbindBackward0>))\n",
      "(6.3561076606958915, tensor([7.2676], grad_fn=<UnbindBackward0>))\n",
      "(6.573680166960646, tensor([9.2066], grad_fn=<UnbindBackward0>))\n",
      "(8.33423142973486, tensor([6.8932], grad_fn=<UnbindBackward0>))\n",
      "(7.64826303090192, tensor([6.6753], grad_fn=<UnbindBackward0>))\n",
      "(8.944028325260595, tensor([7.2179], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.3977], grad_fn=<UnbindBackward0>))\n",
      "(7.7664168980196555, tensor([8.4521], grad_fn=<UnbindBackward0>))\n",
      "(9.807692255081262, tensor([7.7943], grad_fn=<UnbindBackward0>))\n",
      "(8.495560891289124, tensor([6.4050], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([8.0488], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([8.7678], grad_fn=<UnbindBackward0>))\n",
      "(6.740519359606223, tensor([8.4302], grad_fn=<UnbindBackward0>))\n",
      "(7.001245622069476, tensor([8.9066], grad_fn=<UnbindBackward0>))\n",
      "(8.063692634269517, tensor([7.6534], grad_fn=<UnbindBackward0>))\n",
      "(8.96469555531546, tensor([6.1913], grad_fn=<UnbindBackward0>))\n",
      "(7.438971592395862, tensor([8.5894], grad_fn=<UnbindBackward0>))\n",
      "(6.257667587882639, tensor([9.9997], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([7.4829], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([8.3309], grad_fn=<UnbindBackward0>))\n",
      "(6.583409222158765, tensor([7.7547], grad_fn=<UnbindBackward0>))\n",
      "(8.452334619067742, tensor([7.9003], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([9.8720], grad_fn=<UnbindBackward0>))\n",
      "(8.339261982923576, tensor([7.1748], grad_fn=<UnbindBackward0>))\n",
      "(8.566554620953962, tensor([7.8749], grad_fn=<UnbindBackward0>))\n",
      "(8.558718938244736, tensor([7.6520], grad_fn=<UnbindBackward0>))\n",
      "(6.8966943316227125, tensor([7.3168], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([6.2354], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([8.7590], grad_fn=<UnbindBackward0>))\n",
      "(6.51025834052315, tensor([6.3004], grad_fn=<UnbindBackward0>))\n",
      "(7.4821189235521155, tensor([6.4180], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([9.6097], grad_fn=<UnbindBackward0>))\n",
      "(6.947937068614969, tensor([8.2612], grad_fn=<UnbindBackward0>))\n",
      "(8.95402775927046, tensor([6.4070], grad_fn=<UnbindBackward0>))\n",
      "(8.400659375160286, tensor([7.6572], grad_fn=<UnbindBackward0>))\n",
      "(7.5740450053721995, tensor([8.2535], grad_fn=<UnbindBackward0>))\n",
      "(7.102499355774649, tensor([5.9708], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([9.9987], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([6.6942], grad_fn=<UnbindBackward0>))\n",
      "(6.953684210870537, tensor([6.4252], grad_fn=<UnbindBackward0>))\n",
      "(6.251903883165888, tensor([8.2183], grad_fn=<UnbindBackward0>))\n",
      "(8.150756470275551, tensor([9.5009], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([9.5834], grad_fn=<UnbindBackward0>))\n",
      "(7.147559271189454, tensor([7.1358], grad_fn=<UnbindBackward0>))\n",
      "(9.111403624116159, tensor([8.5867], grad_fn=<UnbindBackward0>))\n",
      "(8.395477432732141, tensor([6.3655], grad_fn=<UnbindBackward0>))\n",
      "(9.744374580879056, tensor([8.4463], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([8.4463], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([8.5466], grad_fn=<UnbindBackward0>))\n",
      "(7.537430036586509, tensor([8.4272], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([8.7321], grad_fn=<UnbindBackward0>))\n",
      "(7.547501682814967, tensor([8.4536], grad_fn=<UnbindBackward0>))\n",
      "(7.976938756959434, tensor([8.7649], grad_fn=<UnbindBackward0>))\n",
      "(9.828656241913125, tensor([6.8991], grad_fn=<UnbindBackward0>))\n",
      "(6.678342114654332, tensor([7.2491], grad_fn=<UnbindBackward0>))\n",
      "(7.865955413933502, tensor([6.3596], grad_fn=<UnbindBackward0>))\n",
      "(9.365804474600255, tensor([7.8318], grad_fn=<UnbindBackward0>))\n",
      "(7.419979923661835, tensor([7.8642], grad_fn=<UnbindBackward0>))\n",
      "(8.420902531097951, tensor([8.5576], grad_fn=<UnbindBackward0>))\n",
      "(8.802973456578423, tensor([7.8851], grad_fn=<UnbindBackward0>))\n",
      "(6.137727054086234, tensor([7.7847], grad_fn=<UnbindBackward0>))\n",
      "(9.03931477492408, tensor([8.4600], grad_fn=<UnbindBackward0>))\n",
      "(8.88516409509682, tensor([6.6544], grad_fn=<UnbindBackward0>))\n",
      "(7.419380582918692, tensor([8.7697], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([8.5661], grad_fn=<UnbindBackward0>))\n",
      "(8.445267451844648, tensor([8.3781], grad_fn=<UnbindBackward0>))\n",
      "(6.904750769961838, tensor([7.3957], grad_fn=<UnbindBackward0>))\n",
      "(8.571491964823617, tensor([6.4811], grad_fn=<UnbindBackward0>))\n",
      "(7.735870319952567, tensor([6.7349], grad_fn=<UnbindBackward0>))\n",
      "(8.491465042843506, tensor([7.7784], grad_fn=<UnbindBackward0>))\n",
      "(7.975908360165538, tensor([7.8426], grad_fn=<UnbindBackward0>))\n",
      "(8.769662508112274, tensor([10.1659], grad_fn=<UnbindBackward0>))\n",
      "(7.693025748417888, tensor([7.3602], grad_fn=<UnbindBackward0>))\n",
      "(8.293549515060345, tensor([7.1595], grad_fn=<UnbindBackward0>))\n",
      "(7.784889295655098, tensor([8.2525], grad_fn=<UnbindBackward0>))\n",
      "(7.10085190894405, tensor([7.7924], grad_fn=<UnbindBackward0>))\n",
      "(8.756367559802975, tensor([7.2565], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([9.4001], grad_fn=<UnbindBackward0>))\n",
      "(9.596826513486544, tensor([6.7260], grad_fn=<UnbindBackward0>))\n",
      "(9.142275641062053, tensor([6.8258], grad_fn=<UnbindBackward0>))\n",
      "(7.679713639966372, tensor([6.2684], grad_fn=<UnbindBackward0>))\n",
      "(8.483222671845084, tensor([10.1268], grad_fn=<UnbindBackward0>))\n",
      "(8.37770121259764, tensor([7.2360], grad_fn=<UnbindBackward0>))\n",
      "(7.861341795599989, tensor([6.6593], grad_fn=<UnbindBackward0>))\n",
      "(8.869398159883518, tensor([6.2770], grad_fn=<UnbindBackward0>))\n",
      "(9.13938128303569, tensor([6.7179], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([7.0737], grad_fn=<UnbindBackward0>))\n",
      "(8.200013648175434, tensor([10.2594], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([8.6575], grad_fn=<UnbindBackward0>))\n",
      "(8.617762246337932, tensor([8.3345], grad_fn=<UnbindBackward0>))\n",
      "(7.818027938530729, tensor([7.1934], grad_fn=<UnbindBackward0>))\n",
      "(6.206575926724928, tensor([8.1760], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.9596], grad_fn=<UnbindBackward0>))\n",
      "(9.153134898205403, tensor([7.7460], grad_fn=<UnbindBackward0>))\n",
      "(8.754160749323518, tensor([9.4987], grad_fn=<UnbindBackward0>))\n",
      "(6.947937068614969, tensor([7.7793], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([9.2634], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([8.4545], grad_fn=<UnbindBackward0>))\n",
      "(6.678342114654332, tensor([8.6552], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.3344], grad_fn=<UnbindBackward0>))\n",
      "(7.544861068658458, tensor([7.7818], grad_fn=<UnbindBackward0>))\n",
      "(9.7066816015412, tensor([8.1447], grad_fn=<UnbindBackward0>))\n",
      "(8.383433201236713, tensor([8.2115], grad_fn=<UnbindBackward0>))\n",
      "(8.615951963439501, tensor([9.2072], grad_fn=<UnbindBackward0>))\n",
      "(8.261526448396468, tensor([7.2020], grad_fn=<UnbindBackward0>))\n",
      "(9.39715183404299, tensor([7.6275], grad_fn=<UnbindBackward0>))\n",
      "(9.399803036921504, tensor([6.4410], grad_fn=<UnbindBackward0>))\n",
      "(9.751501195538246, tensor([6.4641], grad_fn=<UnbindBackward0>))\n",
      "(8.332308352219117, tensor([8.5710], grad_fn=<UnbindBackward0>))\n",
      "(8.190354403763262, tensor([7.0047], grad_fn=<UnbindBackward0>))\n",
      "(7.1284959456800365, tensor([8.2581], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([7.2340], grad_fn=<UnbindBackward0>))\n",
      "(8.568836424568076, tensor([8.8358], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([6.5999], grad_fn=<UnbindBackward0>))\n",
      "(9.218407743053941, tensor([7.9108], grad_fn=<UnbindBackward0>))\n",
      "(8.429672593886743, tensor([7.2186], grad_fn=<UnbindBackward0>))\n",
      "(7.356918242356021, tensor([7.1931], grad_fn=<UnbindBackward0>))\n",
      "(8.612139668725192, tensor([8.9584], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([8.7588], grad_fn=<UnbindBackward0>))\n",
      "(7.459914766241105, tensor([8.8252], grad_fn=<UnbindBackward0>))\n",
      "(6.704414354964107, tensor([7.6736], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([9.3554], grad_fn=<UnbindBackward0>))\n",
      "(7.8383433155571165, tensor([7.2663], grad_fn=<UnbindBackward0>))\n",
      "(6.202535517187923, tensor([6.5018], grad_fn=<UnbindBackward0>))\n",
      "(6.838405200847344, tensor([8.4175], grad_fn=<UnbindBackward0>))\n",
      "(7.949444420250626, tensor([6.2886], grad_fn=<UnbindBackward0>))\n",
      "(8.744009988096744, tensor([6.7504], grad_fn=<UnbindBackward0>))\n",
      "(9.677151410322754, tensor([6.2870], grad_fn=<UnbindBackward0>))\n",
      "(6.410174881966167, tensor([9.6675], grad_fn=<UnbindBackward0>))\n",
      "(7.808729306744399, tensor([9.4466], grad_fn=<UnbindBackward0>))\n",
      "(7.416378479192928, tensor([9.4234], grad_fn=<UnbindBackward0>))\n",
      "(6.135564891081739, tensor([6.6125], grad_fn=<UnbindBackward0>))\n",
      "(8.928640371201949, tensor([8.4033], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([9.3134], grad_fn=<UnbindBackward0>))\n",
      "(7.384610383176974, tensor([6.7787], grad_fn=<UnbindBackward0>))\n",
      "(6.752270376141742, tensor([8.3235], grad_fn=<UnbindBackward0>))\n",
      "(7.841885928984623, tensor([6.2812], grad_fn=<UnbindBackward0>))\n",
      "(9.818746324081037, tensor([7.3386], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([8.0331], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([8.9020], grad_fn=<UnbindBackward0>))\n",
      "(9.758346166866458, tensor([6.7218], grad_fn=<UnbindBackward0>))\n",
      "(8.840290669088972, tensor([9.1673], grad_fn=<UnbindBackward0>))\n",
      "(8.24143968982973, tensor([6.8726], grad_fn=<UnbindBackward0>))\n",
      "(6.354370040797351, tensor([6.9056], grad_fn=<UnbindBackward0>))\n",
      "(8.85907931788153, tensor([7.5417], grad_fn=<UnbindBackward0>))\n",
      "(7.544861068658458, tensor([8.4125], grad_fn=<UnbindBackward0>))\n",
      "(7.883069351305753, tensor([6.1861], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([8.8084], grad_fn=<UnbindBackward0>))\n",
      "(7.1891677384203225, tensor([8.5025], grad_fn=<UnbindBackward0>))\n",
      "(7.652070746116482, tensor([6.7083], grad_fn=<UnbindBackward0>))\n",
      "(6.955592608396297, tensor([8.6020], grad_fn=<UnbindBackward0>))\n",
      "(8.257644958208228, tensor([7.3422], grad_fn=<UnbindBackward0>))\n",
      "(8.07402621612406, tensor([7.1467], grad_fn=<UnbindBackward0>))\n",
      "(8.700680734850161, tensor([8.1803], grad_fn=<UnbindBackward0>))\n",
      "(9.43220310339067, tensor([8.2555], grad_fn=<UnbindBackward0>))\n",
      "(6.3835066348840055, tensor([8.7095], grad_fn=<UnbindBackward0>))\n",
      "(7.249925536717988, tensor([7.4764], grad_fn=<UnbindBackward0>))\n",
      "(6.401917196727186, tensor([8.4914], grad_fn=<UnbindBackward0>))\n",
      "(6.452048954437226, tensor([6.5324], grad_fn=<UnbindBackward0>))\n",
      "(9.290352309945568, tensor([7.1382], grad_fn=<UnbindBackward0>))\n",
      "(8.724369949208368, tensor([7.1804], grad_fn=<UnbindBackward0>))\n",
      "(9.355392643675321, tensor([8.6222], grad_fn=<UnbindBackward0>))\n",
      "(9.201299627324033, tensor([6.6824], grad_fn=<UnbindBackward0>))\n",
      "(8.350666240520924, tensor([7.8103], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.3009], grad_fn=<UnbindBackward0>))\n",
      "(6.9726062513017535, tensor([6.3090], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([8.5688], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([8.5840], grad_fn=<UnbindBackward0>))\n",
      "(9.058470422638056, tensor([7.8100], grad_fn=<UnbindBackward0>))\n",
      "(7.499423290592229, tensor([7.9584], grad_fn=<UnbindBackward0>))\n",
      "(7.814399633804487, tensor([8.0143], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([8.4583], grad_fn=<UnbindBackward0>))\n",
      "(7.599401333415815, tensor([7.1421], grad_fn=<UnbindBackward0>))\n",
      "(6.733401891837359, tensor([6.4787], grad_fn=<UnbindBackward0>))\n",
      "(7.655390644826152, tensor([6.7781], grad_fn=<UnbindBackward0>))\n",
      "(8.691146498539675, tensor([7.2743], grad_fn=<UnbindBackward0>))\n",
      "(6.9411900550683745, tensor([8.8537], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([7.1536], grad_fn=<UnbindBackward0>))\n",
      "(7.396948602621014, tensor([8.4012], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([8.2489], grad_fn=<UnbindBackward0>))\n",
      "(7.868254265520613, tensor([6.2323], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([8.3680], grad_fn=<UnbindBackward0>))\n",
      "(7.247792581767846, tensor([8.4100], grad_fn=<UnbindBackward0>))\n",
      "(6.375024819828097, tensor([6.2947], grad_fn=<UnbindBackward0>))\n",
      "(7.0825485693553, tensor([8.6733], grad_fn=<UnbindBackward0>))\n",
      "(6.647688373563329, tensor([6.8018], grad_fn=<UnbindBackward0>))\n",
      "(7.949797216161852, tensor([5.9817], grad_fn=<UnbindBackward0>))\n",
      "(7.884952945759814, tensor([5.8651], grad_fn=<UnbindBackward0>))\n",
      "(7.496652438168283, tensor([6.1398], grad_fn=<UnbindBackward0>))\n",
      "(6.07073772800249, tensor([8.3017], grad_fn=<UnbindBackward0>))\n",
      "(6.045005314036012, tensor([7.8312], grad_fn=<UnbindBackward0>))\n",
      "(6.7464121285733745, tensor([6.3631], grad_fn=<UnbindBackward0>))\n",
      "(8.188133414510478, tensor([6.9940], grad_fn=<UnbindBackward0>))\n",
      "(7.488293515159428, tensor([8.4754], grad_fn=<UnbindBackward0>))\n",
      "(7.706162970199576, tensor([9.1266], grad_fn=<UnbindBackward0>))\n",
      "(7.703007682479236, tensor([9.0369], grad_fn=<UnbindBackward0>))\n",
      "(8.429454277108231, tensor([6.5318], grad_fn=<UnbindBackward0>))\n",
      "(7.0909098220799835, tensor([9.3660], grad_fn=<UnbindBackward0>))\n",
      "(6.742880635791903, tensor([6.2504], grad_fn=<UnbindBackward0>))\n",
      "(6.698268054115413, tensor([6.5706], grad_fn=<UnbindBackward0>))\n",
      "(7.149131598557407, tensor([6.2391], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([7.7347], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([7.2299], grad_fn=<UnbindBackward0>))\n",
      "(6.584791392385716, tensor([9.8360], grad_fn=<UnbindBackward0>))\n",
      "(8.051659556841953, tensor([8.4289], grad_fn=<UnbindBackward0>))\n",
      "(8.875985891325971, tensor([7.8239], grad_fn=<UnbindBackward0>))\n",
      "(7.681560362559537, tensor([6.3276], grad_fn=<UnbindBackward0>))\n",
      "(6.154858094016418, tensor([6.7832], grad_fn=<UnbindBackward0>))\n",
      "(7.631916513071252, tensor([6.4242], grad_fn=<UnbindBackward0>))\n",
      "(7.643003635560718, tensor([7.8248], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([8.7640], grad_fn=<UnbindBackward0>))\n",
      "(9.576510097476705, tensor([7.1185], grad_fn=<UnbindBackward0>))\n",
      "(8.807471889715284, tensor([8.7659], grad_fn=<UnbindBackward0>))\n",
      "(9.809121926500406, tensor([7.2427], grad_fn=<UnbindBackward0>))\n",
      "(6.2878585601617845, tensor([6.4764], grad_fn=<UnbindBackward0>))\n",
      "(8.789203094473724, tensor([9.2493], grad_fn=<UnbindBackward0>))\n",
      "(7.884576510596324, tensor([7.1347], grad_fn=<UnbindBackward0>))\n",
      "(7.98514393119862, tensor([7.1860], grad_fn=<UnbindBackward0>))\n",
      "(6.6895992691789665, tensor([6.8552], grad_fn=<UnbindBackward0>))\n",
      "(6.741700694652055, tensor([8.5177], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([9.4442], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([9.0215], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([8.8025], grad_fn=<UnbindBackward0>))\n",
      "(8.740496729931813, tensor([7.0590], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([7.3976], grad_fn=<UnbindBackward0>))\n",
      "(8.963544291996744, tensor([7.1701], grad_fn=<UnbindBackward0>))\n",
      "(6.07993319509559, tensor([9.2418], grad_fn=<UnbindBackward0>))\n",
      "(7.9294865233142895, tensor([7.3141], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([6.7061], grad_fn=<UnbindBackward0>))\n",
      "(8.669399124305569, tensor([6.2194], grad_fn=<UnbindBackward0>))\n",
      "(7.203405521083095, tensor([9.0045], grad_fn=<UnbindBackward0>))\n",
      "(8.744488113852924, tensor([9.3807], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([8.0570], grad_fn=<UnbindBackward0>))\n",
      "(7.198931240688173, tensor([9.5615], grad_fn=<UnbindBackward0>))\n",
      "(8.254008590564844, tensor([7.4953], grad_fn=<UnbindBackward0>))\n",
      "(7.145196134997171, tensor([8.5352], grad_fn=<UnbindBackward0>))\n",
      "(9.540578933841877, tensor([6.2508], grad_fn=<UnbindBackward0>))\n",
      "(8.413830678421084, tensor([6.2821], grad_fn=<UnbindBackward0>))\n",
      "(9.747710550964285, tensor([9.9448], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.4579], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([7.9995], grad_fn=<UnbindBackward0>))\n",
      "(6.52649485957079, tensor([6.7005], grad_fn=<UnbindBackward0>))\n",
      "(8.414052432496725, tensor([8.2971], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([7.2951], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.4225], grad_fn=<UnbindBackward0>))\n",
      "(6.413458957167357, tensor([8.1131], grad_fn=<UnbindBackward0>))\n",
      "(8.588210678651517, tensor([7.5396], grad_fn=<UnbindBackward0>))\n",
      "(7.062191632286556, tensor([8.5669], grad_fn=<UnbindBackward0>))\n",
      "(8.308199063206446, tensor([6.8072], grad_fn=<UnbindBackward0>))\n",
      "(8.458928283284262, tensor([8.0399], grad_fn=<UnbindBackward0>))\n",
      "(6.887552571664617, tensor([7.2317], grad_fn=<UnbindBackward0>))\n",
      "(8.188133414510478, tensor([8.1691], grad_fn=<UnbindBackward0>))\n",
      "(7.739794458408701, tensor([6.4006], grad_fn=<UnbindBackward0>))\n",
      "(8.789355452209982, tensor([6.2969], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([9.2539], grad_fn=<UnbindBackward0>))\n",
      "(8.643120748014027, tensor([7.2600], grad_fn=<UnbindBackward0>))\n",
      "(6.799055862058796, tensor([6.3014], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([7.9375], grad_fn=<UnbindBackward0>))\n",
      "(7.649216319820633, tensor([8.0334], grad_fn=<UnbindBackward0>))\n",
      "(6.742880635791903, tensor([8.4960], grad_fn=<UnbindBackward0>))\n",
      "(9.418085981110297, tensor([8.8762], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([8.6865], grad_fn=<UnbindBackward0>))\n",
      "(6.452048954437226, tensor([6.3056], grad_fn=<UnbindBackward0>))\n",
      "(6.822197390620491, tensor([7.8210], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([7.9607], grad_fn=<UnbindBackward0>))\n",
      "(9.735956081214809, tensor([8.6372], grad_fn=<UnbindBackward0>))\n",
      "(7.682482446534506, tensor([6.7509], grad_fn=<UnbindBackward0>))\n",
      "(8.796036315200814, tensor([8.6391], grad_fn=<UnbindBackward0>))\n",
      "(7.670428522190693, tensor([6.8186], grad_fn=<UnbindBackward0>))\n",
      "(9.478533767844102, tensor([9.1711], grad_fn=<UnbindBackward0>))\n",
      "(8.886685639065584, tensor([7.8393], grad_fn=<UnbindBackward0>))\n",
      "(8.359603270841466, tensor([6.4282], grad_fn=<UnbindBackward0>))\n",
      "(7.88570539124302, tensor([6.3772], grad_fn=<UnbindBackward0>))\n",
      "(6.182084906716632, tensor([6.6581], grad_fn=<UnbindBackward0>))\n",
      "(6.687108607866515, tensor([6.7956], grad_fn=<UnbindBackward0>))\n",
      "(9.272563715156315, tensor([7.8214], grad_fn=<UnbindBackward0>))\n",
      "(9.038008749211576, tensor([6.3408], grad_fn=<UnbindBackward0>))\n",
      "(8.639587799629844, tensor([7.7229], grad_fn=<UnbindBackward0>))\n",
      "(6.837332814685591, tensor([6.8105], grad_fn=<UnbindBackward0>))\n",
      "(7.018401799069201, tensor([6.7545], grad_fn=<UnbindBackward0>))\n",
      "(9.121181235656364, tensor([8.5342], grad_fn=<UnbindBackward0>))\n",
      "(9.042868018032435, tensor([8.6787], grad_fn=<UnbindBackward0>))\n",
      "(9.696647867928448, tensor([8.7943], grad_fn=<UnbindBackward0>))\n",
      "(6.8690144506657065, tensor([6.7006], grad_fn=<UnbindBackward0>))\n",
      "(9.364090971782101, tensor([6.9389], grad_fn=<UnbindBackward0>))\n",
      "(7.074963197966044, tensor([6.2250], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([9.1128], grad_fn=<UnbindBackward0>))\n",
      "(6.525029657843462, tensor([7.9928], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([7.0044], grad_fn=<UnbindBackward0>))\n",
      "(8.614138397472717, tensor([8.5522], grad_fn=<UnbindBackward0>))\n",
      "(8.07402621612406, tensor([6.2373], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([6.7493], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([9.2982], grad_fn=<UnbindBackward0>))\n",
      "(8.487352349405215, tensor([6.2597], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([6.7316], grad_fn=<UnbindBackward0>))\n",
      "(7.4342573821331355, tensor([6.2927], grad_fn=<UnbindBackward0>))\n",
      "(7.69484807238461, tensor([8.9196], grad_fn=<UnbindBackward0>))\n",
      "(8.499029220788566, tensor([8.5630], grad_fn=<UnbindBackward0>))\n",
      "(9.830594227015803, tensor([8.6672], grad_fn=<UnbindBackward0>))\n",
      "(7.763021309018518, tensor([8.4372], grad_fn=<UnbindBackward0>))\n",
      "(7.071573364211532, tensor([9.1916], grad_fn=<UnbindBackward0>))\n",
      "(7.503289630675082, tensor([8.7068], grad_fn=<UnbindBackward0>))\n",
      "(9.82271124250884, tensor([6.2647], grad_fn=<UnbindBackward0>))\n",
      "(7.344072850573066, tensor([8.3139], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([5.7945], grad_fn=<UnbindBackward0>))\n",
      "(8.022568946988255, tensor([10.3843], grad_fn=<UnbindBackward0>))\n",
      "(7.491087593534876, tensor([7.7134], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([6.7550], grad_fn=<UnbindBackward0>))\n",
      "(9.063926353520646, tensor([6.2457], grad_fn=<UnbindBackward0>))\n",
      "(9.039789270781144, tensor([5.7997], grad_fn=<UnbindBackward0>))\n",
      "(7.163946684342547, tensor([7.1028], grad_fn=<UnbindBackward0>))\n",
      "(5.978885764901122, tensor([9.8050], grad_fn=<UnbindBackward0>))\n",
      "(8.053887083618223, tensor([6.8467], grad_fn=<UnbindBackward0>))\n",
      "(7.711101251840158, tensor([6.4153], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([8.8149], grad_fn=<UnbindBackward0>))\n",
      "(8.774003600200931, tensor([8.6117], grad_fn=<UnbindBackward0>))\n",
      "(8.36822903827628, tensor([6.3622], grad_fn=<UnbindBackward0>))\n",
      "(8.912877287669296, tensor([7.8878], grad_fn=<UnbindBackward0>))\n",
      "(8.643120748014027, tensor([8.6039], grad_fn=<UnbindBackward0>))\n",
      "(7.4109518755836366, tensor([6.6630], grad_fn=<UnbindBackward0>))\n",
      "(7.362010551259734, tensor([8.3942], grad_fn=<UnbindBackward0>))\n",
      "(8.978156076009824, tensor([6.6195], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([9.4241], grad_fn=<UnbindBackward0>))\n",
      "(8.459987717645458, tensor([7.3591], grad_fn=<UnbindBackward0>))\n",
      "(7.394493107219038, tensor([6.5780], grad_fn=<UnbindBackward0>))\n",
      "(6.481577129276431, tensor([6.5761], grad_fn=<UnbindBackward0>))\n",
      "(9.351839934249883, tensor([6.3190], grad_fn=<UnbindBackward0>))\n",
      "(7.433666540166168, tensor([8.3973], grad_fn=<UnbindBackward0>))\n",
      "(7.059617628291383, tensor([7.2123], grad_fn=<UnbindBackward0>))\n",
      "(8.716044050161402, tensor([9.4833], grad_fn=<UnbindBackward0>))\n",
      "(8.708969906980947, tensor([8.5570], grad_fn=<UnbindBackward0>))\n",
      "(8.591929537530255, tensor([7.8160], grad_fn=<UnbindBackward0>))\n",
      "(8.45914025996762, tensor([6.4578], grad_fn=<UnbindBackward0>))\n",
      "(6.78105762593618, tensor([6.2474], grad_fn=<UnbindBackward0>))\n",
      "(8.988695696785708, tensor([7.6787], grad_fn=<UnbindBackward0>))\n",
      "(8.356319965828153, tensor([9.4171], grad_fn=<UnbindBackward0>))\n",
      "(6.7650389767805414, tensor([6.3068], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([6.1298], grad_fn=<UnbindBackward0>))\n",
      "(7.00669522683704, tensor([7.9210], grad_fn=<UnbindBackward0>))\n",
      "(7.419380582918692, tensor([7.1807], grad_fn=<UnbindBackward0>))\n",
      "(9.103979355984773, tensor([6.9783], grad_fn=<UnbindBackward0>))\n",
      "(7.186144304522325, tensor([9.3454], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([7.4057], grad_fn=<UnbindBackward0>))\n",
      "(6.9930151229329605, tensor([8.5355], grad_fn=<UnbindBackward0>))\n",
      "(8.804325112562537, tensor([7.9552], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([6.6996], grad_fn=<UnbindBackward0>))\n",
      "(8.562931083090092, tensor([7.2307], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([6.2644], grad_fn=<UnbindBackward0>))\n",
      "(9.412872969386267, tensor([8.6584], grad_fn=<UnbindBackward0>))\n",
      "(7.465655310134056, tensor([6.3888], grad_fn=<UnbindBackward0>))\n",
      "(7.8160138391590275, tensor([8.7020], grad_fn=<UnbindBackward0>))\n",
      "(7.57198844937744, tensor([8.8897], grad_fn=<UnbindBackward0>))\n",
      "(6.8501261661455, tensor([5.9402], grad_fn=<UnbindBackward0>))\n",
      "(9.802451008358355, tensor([7.8305], grad_fn=<UnbindBackward0>))\n",
      "(6.996681488176539, tensor([9.4836], grad_fn=<UnbindBackward0>))\n",
      "(6.4692503167957724, tensor([8.4629], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([6.7954], grad_fn=<UnbindBackward0>))\n",
      "(6.828712071641684, tensor([6.7978], grad_fn=<UnbindBackward0>))\n",
      "(8.401333305321703, tensor([6.2862], grad_fn=<UnbindBackward0>))\n",
      "(7.400620577371135, tensor([6.3174], grad_fn=<UnbindBackward0>))\n",
      "(6.656726524178391, tensor([9.8131], grad_fn=<UnbindBackward0>))\n",
      "(8.470311205516108, tensor([7.1214], grad_fn=<UnbindBackward0>))\n",
      "(6.447305862541213, tensor([7.1802], grad_fn=<UnbindBackward0>))\n",
      "(8.315566483564277, tensor([9.9937], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([6.4287], grad_fn=<UnbindBackward0>))\n",
      "(7.2640301428995295, tensor([8.3001], grad_fn=<UnbindBackward0>))\n",
      "(6.97914527506881, tensor([9.9697], grad_fn=<UnbindBackward0>))\n",
      "(8.453827315794417, tensor([8.4877], grad_fn=<UnbindBackward0>))\n",
      "(9.23863624113103, tensor([7.7128], grad_fn=<UnbindBackward0>))\n",
      "(7.545918151209323, tensor([6.2549], grad_fn=<UnbindBackward0>))\n",
      "(6.2285110035911835, tensor([8.0232], grad_fn=<UnbindBackward0>))\n",
      "(7.767687277186908, tensor([8.1990], grad_fn=<UnbindBackward0>))\n",
      "(8.790269111478656, tensor([8.2497], grad_fn=<UnbindBackward0>))\n",
      "(9.128804883993665, tensor([8.8709], grad_fn=<UnbindBackward0>))\n",
      "(7.493317248862145, tensor([7.8916], grad_fn=<UnbindBackward0>))\n",
      "(7.781138509845015, tensor([7.6055], grad_fn=<UnbindBackward0>))\n",
      "(8.0861025356691, tensor([7.8647], grad_fn=<UnbindBackward0>))\n",
      "(8.388450315523512, tensor([7.7008], grad_fn=<UnbindBackward0>))\n",
      "(9.323222529032856, tensor([6.4083], grad_fn=<UnbindBackward0>))\n",
      "(7.498869733976931, tensor([6.6311], grad_fn=<UnbindBackward0>))\n",
      "(8.028455164114252, tensor([6.6796], grad_fn=<UnbindBackward0>))\n",
      "(8.48446336679332, tensor([7.7988], grad_fn=<UnbindBackward0>))\n",
      "(8.351138607086154, tensor([8.8529], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([10.0305], grad_fn=<UnbindBackward0>))\n",
      "(8.833608482690892, tensor([9.0420], grad_fn=<UnbindBackward0>))\n",
      "(6.628041376179533, tensor([7.3706], grad_fn=<UnbindBackward0>))\n",
      "(7.271703706887368, tensor([9.3967], grad_fn=<UnbindBackward0>))\n",
      "(6.947937068614969, tensor([8.7203], grad_fn=<UnbindBackward0>))\n",
      "(7.506591780070841, tensor([6.2716], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([7.7859], grad_fn=<UnbindBackward0>))\n",
      "(6.823286122355687, tensor([7.2698], grad_fn=<UnbindBackward0>))\n",
      "(8.514790306799927, tensor([7.8397], grad_fn=<UnbindBackward0>))\n",
      "(9.795790977080754, tensor([7.8881], grad_fn=<UnbindBackward0>))\n",
      "(9.418329675518715, tensor([7.7504], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([7.1725], grad_fn=<UnbindBackward0>))\n",
      "(9.216521231051264, tensor([6.8778], grad_fn=<UnbindBackward0>))\n",
      "(6.699500340161678, tensor([6.4185], grad_fn=<UnbindBackward0>))\n",
      "(7.274479558773871, tensor([7.5258], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([7.1991], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([6.6727], grad_fn=<UnbindBackward0>))\n",
      "(6.866933284461882, tensor([8.5151], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.2742], grad_fn=<UnbindBackward0>))\n",
      "(7.104144092987527, tensor([7.8516], grad_fn=<UnbindBackward0>))\n",
      "(9.767610553984774, tensor([7.7752], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([9.0008], grad_fn=<UnbindBackward0>))\n",
      "(7.126087273299125, tensor([8.6033], grad_fn=<UnbindBackward0>))\n",
      "(8.049107721326406, tensor([8.4954], grad_fn=<UnbindBackward0>))\n",
      "(7.762596048540069, tensor([8.6582], grad_fn=<UnbindBackward0>))\n",
      "(6.326149473155099, tensor([5.9539], grad_fn=<UnbindBackward0>))\n",
      "(6.489204931325317, tensor([7.4371], grad_fn=<UnbindBackward0>))\n",
      "(8.56522116042682, tensor([7.2116], grad_fn=<UnbindBackward0>))\n",
      "(8.862625169408922, tensor([5.8781], grad_fn=<UnbindBackward0>))\n",
      "(7.577633832602728, tensor([6.1852], grad_fn=<UnbindBackward0>))\n",
      "(6.926577033222725, tensor([9.0670], grad_fn=<UnbindBackward0>))\n",
      "(7.450660796211539, tensor([7.9221], grad_fn=<UnbindBackward0>))\n",
      "(6.9440872082295275, tensor([7.5144], grad_fn=<UnbindBackward0>))\n",
      "(7.80954132465341, tensor([6.4153], grad_fn=<UnbindBackward0>))\n",
      "(6.0844994130751715, tensor([6.1990], grad_fn=<UnbindBackward0>))\n",
      "(6.807934943699926, tensor([8.1952], grad_fn=<UnbindBackward0>))\n",
      "(7.017506142941256, tensor([8.7533], grad_fn=<UnbindBackward0>))\n",
      "(8.638702608813434, tensor([6.4137], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([6.6727], grad_fn=<UnbindBackward0>))\n",
      "(9.829087229959878, tensor([7.3747], grad_fn=<UnbindBackward0>))\n",
      "(7.781138509845015, tensor([10.0338], grad_fn=<UnbindBackward0>))\n",
      "(8.80357441813497, tensor([6.1684], grad_fn=<UnbindBackward0>))\n",
      "(8.281723990411392, tensor([7.8775], grad_fn=<UnbindBackward0>))\n",
      "(6.732210706467206, tensor([7.7985], grad_fn=<UnbindBackward0>))\n",
      "(8.444622498581403, tensor([8.9157], grad_fn=<UnbindBackward0>))\n",
      "(5.793013608384144, tensor([6.6608], grad_fn=<UnbindBackward0>))\n",
      "(6.111467339502679, tensor([8.9835], grad_fn=<UnbindBackward0>))\n",
      "(7.546446273746024, tensor([8.4131], grad_fn=<UnbindBackward0>))\n",
      "(8.424858580213442, tensor([6.2038], grad_fn=<UnbindBackward0>))\n",
      "(6.464588303689961, tensor([8.9910], grad_fn=<UnbindBackward0>))\n",
      "(8.063377822367027, tensor([6.3073], grad_fn=<UnbindBackward0>))\n",
      "(8.861633597686627, tensor([8.5915], grad_fn=<UnbindBackward0>))\n",
      "(7.237059026124737, tensor([8.8234], grad_fn=<UnbindBackward0>))\n",
      "(8.449128460502108, tensor([6.6478], grad_fn=<UnbindBackward0>))\n",
      "(8.99342737041261, tensor([7.7552], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.0894], grad_fn=<UnbindBackward0>))\n",
      "(6.693323668269949, tensor([8.7193], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([7.6480], grad_fn=<UnbindBackward0>))\n",
      "(9.508591395777199, tensor([7.8803], grad_fn=<UnbindBackward0>))\n",
      "(7.508238774678663, tensor([7.9058], grad_fn=<UnbindBackward0>))\n",
      "(8.12533508671429, tensor([6.7435], grad_fn=<UnbindBackward0>))\n",
      "(7.166265974133638, tensor([6.7064], grad_fn=<UnbindBackward0>))\n",
      "(9.608445125678054, tensor([7.8862], grad_fn=<UnbindBackward0>))\n",
      "(8.152774052744075, tensor([7.9551], grad_fn=<UnbindBackward0>))\n",
      "(8.696677393390031, tensor([6.7610], grad_fn=<UnbindBackward0>))\n",
      "(8.2960476427647, tensor([9.5622], grad_fn=<UnbindBackward0>))\n",
      "(9.211639527707803, tensor([8.7806], grad_fn=<UnbindBackward0>))\n",
      "(6.573680166960646, tensor([9.8948], grad_fn=<UnbindBackward0>))\n",
      "(8.78615105486974, tensor([5.9947], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([6.6219], grad_fn=<UnbindBackward0>))\n",
      "(8.455104999102815, tensor([8.6487], grad_fn=<UnbindBackward0>))\n",
      "(8.173293438966228, tensor([7.7369], grad_fn=<UnbindBackward0>))\n",
      "(7.4673710669175595, tensor([8.7372], grad_fn=<UnbindBackward0>))\n",
      "(9.549238235812235, tensor([6.3324], grad_fn=<UnbindBackward0>))\n",
      "(7.8567067930958405, tensor([7.7948], grad_fn=<UnbindBackward0>))\n",
      "(7.735870319952567, tensor([7.3691], grad_fn=<UnbindBackward0>))\n",
      "(8.704170559746386, tensor([8.4062], grad_fn=<UnbindBackward0>))\n",
      "(9.551231402759107, tensor([6.3316], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([7.7158], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([8.4618], grad_fn=<UnbindBackward0>))\n",
      "(9.132919225007598, tensor([8.6085], grad_fn=<UnbindBackward0>))\n",
      "(7.478734825567875, tensor([7.1693], grad_fn=<UnbindBackward0>))\n",
      "(7.0192966537150445, tensor([6.3267], grad_fn=<UnbindBackward0>))\n",
      "(8.883501584323207, tensor([6.8385], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([8.4737], grad_fn=<UnbindBackward0>))\n",
      "(8.697679732264463, tensor([7.1392], grad_fn=<UnbindBackward0>))\n",
      "(6.665683717782408, tensor([6.5613], grad_fn=<UnbindBackward0>))\n",
      "(9.121399862679535, tensor([7.3108], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([8.5511], grad_fn=<UnbindBackward0>))\n",
      "(8.332067707289548, tensor([6.7743], grad_fn=<UnbindBackward0>))\n",
      "(7.917900586327916, tensor([8.2491], grad_fn=<UnbindBackward0>))\n",
      "(7.02108396428914, tensor([8.8181], grad_fn=<UnbindBackward0>))\n",
      "(8.46653127661401, tensor([7.1223], grad_fn=<UnbindBackward0>))\n",
      "(6.77078942390898, tensor([8.8403], grad_fn=<UnbindBackward0>))\n",
      "(8.88751459738882, tensor([6.2915], grad_fn=<UnbindBackward0>))\n",
      "(6.1070228877422545, tensor([7.6114], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([8.6689], grad_fn=<UnbindBackward0>))\n",
      "(6.511745329644728, tensor([6.3029], grad_fn=<UnbindBackward0>))\n",
      "(8.266935347610456, tensor([7.3296], grad_fn=<UnbindBackward0>))\n",
      "(7.506591780070841, tensor([7.9628], grad_fn=<UnbindBackward0>))\n",
      "(7.96346006663897, tensor([8.6253], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([7.4232], grad_fn=<UnbindBackward0>))\n",
      "(8.183676582620658, tensor([9.0675], grad_fn=<UnbindBackward0>))\n",
      "(9.754871528207344, tensor([8.9854], grad_fn=<UnbindBackward0>))\n",
      "(7.926602599181384, tensor([7.6827], grad_fn=<UnbindBackward0>))\n",
      "(9.137339479091693, tensor([7.3430], grad_fn=<UnbindBackward0>))\n",
      "(8.475746001502063, tensor([6.2760], grad_fn=<UnbindBackward0>))\n",
      "(6.1092475827643655, tensor([6.3243], grad_fn=<UnbindBackward0>))\n",
      "(7.76046702921342, tensor([6.6148], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([9.0980], grad_fn=<UnbindBackward0>))\n",
      "(7.772331575169614, tensor([7.3426], grad_fn=<UnbindBackward0>))\n",
      "(6.586171654854675, tensor([7.7740], grad_fn=<UnbindBackward0>))\n",
      "(7.271008538280992, tensor([7.8302], grad_fn=<UnbindBackward0>))\n",
      "(8.716863386544805, tensor([7.0982], grad_fn=<UnbindBackward0>))\n",
      "(9.584314812992977, tensor([6.2886], grad_fn=<UnbindBackward0>))\n",
      "(7.872073979866873, tensor([8.7002], grad_fn=<UnbindBackward0>))\n",
      "(8.731013415269564, tensor([8.2486], grad_fn=<UnbindBackward0>))\n",
      "(7.57198844937744, tensor([6.6238], grad_fn=<UnbindBackward0>))\n",
      "(8.65067458279072, tensor([6.6614], grad_fn=<UnbindBackward0>))\n",
      "(6.643789733147672, tensor([6.4302], grad_fn=<UnbindBackward0>))\n",
      "(6.415096959171596, tensor([6.8452], grad_fn=<UnbindBackward0>))\n",
      "(6.437751649736401, tensor([9.8826], grad_fn=<UnbindBackward0>))\n",
      "(7.4645098346365275, tensor([8.2631], grad_fn=<UnbindBackward0>))\n",
      "(7.395721608602045, tensor([7.3377], grad_fn=<UnbindBackward0>))\n",
      "(9.576995349297256, tensor([7.7837], grad_fn=<UnbindBackward0>))\n",
      "(8.297543529356284, tensor([6.7208], grad_fn=<UnbindBackward0>))\n",
      "(9.350189267092581, tensor([7.9280], grad_fn=<UnbindBackward0>))\n",
      "(6.90875477931522, tensor([7.4293], grad_fn=<UnbindBackward0>))\n",
      "(7.195937226475569, tensor([8.6094], grad_fn=<UnbindBackward0>))\n",
      "(6.9584483932976555, tensor([6.6904], grad_fn=<UnbindBackward0>))\n",
      "(6.86171134048073, tensor([8.4996], grad_fn=<UnbindBackward0>))\n",
      "(8.783089671796096, tensor([6.3077], grad_fn=<UnbindBackward0>))\n",
      "(7.564238475170491, tensor([6.5027], grad_fn=<UnbindBackward0>))\n",
      "(8.625509334899697, tensor([6.6725], grad_fn=<UnbindBackward0>))\n",
      "(6.928537818164665, tensor([8.8816], grad_fn=<UnbindBackward0>))\n",
      "(8.730690365678642, tensor([9.5806], grad_fn=<UnbindBackward0>))\n",
      "(8.549466751966532, tensor([8.7254], grad_fn=<UnbindBackward0>))\n",
      "(8.146419323098003, tensor([10.1238], grad_fn=<UnbindBackward0>))\n",
      "(6.717804695023691, tensor([6.9532], grad_fn=<UnbindBackward0>))\n",
      "(7.282761179605593, tensor([9.9972], grad_fn=<UnbindBackward0>))\n",
      "(9.242904362849446, tensor([7.3151], grad_fn=<UnbindBackward0>))\n",
      "(7.454719949364001, tensor([6.4818], grad_fn=<UnbindBackward0>))\n",
      "(9.429957713513835, tensor([5.7492], grad_fn=<UnbindBackward0>))\n",
      "(9.35686202463895, tensor([6.3573], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.6874], grad_fn=<UnbindBackward0>))\n",
      "(8.640825751861513, tensor([9.4571], grad_fn=<UnbindBackward0>))\n",
      "(6.498282149476434, tensor([6.3316], grad_fn=<UnbindBackward0>))\n",
      "(6.80903930604298, tensor([6.7180], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.8724], grad_fn=<UnbindBackward0>))\n",
      "(9.340139809511028, tensor([8.8982], grad_fn=<UnbindBackward0>))\n",
      "(8.60392119492606, tensor([8.9169], grad_fn=<UnbindBackward0>))\n",
      "(8.464635940677562, tensor([8.1579], grad_fn=<UnbindBackward0>))\n",
      "(6.747586526829315, tensor([6.3836], grad_fn=<UnbindBackward0>))\n",
      "(8.063377822367027, tensor([7.3050], grad_fn=<UnbindBackward0>))\n",
      "(9.19339765124587, tensor([6.3430], grad_fn=<UnbindBackward0>))\n",
      "(7.554334823725748, tensor([7.0958], grad_fn=<UnbindBackward0>))\n",
      "(7.879669914604289, tensor([7.7802], grad_fn=<UnbindBackward0>))\n",
      "(6.805722553416985, tensor([7.3408], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([8.0682], grad_fn=<UnbindBackward0>))\n",
      "(9.800235922725653, tensor([10.3616], grad_fn=<UnbindBackward0>))\n",
      "(8.830396801098097, tensor([7.8237], grad_fn=<UnbindBackward0>))\n",
      "(8.70334075304372, tensor([6.2448], grad_fn=<UnbindBackward0>))\n",
      "(8.092851027538384, tensor([9.7023], grad_fn=<UnbindBackward0>))\n",
      "(8.200013648175434, tensor([8.4269], grad_fn=<UnbindBackward0>))\n",
      "(8.887790764195326, tensor([7.0175], grad_fn=<UnbindBackward0>))\n",
      "(6.934397209928558, tensor([8.2133], grad_fn=<UnbindBackward0>))\n",
      "(9.232982102457008, tensor([8.6789], grad_fn=<UnbindBackward0>))\n",
      "(8.037866234709618, tensor([6.6428], grad_fn=<UnbindBackward0>))\n",
      "(7.195937226475569, tensor([8.8327], grad_fn=<UnbindBackward0>))\n",
      "(6.398594934535208, tensor([7.1886], grad_fn=<UnbindBackward0>))\n",
      "(6.668228248417403, tensor([7.7006], grad_fn=<UnbindBackward0>))\n",
      "(8.442469645220301, tensor([9.3064], grad_fn=<UnbindBackward0>))\n",
      "(7.142827401161621, tensor([8.4671], grad_fn=<UnbindBackward0>))\n",
      "(6.588926477533519, tensor([9.4277], grad_fn=<UnbindBackward0>))\n",
      "(8.096512917501594, tensor([5.8917], grad_fn=<UnbindBackward0>))\n",
      "(7.887959336599945, tensor([6.7447], grad_fn=<UnbindBackward0>))\n",
      "(8.138272638530186, tensor([6.7611], grad_fn=<UnbindBackward0>))\n",
      "(8.338066525518801, tensor([7.5966], grad_fn=<UnbindBackward0>))\n",
      "(6.883462586413092, tensor([6.8360], grad_fn=<UnbindBackward0>))\n",
      "(8.329658067569396, tensor([7.1441], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([8.3481], grad_fn=<UnbindBackward0>))\n",
      "(8.469052816088302, tensor([6.0190], grad_fn=<UnbindBackward0>))\n",
      "(8.695506726812653, tensor([9.4168], grad_fn=<UnbindBackward0>))\n",
      "(7.506591780070841, tensor([7.8023], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.4532], grad_fn=<UnbindBackward0>))\n",
      "(8.927977461002001, tensor([6.6838], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([8.5675], grad_fn=<UnbindBackward0>))\n",
      "(6.29156913955832, tensor([7.2551], grad_fn=<UnbindBackward0>))\n",
      "(8.372398606513004, tensor([7.8083], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([9.5559], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([6.7043], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([6.3871], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([8.0297], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([8.3867], grad_fn=<UnbindBackward0>))\n",
      "(7.544861068658458, tensor([6.7135], grad_fn=<UnbindBackward0>))\n",
      "(8.23217423638394, tensor([8.3859], grad_fn=<UnbindBackward0>))\n",
      "(7.00397413672268, tensor([8.5292], grad_fn=<UnbindBackward0>))\n",
      "(8.364275084991524, tensor([6.8106], grad_fn=<UnbindBackward0>))\n",
      "(6.78105762593618, tensor([8.2049], grad_fn=<UnbindBackward0>))\n",
      "(6.875232087276577, tensor([6.7496], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([9.5799], grad_fn=<UnbindBackward0>))\n",
      "(7.00850518208228, tensor([7.1455], grad_fn=<UnbindBackward0>))\n",
      "(8.499029220788566, tensor([7.7824], grad_fn=<UnbindBackward0>))\n",
      "(8.039157390473237, tensor([7.2694], grad_fn=<UnbindBackward0>))\n",
      "(8.28045768658256, tensor([8.5341], grad_fn=<UnbindBackward0>))\n",
      "(8.074649075066652, tensor([8.4663], grad_fn=<UnbindBackward0>))\n",
      "(8.402679804627477, tensor([8.0614], grad_fn=<UnbindBackward0>))\n",
      "(8.462314529906248, tensor([7.3198], grad_fn=<UnbindBackward0>))\n",
      "(6.61472560020376, tensor([7.2041], grad_fn=<UnbindBackward0>))\n",
      "(6.248042874508429, tensor([6.3006], grad_fn=<UnbindBackward0>))\n",
      "(9.120306249272202, tensor([8.8182], grad_fn=<UnbindBackward0>))\n",
      "(7.513163545234075, tensor([6.3892], grad_fn=<UnbindBackward0>))\n",
      "(6.075346031088684, tensor([7.9144], grad_fn=<UnbindBackward0>))\n",
      "(6.86171134048073, tensor([9.8796], grad_fn=<UnbindBackward0>))\n",
      "(6.732210706467206, tensor([7.1772], grad_fn=<UnbindBackward0>))\n",
      "(8.195057690895077, tensor([6.0359], grad_fn=<UnbindBackward0>))\n",
      "(6.835184586147301, tensor([7.5539], grad_fn=<UnbindBackward0>))\n",
      "(8.603737792816423, tensor([7.8054], grad_fn=<UnbindBackward0>))\n",
      "(6.042632833682381, tensor([8.5563], grad_fn=<UnbindBackward0>))\n",
      "(8.643825842349603, tensor([8.0707], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([8.3576], grad_fn=<UnbindBackward0>))\n",
      "(8.74209519574531, tensor([9.5743], grad_fn=<UnbindBackward0>))\n",
      "(6.175867270105761, tensor([6.0798], grad_fn=<UnbindBackward0>))\n",
      "(7.4342573821331355, tensor([8.2274], grad_fn=<UnbindBackward0>))\n",
      "(7.5740450053721995, tensor([9.4998], grad_fn=<UnbindBackward0>))\n",
      "(8.392536586816682, tensor([8.0555], grad_fn=<UnbindBackward0>))\n",
      "(8.45531778769815, tensor([8.2474], grad_fn=<UnbindBackward0>))\n",
      "(9.350189267092581, tensor([9.1131], grad_fn=<UnbindBackward0>))\n",
      "(7.1846291527173145, tensor([9.4651], grad_fn=<UnbindBackward0>))\n",
      "(7.789868559054706, tensor([7.3572], grad_fn=<UnbindBackward0>))\n",
      "(6.3473892096560105, tensor([9.0836], grad_fn=<UnbindBackward0>))\n",
      "(8.251403065380556, tensor([8.3378], grad_fn=<UnbindBackward0>))\n",
      "(6.5638555265321274, tensor([9.4190], grad_fn=<UnbindBackward0>))\n",
      "(8.503905297089302, tensor([8.6204], grad_fn=<UnbindBackward0>))\n",
      "(8.418697944667139, tensor([7.7704], grad_fn=<UnbindBackward0>))\n",
      "(7.4067107301776405, tensor([9.5627], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([6.6015], grad_fn=<UnbindBackward0>))\n",
      "(8.224967478914584, tensor([8.1854], grad_fn=<UnbindBackward0>))\n",
      "(7.509335266016592, tensor([6.6796], grad_fn=<UnbindBackward0>))\n",
      "(8.728102205062104, tensor([7.8419], grad_fn=<UnbindBackward0>))\n",
      "(8.20631072579402, tensor([6.4419], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.8551], grad_fn=<UnbindBackward0>))\n",
      "(8.976893927666076, tensor([6.5736], grad_fn=<UnbindBackward0>))\n",
      "(7.074963197966044, tensor([6.3881], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([6.3625], grad_fn=<UnbindBackward0>))\n",
      "(9.79740455372797, tensor([6.1775], grad_fn=<UnbindBackward0>))\n",
      "(8.3133619511344, tensor([6.2949], grad_fn=<UnbindBackward0>))\n",
      "(6.075346031088684, tensor([7.1507], grad_fn=<UnbindBackward0>))\n",
      "(6.251903883165888, tensor([7.8475], grad_fn=<UnbindBackward0>))\n",
      "(8.774776816043985, tensor([7.1950], grad_fn=<UnbindBackward0>))\n",
      "(7.933438387627489, tensor([9.9103], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([6.5451], grad_fn=<UnbindBackward0>))\n",
      "(8.45914025996762, tensor([9.3119], grad_fn=<UnbindBackward0>))\n",
      "(9.572828387229206, tensor([8.9525], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([6.7476], grad_fn=<UnbindBackward0>))\n",
      "(9.194007717355285, tensor([6.3811], grad_fn=<UnbindBackward0>))\n",
      "(7.660585461703256, tensor([7.4000], grad_fn=<UnbindBackward0>))\n",
      "(7.074963197966044, tensor([6.7720], grad_fn=<UnbindBackward0>))\n",
      "(7.8674885686991285, tensor([7.3906], grad_fn=<UnbindBackward0>))\n",
      "(7.271703706887368, tensor([8.7505], grad_fn=<UnbindBackward0>))\n",
      "(8.434246270595311, tensor([6.7302], grad_fn=<UnbindBackward0>))\n",
      "(6.511745329644728, tensor([7.8555], grad_fn=<UnbindBackward0>))\n",
      "(9.804440398934894, tensor([7.0695], grad_fn=<UnbindBackward0>))\n",
      "(8.810310466357958, tensor([6.2022], grad_fn=<UnbindBackward0>))\n",
      "(7.447751280047908, tensor([8.8791], grad_fn=<UnbindBackward0>))\n",
      "(6.251903883165888, tensor([9.0898], grad_fn=<UnbindBackward0>))\n",
      "(8.85352256068954, tensor([8.9358], grad_fn=<UnbindBackward0>))\n",
      "(8.326032685955079, tensor([8.9121], grad_fn=<UnbindBackward0>))\n",
      "(7.733245646529795, tensor([8.7842], grad_fn=<UnbindBackward0>))\n",
      "(7.00397413672268, tensor([6.7350], grad_fn=<UnbindBackward0>))\n",
      "(7.372118028337787, tensor([7.9228], grad_fn=<UnbindBackward0>))\n",
      "(7.543273346705446, tensor([7.9748], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([7.8201], grad_fn=<UnbindBackward0>))\n",
      "(8.24143968982973, tensor([7.1362], grad_fn=<UnbindBackward0>))\n",
      "(8.445697189711167, tensor([6.7246], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([7.2456], grad_fn=<UnbindBackward0>))\n",
      "(7.615298339825815, tensor([6.1752], grad_fn=<UnbindBackward0>))\n",
      "(6.86171134048073, tensor([5.9985], grad_fn=<UnbindBackward0>))\n",
      "(9.480749115757506, tensor([7.8617], grad_fn=<UnbindBackward0>))\n",
      "(7.00850518208228, tensor([7.1034], grad_fn=<UnbindBackward0>))\n",
      "(9.629774129311743, tensor([8.4604], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([6.1895], grad_fn=<UnbindBackward0>))\n",
      "(8.380685946761574, tensor([6.9004], grad_fn=<UnbindBackward0>))\n",
      "(8.582980931954241, tensor([6.6866], grad_fn=<UnbindBackward0>))\n",
      "(9.336620537886512, tensor([9.0429], grad_fn=<UnbindBackward0>))\n",
      "(8.974618038455112, tensor([8.4504], grad_fn=<UnbindBackward0>))\n",
      "(6.226536669287466, tensor([7.3099], grad_fn=<UnbindBackward0>))\n",
      "(6.5998704992128365, tensor([6.2996], grad_fn=<UnbindBackward0>))\n",
      "(6.982862751468942, tensor([8.4837], grad_fn=<UnbindBackward0>))\n",
      "(7.621684998724611, tensor([8.4654], grad_fn=<UnbindBackward0>))\n",
      "(9.064852065227576, tensor([6.6362], grad_fn=<UnbindBackward0>))\n",
      "(7.589841512182657, tensor([6.0081], grad_fn=<UnbindBackward0>))\n",
      "(7.912423121473705, tensor([8.5829], grad_fn=<UnbindBackward0>))\n",
      "(7.855544677915663, tensor([7.7240], grad_fn=<UnbindBackward0>))\n",
      "(9.319105086767571, tensor([6.7302], grad_fn=<UnbindBackward0>))\n",
      "(8.352318548226004, tensor([6.2859], grad_fn=<UnbindBackward0>))\n",
      "(8.529516941105069, tensor([8.5491], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([9.2729], grad_fn=<UnbindBackward0>))\n",
      "(8.54188580400661, tensor([6.7723], grad_fn=<UnbindBackward0>))\n",
      "(8.227375506834035, tensor([6.5756], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([9.4794], grad_fn=<UnbindBackward0>))\n",
      "(7.080026499922591, tensor([10.2571], grad_fn=<UnbindBackward0>))\n",
      "(8.804175018753625, tensor([9.1117], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([8.5686], grad_fn=<UnbindBackward0>))\n",
      "(8.929832503272403, tensor([8.7040], grad_fn=<UnbindBackward0>))\n",
      "(7.9714309977693505, tensor([6.1969], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([8.1760], grad_fn=<UnbindBackward0>))\n",
      "(9.329367078397823, tensor([8.4093], grad_fn=<UnbindBackward0>))\n",
      "(7.611842399580417, tensor([9.3565], grad_fn=<UnbindBackward0>))\n",
      "(7.160069207596127, tensor([9.4182], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([7.0913], grad_fn=<UnbindBackward0>))\n",
      "(6.1092475827643655, tensor([7.3105], grad_fn=<UnbindBackward0>))\n",
      "(8.080546965824498, tensor([6.2435], grad_fn=<UnbindBackward0>))\n",
      "(8.744009988096744, tensor([6.8069], grad_fn=<UnbindBackward0>))\n",
      "(8.991686725934825, tensor([6.7980], grad_fn=<UnbindBackward0>))\n",
      "(6.508769136971682, tensor([6.0955], grad_fn=<UnbindBackward0>))\n",
      "(9.09963224999176, tensor([9.9355], grad_fn=<UnbindBackward0>))\n",
      "(6.432940092739179, tensor([6.3648], grad_fn=<UnbindBackward0>))\n",
      "(7.68017564043659, tensor([6.7649], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([8.4805], grad_fn=<UnbindBackward0>))\n",
      "(9.589598387915172, tensor([7.7753], grad_fn=<UnbindBackward0>))\n",
      "(6.023447592961033, tensor([6.7853], grad_fn=<UnbindBackward0>))\n",
      "(7.107425474110705, tensor([8.2164], grad_fn=<UnbindBackward0>))\n",
      "(6.734591659972948, tensor([9.3436], grad_fn=<UnbindBackward0>))\n",
      "(6.92951677076365, tensor([6.2349], grad_fn=<UnbindBackward0>))\n",
      "(7.862112211662748, tensor([8.5614], grad_fn=<UnbindBackward0>))\n",
      "(8.50045386741194, tensor([6.7877], grad_fn=<UnbindBackward0>))\n",
      "(8.41715183723601, tensor([6.2467], grad_fn=<UnbindBackward0>))\n",
      "(6.263398262591624, tensor([6.3218], grad_fn=<UnbindBackward0>))\n",
      "(7.685703061234547, tensor([7.1861], grad_fn=<UnbindBackward0>))\n",
      "(8.929435283803425, tensor([8.3698], grad_fn=<UnbindBackward0>))\n",
      "(7.27655640271871, tensor([8.1748], grad_fn=<UnbindBackward0>))\n",
      "(6.07073772800249, tensor([8.7556], grad_fn=<UnbindBackward0>))\n",
      "(9.632990304838447, tensor([9.3076], grad_fn=<UnbindBackward0>))\n",
      "(7.736307096548285, tensor([7.8761], grad_fn=<UnbindBackward0>))\n",
      "(8.397508348470257, tensor([6.6249], grad_fn=<UnbindBackward0>))\n",
      "(7.4977617006225685, tensor([8.1292], grad_fn=<UnbindBackward0>))\n",
      "(7.718240951959316, tensor([7.7356], grad_fn=<UnbindBackward0>))\n",
      "(8.230044310126114, tensor([8.8114], grad_fn=<UnbindBackward0>))\n",
      "(8.102889134640868, tensor([6.2467], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([6.3780], grad_fn=<UnbindBackward0>))\n",
      "(8.871926251117628, tensor([7.8395], grad_fn=<UnbindBackward0>))\n",
      "(6.144185634125646, tensor([6.4179], grad_fn=<UnbindBackward0>))\n",
      "(8.514589805546123, tensor([7.7677], grad_fn=<UnbindBackward0>))\n",
      "(8.897545598709327, tensor([8.3832], grad_fn=<UnbindBackward0>))\n",
      "(8.528330935826693, tensor([7.8857], grad_fn=<UnbindBackward0>))\n",
      "(8.00836557031292, tensor([9.5885], grad_fn=<UnbindBackward0>))\n",
      "(8.353497098745448, tensor([7.9745], grad_fn=<UnbindBackward0>))\n",
      "(7.409741954080923, tensor([6.1986], grad_fn=<UnbindBackward0>))\n",
      "(6.214608098422191, tensor([8.6766], grad_fn=<UnbindBackward0>))\n",
      "(7.8632667240095735, tensor([6.7692], grad_fn=<UnbindBackward0>))\n",
      "(6.642486801367256, tensor([7.4973], grad_fn=<UnbindBackward0>))\n",
      "(6.949856455000773, tensor([7.7902], grad_fn=<UnbindBackward0>))\n",
      "(6.9411900550683745, tensor([6.7199], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([7.8570], grad_fn=<UnbindBackward0>))\n",
      "(8.862908295118627, tensor([9.0889], grad_fn=<UnbindBackward0>))\n",
      "(7.165493475060845, tensor([7.1774], grad_fn=<UnbindBackward0>))\n",
      "(9.064620717626777, tensor([7.8509], grad_fn=<UnbindBackward0>))\n",
      "(8.58615939588096, tensor([7.9466], grad_fn=<UnbindBackward0>))\n",
      "(7.3833681469923835, tensor([7.7872], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([8.1743], grad_fn=<UnbindBackward0>))\n",
      "(8.357259153499912, tensor([7.9418], grad_fn=<UnbindBackward0>))\n",
      "(7.813591552952433, tensor([8.5012], grad_fn=<UnbindBackward0>))\n",
      "(8.856803356728378, tensor([7.8613], grad_fn=<UnbindBackward0>))\n",
      "(8.338544879988579, tensor([7.7918], grad_fn=<UnbindBackward0>))\n",
      "(7.763871287820222, tensor([8.2655], grad_fn=<UnbindBackward0>))\n",
      "(9.752199438322346, tensor([6.1519], grad_fn=<UnbindBackward0>))\n",
      "(7.160069207596127, tensor([8.4571], grad_fn=<UnbindBackward0>))\n",
      "(7.27931883541462, tensor([8.5215], grad_fn=<UnbindBackward0>))\n",
      "(9.826390502151161, tensor([6.2276], grad_fn=<UnbindBackward0>))\n",
      "(6.07073772800249, tensor([8.2854], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([9.3441], grad_fn=<UnbindBackward0>))\n",
      "(8.767173396684006, tensor([7.0957], grad_fn=<UnbindBackward0>))\n",
      "(8.904630097005011, tensor([7.3523], grad_fn=<UnbindBackward0>))\n",
      "(6.823286122355687, tensor([6.6077], grad_fn=<UnbindBackward0>))\n",
      "(9.09504174761272, tensor([6.5144], grad_fn=<UnbindBackward0>))\n",
      "(8.71833650245078, tensor([8.4674], grad_fn=<UnbindBackward0>))\n",
      "(8.691482576512929, tensor([6.1865], grad_fn=<UnbindBackward0>))\n",
      "(7.867871490396322, tensor([9.0709], grad_fn=<UnbindBackward0>))\n",
      "(6.440946540632921, tensor([8.4470], grad_fn=<UnbindBackward0>))\n",
      "(9.679593719831214, tensor([7.8071], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([7.3736], grad_fn=<UnbindBackward0>))\n",
      "(8.663542087751374, tensor([7.9941], grad_fn=<UnbindBackward0>))\n",
      "(9.017847259860732, tensor([7.3451], grad_fn=<UnbindBackward0>))\n",
      "(8.340217320947035, tensor([7.3755], grad_fn=<UnbindBackward0>))\n",
      "(9.460632055209405, tensor([7.2341], grad_fn=<UnbindBackward0>))\n",
      "(8.31090675716845, tensor([8.7467], grad_fn=<UnbindBackward0>))\n",
      "(7.483806687665835, tensor([6.3929], grad_fn=<UnbindBackward0>))\n",
      "(8.344505083590521, tensor([8.6023], grad_fn=<UnbindBackward0>))\n",
      "(8.676075516476429, tensor([7.2826], grad_fn=<UnbindBackward0>))\n",
      "(9.26558584620216, tensor([8.8036], grad_fn=<UnbindBackward0>))\n",
      "(6.802394763324311, tensor([9.4519], grad_fn=<UnbindBackward0>))\n",
      "(8.400209835930418, tensor([7.0253], grad_fn=<UnbindBackward0>))\n",
      "(6.853299093186078, tensor([8.6057], grad_fn=<UnbindBackward0>))\n",
      "(6.522092798170152, tensor([9.5942], grad_fn=<UnbindBackward0>))\n",
      "(9.675205823365795, tensor([6.2159], grad_fn=<UnbindBackward0>))\n",
      "(8.277411998949004, tensor([8.2247], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([6.7576], grad_fn=<UnbindBackward0>))\n",
      "(8.449556542700426, tensor([8.4762], grad_fn=<UnbindBackward0>))\n",
      "(8.183118079394745, tensor([8.1799], grad_fn=<UnbindBackward0>))\n",
      "(8.385260520155413, tensor([9.7658], grad_fn=<UnbindBackward0>))\n",
      "(8.183118079394745, tensor([6.3401], grad_fn=<UnbindBackward0>))\n",
      "(6.131226489483141, tensor([7.3304], grad_fn=<UnbindBackward0>))\n",
      "(7.252053951852814, tensor([8.0300], grad_fn=<UnbindBackward0>))\n",
      "(9.418085981110297, tensor([7.8766], grad_fn=<UnbindBackward0>))\n",
      "(7.030857476116121, tensor([11.1367], grad_fn=<UnbindBackward0>))\n",
      "(6.9411900550683745, tensor([8.8926], grad_fn=<UnbindBackward0>))\n",
      "(7.80016307039296, tensor([6.7644], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([5.9412], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([8.4667], grad_fn=<UnbindBackward0>))\n",
      "(9.30273722124215, tensor([9.3026], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([7.2795], grad_fn=<UnbindBackward0>))\n",
      "(9.735364715141987, tensor([6.5155], grad_fn=<UnbindBackward0>))\n",
      "(7.763871287820222, tensor([10.1087], grad_fn=<UnbindBackward0>))\n",
      "(6.327936783729195, tensor([9.0931], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([7.1552], grad_fn=<UnbindBackward0>))\n",
      "(9.262932822095353, tensor([7.8491], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.6933], grad_fn=<UnbindBackward0>))\n",
      "(6.466144724237619, tensor([7.0459], grad_fn=<UnbindBackward0>))\n",
      "(8.588210678651517, tensor([7.0926], grad_fn=<UnbindBackward0>))\n",
      "(9.322775801305971, tensor([6.8784], grad_fn=<UnbindBackward0>))\n",
      "(8.586346050104554, tensor([7.0685], grad_fn=<UnbindBackward0>))\n",
      "(8.00836557031292, tensor([6.3420], grad_fn=<UnbindBackward0>))\n",
      "(8.252967195000798, tensor([8.6225], grad_fn=<UnbindBackward0>))\n",
      "(8.397508348470257, tensor([7.8393], grad_fn=<UnbindBackward0>))\n",
      "(7.519149957669823, tensor([6.4535], grad_fn=<UnbindBackward0>))\n",
      "(7.749753406274437, tensor([7.2587], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([7.8793], grad_fn=<UnbindBackward0>))\n",
      "(8.662158961666423, tensor([8.0002], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([6.2166], grad_fn=<UnbindBackward0>))\n",
      "(9.42197798310419, tensor([7.6859], grad_fn=<UnbindBackward0>))\n",
      "(9.101752431559284, tensor([8.3394], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([7.5999], grad_fn=<UnbindBackward0>))\n",
      "(7.554334823725748, tensor([6.3343], grad_fn=<UnbindBackward0>))\n",
      "(7.6638772587034705, tensor([7.7423], grad_fn=<UnbindBackward0>))\n",
      "(8.701346403039162, tensor([8.0690], grad_fn=<UnbindBackward0>))\n",
      "(7.077498053569231, tensor([6.7787], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([8.5680], grad_fn=<UnbindBackward0>))\n",
      "(7.3914152346753585, tensor([6.8087], grad_fn=<UnbindBackward0>))\n",
      "(8.325548307161398, tensor([8.3459], grad_fn=<UnbindBackward0>))\n",
      "(8.484049972822984, tensor([6.2994], grad_fn=<UnbindBackward0>))\n",
      "(8.335191583433202, tensor([7.4004], grad_fn=<UnbindBackward0>))\n",
      "(7.9919305198524775, tensor([8.4922], grad_fn=<UnbindBackward0>))\n",
      "(8.646641258603124, tensor([6.4872], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([6.4353], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([7.1620], grad_fn=<UnbindBackward0>))\n",
      "(7.425953657077541, tensor([10.1945], grad_fn=<UnbindBackward0>))\n",
      "(6.732210706467206, tensor([7.3520], grad_fn=<UnbindBackward0>))\n",
      "(6.829793737512425, tensor([9.5396], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([10.1763], grad_fn=<UnbindBackward0>))\n",
      "(7.069874128458572, tensor([9.2606], grad_fn=<UnbindBackward0>))\n",
      "(8.972083182851925, tensor([7.8794], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([6.3087], grad_fn=<UnbindBackward0>))\n",
      "(9.079206103951382, tensor([7.2114], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([7.9545], grad_fn=<UnbindBackward0>))\n",
      "(6.810142450115136, tensor([7.3931], grad_fn=<UnbindBackward0>))\n",
      "(8.430545384690566, tensor([6.3580], grad_fn=<UnbindBackward0>))\n",
      "(8.473241303887054, tensor([6.6121], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([6.6562], grad_fn=<UnbindBackward0>))\n",
      "(6.795705775173514, tensor([7.7755], grad_fn=<UnbindBackward0>))\n",
      "(8.811503250158239, tensor([6.3956], grad_fn=<UnbindBackward0>))\n",
      "(8.393216011596527, tensor([6.7410], grad_fn=<UnbindBackward0>))\n",
      "(9.164191715950203, tensor([9.2135], grad_fn=<UnbindBackward0>))\n",
      "(6.922643891475888, tensor([6.8015], grad_fn=<UnbindBackward0>))\n",
      "(8.389132521348719, tensor([8.4697], grad_fn=<UnbindBackward0>))\n",
      "(7.987864096085687, tensor([8.6252], grad_fn=<UnbindBackward0>))\n",
      "(7.432483807917119, tensor([7.2928], grad_fn=<UnbindBackward0>))\n",
      "(7.734558844354756, tensor([6.1953], grad_fn=<UnbindBackward0>))\n",
      "(9.628984915196462, tensor([6.7535], grad_fn=<UnbindBackward0>))\n",
      "(8.790116892892472, tensor([6.6396], grad_fn=<UnbindBackward0>))\n",
      "(6.762729506931879, tensor([6.4865], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([6.6499], grad_fn=<UnbindBackward0>))\n",
      "(9.536040511615484, tensor([8.8652], grad_fn=<UnbindBackward0>))\n",
      "(9.038246335337664, tensor([9.4967], grad_fn=<UnbindBackward0>))\n",
      "(8.747034264178168, tensor([6.3897], grad_fn=<UnbindBackward0>))\n",
      "(8.291797105048733, tensor([7.9177], grad_fn=<UnbindBackward0>))\n",
      "(8.35490952835879, tensor([7.4907], grad_fn=<UnbindBackward0>))\n",
      "(7.751045117971802, tensor([8.4352], grad_fn=<UnbindBackward0>))\n",
      "(9.033841828485016, tensor([6.1872], grad_fn=<UnbindBackward0>))\n",
      "(9.09873819539488, tensor([9.3347], grad_fn=<UnbindBackward0>))\n",
      "(8.10258642539079, tensor([8.5935], grad_fn=<UnbindBackward0>))\n",
      "(8.237743803890933, tensor([9.9606], grad_fn=<UnbindBackward0>))\n",
      "(7.068172000388042, tensor([6.9720], grad_fn=<UnbindBackward0>))\n",
      "(8.82864061741828, tensor([8.4444], grad_fn=<UnbindBackward0>))\n",
      "(8.571681376700306, tensor([6.1797], grad_fn=<UnbindBackward0>))\n",
      "(8.775240458738327, tensor([6.3175], grad_fn=<UnbindBackward0>))\n",
      "(7.502186486602924, tensor([6.8427], grad_fn=<UnbindBackward0>))\n",
      "(8.760609847570002, tensor([9.5402], grad_fn=<UnbindBackward0>))\n",
      "(8.385260520155413, tensor([9.4036], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([6.3688], grad_fn=<UnbindBackward0>))\n",
      "(6.326149473155099, tensor([6.2166], grad_fn=<UnbindBackward0>))\n",
      "(7.00397413672268, tensor([9.9777], grad_fn=<UnbindBackward0>))\n",
      "(8.024207485778577, tensor([9.5593], grad_fn=<UnbindBackward0>))\n",
      "(8.90598676523643, tensor([8.0164], grad_fn=<UnbindBackward0>))\n",
      "(6.953684210870537, tensor([7.0877], grad_fn=<UnbindBackward0>))\n",
      "(7.470224135899966, tensor([6.8231], grad_fn=<UnbindBackward0>))\n",
      "(7.0967213784947605, tensor([6.5744], grad_fn=<UnbindBackward0>))\n",
      "(8.091933455979893, tensor([6.6676], grad_fn=<UnbindBackward0>))\n",
      "(8.756682421266532, tensor([6.1815], grad_fn=<UnbindBackward0>))\n",
      "(8.09437844497296, tensor([6.1667], grad_fn=<UnbindBackward0>))\n",
      "(6.393590753950631, tensor([7.3594], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([6.9848], grad_fn=<UnbindBackward0>))\n",
      "(8.297045149081827, tensor([8.3729], grad_fn=<UnbindBackward0>))\n",
      "(8.201111644442758, tensor([6.3866], grad_fn=<UnbindBackward0>))\n",
      "(6.85751406254539, tensor([7.4399], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([6.0838], grad_fn=<UnbindBackward0>))\n",
      "(9.680593969977126, tensor([8.5877], grad_fn=<UnbindBackward0>))\n",
      "(7.097548850614793, tensor([7.9879], grad_fn=<UnbindBackward0>))\n",
      "(7.084226422097916, tensor([7.4958], grad_fn=<UnbindBackward0>))\n",
      "(9.075322160298095, tensor([8.0558], grad_fn=<UnbindBackward0>))\n",
      "(8.407601514786142, tensor([6.4220], grad_fn=<UnbindBackward0>))\n",
      "(7.682482446534506, tensor([7.7114], grad_fn=<UnbindBackward0>))\n",
      "(7.671826797878781, tensor([7.2651], grad_fn=<UnbindBackward0>))\n",
      "(8.535229553902337, tensor([9.7330], grad_fn=<UnbindBackward0>))\n",
      "(6.721425700790643, tensor([8.1583], grad_fn=<UnbindBackward0>))\n",
      "(7.730174795246222, tensor([7.1972], grad_fn=<UnbindBackward0>))\n",
      "(7.399398083331354, tensor([6.3039], grad_fn=<UnbindBackward0>))\n",
      "(7.067319848653476, tensor([9.5494], grad_fn=<UnbindBackward0>))\n",
      "(7.759187438507795, tensor([6.0155], grad_fn=<UnbindBackward0>))\n",
      "(7.384610383176974, tensor([8.7573], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([8.7286], grad_fn=<UnbindBackward0>))\n",
      "(7.410347097821024, tensor([7.7482], grad_fn=<UnbindBackward0>))\n",
      "(9.015784277513886, tensor([8.8159], grad_fn=<UnbindBackward0>))\n",
      "(7.68937110752969, tensor([8.4097], grad_fn=<UnbindBackward0>))\n",
      "(8.432070937999402, tensor([9.7851], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([9.1320], grad_fn=<UnbindBackward0>))\n",
      "(8.80086724247048, tensor([7.1028], grad_fn=<UnbindBackward0>))\n",
      "(8.674538762140015, tensor([8.7525], grad_fn=<UnbindBackward0>))\n",
      "(6.959398512133975, tensor([8.9218], grad_fn=<UnbindBackward0>))\n",
      "(6.570882962339584, tensor([9.4451], grad_fn=<UnbindBackward0>))\n",
      "(7.847762537473608, tensor([9.1389], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([8.7558], grad_fn=<UnbindBackward0>))\n",
      "(8.493924564476883, tensor([6.4716], grad_fn=<UnbindBackward0>))\n",
      "(8.301273485191347, tensor([7.8196], grad_fn=<UnbindBackward0>))\n",
      "(7.1846291527173145, tensor([7.8116], grad_fn=<UnbindBackward0>))\n",
      "(8.643297068211963, tensor([6.5210], grad_fn=<UnbindBackward0>))\n",
      "(7.898782356970309, tensor([7.9287], grad_fn=<UnbindBackward0>))\n",
      "(8.567696173589344, tensor([6.5396], grad_fn=<UnbindBackward0>))\n",
      "(7.358830898342354, tensor([8.5406], grad_fn=<UnbindBackward0>))\n",
      "(9.122492281402987, tensor([6.2629], grad_fn=<UnbindBackward0>))\n",
      "(8.911260254572033, tensor([8.1399], grad_fn=<UnbindBackward0>))\n",
      "(7.653020413804189, tensor([8.8063], grad_fn=<UnbindBackward0>))\n",
      "(7.424165281042028, tensor([8.4405], grad_fn=<UnbindBackward0>))\n",
      "(7.519149957669823, tensor([8.9641], grad_fn=<UnbindBackward0>))\n",
      "(8.9445502459405, tensor([8.4435], grad_fn=<UnbindBackward0>))\n",
      "(6.0867747269123065, tensor([6.4932], grad_fn=<UnbindBackward0>))\n",
      "(8.830835365537549, tensor([6.7199], grad_fn=<UnbindBackward0>))\n",
      "(8.487558386286548, tensor([9.5161], grad_fn=<UnbindBackward0>))\n",
      "(7.731053144007127, tensor([8.5272], grad_fn=<UnbindBackward0>))\n",
      "(6.263398262591624, tensor([9.9826], grad_fn=<UnbindBackward0>))\n",
      "(7.469654172932128, tensor([8.4501], grad_fn=<UnbindBackward0>))\n",
      "(9.056839480990421, tensor([8.0906], grad_fn=<UnbindBackward0>))\n",
      "(8.411165786770708, tensor([6.4151], grad_fn=<UnbindBackward0>))\n",
      "(6.90875477931522, tensor([6.6594], grad_fn=<UnbindBackward0>))\n",
      "(8.302513718514158, tensor([5.6810], grad_fn=<UnbindBackward0>))\n",
      "(7.6251071482389, tensor([7.5547], grad_fn=<UnbindBackward0>))\n",
      "(7.417580402414544, tensor([9.9767], grad_fn=<UnbindBackward0>))\n",
      "(6.535241271013659, tensor([7.6849], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([6.3070], grad_fn=<UnbindBackward0>))\n",
      "(7.9599745280805365, tensor([6.6667], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([9.2637], grad_fn=<UnbindBackward0>))\n",
      "(7.138073034044347, tensor([9.4422], grad_fn=<UnbindBackward0>))\n",
      "(8.730851903519232, tensor([6.4675], grad_fn=<UnbindBackward0>))\n",
      "(9.276596167753247, tensor([7.1351], grad_fn=<UnbindBackward0>))\n",
      "(6.683360945766275, tensor([7.9245], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([10.5973], grad_fn=<UnbindBackward0>))\n",
      "(8.596189197642735, tensor([6.4903], grad_fn=<UnbindBackward0>))\n",
      "(8.123261319121745, tensor([9.6258], grad_fn=<UnbindBackward0>))\n",
      "(9.026056891868688, tensor([7.8740], grad_fn=<UnbindBackward0>))\n",
      "(6.923628628138427, tensor([6.2108], grad_fn=<UnbindBackward0>))\n",
      "(6.259581464064923, tensor([7.0275], grad_fn=<UnbindBackward0>))\n",
      "(8.698681067461614, tensor([6.9203], grad_fn=<UnbindBackward0>))\n",
      "(7.067319848653476, tensor([7.8179], grad_fn=<UnbindBackward0>))\n",
      "(6.871091294610546, tensor([9.4666], grad_fn=<UnbindBackward0>))\n",
      "(7.5766097669730375, tensor([9.2645], grad_fn=<UnbindBackward0>))\n",
      "(8.342601680684194, tensor([7.8763], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([7.2432], grad_fn=<UnbindBackward0>))\n",
      "(7.272398392570047, tensor([6.2997], grad_fn=<UnbindBackward0>))\n",
      "(6.943122422819428, tensor([8.1603], grad_fn=<UnbindBackward0>))\n",
      "(8.900412690577083, tensor([7.6917], grad_fn=<UnbindBackward0>))\n",
      "(7.87777633327726, tensor([7.0121], grad_fn=<UnbindBackward0>))\n",
      "(9.69922708822342, tensor([6.4170], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([9.0458], grad_fn=<UnbindBackward0>))\n",
      "(8.30325712085294, tensor([8.5134], grad_fn=<UnbindBackward0>))\n",
      "(7.853216388156072, tensor([8.4821], grad_fn=<UnbindBackward0>))\n",
      "(7.633853559681768, tensor([6.2197], grad_fn=<UnbindBackward0>))\n",
      "(8.027150106832774, tensor([10.2747], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([8.0530], grad_fn=<UnbindBackward0>))\n",
      "(8.353025845202325, tensor([6.5456], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([9.2400], grad_fn=<UnbindBackward0>))\n",
      "(8.112527763478637, tensor([8.9196], grad_fn=<UnbindBackward0>))\n",
      "(8.465899897028686, tensor([7.4690], grad_fn=<UnbindBackward0>))\n",
      "(9.316500567804573, tensor([8.6489], grad_fn=<UnbindBackward0>))\n",
      "(9.02063159674821, tensor([7.8534], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([8.2536], grad_fn=<UnbindBackward0>))\n",
      "(6.848005274576363, tensor([7.1430], grad_fn=<UnbindBackward0>))\n",
      "(9.482959566750917, tensor([8.7353], grad_fn=<UnbindBackward0>))\n",
      "(8.225770799348734, tensor([7.9592], grad_fn=<UnbindBackward0>))\n",
      "(8.766705997750515, tensor([6.0203], grad_fn=<UnbindBackward0>))\n",
      "(6.823286122355687, tensor([6.2915], grad_fn=<UnbindBackward0>))\n",
      "(8.87905466204227, tensor([7.6720], grad_fn=<UnbindBackward0>))\n",
      "(8.849657406639912, tensor([9.3547], grad_fn=<UnbindBackward0>))\n",
      "(6.555356891810665, tensor([8.6757], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([6.4626], grad_fn=<UnbindBackward0>))\n",
      "(7.918992488165245, tensor([8.9139], grad_fn=<UnbindBackward0>))\n",
      "(9.18204377282107, tensor([7.9496], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([8.8733], grad_fn=<UnbindBackward0>))\n",
      "(6.852242569051878, tensor([6.8200], grad_fn=<UnbindBackward0>))\n",
      "(8.585412430393381, tensor([8.5243], grad_fn=<UnbindBackward0>))\n",
      "(7.229113877793302, tensor([7.0882], grad_fn=<UnbindBackward0>))\n",
      "(8.068402958569699, tensor([8.7209], grad_fn=<UnbindBackward0>))\n",
      "(7.650168700845001, tensor([6.3226], grad_fn=<UnbindBackward0>))\n",
      "(6.408528791059498, tensor([6.1941], grad_fn=<UnbindBackward0>))\n",
      "(8.755422380148488, tensor([6.2365], grad_fn=<UnbindBackward0>))\n",
      "(8.490849216076635, tensor([7.9168], grad_fn=<UnbindBackward0>))\n",
      "(8.362175469149628, tensor([10.2080], grad_fn=<UnbindBackward0>))\n",
      "(8.005033344637111, tensor([8.4119], grad_fn=<UnbindBackward0>))\n",
      "(8.195885391314796, tensor([9.0019], grad_fn=<UnbindBackward0>))\n",
      "(6.530877627725885, tensor([6.7725], grad_fn=<UnbindBackward0>))\n",
      "(7.703459047867175, tensor([6.6509], grad_fn=<UnbindBackward0>))\n",
      "(6.727431724850855, tensor([7.6924], grad_fn=<UnbindBackward0>))\n",
      "(8.173575486634153, tensor([7.1220], grad_fn=<UnbindBackward0>))\n",
      "(7.900636613018005, tensor([8.1354], grad_fn=<UnbindBackward0>))\n",
      "(7.477604243197589, tensor([8.1170], grad_fn=<UnbindBackward0>))\n",
      "(7.643003635560718, tensor([7.2065], grad_fn=<UnbindBackward0>))\n",
      "(6.403574197934815, tensor([7.9138], grad_fn=<UnbindBackward0>))\n",
      "(7.826443135456014, tensor([7.8094], grad_fn=<UnbindBackward0>))\n",
      "(8.824089482791823, tensor([7.3193], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([6.2821], grad_fn=<UnbindBackward0>))\n",
      "(9.059866258621348, tensor([8.5006], grad_fn=<UnbindBackward0>))\n",
      "(8.725832056527565, tensor([6.5289], grad_fn=<UnbindBackward0>))\n",
      "(6.807934943699926, tensor([6.2540], grad_fn=<UnbindBackward0>))\n",
      "(8.45638105201948, tensor([7.1370], grad_fn=<UnbindBackward0>))\n",
      "(7.962415680121064, tensor([7.8622], grad_fn=<UnbindBackward0>))\n",
      "(8.469052816088302, tensor([8.6583], grad_fn=<UnbindBackward0>))\n",
      "(7.999007213243955, tensor([6.3591], grad_fn=<UnbindBackward0>))\n",
      "(9.455558477608703, tensor([6.2421], grad_fn=<UnbindBackward0>))\n",
      "(8.017307507688582, tensor([6.2580], grad_fn=<UnbindBackward0>))\n",
      "(8.298290634359283, tensor([7.0520], grad_fn=<UnbindBackward0>))\n",
      "(7.016609683894219, tensor([8.2658], grad_fn=<UnbindBackward0>))\n",
      "(8.568076401730806, tensor([7.3980], grad_fn=<UnbindBackward0>))\n",
      "(8.876544550558378, tensor([9.1996], grad_fn=<UnbindBackward0>))\n",
      "(7.166265974133638, tensor([9.0666], grad_fn=<UnbindBackward0>))\n",
      "(7.943427767876373, tensor([6.3764], grad_fn=<UnbindBackward0>))\n",
      "(7.679251425953058, tensor([8.9694], grad_fn=<UnbindBackward0>))\n",
      "(8.8698199525084, tensor([9.8465], grad_fn=<UnbindBackward0>))\n",
      "(8.491465042843506, tensor([7.1209], grad_fn=<UnbindBackward0>))\n",
      "(7.3777589082278725, tensor([6.2904], grad_fn=<UnbindBackward0>))\n",
      "(6.400257445308821, tensor([7.9118], grad_fn=<UnbindBackward0>))\n",
      "(6.9985096422506015, tensor([9.4359], grad_fn=<UnbindBackward0>))\n",
      "(8.327000740241713, tensor([7.1341], grad_fn=<UnbindBackward0>))\n",
      "(7.01571242048723, tensor([8.5038], grad_fn=<UnbindBackward0>))\n",
      "(8.677950570294351, tensor([9.5275], grad_fn=<UnbindBackward0>))\n",
      "(7.115582126184454, tensor([7.7485], grad_fn=<UnbindBackward0>))\n",
      "(8.157657015196472, tensor([7.5330], grad_fn=<UnbindBackward0>))\n",
      "(8.576781982827894, tensor([6.9106], grad_fn=<UnbindBackward0>))\n",
      "(7.9748769005588755, tensor([9.3472], grad_fn=<UnbindBackward0>))\n",
      "(6.628041376179533, tensor([6.8499], grad_fn=<UnbindBackward0>))\n",
      "(8.505120610181969, tensor([7.7936], grad_fn=<UnbindBackward0>))\n",
      "(7.980023592310645, tensor([6.0407], grad_fn=<UnbindBackward0>))\n",
      "(8.443977129084978, tensor([6.5823], grad_fn=<UnbindBackward0>))\n",
      "(8.051022208190679, tensor([7.7878], grad_fn=<UnbindBackward0>))\n",
      "(9.716133353214099, tensor([8.0953], grad_fn=<UnbindBackward0>))\n",
      "(7.366445148327599, tensor([7.8413], grad_fn=<UnbindBackward0>))\n",
      "(7.419979923661835, tensor([7.4258], grad_fn=<UnbindBackward0>))\n",
      "(9.074978734045507, tensor([6.6308], grad_fn=<UnbindBackward0>))\n",
      "(6.992096427415888, tensor([8.8617], grad_fn=<UnbindBackward0>))\n",
      "(8.639587799629844, tensor([6.5834], grad_fn=<UnbindBackward0>))\n",
      "(8.17188200612782, tensor([6.3241], grad_fn=<UnbindBackward0>))\n",
      "(6.651571873589727, tensor([10.0154], grad_fn=<UnbindBackward0>))\n",
      "(8.22282213081366, tensor([9.7442], grad_fn=<UnbindBackward0>))\n",
      "(6.161207321695077, tensor([8.0410], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.3073], grad_fn=<UnbindBackward0>))\n",
      "(9.66058762285624, tensor([7.9274], grad_fn=<UnbindBackward0>))\n",
      "(9.162619635764772, tensor([8.2023], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([6.8810], grad_fn=<UnbindBackward0>))\n",
      "(7.88570539124302, tensor([6.3658], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([6.3612], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([10.0445], grad_fn=<UnbindBackward0>))\n",
      "(8.435983135990694, tensor([7.1162], grad_fn=<UnbindBackward0>))\n",
      "(6.732210706467206, tensor([7.7624], grad_fn=<UnbindBackward0>))\n",
      "(6.841615476477592, tensor([8.2456], grad_fn=<UnbindBackward0>))\n",
      "(6.411818267709897, tensor([7.1997], grad_fn=<UnbindBackward0>))\n",
      "(6.542471960506805, tensor([8.2249], grad_fn=<UnbindBackward0>))\n",
      "(8.250097752572845, tensor([6.4673], grad_fn=<UnbindBackward0>))\n",
      "(6.111467339502679, tensor([6.3049], grad_fn=<UnbindBackward0>))\n",
      "(7.134890851565884, tensor([8.0431], grad_fn=<UnbindBackward0>))\n",
      "(9.199481628641307, tensor([8.9328], grad_fn=<UnbindBackward0>))\n",
      "(8.185628891147607, tensor([8.4786], grad_fn=<UnbindBackward0>))\n",
      "(8.24301946898925, tensor([8.0209], grad_fn=<UnbindBackward0>))\n",
      "(8.39502555744203, tensor([8.3787], grad_fn=<UnbindBackward0>))\n",
      "(9.245514446983837, tensor([7.2186], grad_fn=<UnbindBackward0>))\n",
      "(6.513230110912307, tensor([10.3497], grad_fn=<UnbindBackward0>))\n",
      "(7.471363088187097, tensor([6.4813], grad_fn=<UnbindBackward0>))\n",
      "(6.721425700790643, tensor([6.7739], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.4337], grad_fn=<UnbindBackward0>))\n",
      "(7.845807502637805, tensor([8.1224], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([7.9401], grad_fn=<UnbindBackward0>))\n",
      "(6.948897222313312, tensor([7.2230], grad_fn=<UnbindBackward0>))\n",
      "(7.699842407396986, tensor([8.5249], grad_fn=<UnbindBackward0>))\n",
      "(7.475905969367397, tensor([6.5060], grad_fn=<UnbindBackward0>))\n",
      "(9.149740749847252, tensor([8.1456], grad_fn=<UnbindBackward0>))\n",
      "(6.259581464064923, tensor([6.7910], grad_fn=<UnbindBackward0>))\n",
      "(8.791941988456118, tensor([6.4774], grad_fn=<UnbindBackward0>))\n",
      "(9.241354425505353, tensor([6.7011], grad_fn=<UnbindBackward0>))\n",
      "(6.761572768804055, tensor([6.8360], grad_fn=<UnbindBackward0>))\n",
      "(8.772765209949785, tensor([9.4834], grad_fn=<UnbindBackward0>))\n",
      "(7.232010331664759, tensor([6.8665], grad_fn=<UnbindBackward0>))\n",
      "(7.717796211013582, tensor([8.4136], grad_fn=<UnbindBackward0>))\n",
      "(6.80903930604298, tensor([6.4598], grad_fn=<UnbindBackward0>))\n",
      "(8.439015410352214, tensor([7.0595], grad_fn=<UnbindBackward0>))\n",
      "(8.068716192714781, tensor([6.5389], grad_fn=<UnbindBackward0>))\n",
      "(6.075346031088684, tensor([8.7823], grad_fn=<UnbindBackward0>))\n",
      "(6.778784897685177, tensor([8.7291], grad_fn=<UnbindBackward0>))\n",
      "(9.028938392241612, tensor([6.4282], grad_fn=<UnbindBackward0>))\n",
      "(6.985641817639208, tensor([8.5636], grad_fn=<UnbindBackward0>))\n",
      "(6.732210706467206, tensor([6.8335], grad_fn=<UnbindBackward0>))\n",
      "(9.225031920719173, tensor([6.2917], grad_fn=<UnbindBackward0>))\n",
      "(9.381853730053534, tensor([6.1990], grad_fn=<UnbindBackward0>))\n",
      "(7.3864708488298945, tensor([9.5856], grad_fn=<UnbindBackward0>))\n",
      "(8.446341450444287, tensor([8.6831], grad_fn=<UnbindBackward0>))\n",
      "(7.428333194190806, tensor([10.1758], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([7.2548], grad_fn=<UnbindBackward0>))\n",
      "(7.821242083523558, tensor([8.6321], grad_fn=<UnbindBackward0>))\n",
      "(9.447386828459384, tensor([8.9076], grad_fn=<UnbindBackward0>))\n",
      "(9.801620926132557, tensor([9.0335], grad_fn=<UnbindBackward0>))\n",
      "(8.60135049942296, tensor([6.8203], grad_fn=<UnbindBackward0>))\n",
      "(7.43543801981455, tensor([8.2596], grad_fn=<UnbindBackward0>))\n",
      "(8.68185981297147, tensor([9.4873], grad_fn=<UnbindBackward0>))\n",
      "(6.504288173536645, tensor([8.5309], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([8.3565], grad_fn=<UnbindBackward0>))\n",
      "(8.814924599721019, tensor([6.6270], grad_fn=<UnbindBackward0>))\n",
      "(9.768583790071327, tensor([6.1795], grad_fn=<UnbindBackward0>))\n",
      "(7.407924322559599, tensor([7.6389], grad_fn=<UnbindBackward0>))\n",
      "(9.59212735272977, tensor([7.8706], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([9.3945], grad_fn=<UnbindBackward0>))\n",
      "(8.55429627936774, tensor([8.5338], grad_fn=<UnbindBackward0>))\n",
      "(7.591357046698551, tensor([7.6838], grad_fn=<UnbindBackward0>))\n",
      "(6.432940092739179, tensor([7.7011], grad_fn=<UnbindBackward0>))\n",
      "(8.784162222270476, tensor([7.3402], grad_fn=<UnbindBackward0>))\n",
      "(6.478509642208569, tensor([6.7803], grad_fn=<UnbindBackward0>))\n",
      "(8.286017468404763, tensor([6.2142], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([6.2537], grad_fn=<UnbindBackward0>))\n",
      "(8.117610746466228, tensor([8.9761], grad_fn=<UnbindBackward0>))\n",
      "(7.475905969367397, tensor([9.0464], grad_fn=<UnbindBackward0>))\n",
      "(8.447843113281444, tensor([9.8396], grad_fn=<UnbindBackward0>))\n",
      "(8.558335134747413, tensor([8.5537], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([8.8574], grad_fn=<UnbindBackward0>))\n",
      "(9.40804303080844, tensor([9.8720], grad_fn=<UnbindBackward0>))\n",
      "(8.56522116042682, tensor([6.1891], grad_fn=<UnbindBackward0>))\n",
      "(7.055312843339752, tensor([9.1023], grad_fn=<UnbindBackward0>))\n",
      "(8.33758794211651, tensor([6.2680], grad_fn=<UnbindBackward0>))\n",
      "(6.1903154058531475, tensor([5.6938], grad_fn=<UnbindBackward0>))\n",
      "(9.06589246761031, tensor([9.4997], grad_fn=<UnbindBackward0>))\n",
      "(8.963288275610298, tensor([6.6352], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([8.6492], grad_fn=<UnbindBackward0>))\n",
      "(8.020927718981577, tensor([6.5546], grad_fn=<UnbindBackward0>))\n",
      "(7.638198244285779, tensor([9.4274], grad_fn=<UnbindBackward0>))\n",
      "(7.253470382684528, tensor([6.7259], grad_fn=<UnbindBackward0>))\n",
      "(8.353261499733874, tensor([7.9704], grad_fn=<UnbindBackward0>))\n",
      "(8.24800570160062, tensor([7.9129], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([7.4348], grad_fn=<UnbindBackward0>))\n",
      "(7.8628820346414905, tensor([7.2354], grad_fn=<UnbindBackward0>))\n",
      "(7.716015266642587, tensor([7.7866], grad_fn=<UnbindBackward0>))\n",
      "(8.424419791263883, tensor([9.4022], grad_fn=<UnbindBackward0>))\n",
      "(7.846980982138788, tensor([6.6585], grad_fn=<UnbindBackward0>))\n",
      "(6.862757913051401, tensor([7.1048], grad_fn=<UnbindBackward0>))\n",
      "(8.500657222776137, tensor([6.7746], grad_fn=<UnbindBackward0>))\n",
      "(8.202482446576537, tensor([6.6098], grad_fn=<UnbindBackward0>))\n",
      "(9.183585634667363, tensor([7.3823], grad_fn=<UnbindBackward0>))\n",
      "(8.511778558714738, tensor([7.0736], grad_fn=<UnbindBackward0>))\n",
      "(7.491087593534876, tensor([6.4173], grad_fn=<UnbindBackward0>))\n",
      "(7.957527402230773, tensor([8.3341], grad_fn=<UnbindBackward0>))\n",
      "(8.886685639065584, tensor([6.8865], grad_fn=<UnbindBackward0>))\n",
      "(6.522092798170152, tensor([8.7453], grad_fn=<UnbindBackward0>))\n",
      "(6.45833828334479, tensor([7.3611], grad_fn=<UnbindBackward0>))\n",
      "(6.57507584059962, tensor([7.9570], grad_fn=<UnbindBackward0>))\n",
      "(9.082166033252674, tensor([7.4908], grad_fn=<UnbindBackward0>))\n",
      "(8.060855752934316, tensor([9.5958], grad_fn=<UnbindBackward0>))\n",
      "(6.437751649736401, tensor([6.1730], grad_fn=<UnbindBackward0>))\n",
      "(7.634820677745543, tensor([5.9525], grad_fn=<UnbindBackward0>))\n",
      "(8.29254851397576, tensor([6.8178], grad_fn=<UnbindBackward0>))\n",
      "(7.158513997329321, tensor([6.6955], grad_fn=<UnbindBackward0>))\n",
      "(8.319717386850606, tensor([6.7542], grad_fn=<UnbindBackward0>))\n",
      "(7.947325027016463, tensor([6.5946], grad_fn=<UnbindBackward0>))\n",
      "(9.22759072538271, tensor([8.1108], grad_fn=<UnbindBackward0>))\n",
      "(8.886547412512042, tensor([8.4453], grad_fn=<UnbindBackward0>))\n",
      "(6.84587987526405, tensor([8.6321], grad_fn=<UnbindBackward0>))\n",
      "(9.422220731011683, tensor([6.8525], grad_fn=<UnbindBackward0>))\n",
      "(8.648221453822641, tensor([6.3862], grad_fn=<UnbindBackward0>))\n",
      "(7.822044008185619, tensor([8.3335], grad_fn=<UnbindBackward0>))\n",
      "(9.680656452403964, tensor([7.6599], grad_fn=<UnbindBackward0>))\n",
      "(8.1886891244442, tensor([8.2282], grad_fn=<UnbindBackward0>))\n",
      "(7.734558844354756, tensor([7.6412], grad_fn=<UnbindBackward0>))\n",
      "(8.951051374025619, tensor([8.4662], grad_fn=<UnbindBackward0>))\n",
      "(6.398594934535208, tensor([6.3110], grad_fn=<UnbindBackward0>))\n",
      "(6.925595197110468, tensor([6.7641], grad_fn=<UnbindBackward0>))\n",
      "(8.164225652265827, tensor([6.2780], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([7.1802], grad_fn=<UnbindBackward0>))\n",
      "(8.710619527942297, tensor([8.4526], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([10.0503], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([9.0243], grad_fn=<UnbindBackward0>))\n",
      "(7.122866658599083, tensor([7.5995], grad_fn=<UnbindBackward0>))\n",
      "(9.315600882633676, tensor([7.0012], grad_fn=<UnbindBackward0>))\n",
      "(6.848005274576363, tensor([6.4476], grad_fn=<UnbindBackward0>))\n",
      "(6.950814768442584, tensor([7.3977], grad_fn=<UnbindBackward0>))\n",
      "(9.468542139105455, tensor([9.0344], grad_fn=<UnbindBackward0>))\n",
      "(7.741099090035366, tensor([9.3706], grad_fn=<UnbindBackward0>))\n",
      "(8.814033201652784, tensor([7.7695], grad_fn=<UnbindBackward0>))\n",
      "(8.635864721133736, tensor([8.6658], grad_fn=<UnbindBackward0>))\n",
      "(7.77779262633883, tensor([6.5707], grad_fn=<UnbindBackward0>))\n",
      "(6.945051063725834, tensor([6.2906], grad_fn=<UnbindBackward0>))\n",
      "(7.730174795246222, tensor([5.9190], grad_fn=<UnbindBackward0>))\n",
      "(7.7336835707759, tensor([6.3824], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([6.4329], grad_fn=<UnbindBackward0>))\n",
      "(8.563695025067657, tensor([8.4298], grad_fn=<UnbindBackward0>))\n",
      "(8.555066843844319, tensor([7.7652], grad_fn=<UnbindBackward0>))\n",
      "(7.501082124259871, tensor([6.3129], grad_fn=<UnbindBackward0>))\n",
      "(7.0967213784947605, tensor([6.7602], grad_fn=<UnbindBackward0>))\n",
      "(6.915723448631314, tensor([8.8882], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([7.9789], grad_fn=<UnbindBackward0>))\n",
      "(8.330381569349418, tensor([9.2871], grad_fn=<UnbindBackward0>))\n",
      "(7.033506484287697, tensor([6.9581], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([8.6291], grad_fn=<UnbindBackward0>))\n",
      "(8.78216942633238, tensor([6.3154], grad_fn=<UnbindBackward0>))\n",
      "(7.461640392208575, tensor([6.2994], grad_fn=<UnbindBackward0>))\n",
      "(9.437874271591685, tensor([9.3723], grad_fn=<UnbindBackward0>))\n",
      "(7.183111701743281, tensor([9.2761], grad_fn=<UnbindBackward0>))\n",
      "(8.820404065485645, tensor([7.2584], grad_fn=<UnbindBackward0>))\n",
      "(8.26796230533871, tensor([8.7163], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([6.8144], grad_fn=<UnbindBackward0>))\n",
      "(7.024649030453636, tensor([8.0800], grad_fn=<UnbindBackward0>))\n",
      "(7.524021415206125, tensor([10.3028], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([6.2325], grad_fn=<UnbindBackward0>))\n",
      "(9.614671504043145, tensor([8.1446], grad_fn=<UnbindBackward0>))\n",
      "(8.810310466357958, tensor([8.2863], grad_fn=<UnbindBackward0>))\n",
      "(6.329720905522696, tensor([7.9344], grad_fn=<UnbindBackward0>))\n",
      "(8.882530508433623, tensor([9.2343], grad_fn=<UnbindBackward0>))\n",
      "(8.098946748943339, tensor([6.5047], grad_fn=<UnbindBackward0>))\n",
      "(8.448271745949816, tensor([9.5280], grad_fn=<UnbindBackward0>))\n",
      "(6.448889394146858, tensor([6.7841], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([9.1268], grad_fn=<UnbindBackward0>))\n",
      "(7.68017564043659, tensor([8.2005], grad_fn=<UnbindBackward0>))\n",
      "(9.469545806435415, tensor([6.3361], grad_fn=<UnbindBackward0>))\n",
      "(9.69609431131315, tensor([9.4147], grad_fn=<UnbindBackward0>))\n",
      "(7.395107546562485, tensor([6.8363], grad_fn=<UnbindBackward0>))\n",
      "(8.4069317971587, tensor([7.1322], grad_fn=<UnbindBackward0>))\n",
      "(7.518607216815252, tensor([9.5672], grad_fn=<UnbindBackward0>))\n",
      "(6.565264970035361, tensor([6.3011], grad_fn=<UnbindBackward0>))\n",
      "(9.434203664214742, tensor([6.5547], grad_fn=<UnbindBackward0>))\n",
      "(8.864605368075784, tensor([7.2092], grad_fn=<UnbindBackward0>))\n",
      "(8.86177531100083, tensor([9.7092], grad_fn=<UnbindBackward0>))\n",
      "(7.6638772587034705, tensor([6.5013], grad_fn=<UnbindBackward0>))\n",
      "(6.9975959829819265, tensor([6.2172], grad_fn=<UnbindBackward0>))\n",
      "(8.146419323098003, tensor([8.6298], grad_fn=<UnbindBackward0>))\n",
      "(6.111467339502679, tensor([6.2001], grad_fn=<UnbindBackward0>))\n",
      "(6.6895992691789665, tensor([7.6435], grad_fn=<UnbindBackward0>))\n",
      "(9.443196602144186, tensor([8.6125], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([6.7602], grad_fn=<UnbindBackward0>))\n",
      "(8.331827004436057, tensor([8.5972], grad_fn=<UnbindBackward0>))\n",
      "(7.53689712956617, tensor([6.6824], grad_fn=<UnbindBackward0>))\n",
      "(5.8916442118257715, tensor([10.0727], grad_fn=<UnbindBackward0>))\n",
      "(8.671629544720656, tensor([6.5463], grad_fn=<UnbindBackward0>))\n",
      "(7.067319848653476, tensor([6.9741], grad_fn=<UnbindBackward0>))\n",
      "(8.68946441235669, tensor([8.8384], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([8.4022], grad_fn=<UnbindBackward0>))\n",
      "(7.861341795599989, tensor([6.2277], grad_fn=<UnbindBackward0>))\n",
      "(8.77770959579525, tensor([7.4206], grad_fn=<UnbindBackward0>))\n",
      "(8.395251520610994, tensor([8.7592], grad_fn=<UnbindBackward0>))\n",
      "(9.373309200254322, tensor([8.2016], grad_fn=<UnbindBackward0>))\n",
      "(6.352629396319567, tensor([6.8479], grad_fn=<UnbindBackward0>))\n",
      "(7.945555428253489, tensor([6.8015], grad_fn=<UnbindBackward0>))\n",
      "(7.392031567514591, tensor([9.0072], grad_fn=<UnbindBackward0>))\n",
      "(6.726233402358747, tensor([6.3524], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([6.5886], grad_fn=<UnbindBackward0>))\n",
      "(8.028455164114252, tensor([7.2671], grad_fn=<UnbindBackward0>))\n",
      "(9.181014542594355, tensor([7.1294], grad_fn=<UnbindBackward0>))\n",
      "(6.47543271670409, tensor([8.1635], grad_fn=<UnbindBackward0>))\n",
      "(6.580639137284949, tensor([8.3864], grad_fn=<UnbindBackward0>))\n",
      "(8.656781205623291, tensor([6.3530], grad_fn=<UnbindBackward0>))\n",
      "(8.633552992532433, tensor([6.7068], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([8.4007], grad_fn=<UnbindBackward0>))\n",
      "(9.526755663250837, tensor([8.9317], grad_fn=<UnbindBackward0>))\n",
      "(8.597297435657898, tensor([6.3309], grad_fn=<UnbindBackward0>))\n",
      "(6.595780513961311, tensor([7.9182], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([6.7617], grad_fn=<UnbindBackward0>))\n",
      "(7.983098940710892, tensor([9.9269], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([7.2581], grad_fn=<UnbindBackward0>))\n",
      "(6.244166900663736, tensor([8.2563], grad_fn=<UnbindBackward0>))\n",
      "(9.306740866262498, tensor([6.4252], grad_fn=<UnbindBackward0>))\n",
      "(9.753826777981722, tensor([7.8326], grad_fn=<UnbindBackward0>))\n",
      "(8.879472402074802, tensor([8.9914], grad_fn=<UnbindBackward0>))\n",
      "(9.325987795502144, tensor([6.1587], grad_fn=<UnbindBackward0>))\n",
      "(7.002155954403621, tensor([8.5272], grad_fn=<UnbindBackward0>))\n",
      "(7.267525427828172, tensor([7.3307], grad_fn=<UnbindBackward0>))\n",
      "(9.10642325855641, tensor([7.8264], grad_fn=<UnbindBackward0>))\n",
      "(6.6240652277998935, tensor([7.3638], grad_fn=<UnbindBackward0>))\n",
      "(6.461468176353717, tensor([6.1746], grad_fn=<UnbindBackward0>))\n",
      "(6.9650803456014065, tensor([8.0566], grad_fn=<UnbindBackward0>))\n",
      "(8.574140471857987, tensor([6.3161], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([7.4399], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([6.4358], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([6.7351], grad_fn=<UnbindBackward0>))\n",
      "(6.154858094016418, tensor([6.3471], grad_fn=<UnbindBackward0>))\n",
      "(6.77078942390898, tensor([8.4017], grad_fn=<UnbindBackward0>))\n",
      "(9.276689752517717, tensor([6.6561], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([9.5121], grad_fn=<UnbindBackward0>))\n",
      "(8.864322722511439, tensor([8.7189], grad_fn=<UnbindBackward0>))\n",
      "(6.709304340258298, tensor([7.4431], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([8.6155], grad_fn=<UnbindBackward0>))\n",
      "(8.312135107648412, tensor([7.9543], grad_fn=<UnbindBackward0>))\n",
      "(6.97354301952014, tensor([7.9242], grad_fn=<UnbindBackward0>))\n",
      "(8.850230965588816, tensor([8.9065], grad_fn=<UnbindBackward0>))\n",
      "(8.534443544822764, tensor([6.9087], grad_fn=<UnbindBackward0>))\n",
      "(7.085064293952548, tensor([8.7552], grad_fn=<UnbindBackward0>))\n",
      "(7.025538314638521, tensor([9.8334], grad_fn=<UnbindBackward0>))\n",
      "(9.568084743872936, tensor([6.2967], grad_fn=<UnbindBackward0>))\n",
      "(9.594513744616725, tensor([9.5354], grad_fn=<UnbindBackward0>))\n",
      "(7.387090235656757, tensor([9.4630], grad_fn=<UnbindBackward0>))\n",
      "(9.220983532074662, tensor([9.3652], grad_fn=<UnbindBackward0>))\n",
      "(8.313607139317558, tensor([9.4593], grad_fn=<UnbindBackward0>))\n",
      "(9.47500967011889, tensor([8.1723], grad_fn=<UnbindBackward0>))\n",
      "(8.009363076630045, tensor([7.2983], grad_fn=<UnbindBackward0>))\n",
      "(7.627544390488503, tensor([7.7030], grad_fn=<UnbindBackward0>))\n",
      "(6.3818160174060985, tensor([7.7753], grad_fn=<UnbindBackward0>))\n",
      "(6.891625897052253, tensor([8.6439], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([7.2373], grad_fn=<UnbindBackward0>))\n",
      "(8.488382109562117, tensor([8.7463], grad_fn=<UnbindBackward0>))\n",
      "(6.890609120147166, tensor([7.9317], grad_fn=<UnbindBackward0>))\n",
      "(7.555905093611346, tensor([10.2162], grad_fn=<UnbindBackward0>))\n",
      "(6.18826412308259, tensor([7.5810], grad_fn=<UnbindBackward0>))\n",
      "(7.179307969504034, tensor([8.4491], grad_fn=<UnbindBackward0>))\n",
      "(8.165647925297504, tensor([8.5393], grad_fn=<UnbindBackward0>))\n",
      "(7.451241684987676, tensor([6.8261], grad_fn=<UnbindBackward0>))\n",
      "(9.45430563898269, tensor([8.3938], grad_fn=<UnbindBackward0>))\n",
      "(8.884610231886873, tensor([6.2991], grad_fn=<UnbindBackward0>))\n",
      "(7.752764808851328, tensor([8.5009], grad_fn=<UnbindBackward0>))\n",
      "(6.234410725718371, tensor([5.9617], grad_fn=<UnbindBackward0>))\n",
      "(8.007034012193408, tensor([6.3235], grad_fn=<UnbindBackward0>))\n",
      "(9.441769502877165, tensor([9.0721], grad_fn=<UnbindBackward0>))\n",
      "(8.382289428951436, tensor([8.7789], grad_fn=<UnbindBackward0>))\n",
      "(8.719644119505357, tensor([8.6939], grad_fn=<UnbindBackward0>))\n",
      "(6.570882962339584, tensor([6.2991], grad_fn=<UnbindBackward0>))\n",
      "(7.516977224604321, tensor([9.9825], grad_fn=<UnbindBackward0>))\n",
      "(7.875499292445208, tensor([7.9559], grad_fn=<UnbindBackward0>))\n",
      "(8.330381569349418, tensor([6.2108], grad_fn=<UnbindBackward0>))\n",
      "(8.319473692442186, tensor([9.6493], grad_fn=<UnbindBackward0>))\n",
      "(8.7937637591133, tensor([7.9400], grad_fn=<UnbindBackward0>))\n",
      "(9.126306376366173, tensor([10.0501], grad_fn=<UnbindBackward0>))\n",
      "(8.930626469173578, tensor([7.2435], grad_fn=<UnbindBackward0>))\n",
      "(8.366370301681654, tensor([6.8488], grad_fn=<UnbindBackward0>))\n",
      "(8.159660737063376, tensor([7.4261], grad_fn=<UnbindBackward0>))\n",
      "(7.344072850573066, tensor([7.9283], grad_fn=<UnbindBackward0>))\n",
      "(7.001245622069476, tensor([7.1678], grad_fn=<UnbindBackward0>))\n",
      "(9.199481628641307, tensor([6.6010], grad_fn=<UnbindBackward0>))\n",
      "(8.762489547371581, tensor([8.8154], grad_fn=<UnbindBackward0>))\n",
      "(8.166500319155052, tensor([8.4190], grad_fn=<UnbindBackward0>))\n",
      "(6.398594934535208, tensor([6.2376], grad_fn=<UnbindBackward0>))\n",
      "(7.431299675155903, tensor([6.2952], grad_fn=<UnbindBackward0>))\n",
      "(7.247792581767846, tensor([8.7913], grad_fn=<UnbindBackward0>))\n",
      "(9.585964638073326, tensor([8.6481], grad_fn=<UnbindBackward0>))\n",
      "(7.761744984658913, tensor([10.0046], grad_fn=<UnbindBackward0>))\n",
      "(6.887552571664617, tensor([7.2622], grad_fn=<UnbindBackward0>))\n",
      "(6.516193076042964, tensor([8.2572], grad_fn=<UnbindBackward0>))\n",
      "(7.443078374348516, tensor([7.2875], grad_fn=<UnbindBackward0>))\n",
      "(8.992806059426483, tensor([7.7609], grad_fn=<UnbindBackward0>))\n",
      "(7.629489916393995, tensor([6.9129], grad_fn=<UnbindBackward0>))\n",
      "(7.3833681469923835, tensor([7.0507], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([6.3900], grad_fn=<UnbindBackward0>))\n",
      "(7.52456122628536, tensor([6.7369], grad_fn=<UnbindBackward0>))\n",
      "(6.804614520062624, tensor([6.0340], grad_fn=<UnbindBackward0>))\n",
      "(8.765926513729443, tensor([6.4897], grad_fn=<UnbindBackward0>))\n",
      "(7.890582534656536, tensor([8.3043], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([8.1162], grad_fn=<UnbindBackward0>))\n",
      "(8.834045641167798, tensor([9.0870], grad_fn=<UnbindBackward0>))\n",
      "(8.867709208039386, tensor([7.1133], grad_fn=<UnbindBackward0>))\n",
      "(6.918695219020472, tensor([6.2802], grad_fn=<UnbindBackward0>))\n",
      "(9.441928170073055, tensor([8.0572], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([8.6198], grad_fn=<UnbindBackward0>))\n",
      "(6.651571873589727, tensor([6.4377], grad_fn=<UnbindBackward0>))\n",
      "(7.236339342754344, tensor([6.4116], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([6.2681], grad_fn=<UnbindBackward0>))\n",
      "(6.688354713946762, tensor([8.2414], grad_fn=<UnbindBackward0>))\n",
      "(8.3707791729607, tensor([6.7012], grad_fn=<UnbindBackward0>))\n",
      "(6.79794041297493, tensor([6.1849], grad_fn=<UnbindBackward0>))\n",
      "(6.3578422665081, tensor([8.8280], grad_fn=<UnbindBackward0>))\n",
      "(6.652863029353347, tensor([8.7357], grad_fn=<UnbindBackward0>))\n",
      "(8.331827004436057, tensor([7.2510], grad_fn=<UnbindBackward0>))\n",
      "(7.524021415206125, tensor([8.4792], grad_fn=<UnbindBackward0>))\n",
      "(8.101677747454572, tensor([8.4318], grad_fn=<UnbindBackward0>))\n",
      "(6.923628628138427, tensor([7.8817], grad_fn=<UnbindBackward0>))\n",
      "(6.78332520060396, tensor([6.7385], grad_fn=<UnbindBackward0>))\n",
      "(8.34972083747249, tensor([9.2314], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([7.0580], grad_fn=<UnbindBackward0>))\n",
      "(7.447751280047908, tensor([6.0587], grad_fn=<UnbindBackward0>))\n",
      "(5.869296913133774, tensor([6.2569], grad_fn=<UnbindBackward0>))\n",
      "(8.198089248956116, tensor([7.2675], grad_fn=<UnbindBackward0>))\n",
      "(8.193400231952097, tensor([6.8887], grad_fn=<UnbindBackward0>))\n",
      "(9.698613591271663, tensor([8.1049], grad_fn=<UnbindBackward0>))\n",
      "(8.831857935197906, tensor([9.2000], grad_fn=<UnbindBackward0>))\n",
      "(6.20050917404269, tensor([7.4412], grad_fn=<UnbindBackward0>))\n",
      "(6.727431724850855, tensor([8.5693], grad_fn=<UnbindBackward0>))\n",
      "(9.648272670221848, tensor([8.8666], grad_fn=<UnbindBackward0>))\n",
      "(7.181591944611865, tensor([8.0258], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([8.6692], grad_fn=<UnbindBackward0>))\n",
      "(6.502790045915623, tensor([9.1170], grad_fn=<UnbindBackward0>))\n",
      "(8.90354343566472, tensor([7.2535], grad_fn=<UnbindBackward0>))\n",
      "(7.638198244285779, tensor([8.1237], grad_fn=<UnbindBackward0>))\n",
      "(8.003363058629947, tensor([6.2268], grad_fn=<UnbindBackward0>))\n",
      "(7.990238185720363, tensor([9.7051], grad_fn=<UnbindBackward0>))\n",
      "(8.679822114864455, tensor([7.1253], grad_fn=<UnbindBackward0>))\n",
      "(7.946617563244473, tensor([7.4545], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([8.9542], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([7.1947], grad_fn=<UnbindBackward0>))\n",
      "(5.820082930352362, tensor([7.8367], grad_fn=<UnbindBackward0>))\n",
      "(8.600062669238532, tensor([6.4786], grad_fn=<UnbindBackward0>))\n",
      "(9.430198532298864, tensor([6.6202], grad_fn=<UnbindBackward0>))\n",
      "(8.138564737261632, tensor([8.8090], grad_fn=<UnbindBackward0>))\n",
      "(7.080867896690782, tensor([8.5619], grad_fn=<UnbindBackward0>))\n",
      "(8.606485298894999, tensor([9.4431], grad_fn=<UnbindBackward0>))\n",
      "(7.853993087224244, tensor([9.3857], grad_fn=<UnbindBackward0>))\n",
      "(7.388327859577107, tensor([9.7692], grad_fn=<UnbindBackward0>))\n",
      "(7.4377951216719325, tensor([8.6673], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([8.7900], grad_fn=<UnbindBackward0>))\n",
      "(7.91862865334224, tensor([7.9721], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([7.2467], grad_fn=<UnbindBackward0>))\n",
      "(7.980707820869669, tensor([7.0198], grad_fn=<UnbindBackward0>))\n",
      "(6.57507584059962, tensor([8.9929], grad_fn=<UnbindBackward0>))\n",
      "(9.577688157971219, tensor([10.1720], grad_fn=<UnbindBackward0>))\n",
      "(6.472346294500901, tensor([8.1081], grad_fn=<UnbindBackward0>))\n",
      "(6.6293632534374485, tensor([6.2672], grad_fn=<UnbindBackward0>))\n",
      "(8.609590040682205, tensor([6.5512], grad_fn=<UnbindBackward0>))\n",
      "(7.745435610274381, tensor([8.7914], grad_fn=<UnbindBackward0>))\n",
      "(9.593355351246755, tensor([7.9197], grad_fn=<UnbindBackward0>))\n",
      "(6.966024187106113, tensor([6.6132], grad_fn=<UnbindBackward0>))\n",
      "(9.69996278809107, tensor([6.2746], grad_fn=<UnbindBackward0>))\n",
      "(7.557472901614746, tensor([6.9455], grad_fn=<UnbindBackward0>))\n",
      "(8.237743803890933, tensor([7.2206], grad_fn=<UnbindBackward0>))\n",
      "(8.862200330487287, tensor([8.5737], grad_fn=<UnbindBackward0>))\n",
      "(6.894670039433482, tensor([9.5726], grad_fn=<UnbindBackward0>))\n",
      "(6.873163834212518, tensor([6.1947], grad_fn=<UnbindBackward0>))\n",
      "(7.3537223303996315, tensor([6.6500], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([8.4863], grad_fn=<UnbindBackward0>))\n",
      "(5.958424693029782, tensor([7.7295], grad_fn=<UnbindBackward0>))\n",
      "(7.911690520708339, tensor([6.2137], grad_fn=<UnbindBackward0>))\n",
      "(8.640648995343156, tensor([6.2967], grad_fn=<UnbindBackward0>))\n",
      "(8.93247660846174, tensor([7.5319], grad_fn=<UnbindBackward0>))\n",
      "(7.5411524551363085, tensor([7.7866], grad_fn=<UnbindBackward0>))\n",
      "(7.080026499922591, tensor([6.6684], grad_fn=<UnbindBackward0>))\n",
      "(9.418085981110297, tensor([6.8533], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([8.2468], grad_fn=<UnbindBackward0>))\n",
      "(9.174505918966966, tensor([7.2828], grad_fn=<UnbindBackward0>))\n",
      "(6.169610732491456, tensor([6.3943], grad_fn=<UnbindBackward0>))\n",
      "(7.085064293952548, tensor([8.6105], grad_fn=<UnbindBackward0>))\n",
      "(6.249975242259483, tensor([6.5929], grad_fn=<UnbindBackward0>))\n",
      "(8.37930948405285, tensor([8.9113], grad_fn=<UnbindBackward0>))\n",
      "(5.971261839790462, tensor([6.5386], grad_fn=<UnbindBackward0>))\n",
      "(6.910750787961936, tensor([9.0710], grad_fn=<UnbindBackward0>))\n",
      "(9.147932911847565, tensor([9.2835], grad_fn=<UnbindBackward0>))\n",
      "(7.241366283322318, tensor([9.5871], grad_fn=<UnbindBackward0>))\n",
      "(7.754052639035757, tensor([7.8596], grad_fn=<UnbindBackward0>))\n",
      "(7.784057002639929, tensor([7.8708], grad_fn=<UnbindBackward0>))\n",
      "(8.534640105019959, tensor([8.4837], grad_fn=<UnbindBackward0>))\n",
      "(8.951440100949855, tensor([7.2529], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([8.6497], grad_fn=<UnbindBackward0>))\n",
      "(7.017506142941256, tensor([9.8789], grad_fn=<UnbindBackward0>))\n",
      "(9.226804098006848, tensor([8.5048], grad_fn=<UnbindBackward0>))\n",
      "(7.5126175446745105, tensor([8.0582], grad_fn=<UnbindBackward0>))\n",
      "(7.034387929915503, tensor([6.7235], grad_fn=<UnbindBackward0>))\n",
      "(6.634633357861686, tensor([7.3736], grad_fn=<UnbindBackward0>))\n",
      "(7.633853559681768, tensor([8.5547], grad_fn=<UnbindBackward0>))\n",
      "(7.07326971745971, tensor([9.5674], grad_fn=<UnbindBackward0>))\n",
      "(9.022563964499264, tensor([8.2770], grad_fn=<UnbindBackward0>))\n",
      "(6.654152520183219, tensor([7.5583], grad_fn=<UnbindBackward0>))\n",
      "(6.315358001522335, tensor([6.3334], grad_fn=<UnbindBackward0>))\n",
      "(7.020190708311925, tensor([6.1997], grad_fn=<UnbindBackward0>))\n",
      "(9.40894529884324, tensor([8.9286], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([6.8272], grad_fn=<UnbindBackward0>))\n",
      "(6.606650186198215, tensor([6.7166], grad_fn=<UnbindBackward0>))\n",
      "(9.510370887608827, tensor([7.2617], grad_fn=<UnbindBackward0>))\n",
      "(8.881002624255569, tensor([8.2222], grad_fn=<UnbindBackward0>))\n",
      "(7.9441374911141125, tensor([7.2677], grad_fn=<UnbindBackward0>))\n",
      "(6.584791392385716, tensor([10.2014], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([8.0283], grad_fn=<UnbindBackward0>))\n",
      "(7.891330757661889, tensor([7.6107], grad_fn=<UnbindBackward0>))\n",
      "(6.07073772800249, tensor([7.8457], grad_fn=<UnbindBackward0>))\n",
      "(9.231808278591423, tensor([7.8277], grad_fn=<UnbindBackward0>))\n",
      "(9.0642735958414, tensor([5.9213], grad_fn=<UnbindBackward0>))\n",
      "(6.863803391452954, tensor([6.6315], grad_fn=<UnbindBackward0>))\n",
      "(6.974478911025045, tensor([6.2492], grad_fn=<UnbindBackward0>))\n",
      "(8.447199819595703, tensor([9.1772], grad_fn=<UnbindBackward0>))\n",
      "(7.506591780070841, tensor([6.2155], grad_fn=<UnbindBackward0>))\n",
      "(7.6544432264701125, tensor([6.9627], grad_fn=<UnbindBackward0>))\n",
      "(8.190077049719049, tensor([6.0864], grad_fn=<UnbindBackward0>))\n",
      "(7.496097345175956, tensor([8.8299], grad_fn=<UnbindBackward0>))\n",
      "(7.907283609426348, tensor([7.1585], grad_fn=<UnbindBackward0>))\n",
      "(8.798454696065095, tensor([6.4332], grad_fn=<UnbindBackward0>))\n",
      "(6.3080984415095305, tensor([7.9131], grad_fn=<UnbindBackward0>))\n",
      "(9.33459133431404, tensor([6.1858], grad_fn=<UnbindBackward0>))\n",
      "(7.166265974133638, tensor([7.2766], grad_fn=<UnbindBackward0>))\n",
      "(7.959625305098115, tensor([10.1985], grad_fn=<UnbindBackward0>))\n",
      "(7.569411792450712, tensor([7.1571], grad_fn=<UnbindBackward0>))\n",
      "(8.314587291319576, tensor([6.2946], grad_fn=<UnbindBackward0>))\n",
      "(6.78332520060396, tensor([7.8039], grad_fn=<UnbindBackward0>))\n",
      "(6.966024187106113, tensor([6.2755], grad_fn=<UnbindBackward0>))\n",
      "(9.652715834104445, tensor([8.7091], grad_fn=<UnbindBackward0>))\n",
      "(8.573951525234847, tensor([6.8697], grad_fn=<UnbindBackward0>))\n",
      "(8.37930948405285, tensor([6.6132], grad_fn=<UnbindBackward0>))\n",
      "(9.193906065512758, tensor([7.3322], grad_fn=<UnbindBackward0>))\n",
      "(6.126869184114185, tensor([6.7388], grad_fn=<UnbindBackward0>))\n",
      "(6.329720905522696, tensor([9.4661], grad_fn=<UnbindBackward0>))\n",
      "(8.537387898701757, tensor([8.3205], grad_fn=<UnbindBackward0>))\n",
      "(7.724888439323074, tensor([8.9726], grad_fn=<UnbindBackward0>))\n",
      "(7.194436851100335, tensor([6.3305], grad_fn=<UnbindBackward0>))\n",
      "(7.941651252930556, tensor([7.6319], grad_fn=<UnbindBackward0>))\n",
      "(9.75927070177572, tensor([6.8082], grad_fn=<UnbindBackward0>))\n",
      "(7.537962659768208, tensor([6.8329], grad_fn=<UnbindBackward0>))\n",
      "(9.709902803463462, tensor([7.1684], grad_fn=<UnbindBackward0>))\n",
      "(8.470101583882387, tensor([8.9421], grad_fn=<UnbindBackward0>))\n",
      "(9.179984251961283, tensor([9.5603], grad_fn=<UnbindBackward0>))\n",
      "(8.63497622707262, tensor([6.5245], grad_fn=<UnbindBackward0>))\n",
      "(7.958227192322312, tensor([6.0459], grad_fn=<UnbindBackward0>))\n",
      "(6.994849985833071, tensor([6.7132], grad_fn=<UnbindBackward0>))\n",
      "(8.65834547117212, tensor([6.5249], grad_fn=<UnbindBackward0>))\n",
      "(7.80994708647679, tensor([6.6682], grad_fn=<UnbindBackward0>))\n",
      "(8.764365720529808, tensor([9.2475], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([8.6113], grad_fn=<UnbindBackward0>))\n",
      "(7.358830898342354, tensor([7.1684], grad_fn=<UnbindBackward0>))\n",
      "(6.893656354602635, tensor([6.5264], grad_fn=<UnbindBackward0>))\n",
      "(8.590257762273243, tensor([9.3955], grad_fn=<UnbindBackward0>))\n",
      "(9.169205828617628, tensor([8.8388], grad_fn=<UnbindBackward0>))\n",
      "(7.633853559681768, tensor([8.9883], grad_fn=<UnbindBackward0>))\n",
      "(8.944158830970403, tensor([9.1095], grad_fn=<UnbindBackward0>))\n",
      "(6.63200177739563, tensor([9.8399], grad_fn=<UnbindBackward0>))\n",
      "(8.129174996911793, tensor([6.2677], grad_fn=<UnbindBackward0>))\n",
      "(6.705639094860003, tensor([8.6351], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([8.8919], grad_fn=<UnbindBackward0>))\n",
      "(8.266421472984554, tensor([6.5106], grad_fn=<UnbindBackward0>))\n",
      "(8.131824785007195, tensor([6.6046], grad_fn=<UnbindBackward0>))\n",
      "(6.472346294500901, tensor([6.7865], grad_fn=<UnbindBackward0>))\n",
      "(6.198478716492308, tensor([6.7329], grad_fn=<UnbindBackward0>))\n",
      "(6.910750787961936, tensor([7.2099], grad_fn=<UnbindBackward0>))\n",
      "(7.016609683894219, tensor([8.4764], grad_fn=<UnbindBackward0>))\n",
      "(6.396929655216146, tensor([7.7131], grad_fn=<UnbindBackward0>))\n",
      "(8.31090675716845, tensor([8.4262], grad_fn=<UnbindBackward0>))\n",
      "(6.1675164908883415, tensor([8.5329], grad_fn=<UnbindBackward0>))\n",
      "(9.837935184295892, tensor([7.2540], grad_fn=<UnbindBackward0>))\n",
      "(6.391917113392602, tensor([9.9110], grad_fn=<UnbindBackward0>))\n",
      "(7.828834527588089, tensor([6.2793], grad_fn=<UnbindBackward0>))\n",
      "(8.69751274553952, tensor([7.7472], grad_fn=<UnbindBackward0>))\n",
      "(8.715388097366482, tensor([6.5057], grad_fn=<UnbindBackward0>))\n",
      "(8.25088114470065, tensor([8.5831], grad_fn=<UnbindBackward0>))\n",
      "(6.16541785423142, tensor([7.0517], grad_fn=<UnbindBackward0>))\n",
      "(6.980075940561763, tensor([8.6220], grad_fn=<UnbindBackward0>))\n",
      "(7.079184394609668, tensor([8.8658], grad_fn=<UnbindBackward0>))\n",
      "(6.642486801367256, tensor([6.3107], grad_fn=<UnbindBackward0>))\n",
      "(6.419994928147142, tensor([8.8088], grad_fn=<UnbindBackward0>))\n",
      "(7.859413154693583, tensor([8.0509], grad_fn=<UnbindBackward0>))\n",
      "(7.815207062189088, tensor([6.8189], grad_fn=<UnbindBackward0>))\n",
      "(8.61974977974133, tensor([6.1379], grad_fn=<UnbindBackward0>))\n",
      "(9.131729971394272, tensor([7.8547], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([9.9175], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([6.4099], grad_fn=<UnbindBackward0>))\n",
      "(7.0707241072602764, tensor([7.9681], grad_fn=<UnbindBackward0>))\n",
      "(9.263975921142093, tensor([6.6072], grad_fn=<UnbindBackward0>))\n",
      "(8.04237800517328, tensor([9.4218], grad_fn=<UnbindBackward0>))\n",
      "(6.854354502255021, tensor([8.9444], grad_fn=<UnbindBackward0>))\n",
      "(7.444248649496705, tensor([6.2098], grad_fn=<UnbindBackward0>))\n",
      "(8.591372589590488, tensor([8.7844], grad_fn=<UnbindBackward0>))\n",
      "(8.261526448396468, tensor([10.3865], grad_fn=<UnbindBackward0>))\n",
      "(8.68609172787805, tensor([6.6485], grad_fn=<UnbindBackward0>))\n",
      "(8.85907931788153, tensor([7.3383], grad_fn=<UnbindBackward0>))\n",
      "(8.34283980427146, tensor([8.7945], grad_fn=<UnbindBackward0>))\n",
      "(8.535229553902337, tensor([8.8546], grad_fn=<UnbindBackward0>))\n",
      "(9.241354425505353, tensor([6.2415], grad_fn=<UnbindBackward0>))\n",
      "(7.716906135298388, tensor([6.7163], grad_fn=<UnbindBackward0>))\n",
      "(8.305236829492593, tensor([10.0199], grad_fn=<UnbindBackward0>))\n",
      "(8.941676305360163, tensor([5.9363], grad_fn=<UnbindBackward0>))\n",
      "(9.252345666121213, tensor([7.4660], grad_fn=<UnbindBackward0>))\n",
      "(8.880446450715093, tensor([6.3394], grad_fn=<UnbindBackward0>))\n",
      "(6.395261598115449, tensor([7.4268], grad_fn=<UnbindBackward0>))\n",
      "(8.250097752572845, tensor([6.9516], grad_fn=<UnbindBackward0>))\n",
      "(6.3080984415095305, tensor([7.9299], grad_fn=<UnbindBackward0>))\n",
      "(7.485491608030754, tensor([9.5398], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([9.8347], grad_fn=<UnbindBackward0>))\n",
      "(6.97354301952014, tensor([6.6354], grad_fn=<UnbindBackward0>))\n",
      "(9.233763886919771, tensor([8.7540], grad_fn=<UnbindBackward0>))\n",
      "(9.13108069099055, tensor([7.3610], grad_fn=<UnbindBackward0>))\n",
      "(8.11552088154677, tensor([6.8488], grad_fn=<UnbindBackward0>))\n",
      "(6.610696044717759, tensor([6.7192], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.3584], grad_fn=<UnbindBackward0>))\n",
      "(7.186901020411631, tensor([5.7648], grad_fn=<UnbindBackward0>))\n",
      "(8.735846677457582, tensor([8.7613], grad_fn=<UnbindBackward0>))\n",
      "(8.933004591578547, tensor([6.8124], grad_fn=<UnbindBackward0>))\n",
      "(6.70073110954781, tensor([7.2071], grad_fn=<UnbindBackward0>))\n",
      "(7.607381425639791, tensor([7.7953], grad_fn=<UnbindBackward0>))\n",
      "(8.54772239645106, tensor([6.3651], grad_fn=<UnbindBackward0>))\n",
      "(7.4342573821331355, tensor([7.7639], grad_fn=<UnbindBackward0>))\n",
      "(8.946114375560743, tensor([8.8482], grad_fn=<UnbindBackward0>))\n",
      "(9.30091207016057, tensor([9.3563], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([7.8641], grad_fn=<UnbindBackward0>))\n",
      "(9.540003766668065, tensor([9.1170], grad_fn=<UnbindBackward0>))\n",
      "(6.920671504248683, tensor([6.3735], grad_fn=<UnbindBackward0>))\n",
      "(9.62397204275074, tensor([9.4351], grad_fn=<UnbindBackward0>))\n",
      "(6.418364935936212, tensor([8.3119], grad_fn=<UnbindBackward0>))\n",
      "(8.860499167616016, tensor([9.4725], grad_fn=<UnbindBackward0>))\n",
      "(7.763446388727362, tensor([6.1849], grad_fn=<UnbindBackward0>))\n",
      "(7.733245646529795, tensor([6.7700], grad_fn=<UnbindBackward0>))\n",
      "(7.358830898342354, tensor([9.0340], grad_fn=<UnbindBackward0>))\n",
      "(8.867145589594143, tensor([6.7435], grad_fn=<UnbindBackward0>))\n",
      "(9.788469440951861, tensor([8.6556], grad_fn=<UnbindBackward0>))\n",
      "(8.353025845202325, tensor([6.4823], grad_fn=<UnbindBackward0>))\n",
      "(8.166784289056151, tensor([6.1414], grad_fn=<UnbindBackward0>))\n",
      "(8.720950028930258, tensor([6.3845], grad_fn=<UnbindBackward0>))\n",
      "(6.80128303447162, tensor([8.3514], grad_fn=<UnbindBackward0>))\n",
      "(6.447305862541213, tensor([8.2499], grad_fn=<UnbindBackward0>))\n",
      "(9.444779883032243, tensor([8.6633], grad_fn=<UnbindBackward0>))\n",
      "(8.88751459738882, tensor([7.2641], grad_fn=<UnbindBackward0>))\n",
      "(7.66105638236183, tensor([8.9180], grad_fn=<UnbindBackward0>))\n",
      "(8.114922974204593, tensor([7.1156], grad_fn=<UnbindBackward0>))\n",
      "(7.646831391430482, tensor([8.1419], grad_fn=<UnbindBackward0>))\n",
      "(7.022868086082641, tensor([10.2670], grad_fn=<UnbindBackward0>))\n",
      "(9.813398717754442, tensor([6.3888], grad_fn=<UnbindBackward0>))\n",
      "(8.827174771362785, tensor([8.9012], grad_fn=<UnbindBackward0>))\n",
      "(8.140315540159985, tensor([8.6740], grad_fn=<UnbindBackward0>))\n",
      "(9.13550906135318, tensor([6.2937], grad_fn=<UnbindBackward0>))\n",
      "(6.907755278982137, tensor([7.7233], grad_fn=<UnbindBackward0>))\n",
      "(8.844624683385302, tensor([10.0047], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([8.4477], grad_fn=<UnbindBackward0>))\n",
      "(6.858565034791365, tensor([7.2359], grad_fn=<UnbindBackward0>))\n",
      "(9.212438170058329, tensor([6.2632], grad_fn=<UnbindBackward0>))\n",
      "(7.7336835707759, tensor([8.8484], grad_fn=<UnbindBackward0>))\n",
      "(7.830425617820331, tensor([7.8141], grad_fn=<UnbindBackward0>))\n",
      "(7.784057002639929, tensor([7.7414], grad_fn=<UnbindBackward0>))\n",
      "(8.620471540869739, tensor([6.2960], grad_fn=<UnbindBackward0>))\n",
      "(9.458059455856434, tensor([8.7664], grad_fn=<UnbindBackward0>))\n",
      "(9.822331725575118, tensor([8.6571], grad_fn=<UnbindBackward0>))\n",
      "(6.642486801367256, tensor([8.3763], grad_fn=<UnbindBackward0>))\n",
      "(6.505784060128229, tensor([8.7383], grad_fn=<UnbindBackward0>))\n",
      "(7.865955413933502, tensor([6.2972], grad_fn=<UnbindBackward0>))\n",
      "(8.332308352219117, tensor([7.2696], grad_fn=<UnbindBackward0>))\n",
      "(6.447305862541213, tensor([9.3407], grad_fn=<UnbindBackward0>))\n",
      "(7.506591780070841, tensor([7.2625], grad_fn=<UnbindBackward0>))\n",
      "(8.676587243566487, tensor([6.4564], grad_fn=<UnbindBackward0>))\n",
      "(9.23824732522919, tensor([9.4839], grad_fn=<UnbindBackward0>))\n",
      "(9.199380531739841, tensor([9.5818], grad_fn=<UnbindBackward0>))\n",
      "(6.838405200847344, tensor([8.3109], grad_fn=<UnbindBackward0>))\n",
      "(8.757311846936409, tensor([7.6960], grad_fn=<UnbindBackward0>))\n",
      "(8.989943046329998, tensor([6.3245], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([7.9963], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([7.8305], grad_fn=<UnbindBackward0>))\n",
      "(7.215239978730097, tensor([8.0148], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([8.4316], grad_fn=<UnbindBackward0>))\n",
      "(8.282230063296685, tensor([8.6471], grad_fn=<UnbindBackward0>))\n",
      "(7.273092595999522, tensor([7.4120], grad_fn=<UnbindBackward0>))\n",
      "(8.067149039910106, tensor([6.6253], grad_fn=<UnbindBackward0>))\n",
      "(7.796880342783522, tensor([8.9032], grad_fn=<UnbindBackward0>))\n",
      "(6.368187186350492, tensor([7.4774], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([8.3676], grad_fn=<UnbindBackward0>))\n",
      "(6.559615237493242, tensor([8.1934], grad_fn=<UnbindBackward0>))\n",
      "(8.16223106548118, tensor([6.7986], grad_fn=<UnbindBackward0>))\n",
      "(6.642486801367256, tensor([8.9302], grad_fn=<UnbindBackward0>))\n",
      "(7.201170883281678, tensor([6.3312], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([7.8794], grad_fn=<UnbindBackward0>))\n",
      "(6.579251212010101, tensor([7.3145], grad_fn=<UnbindBackward0>))\n",
      "(8.648747631156539, tensor([8.2292], grad_fn=<UnbindBackward0>))\n",
      "(7.271703706887368, tensor([7.6786], grad_fn=<UnbindBackward0>))\n",
      "(8.32093496888341, tensor([8.9838], grad_fn=<UnbindBackward0>))\n",
      "(9.08839870117094, tensor([9.0938], grad_fn=<UnbindBackward0>))\n",
      "(6.398594934535208, tensor([7.2784], grad_fn=<UnbindBackward0>))\n",
      "(7.763021309018518, tensor([7.2792], grad_fn=<UnbindBackward0>))\n",
      "(9.351145248567203, tensor([8.0555], grad_fn=<UnbindBackward0>))\n",
      "(6.192362489474872, tensor([8.2498], grad_fn=<UnbindBackward0>))\n",
      "(8.330863613224745, tensor([6.2294], grad_fn=<UnbindBackward0>))\n",
      "(6.349138991379798, tensor([9.2947], grad_fn=<UnbindBackward0>))\n",
      "(7.8582541821860294, tensor([9.7475], grad_fn=<UnbindBackward0>))\n",
      "(6.302618975744905, tensor([7.9629], grad_fn=<UnbindBackward0>))\n",
      "(7.9599745280805365, tensor([9.0631], grad_fn=<UnbindBackward0>))\n",
      "(8.610683534503575, tensor([8.6397], grad_fn=<UnbindBackward0>))\n",
      "(9.157361447185297, tensor([8.3639], grad_fn=<UnbindBackward0>))\n",
      "(8.336150816120663, tensor([8.7233], grad_fn=<UnbindBackward0>))\n",
      "(9.294221855956884, tensor([7.7959], grad_fn=<UnbindBackward0>))\n",
      "(8.722742874329398, tensor([7.1765], grad_fn=<UnbindBackward0>))\n",
      "(7.845416036592485, tensor([8.9209], grad_fn=<UnbindBackward0>))\n",
      "(6.154858094016418, tensor([6.8900], grad_fn=<UnbindBackward0>))\n",
      "(9.617870014133226, tensor([6.3794], grad_fn=<UnbindBackward0>))\n",
      "(7.277247726631484, tensor([9.3409], grad_fn=<UnbindBackward0>))\n",
      "(6.9650803456014065, tensor([8.5353], grad_fn=<UnbindBackward0>))\n",
      "(8.77539495854551, tensor([8.5134], grad_fn=<UnbindBackward0>))\n",
      "(8.96469555531546, tensor([8.3179], grad_fn=<UnbindBackward0>))\n",
      "(7.014814351275545, tensor([8.3087], grad_fn=<UnbindBackward0>))\n",
      "(9.118115427933665, tensor([8.9911], grad_fn=<UnbindBackward0>))\n",
      "(7.937731775260109, tensor([5.9851], grad_fn=<UnbindBackward0>))\n",
      "(7.894690850425624, tensor([6.3718], grad_fn=<UnbindBackward0>))\n",
      "(9.151545325465678, tensor([7.4014], grad_fn=<UnbindBackward0>))\n",
      "(7.246368080102461, tensor([6.2125], grad_fn=<UnbindBackward0>))\n",
      "(8.533066540572527, tensor([8.5726], grad_fn=<UnbindBackward0>))\n",
      "(5.8805329864007, tensor([8.3921], grad_fn=<UnbindBackward0>))\n",
      "(6.3080984415095305, tensor([8.8969], grad_fn=<UnbindBackward0>))\n",
      "(8.599878558034845, tensor([7.9185], grad_fn=<UnbindBackward0>))\n",
      "(7.483806687665835, tensor([6.2214], grad_fn=<UnbindBackward0>))\n",
      "(6.641182169740591, tensor([6.3184], grad_fn=<UnbindBackward0>))\n",
      "(8.25608813381491, tensor([6.4441], grad_fn=<UnbindBackward0>))\n",
      "(8.117610746466228, tensor([10.0592], grad_fn=<UnbindBackward0>))\n",
      "(7.244941546337007, tensor([9.3833], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([7.9029], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([9.0985], grad_fn=<UnbindBackward0>))\n",
      "(7.417580402414544, tensor([6.2850], grad_fn=<UnbindBackward0>))\n",
      "(6.62273632394984, tensor([8.4149], grad_fn=<UnbindBackward0>))\n",
      "(7.9391588179567965, tensor([7.8506], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([8.2695], grad_fn=<UnbindBackward0>))\n",
      "(6.925595197110468, tensor([9.0895], grad_fn=<UnbindBackward0>))\n",
      "(7.763021309018518, tensor([6.3751], grad_fn=<UnbindBackward0>))\n",
      "(7.023758954738443, tensor([9.7284], grad_fn=<UnbindBackward0>))\n",
      "(6.345636360828596, tensor([6.7457], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([7.2372], grad_fn=<UnbindBackward0>))\n",
      "(6.923628628138427, tensor([8.9553], grad_fn=<UnbindBackward0>))\n",
      "(6.964135612418245, tensor([8.4429], grad_fn=<UnbindBackward0>))\n",
      "(6.040254711277414, tensor([6.3353], grad_fn=<UnbindBackward0>))\n",
      "(8.371473537066832, tensor([6.3753], grad_fn=<UnbindBackward0>))\n",
      "(6.726233402358747, tensor([8.9454], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([8.2420], grad_fn=<UnbindBackward0>))\n",
      "(8.355614995760183, tensor([7.1486], grad_fn=<UnbindBackward0>))\n",
      "(9.75347828520942, tensor([6.9202], grad_fn=<UnbindBackward0>))\n",
      "(6.459904454377535, tensor([8.7831], grad_fn=<UnbindBackward0>))\n",
      "(6.982862751468942, tensor([8.5419], grad_fn=<UnbindBackward0>))\n",
      "(9.27453508401818, tensor([7.7889], grad_fn=<UnbindBackward0>))\n",
      "(6.218600119691729, tensor([7.3364], grad_fn=<UnbindBackward0>))\n",
      "(8.224699561967235, tensor([6.3572], grad_fn=<UnbindBackward0>))\n",
      "(8.292298107063221, tensor([9.9611], grad_fn=<UnbindBackward0>))\n",
      "(9.307648554443182, tensor([7.4847], grad_fn=<UnbindBackward0>))\n",
      "(8.2960476427647, tensor([9.1771], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([6.2943], grad_fn=<UnbindBackward0>))\n",
      "(7.796469243086058, tensor([7.7431], grad_fn=<UnbindBackward0>))\n",
      "(8.284504227258497, tensor([6.3139], grad_fn=<UnbindBackward0>))\n",
      "(7.898782356970309, tensor([6.0376], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([8.4527], grad_fn=<UnbindBackward0>))\n",
      "(7.8160138391590275, tensor([6.3854], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([9.7240], grad_fn=<UnbindBackward0>))\n",
      "(8.489821994620105, tensor([7.1804], grad_fn=<UnbindBackward0>))\n",
      "(7.7306140660637395, tensor([6.4414], grad_fn=<UnbindBackward0>))\n",
      "(8.496990484098719, tensor([6.2771], grad_fn=<UnbindBackward0>))\n",
      "(8.786456678344, tensor([10.1414], grad_fn=<UnbindBackward0>))\n",
      "(7.653020413804189, tensor([8.3347], grad_fn=<UnbindBackward0>))\n",
      "(8.790877754224164, tensor([8.6362], grad_fn=<UnbindBackward0>))\n",
      "(8.968141414126814, tensor([9.1638], grad_fn=<UnbindBackward0>))\n",
      "(8.289790583181643, tensor([6.6692], grad_fn=<UnbindBackward0>))\n",
      "(9.76388059814971, tensor([9.0904], grad_fn=<UnbindBackward0>))\n",
      "(6.967909201801884, tensor([8.5232], grad_fn=<UnbindBackward0>))\n",
      "(7.02197642307216, tensor([6.3078], grad_fn=<UnbindBackward0>))\n",
      "(9.01881660441743, tensor([8.5383], grad_fn=<UnbindBackward0>))\n",
      "(7.756195343948118, tensor([6.3299], grad_fn=<UnbindBackward0>))\n",
      "(7.269616749608169, tensor([8.6245], grad_fn=<UnbindBackward0>))\n",
      "(7.580189417944541, tensor([9.4257], grad_fn=<UnbindBackward0>))\n",
      "(6.490723534502507, tensor([6.8716], grad_fn=<UnbindBackward0>))\n",
      "(8.085486772102845, tensor([6.5514], grad_fn=<UnbindBackward0>))\n",
      "(6.386879319362645, tensor([7.3039], grad_fn=<UnbindBackward0>))\n",
      "(6.261491684321042, tensor([6.8053], grad_fn=<UnbindBackward0>))\n",
      "(7.195187320178709, tensor([8.9787], grad_fn=<UnbindBackward0>))\n",
      "(7.689828668736484, tensor([8.4653], grad_fn=<UnbindBackward0>))\n",
      "(6.693323668269949, tensor([9.4786], grad_fn=<UnbindBackward0>))\n",
      "(7.7501841622578365, tensor([7.4850], grad_fn=<UnbindBackward0>))\n",
      "(8.264105763728956, tensor([8.7659], grad_fn=<UnbindBackward0>))\n",
      "(7.396335293800808, tensor([6.1790], grad_fn=<UnbindBackward0>))\n",
      "(7.4489161025442, tensor([8.3583], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([7.2171], grad_fn=<UnbindBackward0>))\n",
      "(8.52456594574565, tensor([6.3456], grad_fn=<UnbindBackward0>))\n",
      "(8.267705664762426, tensor([6.6921], grad_fn=<UnbindBackward0>))\n",
      "(8.718663567048953, tensor([8.7335], grad_fn=<UnbindBackward0>))\n",
      "(6.363028103540465, tensor([6.4742], grad_fn=<UnbindBackward0>))\n",
      "(9.074176947163311, tensor([9.7600], grad_fn=<UnbindBackward0>))\n",
      "(9.399720293137584, tensor([7.3028], grad_fn=<UnbindBackward0>))\n",
      "(7.522400231387125, tensor([7.8679], grad_fn=<UnbindBackward0>))\n",
      "(8.593042503699674, tensor([6.2383], grad_fn=<UnbindBackward0>))\n",
      "(7.781973234434385, tensor([6.8050], grad_fn=<UnbindBackward0>))\n",
      "(8.173293438966228, tensor([6.3463], grad_fn=<UnbindBackward0>))\n",
      "(7.188412736496954, tensor([6.3990], grad_fn=<UnbindBackward0>))\n",
      "(7.780720886117918, tensor([8.7285], grad_fn=<UnbindBackward0>))\n",
      "(7.591861714889934, tensor([6.3456], grad_fn=<UnbindBackward0>))\n",
      "(8.244071270295786, tensor([8.6968], grad_fn=<UnbindBackward0>))\n",
      "(8.030084094267563, tensor([6.3631], grad_fn=<UnbindBackward0>))\n",
      "(9.16858043772795, tensor([6.5790], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([6.1900], grad_fn=<UnbindBackward0>))\n",
      "(8.028455164114252, tensor([7.7899], grad_fn=<UnbindBackward0>))\n",
      "(7.396335293800808, tensor([8.6177], grad_fn=<UnbindBackward0>))\n",
      "(8.37816098272068, tensor([8.0149], grad_fn=<UnbindBackward0>))\n",
      "(6.405228458030842, tensor([8.9003], grad_fn=<UnbindBackward0>))\n",
      "(7.664815785285735, tensor([6.2166], grad_fn=<UnbindBackward0>))\n",
      "(7.783224016336037, tensor([9.2726], grad_fn=<UnbindBackward0>))\n",
      "(8.06777619577889, tensor([7.8013], grad_fn=<UnbindBackward0>))\n",
      "(8.18423477409482, tensor([6.3907], grad_fn=<UnbindBackward0>))\n",
      "(8.221478947267192, tensor([7.8551], grad_fn=<UnbindBackward0>))\n",
      "(8.47907586930311, tensor([9.4369], grad_fn=<UnbindBackward0>))\n",
      "(9.041921720351219, tensor([6.5110], grad_fn=<UnbindBackward0>))\n",
      "(7.653494909661253, tensor([7.1354], grad_fn=<UnbindBackward0>))\n",
      "(6.569481420414296, tensor([6.2797], grad_fn=<UnbindBackward0>))\n",
      "(8.944680683558895, tensor([6.1002], grad_fn=<UnbindBackward0>))\n",
      "(7.922261058353247, tensor([8.6565], grad_fn=<UnbindBackward0>))\n",
      "(9.197356444417888, tensor([7.1031], grad_fn=<UnbindBackward0>))\n",
      "(7.3796321526095525, tensor([7.0401], grad_fn=<UnbindBackward0>))\n",
      "(8.07558263667172, tensor([6.2452], grad_fn=<UnbindBackward0>))\n",
      "(9.60541838719786, tensor([8.4236], grad_fn=<UnbindBackward0>))\n",
      "(7.77779262633883, tensor([9.5317], grad_fn=<UnbindBackward0>))\n",
      "(6.720220155135295, tensor([6.3455], grad_fn=<UnbindBackward0>))\n",
      "(7.507690077819904, tensor([9.4719], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([6.5809], grad_fn=<UnbindBackward0>))\n",
      "(6.767343125265392, tensor([6.2544], grad_fn=<UnbindBackward0>))\n",
      "(8.125926802707886, tensor([9.3218], grad_fn=<UnbindBackward0>))\n",
      "(7.9294865233142895, tensor([6.7586], grad_fn=<UnbindBackward0>))\n",
      "(8.916506080039204, tensor([7.1603], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([6.3184], grad_fn=<UnbindBackward0>))\n",
      "(8.152198015861787, tensor([9.9509], grad_fn=<UnbindBackward0>))\n",
      "(8.963672275615016, tensor([7.9726], grad_fn=<UnbindBackward0>))\n",
      "(7.679713639966372, tensor([9.4342], grad_fn=<UnbindBackward0>))\n",
      "(6.54965074223381, tensor([8.0732], grad_fn=<UnbindBackward0>))\n",
      "(8.53089883847235, tensor([8.2040], grad_fn=<UnbindBackward0>))\n",
      "(6.369900982828227, tensor([6.3168], grad_fn=<UnbindBackward0>))\n",
      "(7.451241684987676, tensor([7.6658], grad_fn=<UnbindBackward0>))\n",
      "(7.393263094763838, tensor([8.6567], grad_fn=<UnbindBackward0>))\n",
      "(8.53817159780143, tensor([6.5015], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([9.4952], grad_fn=<UnbindBackward0>))\n",
      "(6.803505257608338, tensor([7.6167], grad_fn=<UnbindBackward0>))\n",
      "(8.70682132339263, tensor([7.1233], grad_fn=<UnbindBackward0>))\n",
      "(9.423837546238587, tensor([8.7052], grad_fn=<UnbindBackward0>))\n",
      "(7.201170883281678, tensor([8.7950], grad_fn=<UnbindBackward0>))\n",
      "(7.824046010856292, tensor([7.2744], grad_fn=<UnbindBackward0>))\n",
      "(8.021912778985708, tensor([9.1287], grad_fn=<UnbindBackward0>))\n",
      "(6.289715570908998, tensor([8.6441], grad_fn=<UnbindBackward0>))\n",
      "(7.953669778649798, tensor([7.0025], grad_fn=<UnbindBackward0>))\n",
      "(8.543445562560303, tensor([6.5899], grad_fn=<UnbindBackward0>))\n",
      "(8.701845363548474, tensor([6.3068], grad_fn=<UnbindBackward0>))\n",
      "(8.764678074116606, tensor([10.2090], grad_fn=<UnbindBackward0>))\n",
      "(7.503840746698951, tensor([8.1567], grad_fn=<UnbindBackward0>))\n",
      "(8.054204897064407, tensor([8.7559], grad_fn=<UnbindBackward0>))\n",
      "(7.483806687665835, tensor([8.0742], grad_fn=<UnbindBackward0>))\n",
      "(8.291797105048733, tensor([6.2573], grad_fn=<UnbindBackward0>))\n",
      "(6.104793232414985, tensor([8.5769], grad_fn=<UnbindBackward0>))\n",
      "(6.981934677156389, tensor([9.2432], grad_fn=<UnbindBackward0>))\n",
      "(8.714895850248494, tensor([6.2793], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([7.2400], grad_fn=<UnbindBackward0>))\n",
      "(8.01862546504575, tensor([6.8030], grad_fn=<UnbindBackward0>))\n",
      "(6.891625897052253, tensor([6.6510], grad_fn=<UnbindBackward0>))\n",
      "(6.984716320118266, tensor([7.1543], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([8.3180], grad_fn=<UnbindBackward0>))\n",
      "(6.612041034833092, tensor([6.4205], grad_fn=<UnbindBackward0>))\n",
      "(8.657998068007258, tensor([6.2989], grad_fn=<UnbindBackward0>))\n",
      "(8.214735833382303, tensor([8.1433], grad_fn=<UnbindBackward0>))\n",
      "(6.267200548541362, tensor([7.2935], grad_fn=<UnbindBackward0>))\n",
      "(7.771488760117616, tensor([6.4739], grad_fn=<UnbindBackward0>))\n",
      "(8.578476419833136, tensor([6.8422], grad_fn=<UnbindBackward0>))\n",
      "(7.37650812632622, tensor([7.4687], grad_fn=<UnbindBackward0>))\n",
      "(7.3938782901077555, tensor([9.3605], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([6.1826], grad_fn=<UnbindBackward0>))\n",
      "(9.63521587360417, tensor([8.4455], grad_fn=<UnbindBackward0>))\n",
      "(8.4252971767117, tensor([8.7102], grad_fn=<UnbindBackward0>))\n",
      "(5.817111159963204, tensor([7.9538], grad_fn=<UnbindBackward0>))\n",
      "(8.004365564979574, tensor([7.0051], grad_fn=<UnbindBackward0>))\n",
      "(8.791638037085997, tensor([8.1665], grad_fn=<UnbindBackward0>))\n",
      "(9.260653185849772, tensor([9.6266], grad_fn=<UnbindBackward0>))\n",
      "(7.776115477098742, tensor([6.0875], grad_fn=<UnbindBackward0>))\n",
      "(7.10085190894405, tensor([10.2519], grad_fn=<UnbindBackward0>))\n",
      "(7.450660796211539, tensor([8.4670], grad_fn=<UnbindBackward0>))\n",
      "(7.719573989259581, tensor([7.2205], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([9.9842], grad_fn=<UnbindBackward0>))\n",
      "(7.746732907753622, tensor([6.3618], grad_fn=<UnbindBackward0>))\n",
      "(8.922124823918258, tensor([7.8221], grad_fn=<UnbindBackward0>))\n",
      "(6.767343125265392, tensor([9.6816], grad_fn=<UnbindBackward0>))\n",
      "(6.839476438228843, tensor([9.6647], grad_fn=<UnbindBackward0>))\n",
      "(7.052721049232323, tensor([6.6084], grad_fn=<UnbindBackward0>))\n",
      "(9.020026965252676, tensor([7.2181], grad_fn=<UnbindBackward0>))\n",
      "(7.644440761556566, tensor([8.8198], grad_fn=<UnbindBackward0>))\n",
      "(7.035268599281097, tensor([7.4290], grad_fn=<UnbindBackward0>))\n",
      "(9.276315360899716, tensor([6.3681], grad_fn=<UnbindBackward0>))\n",
      "(8.476371196895983, tensor([6.3724], grad_fn=<UnbindBackward0>))\n",
      "(8.004365564979574, tensor([7.0696], grad_fn=<UnbindBackward0>))\n",
      "(9.192278229157774, tensor([6.5181], grad_fn=<UnbindBackward0>))\n",
      "(7.162397497355718, tensor([7.8692], grad_fn=<UnbindBackward0>))\n",
      "(6.616065185132817, tensor([7.7274], grad_fn=<UnbindBackward0>))\n",
      "(7.633853559681768, tensor([9.3867], grad_fn=<UnbindBackward0>))\n",
      "(6.840546529288687, tensor([8.7172], grad_fn=<UnbindBackward0>))\n",
      "(7.409741954080923, tensor([6.8121], grad_fn=<UnbindBackward0>))\n",
      "(9.29908358181057, tensor([8.7145], grad_fn=<UnbindBackward0>))\n",
      "(6.875232087276577, tensor([7.7599], grad_fn=<UnbindBackward0>))\n",
      "(8.170468578330674, tensor([6.8568], grad_fn=<UnbindBackward0>))\n",
      "(9.037057839832897, tensor([8.5140], grad_fn=<UnbindBackward0>))\n",
      "(9.813672248213765, tensor([6.6665], grad_fn=<UnbindBackward0>))\n",
      "(7.215239978730097, tensor([6.1904], grad_fn=<UnbindBackward0>))\n",
      "(9.823524008395744, tensor([7.4259], grad_fn=<UnbindBackward0>))\n",
      "(6.78105762593618, tensor([6.4440], grad_fn=<UnbindBackward0>))\n",
      "(8.310169021981912, tensor([9.3864], grad_fn=<UnbindBackward0>))\n",
      "(6.300785794663244, tensor([6.5242], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([7.8829], grad_fn=<UnbindBackward0>))\n",
      "(7.399398083331354, tensor([8.8021], grad_fn=<UnbindBackward0>))\n",
      "(7.989899374942939, tensor([7.9790], grad_fn=<UnbindBackward0>))\n",
      "(7.424761761823209, tensor([6.2949], grad_fn=<UnbindBackward0>))\n",
      "(8.054204897064407, tensor([6.4533], grad_fn=<UnbindBackward0>))\n",
      "(7.590852123688581, tensor([7.9680], grad_fn=<UnbindBackward0>))\n",
      "(8.443115988019922, tensor([8.2588], grad_fn=<UnbindBackward0>))\n",
      "(7.579167967396076, tensor([6.3501], grad_fn=<UnbindBackward0>))\n",
      "(6.854354502255021, tensor([6.9655], grad_fn=<UnbindBackward0>))\n",
      "(8.393216011596527, tensor([7.2388], grad_fn=<UnbindBackward0>))\n",
      "(7.958576903813898, tensor([9.4822], grad_fn=<UnbindBackward0>))\n",
      "(6.80903930604298, tensor([6.3757], grad_fn=<UnbindBackward0>))\n",
      "(7.465655310134056, tensor([9.6244], grad_fn=<UnbindBackward0>))\n",
      "(6.926577033222725, tensor([7.9732], grad_fn=<UnbindBackward0>))\n",
      "(7.727094484779841, tensor([7.9552], grad_fn=<UnbindBackward0>))\n",
      "(6.963189985870238, tensor([6.7933], grad_fn=<UnbindBackward0>))\n",
      "(9.487593248937424, tensor([7.8488], grad_fn=<UnbindBackward0>))\n",
      "(7.945555428253489, tensor([7.1735], grad_fn=<UnbindBackward0>))\n",
      "(9.13356731317027, tensor([9.4930], grad_fn=<UnbindBackward0>))\n",
      "(6.849066282633458, tensor([6.1534], grad_fn=<UnbindBackward0>))\n",
      "(6.842683282238422, tensor([6.8649], grad_fn=<UnbindBackward0>))\n",
      "(8.021912778985708, tensor([8.2515], grad_fn=<UnbindBackward0>))\n",
      "(5.872117789475416, tensor([7.8568], grad_fn=<UnbindBackward0>))\n",
      "(9.593900644696298, tensor([8.2005], grad_fn=<UnbindBackward0>))\n",
      "(6.672032945461067, tensor([9.8040], grad_fn=<UnbindBackward0>))\n",
      "(9.537699784080726, tensor([9.1391], grad_fn=<UnbindBackward0>))\n",
      "(7.8252452914317745, tensor([7.2788], grad_fn=<UnbindBackward0>))\n",
      "(8.220941168281389, tensor([6.4141], grad_fn=<UnbindBackward0>))\n",
      "(7.409741954080923, tensor([6.5351], grad_fn=<UnbindBackward0>))\n",
      "(9.740556773534106, tensor([10.1665], grad_fn=<UnbindBackward0>))\n",
      "(8.92319149068606, tensor([7.8336], grad_fn=<UnbindBackward0>))\n",
      "(6.775366090936392, tensor([6.5751], grad_fn=<UnbindBackward0>))\n",
      "(8.96610085736724, tensor([7.2426], grad_fn=<UnbindBackward0>))\n",
      "(7.982416346827733, tensor([7.3183], grad_fn=<UnbindBackward0>))\n",
      "(6.1092475827643655, tensor([7.2930], grad_fn=<UnbindBackward0>))\n",
      "(7.010311867307229, tensor([6.8046], grad_fn=<UnbindBackward0>))\n",
      "(7.47477218239787, tensor([8.4674], grad_fn=<UnbindBackward0>))\n",
      "(6.593044534142437, tensor([7.4174], grad_fn=<UnbindBackward0>))\n",
      "(6.380122536899765, tensor([6.6037], grad_fn=<UnbindBackward0>))\n",
      "(8.385260520155413, tensor([9.0774], grad_fn=<UnbindBackward0>))\n",
      "(6.771935555839602, tensor([6.3062], grad_fn=<UnbindBackward0>))\n",
      "(6.478509642208569, tensor([7.3025], grad_fn=<UnbindBackward0>))\n",
      "(6.661854740545311, tensor([6.7889], grad_fn=<UnbindBackward0>))\n",
      "(7.419380582918692, tensor([6.2764], grad_fn=<UnbindBackward0>))\n",
      "(8.236685322712457, tensor([8.0899], grad_fn=<UnbindBackward0>))\n",
      "(9.802008383586275, tensor([7.8407], grad_fn=<UnbindBackward0>))\n",
      "(9.352447388601012, tensor([9.4123], grad_fn=<UnbindBackward0>))\n",
      "(8.797548488481558, tensor([8.4809], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([7.7324], grad_fn=<UnbindBackward0>))\n",
      "(6.703188113240863, tensor([6.3257], grad_fn=<UnbindBackward0>))\n",
      "(8.074337694089515, tensor([6.2967], grad_fn=<UnbindBackward0>))\n",
      "(7.04141166379481, tensor([7.2654], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([6.5249], grad_fn=<UnbindBackward0>))\n",
      "(7.753194269884341, tensor([7.4504], grad_fn=<UnbindBackward0>))\n",
      "(6.80903930604298, tensor([6.6116], grad_fn=<UnbindBackward0>))\n",
      "(8.734077192559303, tensor([6.7869], grad_fn=<UnbindBackward0>))\n",
      "(6.327936783729195, tensor([8.6591], grad_fn=<UnbindBackward0>))\n",
      "(8.91798070997329, tensor([6.7389], grad_fn=<UnbindBackward0>))\n",
      "(7.685703061234547, tensor([6.4456], grad_fn=<UnbindBackward0>))\n",
      "(7.4127640174265625, tensor([9.4963], grad_fn=<UnbindBackward0>))\n",
      "(6.1092475827643655, tensor([6.7561], grad_fn=<UnbindBackward0>))\n",
      "(7.731492029245684, tensor([6.2987], grad_fn=<UnbindBackward0>))\n",
      "(7.508238774678663, tensor([7.1885], grad_fn=<UnbindBackward0>))\n",
      "(6.561030665896573, tensor([6.1736], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.5759], grad_fn=<UnbindBackward0>))\n",
      "(9.37424334324612, tensor([10.3467], grad_fn=<UnbindBackward0>))\n",
      "(8.777092882855566, tensor([6.3034], grad_fn=<UnbindBackward0>))\n",
      "(8.73294952429643, tensor([8.0757], grad_fn=<UnbindBackward0>))\n",
      "(8.927977461002001, tensor([7.8482], grad_fn=<UnbindBackward0>))\n",
      "(8.686598356276965, tensor([9.4780], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([6.5508], grad_fn=<UnbindBackward0>))\n",
      "(8.605936401250625, tensor([6.3350], grad_fn=<UnbindBackward0>))\n",
      "(9.595602772766828, tensor([7.1945], grad_fn=<UnbindBackward0>))\n",
      "(8.297543529356284, tensor([9.1682], grad_fn=<UnbindBackward0>))\n",
      "(7.682482446534506, tensor([7.3424], grad_fn=<UnbindBackward0>))\n",
      "(9.003070169818264, tensor([8.5318], grad_fn=<UnbindBackward0>))\n",
      "(6.313548046277095, tensor([10.4165], grad_fn=<UnbindBackward0>))\n",
      "(8.387312270561717, tensor([6.6125], grad_fn=<UnbindBackward0>))\n",
      "(9.293577970546238, tensor([7.8457], grad_fn=<UnbindBackward0>))\n",
      "(8.614138397472717, tensor([6.6167], grad_fn=<UnbindBackward0>))\n",
      "(7.164720378771857, tensor([6.7459], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([5.8452], grad_fn=<UnbindBackward0>))\n",
      "(7.686162303492906, tensor([6.4976], grad_fn=<UnbindBackward0>))\n",
      "(8.50045386741194, tensor([8.4912], grad_fn=<UnbindBackward0>))\n",
      "(7.528331766707247, tensor([9.5220], grad_fn=<UnbindBackward0>))\n",
      "(7.454141078146678, tensor([9.5062], grad_fn=<UnbindBackward0>))\n",
      "(9.073259831427816, tensor([7.6612], grad_fn=<UnbindBackward0>))\n",
      "(6.144185634125646, tensor([6.2387], grad_fn=<UnbindBackward0>))\n",
      "(6.059123195581797, tensor([7.1951], grad_fn=<UnbindBackward0>))\n",
      "(6.478509642208569, tensor([7.6092], grad_fn=<UnbindBackward0>))\n",
      "(9.26955223160803, tensor([6.7672], grad_fn=<UnbindBackward0>))\n",
      "(6.650279048587422, tensor([7.6367], grad_fn=<UnbindBackward0>))\n",
      "(8.489821994620105, tensor([7.3599], grad_fn=<UnbindBackward0>))\n",
      "(7.195187320178709, tensor([7.4522], grad_fn=<UnbindBackward0>))\n",
      "(8.301521654940728, tensor([6.8095], grad_fn=<UnbindBackward0>))\n",
      "(9.08805973726735, tensor([8.4941], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([8.4322], grad_fn=<UnbindBackward0>))\n",
      "(6.530877627725885, tensor([8.5870], grad_fn=<UnbindBackward0>))\n",
      "(6.71901315438526, tensor([9.2675], grad_fn=<UnbindBackward0>))\n",
      "(8.268475388982598, tensor([9.3721], grad_fn=<UnbindBackward0>))\n",
      "(8.476162841858246, tensor([8.2751], grad_fn=<UnbindBackward0>))\n",
      "(6.2285110035911835, tensor([8.7738], grad_fn=<UnbindBackward0>))\n",
      "(7.026426808699636, tensor([6.7360], grad_fn=<UnbindBackward0>))\n",
      "(6.675823221634848, tensor([9.5203], grad_fn=<UnbindBackward0>))\n",
      "(7.050989447068045, tensor([8.5370], grad_fn=<UnbindBackward0>))\n",
      "(6.302618975744905, tensor([7.7817], grad_fn=<UnbindBackward0>))\n",
      "(6.194405391104672, tensor([7.0913], grad_fn=<UnbindBackward0>))\n",
      "(7.791935956938058, tensor([9.7215], grad_fn=<UnbindBackward0>))\n",
      "(6.583409222158765, tensor([7.1203], grad_fn=<UnbindBackward0>))\n",
      "(8.365206834418355, tensor([5.8678], grad_fn=<UnbindBackward0>))\n",
      "(7.697575346802343, tensor([8.5649], grad_fn=<UnbindBackward0>))\n",
      "(8.58260632996447, tensor([7.6891], grad_fn=<UnbindBackward0>))\n",
      "(6.75110146893676, tensor([7.3290], grad_fn=<UnbindBackward0>))\n",
      "(8.262558973010657, tensor([7.6678], grad_fn=<UnbindBackward0>))\n",
      "(8.738575192110787, tensor([9.5706], grad_fn=<UnbindBackward0>))\n",
      "(7.222566018822171, tensor([8.6820], grad_fn=<UnbindBackward0>))\n",
      "(8.356319965828153, tensor([8.6555], grad_fn=<UnbindBackward0>))\n",
      "(8.853236764745898, tensor([7.1364], grad_fn=<UnbindBackward0>))\n",
      "(6.6039438246004725, tensor([6.6758], grad_fn=<UnbindBackward0>))\n",
      "(7.907651594711089, tensor([7.7879], grad_fn=<UnbindBackward0>))\n",
      "(6.608000625296087, tensor([6.9442], grad_fn=<UnbindBackward0>))\n",
      "(8.580355766373877, tensor([9.0998], grad_fn=<UnbindBackward0>))\n",
      "(6.541029999189903, tensor([7.4001], grad_fn=<UnbindBackward0>))\n",
      "(6.489204931325317, tensor([7.7219], grad_fn=<UnbindBackward0>))\n",
      "(9.288134399416203, tensor([5.9702], grad_fn=<UnbindBackward0>))\n",
      "(6.5337888379333435, tensor([8.3867], grad_fn=<UnbindBackward0>))\n",
      "(7.993619994827744, tensor([8.9591], grad_fn=<UnbindBackward0>))\n",
      "(6.674561391814426, tensor([9.1796], grad_fn=<UnbindBackward0>))\n",
      "(9.03264808356589, tensor([8.4615], grad_fn=<UnbindBackward0>))\n",
      "(6.439350371100098, tensor([6.2248], grad_fn=<UnbindBackward0>))\n",
      "(9.679406061493943, tensor([9.0602], grad_fn=<UnbindBackward0>))\n",
      "(8.515191188745565, tensor([6.7189], grad_fn=<UnbindBackward0>))\n",
      "(7.057036981697891, tensor([6.5889], grad_fn=<UnbindBackward0>))\n",
      "(6.56244409369372, tensor([6.4838], grad_fn=<UnbindBackward0>))\n",
      "(6.625392368007956, tensor([7.4135], grad_fn=<UnbindBackward0>))\n",
      "(8.620832226175724, tensor([8.7623], grad_fn=<UnbindBackward0>))\n",
      "(7.017506142941256, tensor([9.9836], grad_fn=<UnbindBackward0>))\n",
      "(6.040254711277414, tensor([7.2883], grad_fn=<UnbindBackward0>))\n",
      "(6.594413459749778, tensor([8.6049], grad_fn=<UnbindBackward0>))\n",
      "(7.0967213784947605, tensor([6.6290], grad_fn=<UnbindBackward0>))\n",
      "(7.084226422097916, tensor([7.8673], grad_fn=<UnbindBackward0>))\n",
      "(7.792348924113037, tensor([7.9267], grad_fn=<UnbindBackward0>))\n",
      "(6.932447891572509, tensor([8.7097], grad_fn=<UnbindBackward0>))\n",
      "(7.807510042216193, tensor([6.2560], grad_fn=<UnbindBackward0>))\n",
      "(7.419979923661835, tensor([6.6866], grad_fn=<UnbindBackward0>))\n",
      "(7.203405521083095, tensor([7.8331], grad_fn=<UnbindBackward0>))\n",
      "(7.945555428253489, tensor([6.1986], grad_fn=<UnbindBackward0>))\n",
      "(7.565793282428515, tensor([9.3586], grad_fn=<UnbindBackward0>))\n",
      "(7.744569809354496, tensor([6.5258], grad_fn=<UnbindBackward0>))\n",
      "(7.998671361015776, tensor([7.2541], grad_fn=<UnbindBackward0>))\n",
      "(7.123672785204607, tensor([8.8382], grad_fn=<UnbindBackward0>))\n",
      "(8.114025442356757, tensor([8.7489], grad_fn=<UnbindBackward0>))\n",
      "(8.64541048921699, tensor([6.3568], grad_fn=<UnbindBackward0>))\n",
      "(8.896451207355227, tensor([7.2461], grad_fn=<UnbindBackward0>))\n",
      "(8.368693183097793, tensor([5.8181], grad_fn=<UnbindBackward0>))\n",
      "(6.29156913955832, tensor([6.6482], grad_fn=<UnbindBackward0>))\n",
      "(6.9726062513017535, tensor([9.3815], grad_fn=<UnbindBackward0>))\n",
      "(7.887208585813932, tensor([8.9465], grad_fn=<UnbindBackward0>))\n",
      "(6.839476438228843, tensor([9.7384], grad_fn=<UnbindBackward0>))\n",
      "(9.400878083541478, tensor([8.8422], grad_fn=<UnbindBackward0>))\n",
      "(8.327484416188264, tensor([9.0124], grad_fn=<UnbindBackward0>))\n",
      "(6.529418838262226, tensor([9.1160], grad_fn=<UnbindBackward0>))\n",
      "(6.679599185844383, tensor([7.9872], grad_fn=<UnbindBackward0>))\n",
      "(8.345692873253865, tensor([7.4498], grad_fn=<UnbindBackward0>))\n",
      "(8.520985989654934, tensor([8.4790], grad_fn=<UnbindBackward0>))\n",
      "(8.098034756176071, tensor([9.4766], grad_fn=<UnbindBackward0>))\n",
      "(9.543306469268108, tensor([6.2991], grad_fn=<UnbindBackward0>))\n",
      "(6.444131256700441, tensor([6.3343], grad_fn=<UnbindBackward0>))\n",
      "(9.206332350578643, tensor([8.9660], grad_fn=<UnbindBackward0>))\n",
      "(7.807916628926408, tensor([6.7381], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([7.2210], grad_fn=<UnbindBackward0>))\n",
      "(8.648572269472618, tensor([9.1453], grad_fn=<UnbindBackward0>))\n",
      "(9.388988523403828, tensor([10.0103], grad_fn=<UnbindBackward0>))\n",
      "(8.642062173462106, tensor([6.2974], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([6.9734], grad_fn=<UnbindBackward0>))\n",
      "(9.352360631996612, tensor([6.2463], grad_fn=<UnbindBackward0>))\n",
      "(7.0192966537150445, tensor([6.3637], grad_fn=<UnbindBackward0>))\n",
      "(7.2640301428995295, tensor([8.5934], grad_fn=<UnbindBackward0>))\n",
      "(7.589335823170617, tensor([9.0997], grad_fn=<UnbindBackward0>))\n",
      "(9.150059441091253, tensor([9.4838], grad_fn=<UnbindBackward0>))\n",
      "(6.597145701886651, tensor([8.3772], grad_fn=<UnbindBackward0>))\n",
      "(6.96979066990159, tensor([8.4548], grad_fn=<UnbindBackward0>))\n",
      "(6.675823221634848, tensor([8.9308], grad_fn=<UnbindBackward0>))\n",
      "(7.632885505395133, tensor([8.6829], grad_fn=<UnbindBackward0>))\n",
      "(8.54772239645106, tensor([6.8703], grad_fn=<UnbindBackward0>))\n",
      "(8.57035473953047, tensor([10.5047], grad_fn=<UnbindBackward0>))\n",
      "(8.147288258706624, tensor([8.2307], grad_fn=<UnbindBackward0>))\n",
      "(9.598727138549595, tensor([5.9780], grad_fn=<UnbindBackward0>))\n",
      "(8.344029572407049, tensor([8.4240], grad_fn=<UnbindBackward0>))\n",
      "(8.076515327552329, tensor([6.1880], grad_fn=<UnbindBackward0>))\n",
      "(8.201111644442758, tensor([6.4137], grad_fn=<UnbindBackward0>))\n",
      "(8.711443319075466, tensor([6.8515], grad_fn=<UnbindBackward0>))\n",
      "(9.072227069846548, tensor([8.1947], grad_fn=<UnbindBackward0>))\n",
      "(8.585225601808064, tensor([8.9308], grad_fn=<UnbindBackward0>))\n",
      "(8.305236829492593, tensor([8.2297], grad_fn=<UnbindBackward0>))\n",
      "(9.06681636189014, tensor([6.8088], grad_fn=<UnbindBackward0>))\n",
      "(9.059168584174444, tensor([6.6626], grad_fn=<UnbindBackward0>))\n",
      "(7.519149957669823, tensor([6.4538], grad_fn=<UnbindBackward0>))\n",
      "(6.877296071497429, tensor([7.3654], grad_fn=<UnbindBackward0>))\n",
      "(6.318968113746434, tensor([8.7126], grad_fn=<UnbindBackward0>))\n",
      "(7.801800401908973, tensor([6.2489], grad_fn=<UnbindBackward0>))\n",
      "(6.470799503782602, tensor([8.8771], grad_fn=<UnbindBackward0>))\n",
      "(9.72865786150047, tensor([9.0415], grad_fn=<UnbindBackward0>))\n",
      "(6.393590753950631, tensor([7.3978], grad_fn=<UnbindBackward0>))\n",
      "(9.172949982757624, tensor([8.6724], grad_fn=<UnbindBackward0>))\n",
      "(6.879355804460439, tensor([7.0519], grad_fn=<UnbindBackward0>))\n",
      "(9.619399015594476, tensor([8.5141], grad_fn=<UnbindBackward0>))\n",
      "(6.921658184151129, tensor([8.4187], grad_fn=<UnbindBackward0>))\n",
      "(8.837536046107568, tensor([8.5735], grad_fn=<UnbindBackward0>))\n",
      "(8.901230352110781, tensor([8.5007], grad_fn=<UnbindBackward0>))\n",
      "(6.776506992372183, tensor([9.0389], grad_fn=<UnbindBackward0>))\n",
      "(7.754481547470383, tensor([6.7899], grad_fn=<UnbindBackward0>))\n",
      "(8.361007108226909, tensor([10.0648], grad_fn=<UnbindBackward0>))\n",
      "(9.765718622791232, tensor([8.6552], grad_fn=<UnbindBackward0>))\n",
      "(6.476972362889683, tensor([7.4674], grad_fn=<UnbindBackward0>))\n",
      "(7.388946097618437, tensor([7.8576], grad_fn=<UnbindBackward0>))\n",
      "(6.739336627357174, tensor([9.3534], grad_fn=<UnbindBackward0>))\n",
      "(7.347943823148687, tensor([7.4306], grad_fn=<UnbindBackward0>))\n",
      "(8.121480374750751, tensor([6.4543], grad_fn=<UnbindBackward0>))\n",
      "(6.787844982309579, tensor([9.7717], grad_fn=<UnbindBackward0>))\n",
      "(8.4071550862073, tensor([10.1579], grad_fn=<UnbindBackward0>))\n",
      "(8.123261319121745, tensor([6.3432], grad_fn=<UnbindBackward0>))\n",
      "(7.902487437162855, tensor([6.3390], grad_fn=<UnbindBackward0>))\n",
      "(6.186208623900494, tensor([8.2467], grad_fn=<UnbindBackward0>))\n",
      "(8.943114308091785, tensor([8.4273], grad_fn=<UnbindBackward0>))\n",
      "(7.344072850573066, tensor([7.2483], grad_fn=<UnbindBackward0>))\n",
      "(9.161990111035136, tensor([5.9715], grad_fn=<UnbindBackward0>))\n",
      "(7.191429330036379, tensor([6.2458], grad_fn=<UnbindBackward0>))\n",
      "(7.628031126930335, tensor([6.7562], grad_fn=<UnbindBackward0>))\n",
      "(8.516392871245468, tensor([8.4720], grad_fn=<UnbindBackward0>))\n",
      "(7.513163545234075, tensor([9.4426], grad_fn=<UnbindBackward0>))\n",
      "(6.570882962339584, tensor([7.2508], grad_fn=<UnbindBackward0>))\n",
      "(6.406879986069314, tensor([9.2149], grad_fn=<UnbindBackward0>))\n",
      "(6.760414691083428, tensor([10.2419], grad_fn=<UnbindBackward0>))\n",
      "(7.776535028185241, tensor([9.9549], grad_fn=<UnbindBackward0>))\n",
      "(8.36264243156764, tensor([9.5861], grad_fn=<UnbindBackward0>))\n",
      "(8.28374674710613, tensor([9.0293], grad_fn=<UnbindBackward0>))\n",
      "(8.752897524775305, tensor([8.9086], grad_fn=<UnbindBackward0>))\n",
      "(9.442166123673227, tensor([6.7599], grad_fn=<UnbindBackward0>))\n",
      "(6.075346031088684, tensor([8.4760], grad_fn=<UnbindBackward0>))\n",
      "(8.821732380934442, tensor([8.9612], grad_fn=<UnbindBackward0>))\n",
      "(6.813444599510896, tensor([6.9370], grad_fn=<UnbindBackward0>))\n",
      "(6.55250788703459, tensor([8.0362], grad_fn=<UnbindBackward0>))\n",
      "(6.293419278846481, tensor([8.7851], grad_fn=<UnbindBackward0>))\n",
      "(6.889591308354466, tensor([10.1659], grad_fn=<UnbindBackward0>))\n",
      "(7.428927194802272, tensor([5.9505], grad_fn=<UnbindBackward0>))\n",
      "(7.403061091090091, tensor([6.2508], grad_fn=<UnbindBackward0>))\n",
      "(6.566672429803241, tensor([9.5779], grad_fn=<UnbindBackward0>))\n",
      "(7.467942332285852, tensor([6.5482], grad_fn=<UnbindBackward0>))\n",
      "(9.705828982731935, tensor([8.6853], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([7.7089], grad_fn=<UnbindBackward0>))\n",
      "(9.359277484750553, tensor([7.2304], grad_fn=<UnbindBackward0>))\n",
      "(7.126087273299125, tensor([6.6373], grad_fn=<UnbindBackward0>))\n",
      "(7.395107546562485, tensor([9.9986], grad_fn=<UnbindBackward0>))\n",
      "(6.877296071497429, tensor([8.7698], grad_fn=<UnbindBackward0>))\n",
      "(8.521384396034705, tensor([6.3829], grad_fn=<UnbindBackward0>))\n",
      "(8.509967146324497, tensor([7.8414], grad_fn=<UnbindBackward0>))\n",
      "(7.669961995473577, tensor([8.5341], grad_fn=<UnbindBackward0>))\n",
      "(6.957497370876951, tensor([6.1858], grad_fn=<UnbindBackward0>))\n",
      "(7.829630389150193, tensor([8.4042], grad_fn=<UnbindBackward0>))\n",
      "(9.428993858017527, tensor([8.1599], grad_fn=<UnbindBackward0>))\n",
      "(6.154858094016418, tensor([8.4249], grad_fn=<UnbindBackward0>))\n",
      "(6.639875833826536, tensor([7.9820], grad_fn=<UnbindBackward0>))\n",
      "(6.891625897052253, tensor([9.1455], grad_fn=<UnbindBackward0>))\n",
      "(9.30228124567454, tensor([7.3921], grad_fn=<UnbindBackward0>))\n",
      "(6.984716320118266, tensor([7.1076], grad_fn=<UnbindBackward0>))\n",
      "(8.628197749459149, tensor([6.1770], grad_fn=<UnbindBackward0>))\n",
      "(6.892641641172089, tensor([7.8743], grad_fn=<UnbindBackward0>))\n",
      "(6.523562306149512, tensor([9.0922], grad_fn=<UnbindBackward0>))\n",
      "(8.5016733797582, tensor([7.0832], grad_fn=<UnbindBackward0>))\n",
      "(7.780303087908373, tensor([9.8044], grad_fn=<UnbindBackward0>))\n",
      "(6.655440350367647, tensor([6.9074], grad_fn=<UnbindBackward0>))\n",
      "(6.836259277277067, tensor([6.2116], grad_fn=<UnbindBackward0>))\n",
      "(7.943072717277933, tensor([6.3545], grad_fn=<UnbindBackward0>))\n",
      "(6.331501849893691, tensor([7.2243], grad_fn=<UnbindBackward0>))\n",
      "(7.784057002639929, tensor([10.0545], grad_fn=<UnbindBackward0>))\n",
      "(8.648045999835, tensor([7.5183], grad_fn=<UnbindBackward0>))\n",
      "(8.276394704863307, tensor([6.5601], grad_fn=<UnbindBackward0>))\n",
      "(6.9865664594064265, tensor([6.3074], grad_fn=<UnbindBackward0>))\n",
      "(8.514990767861038, tensor([8.8265], grad_fn=<UnbindBackward0>))\n",
      "(8.628555659269699, tensor([8.3518], grad_fn=<UnbindBackward0>))\n",
      "(6.736966958001855, tensor([6.2104], grad_fn=<UnbindBackward0>))\n",
      "(7.938088726896952, tensor([7.1953], grad_fn=<UnbindBackward0>))\n",
      "(8.232440158470336, tensor([8.8740], grad_fn=<UnbindBackward0>))\n",
      "(6.621405651764134, tensor([10.0452], grad_fn=<UnbindBackward0>))\n",
      "(7.817222785508166, tensor([9.9541], grad_fn=<UnbindBackward0>))\n",
      "(8.03365842788615, tensor([8.4037], grad_fn=<UnbindBackward0>))\n",
      "(7.848933726364071, tensor([9.0024], grad_fn=<UnbindBackward0>))\n",
      "(8.069655306886165, tensor([7.6473], grad_fn=<UnbindBackward0>))\n",
      "(8.989818381366925, tensor([7.3330], grad_fn=<UnbindBackward0>))\n",
      "(7.275172319452771, tensor([6.3255], grad_fn=<UnbindBackward0>))\n",
      "(8.843903650835497, tensor([8.0885], grad_fn=<UnbindBackward0>))\n",
      "(6.962243464266207, tensor([8.8634], grad_fn=<UnbindBackward0>))\n",
      "(9.530174967444372, tensor([7.4408], grad_fn=<UnbindBackward0>))\n",
      "(9.792332503169966, tensor([8.2984], grad_fn=<UnbindBackward0>))\n",
      "(6.171700597410915, tensor([7.8028], grad_fn=<UnbindBackward0>))\n",
      "(7.270312886079025, tensor([10.1509], grad_fn=<UnbindBackward0>))\n",
      "(7.995306620290822, tensor([8.0864], grad_fn=<UnbindBackward0>))\n",
      "(9.531191227694439, tensor([7.4197], grad_fn=<UnbindBackward0>))\n",
      "(6.52795791762255, tensor([8.9444], grad_fn=<UnbindBackward0>))\n",
      "(8.282230063296685, tensor([7.2619], grad_fn=<UnbindBackward0>))\n",
      "(7.680637427560936, tensor([6.2244], grad_fn=<UnbindBackward0>))\n",
      "(8.3774712482411, tensor([6.8788], grad_fn=<UnbindBackward0>))\n",
      "(9.25569607369698, tensor([10.0211], grad_fn=<UnbindBackward0>))\n",
      "(6.925595197110468, tensor([6.3077], grad_fn=<UnbindBackward0>))\n",
      "(7.681560362559537, tensor([7.6222], grad_fn=<UnbindBackward0>))\n",
      "(6.79346613258001, tensor([6.3051], grad_fn=<UnbindBackward0>))\n",
      "(7.878155336503324, tensor([6.7742], grad_fn=<UnbindBackward0>))\n",
      "(7.865187954187467, tensor([7.7689], grad_fn=<UnbindBackward0>))\n",
      "(7.9748769005588755, tensor([6.1753], grad_fn=<UnbindBackward0>))\n",
      "(8.149890544402423, tensor([8.6201], grad_fn=<UnbindBackward0>))\n",
      "(8.495152060539358, tensor([8.3192], grad_fn=<UnbindBackward0>))\n",
      "(9.738671869871839, tensor([6.6983], grad_fn=<UnbindBackward0>))\n",
      "(7.9229859587111955, tensor([7.4151], grad_fn=<UnbindBackward0>))\n",
      "(8.733594061863055, tensor([9.5377], grad_fn=<UnbindBackward0>))\n",
      "(8.36799688505411, tensor([9.4166], grad_fn=<UnbindBackward0>))\n",
      "(9.036225051729327, tensor([9.4016], grad_fn=<UnbindBackward0>))\n",
      "(6.22455842927536, tensor([7.9385], grad_fn=<UnbindBackward0>))\n",
      "(7.164720378771857, tensor([8.0110], grad_fn=<UnbindBackward0>))\n",
      "(7.155396301896734, tensor([6.2943], grad_fn=<UnbindBackward0>))\n",
      "(7.177782416195197, tensor([9.6434], grad_fn=<UnbindBackward0>))\n",
      "(7.935945103353701, tensor([9.5967], grad_fn=<UnbindBackward0>))\n",
      "(7.986164860332727, tensor([7.8191], grad_fn=<UnbindBackward0>))\n",
      "(7.095064377287131, tensor([8.6436], grad_fn=<UnbindBackward0>))\n",
      "(7.2196420401307355, tensor([9.7070], grad_fn=<UnbindBackward0>))\n",
      "(8.528924114291936, tensor([8.7694], grad_fn=<UnbindBackward0>))\n",
      "(7.653969180478774, tensor([9.6121], grad_fn=<UnbindBackward0>))\n",
      "(7.75491027202143, tensor([9.3675], grad_fn=<UnbindBackward0>))\n",
      "(7.907651594711089, tensor([6.6294], grad_fn=<UnbindBackward0>))\n",
      "(6.473890696352274, tensor([6.3040], grad_fn=<UnbindBackward0>))\n",
      "(9.158520623246385, tensor([7.1408], grad_fn=<UnbindBackward0>))\n",
      "(8.649098262296176, tensor([8.6007], grad_fn=<UnbindBackward0>))\n",
      "(7.01571242048723, tensor([7.6311], grad_fn=<UnbindBackward0>))\n",
      "(5.908082938168931, tensor([6.0913], grad_fn=<UnbindBackward0>))\n",
      "(9.142596719889664, tensor([6.3060], grad_fn=<UnbindBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print('=' * 40)\n",
    "\n",
    "print(('정답', '예측'))\n",
    "for eval in zip(y_train, H):\n",
    "    print(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0b84b4-32cb-4e32-9ada-870078fde653",
   "metadata": {},
   "source": [
    "### 💡 OLS (Ordinary Least Square)\n",
    "- 최소제곱법 또는 최소자승법, 최소제곱근사법, 최소자승근사법을 의미하는 기술로서, 가장 근접한 해를 구하는 방식이다.\n",
    "- 선형 회귀 모델을 평가하는 데 유용한 방법이며, 모델 전체와 모델의 각 feature에 대한 통계적 성능 지표를 사용하여 수행된다.\n",
    "- 다양한 유형의 통계 모델을 추정하고 여러 통계 테스트를 수행하는 여러 개와 기능을 제공한다.\n",
    "- 관측된 데이터에 선형 방정식을 적용해서 생성되며, 가장 일반적인 방법이다.\n",
    "- P > |t| (p-value) : 해당 독립변수가 0.05보다 작으면 종속 변수에 영향을 미치는 것이 유의미 하다 라는 것을 뜻한다.\n",
    "- Durbin-Watson : 보통 1.5에서 2.5 사이라면, 독립으로 판단하고 회귀 모형이 적합하다는 것을 의미한다.\n",
    "- R<sup>2</sup>(R-squared) 값을 유지 또는 개선하는 방향으로만 수행해야 한다.\n",
    "- coef가 높을수록 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a59fe-7941-47db-ad5c-dd0ca770fc29",
   "metadata": {},
   "source": [
    "### VIF(Variance Inflation Factor)\n",
    "- 분산 팽창 요인 수치가 5 또는 10 이상일 경우, 다중 공선성의 문제가 있다는 뜻이다.\n",
    "- 전체적으로 5를 넘어가면 10 이상을 판단한다.\n",
    "- 다중 공선성(Multicollinearity)이란 회귀 분석에서 독립변수들 간에 강한 상관관계가 나타나는 문제를 뜻한다.\n",
    "\n",
    "<img src='./images/multicollinearity.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79224126-9046-4260-b61d-09f8af96045e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853c83b2-6942-4678-8eb6-d0043c7d3526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357fb18-4629-4951-a768-aad4dddb8ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1687d50-4e46-4141-93ac-280f7092d228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8829acfb-f8a1-421f-ba47-e355c98a0a3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7dfc5f-cd49-4e35-a708-9973741e8fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559c3e67-ebfc-4087-9806-33485bfe030c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
